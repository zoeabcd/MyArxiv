{"2025-02-21T00:00:00Z":{"Computation and Language":[{"id":"http://arxiv.org/abs/2502.15680v1","updated":"2025-02-21T18:59:14Z","published":"2025-02-21T18:59:14Z","title":"Privacy Ripple Effects from Adding or Removing Personal Information in\n  Language Model Training","summary":"  Due to the sensitive nature of personally identifiable information (PII), its\nowners may have the authority to control its inclusion or request its removal\nfrom large-language model (LLM) training. Beyond this, PII may be added or\nremoved from training datasets due to evolving dataset curation techniques,\nbecause they were newly scraped for retraining, or because they were included\nin a new downstream fine-tuning stage. We find that the amount and ease of PII\nmemorization is a dynamic property of a model that evolves throughout training\npipelines and depends on commonly altered design choices. We characterize three\nsuch novel phenomena: (1) similar-appearing PII seen later in training can\nelicit memorization of earlier-seen sequences in what we call assisted\nmemorization, and this is a significant factor (in our settings, up to 1/3);\n(2) adding PII can increase memorization of other PII significantly (in our\nsettings, as much as $\\approx\\!7.5\\times$); and (3) removing PII can lead to\nother PII being memorized. Model creators should consider these first- and\nsecond-order privacy risks when training models to avoid the risk of new PII\nregurgitation.\n","authors":["Jaydeep Borkar","Matthew Jagielski","Katherine Lee","Niloofar Mireshghallah","David A. Smith","Christopher A. Choquette-Choo"],"pdf_url":"https://arxiv.org/pdf/2502.15680v1.pdf","comment":"23 pages, 26 figures"},{"id":"http://arxiv.org/abs/2502.15677v1","updated":"2025-02-21T18:58:06Z","published":"2025-02-21T18:58:06Z","title":"FLEKE: Federated Locate-then-Edit Knowledge Editing","summary":"  Locate-then-Edit Knowledge Editing (LEKE) is a key technique for updating\nlarge language models (LLMs) without full retraining. However, existing methods\nassume a single-user setting and become inefficient in real-world multi-client\nscenarios, where decentralized organizations (e.g., hospitals, financial\ninstitutions) independently update overlapping knowledge, leading to redundant\nmediator knowledge vector (MKV) computations and privacy concerns. To address\nthese challenges, we introduce Federated Locate-then-Edit Knowledge Editing\n(FLEKE), a novel task that enables multiple clients to collaboratively perform\nLEKE while preserving privacy and reducing computational overhead. To achieve\nthis, we propose FedEdit, a two-stage framework that optimizes MKV selection\nand reuse. In the first stage, clients locally apply LEKE and upload the\ncomputed MKVs. In the second stage, rather than relying solely on server-based\nMKV sharing, FLEKE allows clients retrieve relevant MKVs based on cosine\nsimilarity, enabling knowledge re-edit and minimizing redundant computations.\nExperimental results on two benchmark datasets demonstrate that FedEdit retains\nover 96% of the performance of non-federated LEKE while significantly\noutperforming a FedAvg-based baseline by approximately twofold. Besides, we\nfind that MEMIT performs more consistently than PMET in the FLEKE task with our\nFedEdit framework. Our code is available at https://github.com/zongkaiz/FLEKE.\n","authors":["Zongkai Zhao","Guozeng Xu","Xiuhua Li","Kaiwen Wei","Jiang Zhong"],"pdf_url":"https://arxiv.org/pdf/2502.15677v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.15676v1","updated":"2025-02-21T18:57:52Z","published":"2025-02-21T18:57:52Z","title":"AutoToM: Automated Bayesian Inverse Planning and Model Discovery for\n  Open-ended Theory of Mind","summary":"  Theory of Mind (ToM), the ability to understand people's mental variables\nbased on their behavior, is key to developing socially intelligent agents.\nCurrent approaches to Theory of Mind reasoning either rely on prompting Large\nLanguage Models (LLMs), which are prone to systematic errors, or use rigid,\nhandcrafted Bayesian Theory of Mind (BToM) models, which are more robust but\ncannot generalize across different domains. In this work, we introduce AutoToM,\nan automated Bayesian Theory of Mind method for achieving open-ended machine\nTheory of Mind. AutoToM can operate in any domain, infer any mental variable,\nand conduct robust Theory of Mind reasoning of any order. Given a Theory of\nMind inference problem, AutoToM first proposes an initial BToM model. It then\nconducts automated Bayesian inverse planning based on the proposed model,\nleveraging an LLM as the backend. Based on the uncertainty of the inference, it\niteratively refines the model, by introducing additional mental variables\nand/or incorporating more timesteps in the context. Empirical evaluations\nacross multiple Theory of Mind benchmarks demonstrate that AutoToM consistently\nachieves state-of-the-art performance, offering a scalable, robust, and\ninterpretable approach to machine Theory of Mind.\n","authors":["Zhining Zhang","Chuanyang Jin","Mung Yao Jia","Tianmin Shu"],"pdf_url":"https://arxiv.org/pdf/2502.15676v1.pdf","comment":"23 pages, 6 figures, 11 tables. Website at\n  https://chuanyangjin.com/AutoToM/"},{"id":"http://arxiv.org/abs/2411.00150v2","updated":"2025-02-21T18:54:37Z","published":"2024-10-31T18:57:59Z","title":"Schema Augmentation for Zero-Shot Domain Adaptation in Dialogue State\n  Tracking","summary":"  Zero-shot domain adaptation for dialogue state tracking (DST) remains a\nchallenging problem in task-oriented dialogue (TOD) systems, where models must\ngeneralize to target domains unseen at training time. Current large language\nmodel approaches for zero-shot domain adaptation rely on prompting to introduce\nknowledge pertaining to the target domains. However, their efficacy strongly\ndepends on prompt engineering, as well as the zero-shot ability of the\nunderlying language model. In this work, we devise a novel data augmentation\napproach, Schema Augmentation, that improves the zero-shot domain adaptation of\nlanguage models through fine-tuning. Schema Augmentation is a simple but\neffective technique that enhances generalization by introducing variations of\nslot names within the schema provided in the prompt. Experiments on MultiWOZ\nand SpokenWOZ showed that the proposed approach resulted in a substantial\nimprovement over the baseline, in some experiments achieving over a twofold\naccuracy gain over unseen domains while maintaining equal or superior\nperformance over all domains.\n","authors":["Christopher Richardson","Roshan Sharma","Neeraj Gaur","Parisa Haghani","Anirudh Sundar","Bhuvana Ramabhadran"],"pdf_url":"https://arxiv.org/pdf/2411.00150v2.pdf","comment":"short paper (4 pages) submitted to ARR"},{"id":"http://arxiv.org/abs/2502.15666v1","updated":"2025-02-21T18:45:37Z","published":"2025-02-21T18:45:37Z","title":"Almost AI, Almost Human: The Challenge of Detecting AI-Polished Writing","summary":"  The growing use of large language models (LLMs) for text generation has led\nto widespread concerns about AI-generated content detection. However, an\noverlooked challenge is AI-polished text, where human-written content undergoes\nsubtle refinements using AI tools. This raises a critical question: should\nminimally polished text be classified as AI-generated? Misclassification can\nlead to false plagiarism accusations and misleading claims about AI prevalence\nin online content. In this study, we systematically evaluate eleven\nstate-of-the-art AI-text detectors using our AI-Polished-Text Evaluation\n(APT-Eval) dataset, which contains $11.7K$ samples refined at varying\nAI-involvement levels. Our findings reveal that detectors frequently\nmisclassify even minimally polished text as AI-generated, struggle to\ndifferentiate between degrees of AI involvement, and exhibit biases against\nolder and smaller models. These limitations highlight the urgent need for more\nnuanced detection methodologies.\n","authors":["Shoumik Saha","Soheil Feizi"],"pdf_url":"https://arxiv.org/pdf/2502.15666v1.pdf","comment":"17 pages, 17 figures"},{"id":"http://arxiv.org/abs/2409.09760v3","updated":"2025-02-21T18:40:52Z","published":"2024-09-15T15:01:00Z","title":"ELMI: Interactive and Intelligent Sign Language Translation of Lyrics\n  for Song Signing","summary":"  d/Deaf and hearing song-signers have become prevalent across video-sharing\nplatforms, but translating songs into sign language remains cumbersome and\ninaccessible. Our formative study revealed the challenges song-signers face,\nincluding semantic, syntactic, expressive, and rhythmic considerations in\ntranslations. We present ELMI, an accessible song-signing tool that assists in\ntranslating lyrics into sign language. ELMI enables users to edit glosses\nline-by-line, with real-time synced lyric and music video snippets. Users can\nalso chat with a large language model-driven AI to discuss meaning, glossing,\nemoting, and timing. Through an exploratory study with 13 song-signers, we\nexamined how ELMI facilitates their workflows and how song-signers leverage and\nreceive an LLM-driven chat for translation. Participants successfully adopted\nELMI to song-signing, with active discussions throughout. They also reported\nimproved confidence and independence in their translations, finding ELMI\nencouraging, constructive, and informative. We discuss research and design\nimplications for accessible and culturally sensitive song-signing translation\ntools.\n","authors":["Suhyeon Yoo","Khai N. Truong","Young-Ho Kim"],"pdf_url":"https://arxiv.org/pdf/2409.09760v3.pdf","comment":"17 pages excluding reference and appendix. Accepted at ACM CHI 2025.\n  https://naver-ai.github.io/elmi"},{"id":"http://arxiv.org/abs/2501.10582v2","updated":"2025-02-21T18:27:19Z","published":"2025-01-17T22:20:55Z","title":"Adapting Large Language Models for Character-based Augmentative and\n  Alternative Communication","summary":"  Users of Augmentative and Alternative Communication (AAC) may write\nletter-by-letter via an interface that uses a character language model.\nHowever, most state-of-the-art large pretrained language models predict subword\ntokens of variable length. We investigate how to practically use such models to\nmake accurate and efficient character predictions. We fine-tune models using a\nlarge dataset of sentences we curated in which each sentence is rated according\nto how useful it might be for spoken or written AAC communication. We find that\nusing an algorithm to produce character predictions from a subword large\nlanguage model provides more accurate predictions than adding a classification\nlayer or using a byte-level model. We also find that our domain adaptation\nprocedure is effective at improving model performance on simple, conversational\ntext.\n","authors":["Dylan Gaines","Keith Vertanen"],"pdf_url":"https://arxiv.org/pdf/2501.10582v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.15654v1","updated":"2025-02-21T18:22:36Z","published":"2025-02-21T18:22:36Z","title":"Machine-generated text detection prevents language model collapse","summary":"  As Large Language Models (LLMs) become increasingly prevalent, their\ngenerated outputs are proliferating across the web, risking a future where\nmachine-generated content dilutes human-authored text. Since web data is the\nprimary resource for LLM pretraining, future models will be trained on an\nunknown portion of synthetic data. This will lead to model collapse, a\ndegenerative process which causes models to reinforce their own errors and\nexperience a drop in model performance. In this study, we investigate the\nimpact of decoding strategy on model collapse, where we analyse the\ncharacteristics of the generated data during recursive training, its similarity\nto human references and the resulting model performance. Using the decoding\nstrategies that lead to the most significant model degradation, we tackle the\nquestion: how to avoid model collapse when the origin (human or synthetic) of\nthe training data is unknown. We design a novel methodology based on resampling\nthe data distribution using importance weights from our machine-generated text\ndetector. Our method is validated on two LLM variants (GPT-2 and SmolLM2) on\nthe open-ended text generation task, demonstrating that we can successfully\nprevent model collapse and when there is enough human-authored data in the\ntraining dataset, our method improves model performance.\n","authors":["George Drayson","Vasileios Lampos"],"pdf_url":"https://arxiv.org/pdf/2502.15654v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.15652v1","updated":"2025-02-21T18:20:35Z","published":"2025-02-21T18:20:35Z","title":"Empowering LLMs with Logical Reasoning: A Comprehensive Survey","summary":"  Large language models (LLMs) have achieved remarkable successes on various\nnatural language tasks. However, recent studies have found that there are still\nsignificant challenges to the logical reasoning abilities of LLMs. This paper\nsummarizes and categorizes the main challenges into two aspects: (1) Logical\nquestion answering, LLMs often fail to generate the correct answer within\ncomplex logical problem which requires sophisticated deductive, inductive or\nabductive reasoning given a collection of premises and constrains. (2) Logical\nconsistency, LLMs are prone to producing responses contradicting themselves\nacross different questions. For example, a state-of-the-art Macaw\nquestion-answering LLM answers Yes to both questions Is a magpie a bird? and\nDoes a bird have wings? but answers No to Does a magpie have wings?. To\nfacilitate this research direction, we comprehensively investigate the most\ncutting-edge methods and propose detailed taxonomies of these methods.\nSpecifically, to accurately answer complex logic questions, previous methods\ncan be categorized based on reliance on external solvers, prompts, pretraining,\nand fine-tuning. To avoid logical contradictions, we discuss concepts and\nsolutions of various logical consistencies, including implication, negation,\ntransitivity, factuality consistency, and their composites. In addition, we\nreview commonly used benchmark datasets and evaluation metrics, and discuss\npromising research directions, such as extensions to modal logic to account for\nuncertainty, and efficient algorithms satisfying multiple logical consistencies\nsimultaneously.\n","authors":["Fengxiang Cheng","Haoxuan Li","Fenrong Liu","Robert van Rooij","Kun Zhang","Zhouchen Lin"],"pdf_url":"https://arxiv.org/pdf/2502.15652v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.04628v3","updated":"2025-02-21T18:12:34Z","published":"2024-12-05T21:50:22Z","title":"SWEPO: Simultaneous Weighted Preference Optimization for Group\n  Contrastive Alignment","summary":"  Direct Preference Optimization (DPO) has proven effective in aligning large\nlanguage models with human preferences but is often constrained to pairwise\ncomparisons -- overlooking additional positive and negative responses that are\ncommonly available in real-world settings. We propose Simultaneous Weighted\nPreference Optimization (SWEPO), which incorporates multiple responses per\nquery and prioritizes those that deviate most from the average reward. This\ndeviation-based weighting focuses training on the most informative outliers,\nakin to a built-in curriculum. Theoretically, we prove that such\nmulti-preference sampling lowers alignment bias, bounding the expected\ndeviation from the true acceptable-response distribution at a rate of\n$\\mathcal{O}(\\tfrac{1}{\\sqrt{k}})$. Empirically, SWEPO outperforms\nstate-of-the-art baselines on the Ultra-Feedback dataset and demonstrates\nsubstantial improvements over DPO and InfoNCA, yielding boosts of up to $\\sim\n4$% on length-controlled win-rate on AlpacaEval.\n","authors":["Taneesh Gupta","Rahul Madhavan","Xuchao Zhang","Chetan Bansal","Saravan Rajmohan"],"pdf_url":"https://arxiv.org/pdf/2412.04628v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.15639v1","updated":"2025-02-21T18:09:54Z","published":"2025-02-21T18:09:54Z","title":"Steering into New Embedding Spaces: Analyzing Cross-Lingual Alignment\n  Induced by Model Interventions in Multilingual Language Models","summary":"  Aligned representations across languages is a desired property in\nmultilingual large language models (mLLMs), as alignment can improve\nperformance in cross-lingual tasks. Typically alignment requires fine-tuning a\nmodel, which is computationally expensive, and sizable language data, which\noften may not be available. A data-efficient alternative to fine-tuning is\nmodel interventions -- a method for manipulating model activations to steer\ngeneration into the desired direction. We analyze the effect of a popular\nintervention (finding experts) on the alignment of cross-lingual\nrepresentations in mLLMs. We identify the neurons to manipulate for a given\nlanguage and introspect the embedding space of mLLMs pre- and\npost-manipulation. We show that modifying the mLLM's activations changes its\nembedding space such that cross-lingual alignment is enhanced. Further, we show\nthat the changes to the embedding space translate into improved downstream\nperformance on retrieval tasks, with up to 2x improvements in top-1 accuracy on\ncross-lingual retrieval.\n","authors":["Anirudh Sundar","Sinead Williamson","Katherine Metcalf","Barry-John Theobald","Skyler Seto","Masha Fedzechkina"],"pdf_url":"https://arxiv.org/pdf/2502.15639v1.pdf","comment":"34 pages"},{"id":"http://arxiv.org/abs/2502.06556v4","updated":"2025-02-21T17:55:32Z","published":"2025-02-10T15:24:30Z","title":"ProjectTest: A Project-level LLM Unit Test Generation Benchmark and\n  Impact of Error Fixing Mechanisms","summary":"  Unit test generation has become a promising and important use case of LLMs.\nHowever, existing evaluation benchmarks for assessing LLM unit test generation\ncapabilities focus on function- or class-level code rather than more practical\nand challenging project-level codebases. To address such limitation, we propose\nProjectTest, a project-level benchmark for unit test generation covering\nPython, Java, and JavaScript. ProjectTest features 20 moderate-sized and\nhigh-quality projects per language. We evaluate nine frontier LLMs on\nProjectTest and the results show that all frontier LLMs tested exhibit moderate\nperformance on ProjectTest on Python and Java, highlighting the difficulty of\nProjectTest. We also conduct a thorough error analysis, which shows that even\nfrontier LLMs, such as Claude-3.5-Sonnet, have significant basic yet critical\nerrors, including compilation and cascade errors. Motivated by this\nobservation, we further evaluate all frontier LLMs under manual error-fixing\nand self-error-fixing scenarios to assess their potential when equipped with\nerror-fixing mechanisms. Our code and dataset is available at\n\\href{https://github.com/YiboWANG214/ProjectTest}{ProjectTest}.\n","authors":["Yibo Wang","Congying Xia","Wenting Zhao","Jiangshu Du","Chunyu Miao","Zhongfen Deng","Philip S. Yu","Chen Xing"],"pdf_url":"https://arxiv.org/pdf/2502.06556v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.15619v1","updated":"2025-02-21T17:42:51Z","published":"2025-02-21T17:42:51Z","title":"Extraction multi-étiquettes de relations en utilisant des couches de\n  Transformer","summary":"  In this article, we present the BTransformer18 model, a deep learning\narchitecture designed for multi-label relation extraction in French texts. Our\napproach combines the contextual representation capabilities of pre-trained\nlanguage models from the BERT family - such as BERT, RoBERTa, and their French\ncounterparts CamemBERT and FlauBERT - with the power of Transformer encoders to\ncapture long-term dependencies between tokens. Experiments conducted on the\ndataset from the TextMine'25 challenge show that our model achieves superior\nperformance, particularly when using CamemBERT-Large, with a macro F1 score of\n0.654, surpassing the results obtained with FlauBERT-Large. These results\ndemonstrate the effectiveness of our approach for the automatic extraction of\ncomplex relations in intelligence reports.\n","authors":["Ngoc Luyen Le","Gildas Tagny Ngompé"],"pdf_url":"https://arxiv.org/pdf/2502.15619v1.pdf","comment":"in French language"},{"id":"http://arxiv.org/abs/2502.15618v1","updated":"2025-02-21T17:41:21Z","published":"2025-02-21T17:41:21Z","title":"Probe Pruning: Accelerating LLMs through Dynamic Pruning via\n  Model-Probing","summary":"  We introduce Probe Pruning (PP), a novel framework for online, dynamic,\nstructured pruning of Large Language Models (LLMs) applied in a batch-wise\nmanner. PP leverages the insight that not all samples and tokens contribute\nequally to the model's output, and probing a small portion of each batch\neffectively identifies crucial weights, enabling tailored dynamic pruning for\ndifferent batches. It comprises three main stages: probing, history-informed\npruning, and full inference. In the probing stage, PP selects a small yet\ncrucial set of hidden states, based on residual importance, to run a few model\nlayers ahead. During the history-informed pruning stage, PP strategically\nintegrates the probing states with historical states. Subsequently, it\nstructurally prunes weights based on the integrated states and the PP\nimportance score, a metric developed specifically to assess the importance of\neach weight channel in maintaining performance. In the final stage, full\ninference is conducted on the remaining weights. A major advantage of PP is its\ncompatibility with existing models, as it operates without requiring additional\nneural network modules or fine-tuning. Comprehensive evaluations of PP on\nLLaMA-2/3 and OPT models reveal that even minimal probing-using just 1.5% of\nFLOPs-can substantially enhance the efficiency of structured pruning of LLMs.\nFor instance, when evaluated on LLaMA-2-7B with WikiText2, PP achieves a 2.56\ntimes lower ratio of performance degradation per unit of runtime reduction\ncompared to the state-of-the-art method at a 40% pruning ratio. Our code is\navailable at https://github.com/Qi-Le1/Probe_Pruning.\n","authors":["Qi Le","Enmao Diao","Ziyan Wang","Xinran Wang","Jie Ding","Li Yang","Ali Anwar"],"pdf_url":"https://arxiv.org/pdf/2502.15618v1.pdf","comment":"ICLR 2025"},{"id":"http://arxiv.org/abs/2502.15616v1","updated":"2025-02-21T17:40:42Z","published":"2025-02-21T17:40:42Z","title":"Pastiche Novel Generation Creating: Fan Fiction You Love in Your\n  Favorite Author's Style","summary":"  Great novels create immersive worlds with rich character arcs,\nwell-structured plots, and nuanced writing styles. However, current novel\ngeneration methods often rely on brief, simplistic story outlines and generate\ndetails using plain, generic language. To bridge this gap, we introduce the\ntask of Pastiche Novel Generation, which requires the generated novels to\nimitate the distinctive features of the original work, including understanding\ncharacter profiles, predicting plausible plot developments, and writing\nconcrete details using vivid, expressive language. To achieve this, we propose\nWriterAgent, a novel generation system designed to master the core aspects of\nliterary pastiche. WriterAgent is trained through a curriculum learning\nparadigm, progressing from low-level stylistic mastery to high-level narrative\ncoherence. Its key tasks include language style learning, character modeling,\nplot planning, and stylish writing, ensuring comprehensive narrative control.\nTo support this, WriterAgent leverages the WriterLoRA framework, an extension\nof LoRA with hierarchical and cumulative task-specific modules, each\nspecializing in a different narrative aspect. We evaluate WriterAgent on\nmultilingual classics like Harry Potter and Dream of the Red Chamber,\ndemonstrating its superiority over baselines in capturing the target author's\nsettings, character dynamics, and writing style to produce coherent, faithful\nnarratives.\n","authors":["Xueran Han","Yuhan Liu","Mingzhe Li","Wei Liu","Sen Hu","Rui Yan","Zhiqiang Xu","Xiuying Chen"],"pdf_url":"https://arxiv.org/pdf/2502.15616v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.15612v1","updated":"2025-02-21T17:33:59Z","published":"2025-02-21T17:33:59Z","title":"LaTIM: Measuring Latent Token-to-Token Interactions in Mamba Models","summary":"  State space models (SSMs), such as Mamba, have emerged as an efficient\nalternative to transformers for long-context sequence modeling. However,\ndespite their growing adoption, SSMs lack the interpretability tools that have\nbeen crucial for understanding and improving attention-based architectures.\nWhile recent efforts provide insights into Mamba's internal mechanisms, they do\nnot explicitly decompose token-wise contributions, leaving gaps in\nunderstanding how Mamba selectively processes sequences across layers. In this\nwork, we introduce LaTIM, a novel token-level decomposition method for both\nMamba-1 and Mamba-2 that enables fine-grained interpretability. We extensively\nevaluate our method across diverse tasks, including machine translation,\ncopying, and retrieval-based generation, demonstrating its effectiveness in\nrevealing Mamba's token-to-token interaction patterns.\n","authors":["Hugo Pitorro","Marcos Treviso"],"pdf_url":"https://arxiv.org/pdf/2502.15612v1.pdf","comment":"8 pages, 10 figures in the main paper"},{"id":"http://arxiv.org/abs/2502.15609v1","updated":"2025-02-21T17:31:00Z","published":"2025-02-21T17:31:00Z","title":"On the Robustness of Transformers against Context Hijacking for Linear\n  Classification","summary":"  Transformer-based Large Language Models (LLMs) have demonstrated powerful\nin-context learning capabilities. However, their predictions can be disrupted\nby factually correct context, a phenomenon known as context hijacking,\nrevealing a significant robustness issue. To understand this phenomenon\ntheoretically, we explore an in-context linear classification problem based on\nrecent advances in linear transformers. In our setup, context tokens are\ndesigned as factually correct query-answer pairs, where the queries are similar\nto the final query but have opposite labels. Then, we develop a general\ntheoretical analysis on the robustness of the linear transformers, which is\nformulated as a function of the model depth, training context lengths, and\nnumber of hijacking context tokens. A key finding is that a well-trained deeper\ntransformer can achieve higher robustness, which aligns with empirical\nobservations. We show that this improvement arises because deeper layers enable\nmore fine-grained optimization steps, effectively mitigating interference from\ncontext hijacking. This is also well supported by our numerical experiments.\nOur findings provide theoretical insights into the benefits of deeper\narchitectures and contribute to enhancing the understanding of transformer\narchitectures.\n","authors":["Tianle Li","Chenyang Zhang","Xingwu Chen","Yuan Cao","Difan Zou"],"pdf_url":"https://arxiv.org/pdf/2502.15609v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.13311v2","updated":"2025-02-21T17:25:44Z","published":"2025-02-18T22:13:00Z","title":"Training Turn-by-Turn Verifiers for Dialogue Tutoring Agents: The\n  Curious Case of LLMs as Your Coding Tutors","summary":"  Intelligent tutoring agents powered by large language models (LLMs) have been\nincreasingly explored to deliver personalized guidance in areas such as\nlanguage learning and science education. However, their capabilities in guiding\nusers to solve complex real-world tasks remain underexplored. To address this\nlimitation, in this work, we focus on coding tutoring, a challenging problem\nthat requires tutors to proactively guide students toward completing predefined\ncoding tasks. We propose a novel agent workflow, Trace-and-Verify (TRAVER),\nwhich combines knowledge tracing to estimate a student's knowledge state and\nturn-by-turn verification to ensure effective guidance toward task completion.\nWe introduce DICT, an automatic evaluation protocol that assesses tutor agents\nholistically using controlled student simulation and code generation tests.\nExtensive experiments reveal the challenges of coding tutoring and demonstrate\nthat TRAVER achieves a significantly higher success rate. Although we use code\ntutoring as an example in this paper, our results and findings can be extended\nbeyond coding, providing valuable insights into advancing tutoring agents for a\nvariety of tasks.\n","authors":["Jian Wang","Yinpei Dai","Yichi Zhang","Ziqiao Ma","Wenjie Li","Joyce Chai"],"pdf_url":"https://arxiv.org/pdf/2502.13311v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.15603v1","updated":"2025-02-21T17:19:23Z","published":"2025-02-21T17:19:23Z","title":"Do Multilingual LLMs Think In English?","summary":"  Large language models (LLMs) have multilingual capabilities and can solve\ntasks across various languages. However, we show that current LLMs make key\ndecisions in a representation space closest to English, regardless of their\ninput and output languages. Exploring the internal representations with a logit\nlens for sentences in French, German, Dutch, and Mandarin, we show that the LLM\nfirst emits representations close to English for semantically-loaded words\nbefore translating them into the target language. We further show that\nactivation steering in these LLMs is more effective when the steering vectors\nare computed in English rather than in the language of the inputs and outputs.\nThis suggests that multilingual LLMs perform key reasoning steps in a\nrepresentation that is heavily shaped by English in a way that is not\ntransparent to system users.\n","authors":["Lisa Schut","Yarin Gal","Sebastian Farquhar"],"pdf_url":"https://arxiv.org/pdf/2502.15603v1.pdf","comment":"Main paper 9 pages; including appendix 48 pages"},{"id":"http://arxiv.org/abs/2502.15600v1","updated":"2025-02-21T17:18:02Z","published":"2025-02-21T17:18:02Z","title":"Robust Bias Detection in MLMs and its Application to Human Trait Ratings","summary":"  There has been significant prior work using templates to study bias against\ndemographic attributes in MLMs. However, these have limitations: they overlook\nrandom variability of templates and target concepts analyzed, assume equality\namongst templates, and overlook bias quantification. Addressing these, we\npropose a systematic statistical approach to assess bias in MLMs, using mixed\nmodels to account for random effects, pseudo-perplexity weights for sentences\nderived from templates and quantify bias using statistical effect sizes.\nReplicating prior studies, we match on bias scores in magnitude and direction\nwith small to medium effect sizes. Next, we explore the novel problem of gender\nbias in the context of $\\textit{personality}$ and $\\textit{character}$ traits,\nacross seven MLMs (base and large). We find that MLMs vary; ALBERT is unbiased\nfor binary gender but the most biased for non-binary $\\textit{neo}$, while\nRoBERTa-large is the most biased for binary gender but shows small to no bias\nfor $\\textit{neo}$. There is some alignment of MLM bias and findings in\npsychology (human perspective) - in $\\textit{agreeableness}$ with RoBERTa-large\nand $\\textit{emotional stability}$ with BERT-large. There is general agreement\nfor the remaining 3 personality dimensions: both sides observe at most small\ndifferences across gender. For character traits, human studies on gender bias\nare limited thus comparisons are not feasible.\n","authors":["Ingroj Shrestha","Louis Tay","Padmini Srinivasan"],"pdf_url":"https://arxiv.org/pdf/2502.15600v1.pdf","comment":"To appear at Findings of NAACL 2025"},{"id":"http://arxiv.org/abs/2502.15594v1","updated":"2025-02-21T17:12:35Z","published":"2025-02-21T17:12:35Z","title":"SafeInt: Shielding Large Language Models from Jailbreak Attacks via\n  Safety-Aware Representation Intervention","summary":"  With the widespread real-world deployment of large language models (LLMs),\nensuring their behavior complies with safety standards has become crucial.\nJailbreak attacks exploit vulnerabilities in LLMs to induce undesirable\nbehavior, posing a significant threat to LLM safety. Previous defenses often\nfail to achieve both effectiveness and efficiency simultaneously. Defenses from\na representation perspective offer new insights, but existing interventions\ncannot dynamically adjust representations based on the harmfulness of the\nqueries. To address this limitation while ensuring both effectiveness and\nefficiency, we propose SafeIntervention (SafeInt), a novel defense method that\nshields LLMs from jailbreak attacks through safety-aware representation\nintervention. SafeInt is built on our analysis of the representations of\njailbreak samples. It adjusts representation distributions of jailbreak samples\nthrough intervention to align them with the representations of unsafe samples\nwhile minimizing unnecessary perturbations to jailbreak-irrelevant\nrepresentations. We conduct comprehensive experiments covering six jailbreak\nattacks, two jailbreak datasets, and two utility benchmarks. Experimental\nresults demonstrate that SafeInt outperforms all baselines in defending LLMs\nagainst jailbreak attacks while largely maintaining utility. Additionally, we\nevaluate SafeInt against adaptive attacks and verify its effectiveness in\nmitigating real-time attacks.\n","authors":["Jiaqi Wu","Chen Chen","Chunyan Hou","Xiaojie Yuan"],"pdf_url":"https://arxiv.org/pdf/2502.15594v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.15592v1","updated":"2025-02-21T17:02:40Z","published":"2025-02-21T17:02:40Z","title":"Generalizing From Short to Long: Effective Data Synthesis for\n  Long-Context Instruction Tuning","summary":"  Long-context modelling for large language models (LLMs) has been a key area\nof recent research because many real world use cases require reasoning over\nlonger inputs such as documents. The focus of research into modelling long\ncontext has been on how to model position and there has been little\ninvestigation into other important aspects of language modelling such as\ninstruction tuning. Long context training examples are challenging and\nexpensive to create and use. In this paper, we investigate how to design\ninstruction data for the post-training phase of a long context pre-trained\nmodel: how much and what type of context is needed for optimal and efficient\npost-training. Our controlled study reveals that models instruction-tuned on\nshort contexts can effectively generalize to longer ones, while also\nidentifying other critical factors such as instruction difficulty and context\ncomposition. Based on these findings, we propose context synthesis, a novel\ndata synthesis framework that leverages off-the-shelf LLMs to generate extended\nbackground contexts for high-quality instruction-answer pairs. Experiment\nresults on the document-level benchmark (LongBench) demonstrate that our\nproposed approach outperforms previous instruction synthesis approaches and\ncomes close to the performance of human-annotated long-context instruction\ndata. The project will be available at:\nhttps://github.com/NJUNLP/context-synthesis.\n","authors":["Wenhao Zhu","Pinzhen Chen","Hanxu Hu","Shujian Huang","Fei Yuan","Jiajun Chen","Alexandra Birch"],"pdf_url":"https://arxiv.org/pdf/2502.15592v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.10508v2","updated":"2025-02-21T17:00:16Z","published":"2024-10-14T13:48:36Z","title":"Everyday Speech in the Indian Subcontinent","summary":"  India has 1369 languages of which 22 are official. About 13 different scripts\nare used to represent these languages. A Common Label Set (CLS) was developed\nbased on phonetics to address the issue of large vocabulary of units required\nin the End-to-End (E2E) framework for multilingual synthesis. The Indian\nlanguage text is first converted to CLS. This approach enables seamless code\nswitching across 13 Indian languages and English in a given native speaker's\nvoice, which corresponds to everyday speech in the Indian subcontinent, where\nthe population is multilingual.\n","authors":["Utkarsh P"],"pdf_url":"https://arxiv.org/pdf/2410.10508v2.pdf","comment":"3 Pages"},{"id":"http://arxiv.org/abs/2502.14677v2","updated":"2025-02-21T16:58:44Z","published":"2025-02-20T16:09:27Z","title":"Data-Constrained Synthesis of Training Data for De-Identification","summary":"  Many sensitive domains -- such as the clinical domain -- lack widely\navailable datasets due to privacy risks. The increasing generative capabilities\nof large language models (LLMs) have made synthetic datasets a viable path\nforward. In this study, we domain-adapt LLMs to the clinical domain and\ngenerate synthetic clinical texts that are machine-annotated with tags for\npersonally identifiable information using capable encoder-based NER models. The\nsynthetic corpora are then used to train synthetic NER models. The results show\nthat training NER models using synthetic corpora incurs only a small drop in\npredictive performance. The limits of this process are investigated in a\nsystematic ablation study -- using both Swedish and Spanish data. Our analysis\nshows that smaller datasets can be sufficient for domain-adapting LLMs for data\nsynthesis. Instead, the effectiveness of this process is almost entirely\ncontingent on the performance of the machine-annotating NER models trained\nusing the original data.\n","authors":["Thomas Vakili","Aron Henriksson","Hercules Dalianis"],"pdf_url":"https://arxiv.org/pdf/2502.14677v2.pdf","comment":"Under review"},{"id":"http://arxiv.org/abs/2502.15589v1","updated":"2025-02-21T16:57:22Z","published":"2025-02-21T16:57:22Z","title":"LightThinker: Thinking Step-by-Step Compression","summary":"  Large language models (LLMs) have shown remarkable performance in complex\nreasoning tasks, but their efficiency is hindered by the substantial memory and\ncomputational costs associated with generating lengthy tokens. In this paper,\nwe propose LightThinker, a novel method that enables LLMs to dynamically\ncompress intermediate thoughts during reasoning. Inspired by human cognitive\nprocesses, LightThinker compresses verbose thought steps into compact\nrepresentations and discards the original reasoning chains, thereby\nsignificantly reducing the number of tokens stored in the context window. This\nis achieved by training the model on when and how to perform compression\nthrough data construction, mapping hidden states to condensed gist tokens, and\ncreating specialized attention masks. Additionally, we introduce the Dependency\n(Dep) metric to quantify the degree of compression by measuring the reliance on\nhistorical tokens during generation. Extensive experiments on four datasets and\ntwo models show that LightThinker reduces peak memory usage and inference time,\nwhile maintaining competitive accuracy. Our work provides a new direction for\nimproving the efficiency of LLMs in complex reasoning tasks without sacrificing\nperformance. Code will be released at https://github.com/zjunlp/LightThinker.\n","authors":["Jintian Zhang","Yuqi Zhu","Mengshu Sun","Yujie Luo","Shuofei Qiao","Lun Du","Da Zheng","Huajun Chen","Ningyu Zhang"],"pdf_url":"https://arxiv.org/pdf/2502.15589v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.01030v2","updated":"2025-02-21T16:53:35Z","published":"2025-01-02T03:21:32Z","title":"Reasoning based on symbolic and parametric knowledge bases: a survey","summary":"  Reasoning is fundamental to human intelligence, and critical for\nproblem-solving, decision-making, and critical thinking. Reasoning refers to\ndrawing new conclusions based on existing knowledge, which can support various\napplications like clinical diagnosis, basic education, and financial analysis.\nThough a good number of surveys have been proposed for reviewing\nreasoning-related methods, none of them has systematically investigated these\nmethods from the viewpoint of their dependent knowledge base. Both the\nscenarios to which the knowledge bases are applied and their storage formats\nare significantly different. Hence, investigating reasoning methods from the\nknowledge base perspective helps us better understand the challenges and future\ndirections. To fill this gap, this paper first classifies the knowledge base\ninto symbolic and parametric ones. The former explicitly stores information in\nhuman-readable symbols, and the latter implicitly encodes knowledge within\nparameters. Then, we provide a comprehensive overview of reasoning methods\nusing symbolic knowledge bases, parametric knowledge bases, and both of them.\nFinally, we identify the future direction toward enhancing reasoning\ncapabilities to bridge the gap between human and machine intelligence.\n","authors":["Mayi Xu","Yunfeng Ning","Yongqi Li","Jianhao Chen","Jintao Wen","Yao Xiao","Shen Zhou","Birong Pan","Zepeng Bao","Xin Miao","Hankun Kang","Ke Sun","Tieyun Qian"],"pdf_url":"https://arxiv.org/pdf/2501.01030v2.pdf","comment":"There are imperfections in some parts of the paper, which may lead to\n  misunderstandings among readers. To be rigorous, we apply for the withdrawal\n  of this paper."},{"id":"http://arxiv.org/abs/2502.15583v1","updated":"2025-02-21T16:47:01Z","published":"2025-02-21T16:47:01Z","title":"Chats-Grid: An Iterative Retrieval Q&A Optimization Scheme Leveraging\n  Large Model and Retrieval Enhancement Generation in smart grid","summary":"  With rapid advancements in artificial intelligence, question-answering (Q&A)\nsystems have become essential in intelligent search engines, virtual\nassistants, and customer service platforms. However, in dynamic domains like\nsmart grids, conventional retrieval-augmented generation(RAG) Q&A systems face\nchallenges such as inadequate retrieval quality, irrelevant responses, and\ninefficiencies in handling large-scale, real-time data streams. This paper\nproposes an optimized iterative retrieval-based Q&A framework called Chats-Grid\ntailored for smart grid environments. In the pre-retrieval phase, Chats-Grid\nadvanced query expansion ensures comprehensive coverage of diverse data\nsources, including sensor readings, meter records, and control system\nparameters. During retrieval, Best Matching 25(BM25) sparse retrieval and BAAI\nGeneral Embedding(BGE) dense retrieval in Chats-Grid are combined to process\nvast, heterogeneous datasets effectively. Post-retrieval, a fine-tuned large\nlanguage model uses prompt engineering to assess relevance, filter irrelevant\nresults, and reorder documents based on contextual accuracy. The model further\ngenerates precise, context-aware answers, adhering to quality criteria and\nemploying a self-checking mechanism for enhanced reliability. Experimental\nresults demonstrate Chats-Grid's superiority over state-of-the-art methods in\nfidelity, contextual recall, relevance, and accuracy by 2.37%, 2.19%, and 3.58%\nrespectively. This framework advances smart grid management by improving\ndecision-making and user interactions, fostering resilient and adaptive smart\ngrid infrastructures.\n","authors":["Yunfeng Li","Jiqun Zhang","Guofu Liao","Xue Shi","Junhong Liu"],"pdf_url":"https://arxiv.org/pdf/2502.15583v1.pdf","comment":"12 pages, 10 figures"},{"id":"http://arxiv.org/abs/2407.18328v2","updated":"2025-02-21T16:45:17Z","published":"2024-07-04T22:26:20Z","title":"Unveiling Scoring Processes: Dissecting the Differences between LLMs and\n  Human Graders in Automatic Scoring","summary":"  Large language models (LLMs) have demonstrated strong potential in performing\nautomatic scoring for constructed response assessments. While constructed\nresponses graded by humans are usually based on given grading rubrics, the\nmethods by which LLMs assign scores remain largely unclear. It is also\nuncertain how closely AI's scoring process mirrors that of humans or if it\nadheres to the same grading criteria. To address this gap, this paper uncovers\nthe grading rubrics that LLMs used to score students' written responses to\nscience tasks and their alignment with human scores. We also examine whether\nenhancing the alignments can improve scoring accuracy. Specifically, we prompt\nLLMs to generate analytic rubrics that they use to assign scores and study the\nalignment gap with human grading rubrics. Based on a series of experiments with\nvarious configurations of LLM settings, we reveal a notable alignment gap\nbetween human and LLM graders. While LLMs can adapt quickly to scoring tasks,\nthey often resort to shortcuts, bypassing deeper logical reasoning expected in\nhuman grading. We found that incorporating high-quality analytical rubrics\ndesigned to reflect human grading logic can mitigate this gap and enhance LLMs'\nscoring accuracy. These results underscore the need for a nuanced approach when\napplying LLMs in science education and highlight the importance of aligning LLM\noutputs with human expectations to ensure efficient and accurate automatic\nscoring.\n","authors":["Xuansheng Wu","Padmaja Pravin Saraf","Gyeonggeon Lee","Ehsan Latif","Ninghao Liu","Xiaoming Zhai"],"pdf_url":"https://arxiv.org/pdf/2407.18328v2.pdf","comment":"Accepted by Technology, Knowledge, and Learning (TKNL)"},{"id":"http://arxiv.org/abs/2412.00966v3","updated":"2025-02-21T16:42:44Z","published":"2024-12-01T21:06:08Z","title":"From Priest to Doctor: Domain Adaptation for Low-Resource Neural Machine\n  Translation","summary":"  Many of the world's languages have insufficient data to train high-performing\ngeneral neural machine translation (NMT) models, let alone domain-specific\nmodels, and often the only available parallel data are small amounts of\nreligious texts. Hence, domain adaptation (DA) is a crucial issue faced by\ncontemporary NMT and has, so far, been underexplored for low-resource\nlanguages. In this paper, we evaluate a set of methods from both low-resource\nNMT and DA in a realistic setting, in which we aim to translate between a\nhigh-resource and a low-resource language with access to only: a) parallel\nBible data, b) a bilingual dictionary, and c) a monolingual target-domain\ncorpus in the high-resource language. Our results show that the effectiveness\nof the tested methods varies, with the simplest one, DALI, being most\neffective. We follow up with a small human evaluation of DALI, which shows that\nthere is still a need for more careful investigation of how to accomplish DA\nfor low-resource NMT.\n","authors":["Ali Marashian","Enora Rice","Luke Gessler","Alexis Palmer","Katharina von der Wense"],"pdf_url":"https://arxiv.org/pdf/2412.00966v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.15576v1","updated":"2025-02-21T16:36:42Z","published":"2025-02-21T16:36:42Z","title":"Interpreting and Steering LLMs with Mutual Information-based\n  Explanations on Sparse Autoencoders","summary":"  Large language models (LLMs) excel at handling human queries, but they can\noccasionally generate flawed or unexpected responses. Understanding their\ninternal states is crucial for understanding their successes, diagnosing their\nfailures, and refining their capabilities. Although sparse autoencoders (SAEs)\nhave shown promise for interpreting LLM internal representations, limited\nresearch has explored how to better explain SAE features, i.e., understanding\nthe semantic meaning of features learned by SAE. Our theoretical analysis\nreveals that existing explanation methods suffer from the frequency bias issue,\nwhere they emphasize linguistic patterns over semantic concepts, while the\nlatter is more critical to steer LLM behaviors. To address this, we propose\nusing a fixed vocabulary set for feature interpretations and designing a mutual\ninformation-based objective, aiming to better capture the semantic meaning\nbehind these features. We further propose two runtime steering strategies that\nadjust the learned feature activations based on their corresponding\nexplanations. Empirical results show that, compared to baselines, our method\nprovides more discourse-level explanations and effectively steers LLM behaviors\nto defend against jailbreak attacks. These findings highlight the value of\nexplanations for steering LLM behaviors in downstream applications. We will\nrelease our code and data once accepted.\n","authors":["Xuansheng Wu","Jiayi Yuan","Wenlin Yao","Xiaoming Zhai","Ninghao Liu"],"pdf_url":"https://arxiv.org/pdf/2502.15576v1.pdf","comment":"Pre-print. 20 pages, 5 figures"},{"id":"http://arxiv.org/abs/2502.15572v1","updated":"2025-02-21T16:32:28Z","published":"2025-02-21T16:32:28Z","title":"DReSD: Dense Retrieval for Speculative Decoding","summary":"  Speculative decoding (SD) accelerates Large Language Model (LLM) generation\nby using an efficient draft model to propose the next few tokens, which are\nverified by the LLM in a single forward call, reducing latency while preserving\nits outputs. We focus on retrieval-based SD where the draft model retrieves the\nnext tokens from a non-parametric datastore. Sparse retrieval (REST), which\noperates on the surface form of strings, is currently the dominant paradigm due\nto its simplicity and scalability. However, its effectiveness is limited due to\nthe usage of short contexts and exact string matching. Instead, we introduce\nDense Retrieval for Speculative Decoding (DReSD), a novel framework that uses\napproximate nearest neighbour search with contextualised token embeddings to\nretrieve the most semantically relevant token sequences for SD. Extensive\nexperiments show that DReSD achieves (on average) 87% higher acceptance rates,\n65% longer accepted tokens and 19% faster generation speeds compared to sparse\nretrieval (REST).\n","authors":["Milan Gritta","Huiyin Xue","Gerasimos Lampouras"],"pdf_url":"https://arxiv.org/pdf/2502.15572v1.pdf","comment":"Under Review"},{"id":"http://arxiv.org/abs/2502.15563v1","updated":"2025-02-21T16:24:10Z","published":"2025-02-21T16:24:10Z","title":"Bridging vision language model (VLM) evaluation gaps with a framework\n  for scalable and cost-effective benchmark generation","summary":"  Reliable evaluation of AI models is critical for scientific progress and\npractical application. While existing VLM benchmarks provide general insights\ninto model capabilities, their heterogeneous designs and limited focus on a few\nimaging domains pose significant challenges for both cross-domain performance\ncomparison and targeted domain-specific evaluation. To address this, we propose\nthree key contributions: (1) a framework for the resource-efficient creation of\ndomain-specific VLM benchmarks enabled by task augmentation for creating\nmultiple diverse tasks from a single existing task, (2) the release of new VLM\nbenchmarks for seven domains, created according to the same homogeneous\nprotocol and including 162,946 thoroughly human-validated answers, and (3) an\nextensive benchmarking of 22 state-of-the-art VLMs on a total of 37,171 tasks,\nrevealing performance variances across domains and tasks, thereby supporting\nthe need for tailored VLM benchmarks. Adoption of our methodology will pave the\nway for the resource-efficient domain-specific selection of models and guide\nfuture research efforts toward addressing core open questions.\n","authors":["Tim Rädsch","Leon Mayer","Simon Pavicic","A. Emre Kavur","Marcel Knopp","Barış Öztürk","Klaus Maier-Hein","Paul F. Jaeger","Fabian Isensee","Annika Reinke","Lena Maier-Hein"],"pdf_url":"https://arxiv.org/pdf/2502.15563v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.05673v4","updated":"2025-02-21T16:17:17Z","published":"2024-06-09T07:06:58Z","title":"Flow of Reasoning:Training LLMs for Divergent Problem Solving with\n  Minimal Examples","summary":"  The ability to generate diverse solutions to a given problem is a hallmark of\nhuman creativity. This divergent reasoning is also crucial for machines,\nenhancing their robustness and enabling them to assist humans in many\napplications such as scientific discovery. However, existing approaches to\nmulti-step reasoning with large language models (LLMs) have mostly focused only\non reasoning accuracy, without further discovering more diverse valid\nsolutions. For example, supervised fine-tuning can improve LLM reasoning\nquality, but requires extensive supervised data to capture the full range of\npossible solutions. Reward-maximization reinforcement learning aims to find\nlimited highest-reward solutions while neglecting the solution diversity. To\nfill this gap, we propose Flow of Reasoning (FoR), an efficient\ndiversity-seeking LLM finetuning method aimed at improving reasoning quality\nand diversity with minimal data. FoR formulates multi-step LLM reasoning as a\nMarkovian flow on a DAG-structured reasoning graph. This formulation allows us\nto incorporate and adapt principled GFlowNet approaches, for finetuning LLMs to\nsample divergent paths with probabilities proportional to the (unnormalized)\nreward of target problems. Extensive experiments show that, with limited\ntraining examples (e.g., 15 examples), FoR enables the discovery of diverse,\ncreative, high-quality solutions, greatly outperforming a wide range of\nexisting inference and training methods across six challenging reasoning tasks,\nincluding BlocksWorld (embodied reasoning), Game24 (math puzzle solving),\nRubik's Cube (spatial reasoning), 1D-ARC (abstraction reasoning), GSM8k (math\nreasoning), and ProntoQA (logical reasoning). Code is available at\nhttps://github.com/Yu-Fangxu/FoR.\n","authors":["Fangxu Yu","Lai Jiang","Haoqiang Kang","Shibo Hao","Lianhui Qin"],"pdf_url":"https://arxiv.org/pdf/2406.05673v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.03930v2","updated":"2025-02-21T16:15:12Z","published":"2024-10-04T21:13:58Z","title":"Reverb: Open-Source ASR and Diarization from Rev","summary":"  Today, we are open-sourcing our core speech recognition and diarization\nmodels for non-commercial use. We are releasing both a full production pipeline\nfor developers as well as pared-down research models for experimentation. Rev\nhopes that these releases will spur research and innovation in the fast-moving\ndomain of voice technology. The speech recognition models released today\noutperform all existing open source speech recognition models across a variety\nof long-form speech recognition domains.\n","authors":["Nishchal Bhandari","Danny Chen","Miguel Ángel del Río Fernández","Natalie Delworth","Jennifer Drexler Fox","Migüel Jetté","Quinten McNamara","Corey Miller","Ondřej Novotný","Ján Profant","Nan Qin","Martin Ratajczak","Jean-Philippe Robichaud"],"pdf_url":"https://arxiv.org/pdf/2410.03930v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.01870v2","updated":"2025-02-21T16:07:00Z","published":"2024-10-02T17:29:23Z","title":"NEAT: Nonlinear Parameter-efficient Adaptation of Pre-trained Models","summary":"  Fine-tuning pre-trained models often yields state-of-the-art performance but\nis computationally expensive when updating all parameters. Parameter-efficient\nfine-tuning (PEFT) methods, such as Low-Rank Adaptation (LoRA), address this by\nfreezing pre-trained weights and introducing low-rank matrices. However,\nbecause LoRA relies on low-rank decomposition, it struggles to capture complex\nnonlinear dynamics and optimal optimization trajectories, resulting in a\nperformance gap relative to full fine-tuning and inefficient parameter\nutilization. To overcome these issues, we propose NEAT, a nonlinear PEFT\napproach that employs a lightweight neural network to learn a nonlinear\ntransformation of the pre-trained weights, thereby better approximating\ncumulative weight updates. Our theoretical analysis shows that NEAT achieves\ngreater efficiency than LoRA while maintaining equivalent expressivity.\nExtensive experiments on four benchmarks and over twenty datasets demonstrate\nthat NEAT significantly outperforms state-of-the-art baselines in both vision\nand text tasks.\n","authors":["Yibo Zhong","Haoxiang Jiang","Lincan Li","Ryumei Nakada","Tianci Liu","Linjun Zhang","Huaxiu Yao","Haoyu Wang"],"pdf_url":"https://arxiv.org/pdf/2410.01870v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.13124v2","updated":"2025-02-21T16:02:42Z","published":"2025-02-18T18:46:57Z","title":"NaturalReasoning: Reasoning in the Wild with 2.8M Challenging Questions","summary":"  Scaling reasoning capabilities beyond traditional domains such as math and\ncoding is hindered by the lack of diverse and high-quality questions. To\novercome this limitation, we introduce a scalable approach for generating\ndiverse and challenging reasoning questions, accompanied by reference answers.\nWe present NaturalReasoning, a comprehensive dataset comprising 2.8 million\nquestions that span multiple domains, including STEM fields (e.g., Physics,\nComputer Science), Economics, Social Sciences, and more. We demonstrate the\nutility of the questions in NaturalReasoning through knowledge distillation\nexperiments which show that NaturalReasoning can effectively elicit and\ntransfer reasoning capabilities from a strong teacher model. Furthermore, we\ndemonstrate that NaturalReasoning is also effective for unsupervised\nself-training using external reward models or self-rewarding.\n","authors":["Weizhe Yuan","Jane Yu","Song Jiang","Karthik Padthe","Yang Li","Dong Wang","Ilia Kulikov","Kyunghyun Cho","Yuandong Tian","Jason E Weston","Xian Li"],"pdf_url":"https://arxiv.org/pdf/2502.13124v2.pdf","comment":"Dataset at https://huggingface.co/datasets/facebook/natural_reasoning"},{"id":"http://arxiv.org/abs/2502.15543v1","updated":"2025-02-21T15:50:41Z","published":"2025-02-21T15:50:41Z","title":"PIP-KAG: Mitigating Knowledge Conflicts in Knowledge-Augmented\n  Generation via Parametric Pruning","summary":"  Knowledge-Augmented Generation (KAG) has shown great promise in updating the\ninternal memory of Large Language Models (LLMs) by integrating external\nknowledge. However, KAG inevitably faces knowledge conflicts when the internal\nmemory contradicts external information. Current approaches to mitigating these\nconflicts mainly focus on improving external knowledge utilization. However,\nthese methods have shown only limited effectiveness in mitigating the knowledge\nconflict problem, as internal knowledge continues to influence the generation\nprocess of LLMs. In this paper, we propose a ParametrIc Pruning-based\nKnowledge-Augmented Generation (PIP-KAG) approach, which prunes internal\nknowledge of LLMs and incorporates a plug-and-play adaptation module to help\nLLMs better leverage external sources. Additionally, we construct the\nCoConflictQA benchmark based on the hallucination of LLMs to better evaluate\ncontextual faithfulness during answering questions. Experimental results on\nCoConflictQA demonstrate that PIP-KAG significantly reduces knowledge conflicts\nand improves context fidelity. Notably, PIP-KAG reduces LLM's parameters by\n13%, enhancing parameter efficiency in LLMs within the KAG framework. All codes\nare available at https://github.com/OpenBMB/PIP-KAG.\n","authors":["Pengcheng Huang","Zhenghao Liu","Yukun Yan","Xiaoyuan Yi","Hao Chen","Zhiyuan Liu","Maosong Sun","Tong Xiao","Ge Yu","Chenyan Xiong"],"pdf_url":"https://arxiv.org/pdf/2502.15543v1.pdf","comment":"20 pages, 7 figures, 7 tables"},{"id":"http://arxiv.org/abs/2412.21102v2","updated":"2025-02-21T15:48:44Z","published":"2024-12-30T17:25:58Z","title":"Exploring and Controlling Diversity in LLM-Agent Conversation","summary":"  Controlling diversity in LLM-agent world simulations is essential for\nmaintaining stability in structured tasks while enabling variation where\ncreativity is needed. However, we observe that dialogue diversity declines\nsignificantly over long-term simulation. To investigate the role of prompt\ndesign in conversational diversity, we modularized the utterance generation\nprompt and found that reducing the given information leads to more diverse\noutputs. Based on this insight, we propose Adaptive Prompt Pruning (APP), a\nnovel method that allows users to control diversity through a single parameter,\nlambda. APP dynamically prunes the utterance generation prompt based on their\nattention weights and is compatible with traditional diversity control\ntechniques. We demonstrate that APP effectively controls output diversity\nthrough extensive experiments, and propose a method to balance the control\ntrade-offs. Additionally, we provide an in-depth analysis to offer insights\ninto optimizing diversity control in multi-agent simulation.\n","authors":["KuanChao Chu","Yi-Pei Chen","Hideki Nakayama"],"pdf_url":"https://arxiv.org/pdf/2412.21102v2.pdf","comment":"Accepted for the AAAI 2025 Workshop on Advancing LLM-Based\n  Multi-Agent Collaboration (v1); updated version (v2)"},{"id":"http://arxiv.org/abs/2409.17179v2","updated":"2025-02-21T15:48:32Z","published":"2024-09-23T17:40:24Z","title":"Fully automatic extraction of morphological traits from the Web: utopia\n  or reality?","summary":"  Plant morphological traits, their observable characteristics, are fundamental\nto understand the role played by each species within their ecosystem. However,\ncompiling trait information for even a moderate number of species is a\ndemanding task that may take experts years to accomplish. At the same time,\nmassive amounts of information about species descriptions is available online\nin the form of text, although the lack of structure makes this source of data\nimpossible to use at scale. To overcome this, we propose to leverage recent\nadvances in large language models (LLMs) and devise a mechanism for gathering\nand processing information on plant traits in the form of unstructured textual\ndescriptions, without manual curation. We evaluate our approach by\nautomatically replicating three manually created species-trait matrices. Our\nmethod managed to find values for over half of all species-trait pairs, with an\nF1-score of over 75%. Our results suggest that large-scale creation of\nstructured trait databases from unstructured online text is currently feasible\nthanks to the information extraction capabilities of LLMs, being limited by the\navailability of textual descriptions covering all the traits of interest.\n","authors":["Diego Marcos","Robert van de Vlasakker","Ioannis N. Athanasiadis","Pierre Bonnet","Hervé Goeau","Alexis Joly","W. Daniel Kissling","César Leblanc","André S. J. van Proosdij","Konstantinos P. Panousis"],"pdf_url":"https://arxiv.org/pdf/2409.17179v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.15538v1","updated":"2025-02-21T15:40:37Z","published":"2025-02-21T15:40:37Z","title":"SOTOPIA-Ω: Dynamic Strategy Injection Learning and Social\n  Instrucion Following Evaluation for Social Agents","summary":"  Despite the abundance of prior social strategies possessed by humans, there\nremains a paucity of research dedicated to their transfer and integration into\nsocial agents. Our proposed SOTOPIA-{\\Omega} framework aims to address and\nbridge this gap, with a particular focus on enhancing the social capabilities\nof language agents. This framework dynamically injects multi-step reasoning\nstrategies inspired by negotiation theory, along with two simple direct\nstrategies, into expert agents, thereby automating the construction of\nhigh-quality social dialogue training corpus. Additionally, we introduce the\nconcept of Social Instruction Following (S-IF) and propose two new S-IF\nevaluation metrics that are complementary to social capability. We demonstrate\nthat several 7B models trained on high-quality corpus not only significantly\nsurpass the expert agent (GPT-4) in achieving social goals but also enhance\nS-IF performance. Analysis and variant experiments validate the advantages of\ndynamic construction, which can especially break the agent's prolonged\ndeadlock.\n","authors":["Wenyuan Zhang","Tianyun Liu","Mengxiao Song","Xiaodong Li","Tingwen Liu"],"pdf_url":"https://arxiv.org/pdf/2502.15538v1.pdf","comment":"26 pages, 5 figures, 23 tables"},{"id":"http://arxiv.org/abs/2502.15507v1","updated":"2025-02-21T15:04:48Z","published":"2025-02-21T15:04:48Z","title":"Activation Steering in Neural Theorem Provers","summary":"  Large Language Models (LLMs) have shown promise in proving formal theorems\nusing proof assistants like Lean. However, current state of the art language\nmodels struggles to predict next step in proofs leading practitioners to use\ndifferent sampling techniques to improve LLMs capabilities. We observe that the\nLLM is capable of predicting the correct tactic; however, it faces challenges\nin ranking it appropriately within the set of candidate tactics, affecting the\noverall selection process. To overcome this hurdle, we use activation steering\nto guide LLMs responses to improve the generations at the time of inference.\nOur results suggest that activation steering offers a promising lightweight\nalternative to specialized fine-tuning for enhancing theorem proving\ncapabilities in LLMs, particularly valuable in resource-constrained\nenvironments.\n","authors":["Shashank Kirtania"],"pdf_url":"https://arxiv.org/pdf/2502.15507v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.14155v2","updated":"2025-02-21T14:57:14Z","published":"2025-02-19T23:51:23Z","title":"Giving AI Personalities Leads to More Human-Like Reasoning","summary":"  In computational cognitive modeling, capturing the full spectrum of human\njudgment and decision-making processes, beyond just optimal behaviors, is a\nsignificant challenge. This study explores whether Large Language Models (LLMs)\ncan emulate the breadth of human reasoning by predicting both intuitive, fast\nSystem 1 and deliberate, slow System 2 processes. We investigate the potential\nof AI to mimic diverse reasoning behaviors across a human population,\naddressing what we call the \"full reasoning spectrum problem\". We designed\nreasoning tasks using a novel generalization of the Natural Language Inference\n(NLI) format to evaluate LLMs' ability to replicate human reasoning. The\nquestions were crafted to elicit both System 1 and System 2 responses. Human\nresponses were collected through crowd-sourcing and the entire distribution was\nmodeled, rather than just the majority of the answers. We used\npersonality-based prompting inspired by the Big Five personality model to\nelicit AI responses reflecting specific personality traits, capturing the\ndiversity of human reasoning, and exploring how personality traits influence\nLLM outputs. Combined with genetic algorithms to optimize the weighting of\nthese prompts, this method was tested alongside traditional machine learning\nmodels. The results show that LLMs can mimic human response distributions, with\nopen-source models like Llama and Mistral outperforming proprietary GPT models.\nPersonality-based prompting, especially when optimized with genetic algorithms,\nsignificantly enhanced LLMs' ability to predict human response distributions,\nsuggesting that capturing suboptimal, naturalistic reasoning may require\nmodeling techniques incorporating diverse reasoning styles and psychological\nprofiles. The study concludes that personality-based prompting combined with\ngenetic algorithms is promising for enhancing AI's 'human-ness' in reasoning.\n","authors":["Animesh Nighojkar","Bekhzodbek Moydinboyev","My Duong","John Licato"],"pdf_url":"https://arxiv.org/pdf/2502.14155v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.03642v2","updated":"2025-02-21T14:54:05Z","published":"2024-06-05T22:35:17Z","title":"Is Free Self-Alignment Possible?","summary":"  Aligning pretrained language models (LMs) often requires large-scale\npreference data and substantial computational resources. These costs become\neven more prohibitive for multi-objective or pluralistic alignment. Is this\ntruly necessary? Can we perform efficient alignment using only internal model\ncapabilities, and without additional training? To answer this question, we\npropose AlignEZ, a novel approach that leverages (1) self-generated preference\ndata and (2) representation editing to achieve cost-effective, efficient\nalignment. By operating directly on learned representations, AlignEZ\nindependently targets different behavioral aspects without the overhead of\ntraditional alignment methods. Our experiments reveal that this cost-efficient\nprocedure improves performance across diverse tasks: up to 19.9% on general\nalignment and 1.9% on challenging mathematical reasoning tasks, even when\nstarting from a strong base model. AlignEZ can also align models to multiple\nobjectives simultaneously, granting fine-grained control over multiple\npreference axes. Finally, we show that AlignEZ can accelerate more expensive\nalignment procedures--such as DPO--even under limited availability of\nground-truth preference data.\n","authors":["Dyah Adila","Changho Shin","Yijing Zhang","Frederic Sala"],"pdf_url":"https://arxiv.org/pdf/2406.03642v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.15499v1","updated":"2025-02-21T14:49:34Z","published":"2025-02-21T14:49:34Z","title":"Scale-Distribution Decoupling: Enabling Stable and Effective Training of\n  Large Language Models","summary":"  Training stability is a persistent challenge in the pre-training of large\nlanguage models (LLMs), particularly for architectures such as Post-Norm\nTransformers, which are prone to gradient explosion and dissipation. In this\npaper, we propose Scale-Distribution Decoupling (SDD), a novel approach that\nstabilizes training by explicitly decoupling the scale and distribution of the\nweight matrix in fully-connected layers. SDD applies a normalization mechanism\nto regulate activations and a learnable scaling vector to maintain\nwell-conditioned gradients, effectively preventing $\\textbf{gradient explosion\nand dissipation}$. This separation improves optimization efficiency,\nparticularly in deep networks, by ensuring stable gradient propagation.\nExperimental results demonstrate that our method stabilizes training across\nvarious LLM architectures and outperforms existing techniques in different\nnormalization configurations. Furthermore, the proposed method is lightweight\nand compatible with existing frameworks, making it a practical solution for\nstabilizing LLM training. Code is available at https://github.com/kaihemo/SDD.\n","authors":["Ya Wang","Zhijian Zhuo","Yutao Zeng","Xun Zhou","Jian Yang","Xiaoqing Li"],"pdf_url":"https://arxiv.org/pdf/2502.15499v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07780v2","updated":"2025-02-21T14:41:48Z","published":"2025-02-11T18:59:35Z","title":"DarwinLM: Evolutionary Structured Pruning of Large Language Models","summary":"  Large Language Models (LLMs) have achieved significant success across various\nNLP tasks. However, their massive computational costs limit their widespread\nuse, particularly in real-time applications. Structured pruning offers an\neffective solution by compressing models and directly providing end-to-end\nspeed improvements, regardless of the hardware environment. Meanwhile,\ndifferent components of the model exhibit varying sensitivities towards\npruning, calling for \\emph{non-uniform} model compression. However, a pruning\nmethod should not only identify a capable substructure, but also account for\npost-compression training. To this end, we propose \\sysname, a method for\n\\emph{training-aware} structured pruning. \\sysname builds upon an evolutionary\nsearch process, generating multiple offspring models in each generation through\nmutation, and selecting the fittest for survival. To assess the effect of\npost-training, we incorporate a lightweight, multistep training process within\nthe offspring population, progressively increasing the number of tokens and\neliminating poorly performing models in each selection stage. We validate our\nmethod through extensive experiments on Llama-2-7B, Llama-3.1-8B and\nQwen-2.5-14B-Instruct, achieving state-of-the-art performance for structured\npruning. For instance, \\sysname surpasses ShearedLlama while requiring\n$5\\times$ less training data during post-compression training. Code is at:\nhttps://github.com/IST-DASLab/DarwinLM\n","authors":["Shengkun Tang","Oliver Sieberling","Eldar Kurtic","Zhiqiang Shen","Dan Alistarh"],"pdf_url":"https://arxiv.org/pdf/2502.07780v2.pdf","comment":"Code: https://github.com/IST-DASLab/DarwinLM"},{"id":"http://arxiv.org/abs/2502.10921v2","updated":"2025-02-21T14:41:21Z","published":"2025-02-15T22:46:50Z","title":"Evolving Hate Speech Online: An Adaptive Framework for Detection and\n  Mitigation","summary":"  The proliferation of social media platforms has led to an increase in the\nspread of hate speech, particularly targeting vulnerable communities.\nUnfortunately, existing methods for automatically identifying and blocking\ntoxic language rely on pre-constructed lexicons, making them reactive rather\nthan adaptive. As such, these approaches become less effective over time,\nespecially when new communities are targeted with slurs not included in the\noriginal datasets. To address this issue, we present an adaptive approach that\nuses word embeddings to update lexicons and develop a hybrid model that adjusts\nto emerging slurs and new linguistic patterns. This approach can effectively\ndetect toxic language, including intentional spelling mistakes employed by\naggressors to avoid detection. Our hybrid model, which combines BERT with\nlexicon-based techniques, achieves an accuracy of 95% for most state-of-the-art\ndatasets. Our work has significant implications for creating safer online\nenvironments by improving the detection of toxic content and proactively\nupdating the lexicon. Content Warning: This paper contains examples of hate\nspeech that may be triggering.\n","authors":["Shiza Ali","Jeremy Blackburn","Gianluca Stringhini"],"pdf_url":"https://arxiv.org/pdf/2502.10921v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.12560v2","updated":"2025-02-21T14:41:19Z","published":"2025-02-18T05:54:56Z","title":"How does a Language-Specific Tokenizer affect LLMs?","summary":"  The necessity of language-specific tokenizers intuitively appears crucial for\neffective natural language processing, yet empirical analyses on their\nsignificance and underlying reasons are lacking. This study explores how\nlanguage-specific tokenizers influence the behavior of Large Language Models\npredominantly trained with English text data, through the case study of Korean.\nThe research unfolds in two main stages: (1) the development of a\nKorean-specific extended tokenizer and (2) experiments to compare models with\nthe basic tokenizer and the extended tokenizer through various Next Token\nPrediction tasks. Our in-depth analysis reveals that the extended tokenizer\ndecreases confidence in incorrect predictions during generation and reduces\ncross-entropy in complex tasks, indicating a tendency to produce less\nnonsensical outputs. Consequently, the extended tokenizer provides stability\nduring generation, potentially leading to higher performance in downstream\ntasks.\n","authors":["Jean Seo","Jaeyoon Kim","SungJoo Byun","Hyopil Shin"],"pdf_url":"https://arxiv.org/pdf/2502.12560v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.14313v2","updated":"2025-02-21T14:35:19Z","published":"2024-06-20T13:43:38Z","title":"Iterative Repair with Weak Verifiers for Few-shot Transfer in KBQA with\n  Unanswerability","summary":"  Real-world applications of KBQA require models to handle unanswerable\nquestions with a limited volume of in-domain labeled training data. We propose\nthe novel task of few-shot transfer for KBQA with unanswerable questions and\ncontribute two new datasets for performance evaluation. We present FUn-FuSIC -\na novel solution for our task that extends FuSIC KBQA, the state-of-the-art\nfew-shot transfer model for answerable-only KBQA. We first note that\nFuSIC-KBQA's iterative repair makes a strong assumption that all questions are\nunanswerable. As a remedy, we propose Feedback for Unanswerability (FUn), which\nuses iterative repair using feedback from a suite of strong and weak verifiers,\nand an adaptation of self consistency for unanswerabilty to better assess the\nanswerability of a question. Our experiments show that FUn-FuSIC significantly\noutperforms suitable adaptations of multiple LLM based and supervised SoTA\nmodels on our task, while establishing a new SoTA for answerable few-shot\ntransfer as well.\n","authors":["Riya Sawhney","Samrat Yadav","Indrajit Bhattacharya"," Mausam"],"pdf_url":"https://arxiv.org/pdf/2406.14313v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.15487v1","updated":"2025-02-21T14:23:14Z","published":"2025-02-21T14:23:14Z","title":"ExpliCa: Evaluating Explicit Causal Reasoning in Large Language Models","summary":"  Large Language Models (LLMs) are increasingly used in tasks requiring\ninterpretive and inferential accuracy. In this paper, we introduce ExpliCa, a\nnew dataset for evaluating LLMs in explicit causal reasoning. ExpliCa uniquely\nintegrates both causal and temporal relations presented in different linguistic\norders and explicitly expressed by linguistic connectives. The dataset is\nenriched with crowdsourced human acceptability ratings. We tested LLMs on\nExpliCa through prompting and perplexity-based metrics. We assessed seven\ncommercial and open-source LLMs, revealing that even top models struggle to\nreach 0.80 accuracy. Interestingly, models tend to confound temporal relations\nwith causal ones, and their performance is also strongly influenced by the\nlinguistic order of the events. Finally, perplexity-based scores and prompting\nperformance are differently affected by model size.\n","authors":["Martina Miliani","Serenna Auriemma","Alessandro Bondielli","Emmanuele Chersoni","Lucia Passaro","Irene Sucameli","Alessandro Lenci"],"pdf_url":"https://arxiv.org/pdf/2502.15487v1.pdf","comment":"Submitted to ACL 2025"},{"id":"http://arxiv.org/abs/2501.13773v2","updated":"2025-02-21T13:50:25Z","published":"2025-01-23T15:52:34Z","title":"Do Large Language Models Truly Understand Geometric Structures?","summary":"  Geometric ability is a significant challenge for large language models (LLMs)\ndue to the need for advanced spatial comprehension and abstract thinking.\nExisting datasets primarily evaluate LLMs on their final answers, but they\ncannot truly measure their true understanding of geometric structures, as LLMs\ncan arrive at correct answers by coincidence. To fill this gap, we introduce\nthe GeomRel dataset, designed to evaluate LLMs' understanding of geometric\nstructures by isolating the core step of geometric relationship identification\nin problem-solving. Using this benchmark, we conduct thorough evaluations of\ndiverse LLMs and identify key limitations in understanding geometric\nstructures. We further propose the Geometry Chain-of-Thought (GeoCoT) method,\nwhich enhances LLMs' ability to identify geometric relationships, resulting in\nsignificant performance improvements.\n","authors":["Xiaofeng Wang","Yiming Wang","Wenhong Zhu","Rui Wang"],"pdf_url":"https://arxiv.org/pdf/2501.13773v2.pdf","comment":"Accepted to ICLR 2025"},{"id":"http://arxiv.org/abs/2410.03296v2","updated":"2025-02-21T13:45:51Z","published":"2024-10-04T10:14:12Z","title":"Comparing zero-shot self-explanations with human rationales in text\n  classification","summary":"  Instruction-tuned LLMs are able to provide an explanation about their output\nto users by generating self-explanations. These do not require gradient\ncomputations or the application of possibly complex XAI methods. In this paper,\nwe analyse whether this ability results in a good explanation. We evaluate\nself-explanations in the form of input rationales with respect to their\nplausibility to humans as well as their faithfulness to models. We study two\ntext classification tasks: sentiment classification and forced labour\ndetection, i.e., identifying pre-defined risk indicators of forced labour. In\naddition to English, we include Danish and Italian translations of the\nsentiment classification task and compare self-explanations to human\nannotations for all samples. To allow for direct comparisons, we also compute\npost-hoc feature attribution, i.e., layer-wise relevance propagation (LRP) and\nanalyse 4 LLMs. We show that self-explanations align more closely with human\nannotations compared to LRP, while maintaining a comparable level of\nfaithfulness. This finding suggests that self-explanations indeed provide good\nexplanations for text classification.\n","authors":["Stephanie Brandl","Oliver Eberle"],"pdf_url":"https://arxiv.org/pdf/2410.03296v2.pdf","comment":"preprint"},{"id":"http://arxiv.org/abs/2410.15365v4","updated":"2025-02-21T13:33:03Z","published":"2024-10-20T11:47:17Z","title":"BERTtime Stories: Investigating the Role of Synthetic Story Data in\n  Language Pre-training","summary":"  We describe our contribution to the Strict and Strict-Small tracks of the 2nd\niteration of the BabyLM Challenge. The shared task is centered around efficient\npre-training given data constraints motivated by human development. In\nresponse, we study the effect of synthetic story data in language pre-training\nusing TinyStories: a recently introduced dataset of short stories. Initially,\nwe train GPT-Neo models on subsets of TinyStories, while varying the amount of\navailable data. We find that, even with access to less than 100M words, the\nmodels are able to generate high-quality, original completions to a given\nstory, and acquire substantial linguistic knowledge. To measure the effect of\nsynthetic story data, we train LTG-BERT encoder models on a combined dataset\nof: a subset of TinyStories, story completions generated by GPT-Neo, and a\nsubset of the BabyLM dataset. Our experimentation reveals that synthetic data\ncan occasionally offer modest gains, but overall have a negative influence on\nlinguistic understanding. Our work offers an initial study on synthesizing\nstory data in low resource settings and underscores their potential for\naugmentation in data-constrained language modeling. We publicly release our\nmodels and implementation on our GitHub.\n","authors":["Nikitas Theodoropoulos","Giorgos Filandrianos","Vassilis Lyberatos","Maria Lymperaiou","Giorgos Stamou"],"pdf_url":"https://arxiv.org/pdf/2410.15365v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.15766v2","updated":"2025-02-21T13:29:42Z","published":"2024-09-24T05:44:46Z","title":"CHBench: A Chinese Dataset for Evaluating Health in Large Language\n  Models","summary":"  With the rapid development of large language models (LLMs), assessing their\nperformance on health-related inquiries has become increasingly essential. The\nuse of these models in real-world contexts-where misinformation can lead to\nserious consequences for individuals seeking medical advice and\nsupport-necessitates a rigorous focus on safety and trustworthiness. In this\nwork, we introduce CHBench, the first comprehensive safety-oriented Chinese\nhealth-related benchmark designed to evaluate LLMs' capabilities in\nunderstanding and addressing physical and mental health issues with a safety\nperspective across diverse scenarios. CHBench comprises 6,493 entries on mental\nhealth and 2,999 entries on physical health, spanning a wide range of topics.\nOur extensive evaluations of four popular Chinese LLMs highlight significant\ngaps in their capacity to deliver safe and accurate health information,\nunderscoring the urgent need for further advancements in this critical domain.\nThe code is available at https://github.com/TracyGuo2001/CHBench.\n","authors":["Chenlu Guo","Nuo Xu","Yi Chang","Yuan Wu"],"pdf_url":"https://arxiv.org/pdf/2409.15766v2.pdf","comment":"11 pages"},{"id":"http://arxiv.org/abs/2502.15451v1","updated":"2025-02-21T13:25:00Z","published":"2025-02-21T13:25:00Z","title":"A fast convergence algorithm based on binary integer programming for\n  expert load balancing in MoE LLMs","summary":"  MoE (Mixture-of-Expert) architectures appear frequently in large language\nmodels, and the number of experts can be over one hundred recently. However,\nthe expert load imbalance problem always happens in MoE model pre-training,\nwhich will cause routing collapse or increased computational overhead. In order\nto balance loads on experts, we propose BIP-Based Balancing, an expert load\nbalancing algorithm based on binary integer programming (BIP). The algorithm\nmaintains an additional vector q that can help change the top-K order of s by\nsolving a binary integer programming with very small time costs. In simulation\nexperiments, we observe that BIP-Based Balancing make imbalance disappoint very\nfast, while the final sum of routine scores decreases very little. Our\nalgorithm achieves nearly perfect trade-off between expert load balance and\npre-training efficiency under the simulation view.\n","authors":["Yuan Sun"],"pdf_url":"https://arxiv.org/pdf/2502.15451v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.15443v1","updated":"2025-02-21T13:11:22Z","published":"2025-02-21T13:11:22Z","title":"When Compression Meets Model Compression: Memory-Efficient Double\n  Compression for Large Language Models","summary":"  Large language models (LLMs) exhibit excellent performance in various tasks.\nHowever, the memory requirements of LLMs present a great challenge when\ndeploying on memory-limited devices, even for quantized LLMs. This paper\nintroduces a framework to compress LLM after quantization further, achieving\nabout 2.2x compression ratio. A compression-aware quantization is first\nproposed to enhance model weight compressibility by re-scaling the model\nparameters before quantization, followed by a pruning method to improve\nfurther. Upon this, we notice that decompression can be a bottleneck during\npractical scenarios. We then give a detailed analysis of the trade-off between\nmemory usage and latency brought by the proposed method. A speed-adaptive\nmethod is proposed to overcome it. The experimental results show inference with\nthe compressed model can achieve a 40% reduction in memory size with negligible\nloss in accuracy and inference speed.\n","authors":["Weilan Wang","Yu Mao","Dongdong Tang","Hongchao Du","Nan Guan","Chun Jason Xue"],"pdf_url":"https://arxiv.org/pdf/2502.15443v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.15436v1","updated":"2025-02-21T13:05:19Z","published":"2025-02-21T13:05:19Z","title":"Fed-SB: A Silver Bullet for Extreme Communication Efficiency and\n  Performance in (Private) Federated LoRA Fine-Tuning","summary":"  Low-Rank Adaptation (LoRA) has become ubiquitous for efficiently fine-tuning\nfoundation models. However, federated fine-tuning using LoRA is challenging due\nto suboptimal updates arising from traditional federated averaging of\nindividual adapters. Existing solutions either incur prohibitively high\ncommunication cost that scales linearly with the number of clients or suffer\nfrom performance degradation due to limited expressivity. We introduce\nFederated Silver Bullet (Fed-SB), a novel approach for federated fine-tuning of\nLLMs using LoRA-SB, a recently proposed low-rank adaptation method. LoRA-SB\noptimally aligns the optimization trajectory with the ideal low-rank full\nfine-tuning projection by learning a small square matrix (R) between adapters B\nand A, keeping other components fixed. Direct averaging of R guarantees exact\nupdates, substantially reducing communication cost, which remains independent\nof the number of clients, and enables scalability. Fed-SB achieves\nstate-of-the-art performance across commonsense reasoning, arithmetic\nreasoning, and language inference tasks while reducing communication costs by\nup to 230x. In private settings, Fed-SB further improves performance by (1)\nreducing trainable parameters, thereby lowering the noise required for\ndifferential privacy and (2) avoiding noise amplification introduced by other\nmethods. Overall, Fed-SB establishes a new Pareto frontier in the tradeoff\nbetween communication and performance, offering an efficient and scalable\nsolution for both private and non-private federated fine-tuning. Our code is\npublicly available at https://github.com/CERT-Lab/fed-sb.\n","authors":["Raghav Singhal","Kaustubh Ponkshe","Rohit Vartak","Lav R. Varshney","Praneeth Vepakomma"],"pdf_url":"https://arxiv.org/pdf/2502.15436v1.pdf","comment":"Raghav Singhal and Kaustubh Ponkshe contributed equally to this work"},{"id":"http://arxiv.org/abs/2502.15435v1","updated":"2025-02-21T13:04:13Z","published":"2025-02-21T13:04:13Z","title":"Single-pass Detection of Jailbreaking Input in Large Language Models","summary":"  Defending aligned Large Language Models (LLMs) against jailbreaking attacks\nis a challenging problem, with existing approaches requiring multiple requests\nor even queries to auxiliary LLMs, making them computationally heavy. Instead,\nwe focus on detecting jailbreaking input in a single forward pass. Our method,\ncalled Single Pass Detection SPD, leverages the information carried by the\nlogits to predict whether the output sentence will be harmful. This allows us\nto defend in just one forward pass. SPD can not only detect attacks effectively\non open-source models, but also minimizes the misclassification of harmless\ninputs. Furthermore, we show that SPD remains effective even without complete\nlogit access in GPT-3.5 and GPT-4. We believe that our proposed method offers a\npromising approach to efficiently safeguard LLMs against adversarial attacks.\n","authors":["Leyla Naz Candogan","Yongtao Wu","Elias Abad Rocamora","Grigorios G. Chrysos","Volkan Cevher"],"pdf_url":"https://arxiv.org/pdf/2502.15435v1.pdf","comment":"Accepted in TMLR 2025"},{"id":"http://arxiv.org/abs/2502.15434v1","updated":"2025-02-21T13:01:26Z","published":"2025-02-21T13:01:26Z","title":"Mixup Model Merge: Enhancing Model Merging Performance through\n  Randomized Linear Interpolation","summary":"  Model merging integrates the parameters of multiple models into a unified\nmodel, combining their diverse capabilities. Existing model merging methods are\noften constrained by fixed parameter merging ratios. In this study, we propose\nMixup Model Merge (M$^3$), an innovative approach inspired by the Mixup data\naugmentation technique. This method merges the parameters of two large language\nmodels (LLMs) by randomly generating linear interpolation ratios, allowing for\na more flexible and comprehensive exploration of the parameter space. Extensive\nexperiments demonstrate the superiority of our proposed M$^3$ method in merging\nfine-tuned LLMs: (1) it significantly improves performance across multiple\ntasks, (2) it enhances LLMs' out-of-distribution (OOD) robustness and\nadversarial robustness, (3) it achieves superior results when combined with\nsparsification techniques such as DARE, and (4) it offers a simple yet\nefficient solution that does not require additional computational resources. In\nconclusion, M$^3$ is a simple yet effective model merging method that\nsignificantly enhances the performance of the merged model by randomly\ngenerating contribution ratios for two fine-tuned LLMs. The code is available\nat https://github.com/MLGroupJLU/MixupModelMerge.\n","authors":["Yue Zhou","Yi Chang","Yuan Wu"],"pdf_url":"https://arxiv.org/pdf/2502.15434v1.pdf","comment":"15 pages"},{"id":"http://arxiv.org/abs/2502.15429v1","updated":"2025-02-21T12:54:56Z","published":"2025-02-21T12:54:56Z","title":"Pub-Guard-LLM: Detecting Fraudulent Biomedical Articles with Reliable\n  Explanations","summary":"  A significant and growing number of published scientific articles is found to\ninvolve fraudulent practices, posing a serious threat to the credibility and\nsafety of research in fields such as medicine. We propose Pub-Guard-LLM, the\nfirst large language model-based system tailored to fraud detection of\nbiomedical scientific articles. We provide three application modes for\ndeploying Pub-Guard-LLM: vanilla reasoning, retrieval-augmented generation, and\nmulti-agent debate. Each mode allows for textual explanations of predictions.\nTo assess the performance of our system, we introduce an open-source benchmark,\nPubMed Retraction, comprising over 11K real-world biomedical articles,\nincluding metadata and retraction labels. We show that, across all modes,\nPub-Guard-LLM consistently surpasses the performance of various baselines and\nprovides more reliable explanations, namely explanations which are deemed more\nrelevant and coherent than those generated by the baselines when evaluated by\nmultiple assessment methods. By enhancing both detection performance and\nexplainability in scientific fraud detection, Pub-Guard-LLM contributes to\nsafeguarding research integrity with a novel, effective, open-source tool.\n","authors":["Lihu Chen","Shuojie Fu","Gabriel Freedman","Cemre Zor","Guy Martin","James Kinross","Uddhav Vaghela","Ovidiu Serban","Francesca Toni"],"pdf_url":"https://arxiv.org/pdf/2502.15429v1.pdf","comment":"long paper under review"},{"id":"http://arxiv.org/abs/2502.15422v1","updated":"2025-02-21T12:46:40Z","published":"2025-02-21T12:46:40Z","title":"Evaluating Multimodal Generative AI with Korean Educational Standards","summary":"  This paper presents the Korean National Educational Test Benchmark (KoNET), a\nnew benchmark designed to evaluate Multimodal Generative AI Systems using\nKorean national educational tests. KoNET comprises four exams: the Korean\nElementary General Educational Development Test (KoEGED), Middle (KoMGED), High\n(KoHGED), and College Scholastic Ability Test (KoCSAT). These exams are\nrenowned for their rigorous standards and diverse questions, facilitating a\ncomprehensive analysis of AI performance across different educational levels.\nBy focusing on Korean, KoNET provides insights into model performance in\nless-explored languages. We assess a range of models - open-source,\nopen-access, and closed APIs - by examining difficulties, subject diversity,\nand human error rates. The code and dataset builder will be made fully\nopen-sourced at https://github.com/naver-ai/KoNET.\n","authors":["Sanghee Park","Geewook Kim"],"pdf_url":"https://arxiv.org/pdf/2502.15422v1.pdf","comment":"18 pages; To appear at NAACL 2025 Main Conference (Project page:\n  https://github.com/naver-ai/KoNET )"},{"id":"http://arxiv.org/abs/2501.12835v2","updated":"2025-02-21T12:41:06Z","published":"2025-01-22T12:21:17Z","title":"Adaptive Retrieval Without Self-Knowledge? Bringing Uncertainty Back\n  Home","summary":"  Retrieval Augmented Generation (RAG) improves correctness of Question\nAnswering (QA) and addresses hallucinations in Large Language Models (LLMs),\nyet greatly increase computational costs. Besides, RAG is not always needed as\nmay introduce irrelevant information. Recent adaptive retrieval methods\nintegrate LLMs' intrinsic knowledge with external information appealing to LLM\nself-knowledge, but they often neglect efficiency evaluations and comparisons\nwith uncertainty estimation techniques. We bridge this gap by conducting a\ncomprehensive analysis of 35 adaptive retrieval methods, including 8 recent\napproaches and 27 uncertainty estimation techniques, across 6 datasets using 10\nmetrics for QA performance, self-knowledge, and efficiency. Our findings show\nthat uncertainty estimation techniques often outperform complex pipelines in\nterms of efficiency and self-knowledge, while maintaining comparable QA\nperformance.\n","authors":["Viktor Moskvoretskii","Maria Lysyuk","Mikhail Salnikov","Nikolay Ivanov","Sergey Pletenev","Daria Galimzianova","Nikita Krayko","Vasily Konovalov","Irina Nikishina","Alexander Panchenko"],"pdf_url":"https://arxiv.org/pdf/2501.12835v2.pdf","comment":"The code and data are at https://github.com/s-nlp/AdaRAGUE"},{"id":"http://arxiv.org/abs/2502.15419v1","updated":"2025-02-21T12:38:26Z","published":"2025-02-21T12:38:26Z","title":"Beyond Translation: LLM-Based Data Generation for Multilingual\n  Fact-Checking","summary":"  Robust automatic fact-checking systems have the potential to combat online\nmisinformation at scale. However, most existing research primarily focuses on\nEnglish. In this paper, we introduce MultiSynFact, the first large-scale\nmultilingual fact-checking dataset containing 2.2M claim-source pairs designed\nto support Spanish, German, English, and other low-resource languages. Our\ndataset generation pipeline leverages Large Language Models (LLMs), integrating\nexternal knowledge from Wikipedia and incorporating rigorous claim validation\nsteps to ensure data quality. We evaluate the effectiveness of MultiSynFact\nacross multiple models and experimental settings. Additionally, we open-source\na user-friendly framework to facilitate further research in multilingual\nfact-checking and dataset generation.\n","authors":["Yi-Ling Chung","Aurora Cobo","Pablo Serna"],"pdf_url":"https://arxiv.org/pdf/2502.15419v1.pdf","comment":"15 pages, 1 figure, 18 tables"},{"id":"http://arxiv.org/abs/2502.15418v1","updated":"2025-02-21T12:37:58Z","published":"2025-02-21T12:37:58Z","title":"MHQA: A Diverse, Knowledge Intensive Mental Health Question Answering\n  Challenge for Language Models","summary":"  Mental health remains a challenging problem all over the world, with issues\nlike depression, anxiety becoming increasingly common. Large Language Models\n(LLMs) have seen a vast application in healthcare, specifically in answering\nmedical questions. However, there is a lack of standard benchmarking datasets\nfor question answering (QA) in mental health. Our work presents a novel\nmultiple choice dataset, MHQA (Mental Health Question Answering), for\nbenchmarking Language models (LMs). Previous mental health datasets have\nfocused primarily on text classification into specific labels or disorders.\nMHQA, on the other hand, presents question-answering for mental health focused\non four key domains: anxiety, depression, trauma, and obsessive/compulsive\nissues, with diverse question types, namely, factoid, diagnostic, prognostic,\nand preventive. We use PubMed abstracts as the primary source for QA. We\ndevelop a rigorous pipeline for LLM-based identification of information from\nabstracts based on various selection criteria and converting it into QA pairs.\nFurther, valid QA pairs are extracted based on post-hoc validation criteria.\nOverall, our MHQA dataset consists of 2,475 expert-verified gold standard\ninstances called MHQA-gold and ~56.1k pairs pseudo labeled using external\nmedical references. We report F1 scores on different LLMs along with few-shot\nand supervised fine-tuning experiments, further discussing the insights for the\nscores.\n","authors":["Suraj Racha","Prashant Joshi","Anshika Raman","Nikita Jangid","Mridul Sharma","Ganesh Ramakrishnan","Nirmal Punjabi"],"pdf_url":"https://arxiv.org/pdf/2502.15418v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.15412v1","updated":"2025-02-21T12:21:09Z","published":"2025-02-21T12:21:09Z","title":"Textual-to-Visual Iterative Self-Verification for Slide Generation","summary":"  Generating presentation slides is a time-consuming task that urgently\nrequires automation. Due to their limited flexibility and lack of automated\nrefinement mechanisms, existing autonomous LLM-based agents face constraints in\nreal-world applicability. We decompose the task of generating missing\npresentation slides into two key components: content generation and layout\ngeneration, aligning with the typical process of creating academic slides.\nFirst, we introduce a content generation approach that enhances coherence and\nrelevance by incorporating context from surrounding slides and leveraging\nsection retrieval strategies. For layout generation, we propose a\ntextual-to-visual self-verification process using a LLM-based Reviewer +\nRefiner workflow, transforming complex textual layouts into intuitive visual\nformats. This modality transformation simplifies the task, enabling accurate\nand human-like review and refinement. Experiments show that our approach\nsignificantly outperforms baseline methods in terms of alignment, logical flow,\nvisual appeal, and readability.\n","authors":["Yunqing Xu","Xinbei Ma","Jiyang Qiu","Hai Zhao"],"pdf_url":"https://arxiv.org/pdf/2502.15412v1.pdf","comment":"Work in progress"},{"id":"http://arxiv.org/abs/2502.15401v1","updated":"2025-02-21T12:00:10Z","published":"2025-02-21T12:00:10Z","title":"Problem-Solving Logic Guided Curriculum In-Context Learning for LLMs\n  Complex Reasoning","summary":"  In-context learning (ICL) can significantly enhance the complex reasoning\ncapabilities of large language models (LLMs), with the key lying in the\nselection and ordering of demonstration examples. Previous methods typically\nrelied on simple features to measure the relevance between examples. We argue\nthat these features are not sufficient to reflect the intrinsic connections\nbetween examples. In this study, we propose a curriculum ICL strategy guided by\nproblem-solving logic. We select demonstration examples by analyzing the\nproblem-solving logic and order them based on curriculum learning.\nSpecifically, we constructed a problem-solving logic instruction set based on\nthe BREAK dataset and fine-tuned a language model to analyze the\nproblem-solving logic of examples. Subsequently, we selected appropriate\ndemonstration examples based on problem-solving logic and assessed their\ndifficulty according to the number of problem-solving steps. In accordance with\nthe principles of curriculum learning, we ordered the examples from easy to\nhard to serve as contextual prompts. Experimental results on multiple\nbenchmarks indicate that our method outperforms previous ICL approaches in\nterms of performance and efficiency, effectively enhancing the complex\nreasoning capabilities of LLMs. Our project will be publicly available\nsubsequently.\n","authors":["Xuetao Ma","Wenbin Jiang","Hua Huang"],"pdf_url":"https://arxiv.org/pdf/2502.15401v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.02320v3","updated":"2025-02-21T11:53:53Z","published":"2024-10-03T08:56:29Z","title":"Post-edits Are Preferences Too","summary":"  Preference Optimization (PO) techniques are currently one of the state of the\nart techniques for fine-tuning large language models (LLMs) on pairwise\npreference feedback from human annotators. However, in machine translation,\nthis sort of feedback can be difficult to solicit. Additionally, Kreutzer et\nal. (2018) have shown that, for machine translation, pairwise preferences are\nless reliable than other forms of human feedback, such as 5-point ratings.\n  We examine post-edits to see if they can be a source of reliable human\npreferences by construction. In PO, a human annotator is shown sequences $s_1$\nand $s_2$ and asked for a preference judgment, %$s_1 > s_2$; while for\npost-editing, editors create $s_1$ and know that it should be better than\n$s_2$. We attempt to use these implicit preferences for PO and show that it\nhelps the model move towards post-edit-like hypotheses and away from machine\ntranslation-like hypotheses. Furthermore, we show that best results are\nobtained by pre-training the model with supervised fine-tuning (SFT) on\npost-edits in order to promote post-edit-like hypotheses to the top output\nranks.\n","authors":["Nathaniel Berger","Miriam Exel","Matthias Huck","Stefan Riezler"],"pdf_url":"https://arxiv.org/pdf/2410.02320v3.pdf","comment":"To appear at the Ninth Conference on Machine Translation (WMT24)"},{"id":"http://arxiv.org/abs/2502.15392v1","updated":"2025-02-21T11:38:40Z","published":"2025-02-21T11:38:40Z","title":"Chitrarth: Bridging Vision and Language for a Billion People","summary":"  Recent multimodal foundation models are primarily trained on English or high\nresource European language data, which hinders their applicability to other\nmedium and low-resource languages. To address this limitation, we introduce\nChitrarth (Chitra: Image; Artha: Meaning), an inclusive Vision-Language Model\n(VLM), specifically targeting the rich linguistic diversity and visual\nreasoning across 10 prominent Indian languages. Our model effectively\nintegrates a state-of-the-art (SOTA) multilingual Large Language Model (LLM)\nwith a vision module, primarily trained on multilingual image-text data.\nFurthermore, we also introduce BharatBench, a comprehensive framework for\nevaluating VLMs across various Indian languages, ultimately contributing to\nmore diverse and effective AI systems. Our model achieves SOTA results for\nbenchmarks across low resource languages while retaining its efficiency in\nEnglish. Through our research, we aim to set new benchmarks in\nmultilingual-multimodal capabilities, offering substantial improvements over\nexisting models and establishing a foundation to facilitate future advancements\nin this arena.\n","authors":["Shaharukh Khan","Ayush Tarun","Abhinav Ravi","Ali Faraz","Akshat Patidar","Praveen Kumar Pokala","Anagha Bhangare","Raja Kolla","Chandra Khatri","Shubham Agarwal"],"pdf_url":"https://arxiv.org/pdf/2502.15392v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.22886v2","updated":"2025-02-21T11:11:37Z","published":"2024-10-30T10:31:54Z","title":"Less is More: Pre-Training Cross-Lingual Small-Scale Language Models\n  with Cognitively-Plausible Curriculum Learning Strategies","summary":"  Curriculum Learning has been a popular strategy to improve the cognitive\nplausibility of Small-Scale Language Models (SSLMs) in the BabyLM Challenge.\nHowever, it has not led to considerable improvements over non-curriculum\nmodels. We assess whether theoretical linguistic acquisition theories can be\nused to specify more fine-grained curriculum learning strategies, creating\nage-ordered corpora of Child-Directed Speech for four typologically distant\nlanguage families to implement SSLMs and acquisition-inspired curricula\ncross-lingually. Comparing the success of three objective curricula (Growing,\nInwards and MMM) that precisely replicate the predictions of acquisition\ntheories on a standard SSLM architecture, we find fine-grained\nacquisition-inspired curricula can outperform non-curriculum baselines and\nperformance benefits of curricula strategies in SSLMs can be derived by\nspecifying fine-grained language-specific curricula that precisely replicate\nlanguage acquisition theories.\n","authors":["Suchir Salhan","Richard Diehl Martinez","Zébulon Goriely","Paula Buttery"],"pdf_url":"https://arxiv.org/pdf/2410.22886v2.pdf","comment":"BabyLM Shared Task 2024 (Accepted, Poster), co-located in EMNLP 2024"},{"id":"http://arxiv.org/abs/2407.06606v2","updated":"2025-02-21T10:52:07Z","published":"2024-07-09T07:15:56Z","title":"Tailored Design of Audio-Visual Speech Recognition Models using\n  Branchformers","summary":"  Recent advances in Audio-Visual Speech Recognition (AVSR) have led to\nunprecedented achievements in the field, improving the robustness of this type\nof system in adverse, noisy environments. In most cases, this task has been\naddressed through the design of models composed of two independent encoders,\neach dedicated to a specific modality. However, while recent works have\nexplored unified audio-visual encoders, determining the optimal cross-modal\narchitecture remains an ongoing challenge. Furthermore, such approaches often\nrely on models comprising vast amounts of parameters and high computational\ncost training processes. In this paper, we aim to bridge this research gap by\nintroducing a novel audio-visual framework. Our proposed method constitutes, to\nthe best of our knowledge, the first attempt to harness the flexibility and\ninterpretability offered by encoder architectures, such as the Branchformer, in\nthe design of parameter-efficient AVSR systems. To be more precise, the\nproposed framework consists of two steps: first, estimating audio- and\nvideo-only systems, and then designing a tailored audio-visual unified encoder\nbased on the layer-level branch scores provided by the modality-specific\nmodels. Extensive experiments on English and Spanish AVSR benchmarks covering\nmultiple data conditions and scenarios demonstrated the effectiveness of our\nproposed method. Even when trained on a moderate scale of data, our models\nachieve competitive word error rates (WER) of approximately 2.5\\% for English\nand surpass existing approaches for Spanish, establishing a new benchmark with\nan average WER of around 9.1\\%. These results reflect how our tailored AVSR\nsystem is able to reach state-of-the-art recognition rates while significantly\nreducing the model complexity w.r.t. the prevalent approach in the field. Code\nand pre-trained models are available at\nhttps://github.com/david-gimeno/tailored-avsr.\n","authors":["David Gimeno-Gómez","Carlos-D. Martínez-Hinarejos"],"pdf_url":"https://arxiv.org/pdf/2407.06606v2.pdf","comment":"Submitted and under review for the Computer Speech and Language\n  journal of Elsevier"},{"id":"http://arxiv.org/abs/2502.11187v2","updated":"2025-02-21T10:33:37Z","published":"2025-02-16T16:22:23Z","title":"TituLLMs: A Family of Bangla LLMs with Comprehensive Benchmarking","summary":"  In this paper, we present TituLLMs, the first large pretrained Bangla LLMs,\navailable in 1b and 3b parameter sizes. Due to computational constraints during\nboth training and inference, we focused on smaller models. To train TituLLMs,\nwe collected a pretraining dataset of approximately ~37 billion tokens. We\nextended the Llama-3.2 tokenizer to incorporate language- and culture-specific\nknowledge, which also enables faster training and inference. There was a lack\nof benchmarking datasets to benchmark LLMs for Bangla. To address this gap, we\ndeveloped five benchmarking datasets. We benchmarked various LLMs, including\nTituLLMs, and demonstrated that TituLLMs outperforms its initial multilingual\nversions. However, this is not always the case, highlighting the complexities\nof language adaptation. Our work lays the groundwork for adapting existing\nmultilingual open models to other low-resource languages. To facilitate broader\nadoption and further research, we have made the TituLLMs models and\nbenchmarking datasets publicly available\n(https://huggingface.co/collections/hishab/titulm-llama-family-6718d31fc1b83529276f490a).\n","authors":["Shahriar Kabir Nahin","Rabindra Nath Nandi","Sagor Sarker","Quazi Sarwar Muhtaseem","Md Kowsher","Apu Chandraw Shill","Md Ibrahim","Mehadi Hasan Menon","Tareq Al Muntasir","Firoj Alam"],"pdf_url":"https://arxiv.org/pdf/2502.11187v2.pdf","comment":"LLMs, Benchmarking, Large Language Models, Bangla"},{"id":"http://arxiv.org/abs/2502.15365v1","updated":"2025-02-21T10:27:28Z","published":"2025-02-21T10:27:28Z","title":"Identifying Features that Shape Perceived Consciousness in Large\n  Language Model-based AI: A Quantitative Study of Human Responses","summary":"  This study quantitively examines which features of AI-generated text lead\nhumans to perceive subjective consciousness in large language model (LLM)-based\nAI systems. Drawing on 99 passages from conversations with Claude 3 Opus and\nfocusing on eight features -- metacognitive self-reflection, logical reasoning,\nempathy, emotionality, knowledge, fluency, unexpectedness, and subjective\nexpressiveness -- we conducted a survey with 123 participants. Using regression\nand clustering analyses, we investigated how these features influence\nparticipants' perceptions of AI consciousness. The results reveal that\nmetacognitive self-reflection and the AI's expression of its own emotions\nsignificantly increased perceived consciousness, while a heavy emphasis on\nknowledge reduced it. Participants clustered into seven subgroups, each showing\ndistinct feature-weighting patterns. Additionally, higher prior knowledge of\nLLMs and more frequent usage of LLM-based chatbots were associated with greater\noverall likelihood assessments of AI consciousness. This study underscores the\nmultidimensional and individualized nature of perceived AI consciousness and\nprovides a foundation for better understanding the psychosocial implications of\nhuman-AI interaction.\n","authors":["Kang Bongsu","Kim Jundong","Yun Tae-Rim","Bae Hyojin","Kim Chang-Eop"],"pdf_url":"https://arxiv.org/pdf/2502.15365v1.pdf","comment":"11 pages, 3 figures, 4 tables"},{"id":"http://arxiv.org/abs/2502.14669v2","updated":"2025-02-21T10:27:10Z","published":"2025-02-20T16:05:18Z","title":"AlphaMaze: Enhancing Large Language Models' Spatial Intelligence via\n  GRPO","summary":"  Large Language Models (LLMs) have demonstrated impressive capabilities in\nlanguage processing, yet they often struggle with tasks requiring genuine\nvisual spatial reasoning. In this paper, we introduce a novel two-stage\ntraining framework designed to equip standard LLMs with visual reasoning\nabilities for maze navigation. First, we leverage Supervised Fine Tuning (SFT)\non a curated dataset of tokenized maze representations to teach the model to\npredict step-by-step movement commands. Next, we apply Group Relative Policy\nOptimization (GRPO)-a technique used in DeepSeekR1-with a carefully crafted\nreward function to refine the model's sequential decision-making and encourage\nemergent chain-of-thought behaviors. Experimental results on synthetically\ngenerated mazes show that while a baseline model fails to navigate the maze,\nthe SFT-trained model achieves 86% accuracy, and further GRPO fine-tuning\nboosts accuracy to 93%. Qualitative analyses reveal that GRPO fosters more\nrobust and self-corrective reasoning, highlighting the potential of our\napproach to bridge the gap between language models and visual spatial tasks.\nThese findings offer promising implications for applications in robotics,\nautonomous navigation, and other domains that require integrated visual and\nsequential reasoning.\n","authors":["Alan Dao","Dinh Bach Vu"],"pdf_url":"https://arxiv.org/pdf/2502.14669v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.15361v1","updated":"2025-02-21T10:16:07Z","published":"2025-02-21T10:16:07Z","title":"Evaluating Social Biases in LLM Reasoning","summary":"  In the recent development of AI reasoning, large language models (LLMs) are\ntrained to automatically generate chain-of-thought reasoning steps, which have\ndemonstrated compelling performance on math and coding tasks. However, when\nbias is mixed within the reasoning process to form strong logical arguments, it\ncould cause even more harmful results and further induce hallucinations. In\nthis paper, we have evaluated the 8B and 32B variants of DeepSeek-R1 against\ntheir instruction tuned counterparts on the BBQ dataset, and investigated the\nbias that is elicited out and being amplified through reasoning steps. To the\nbest of our knowledge, this empirical study is the first to assess bias issues\nin LLM reasoning.\n","authors":["Xuyang Wu","Jinming Nian","Zhiqiang Tao","Yi Fang"],"pdf_url":"https://arxiv.org/pdf/2502.15361v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.15359v1","updated":"2025-02-21T10:14:55Z","published":"2025-02-21T10:14:55Z","title":"ARS: Automatic Routing Solver with Large Language Models","summary":"  Real-world Vehicle Routing Problems (VRPs) are characterized by a variety of\npractical constraints, making manual solver design both knowledge-intensive and\ntime-consuming. Although there is increasing interest in automating the design\nof routing algorithms, existing research has explored only a limited array of\nVRP variants and fails to adequately address the complex and prevalent\nconstraints encountered in real-world situations. To fill this gap, this paper\nintroduces RoutBench, a benchmark of 1,000 VRP variants derived from 24\nattributes, for evaluating the effectiveness of automatic routing solvers in\naddressing complex constraints. Along with RoutBench, we present the Automatic\nRouting Solver (ARS), which employs Large Language Model (LLM) agents to\nenhance a backbone algorithm framework by automatically generating\nconstraint-aware heuristic code, based on problem descriptions and several\nrepresentative constraints selected from a database. Our experiments show that\nARS outperforms state-of-the-art LLM-based methods and commonly used solvers,\nautomatically solving 91.67% of common VRPs and achieving at least a 30%\nimprovement across all benchmarks.\n","authors":["Kai Li","Fei Liu","Zhenkun Wang","Xialiang Tong","Xiongwei Han","Mingxuan Yuan"],"pdf_url":"https://arxiv.org/pdf/2502.15359v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.15349v1","updated":"2025-02-21T10:06:41Z","published":"2025-02-21T10:06:41Z","title":"AttentionEngine: A Versatile Framework for Efficient Attention\n  Mechanisms on Diverse Hardware Platforms","summary":"  Transformers and large language models (LLMs) have revolutionized machine\nlearning, with attention mechanisms at the core of their success. As the\nlandscape of attention variants expands, so too do the challenges of optimizing\ntheir performance, particularly across different hardware platforms. Current\noptimization strategies are often narrowly focused, requiring extensive manual\nintervention to accommodate changes in model configurations or hardware\nenvironments. In this paper, we introduce AttentionEngine, a comprehensive\nframework designed to streamline the optimization of attention mechanisms\nacross heterogeneous hardware backends. By decomposing attention computation\ninto modular operations with customizable components, AttentionEngine enables\nflexible adaptation to diverse algorithmic requirements. The framework further\nautomates kernel optimization through a combination of programmable templates\nand a robust cross-platform scheduling strategy. Empirical results reveal\nperformance gains of up to 10x on configurations beyond the reach of existing\nmethods. AttentionEngine offers a scalable, efficient foundation for developing\nand deploying attention mechanisms with minimal manual tuning. Our code has\nbeen open-sourced and is available at\nhttps://github.com/microsoft/AttentionEngine.\n","authors":["Feiyang Chen","Yu Cheng","Lei Wang","Yuqing Xia","Ziming Miao","Lingxiao Ma","Fan Yang","Jilong Xue","Zhi Yang","Mao Yang","Haibo Chen"],"pdf_url":"https://arxiv.org/pdf/2502.15349v1.pdf","comment":"15 pages"},{"id":"http://arxiv.org/abs/2502.15348v1","updated":"2025-02-21T10:02:15Z","published":"2025-02-21T10:02:15Z","title":"Constructing a Norm for Children's Scientific Drawing: Distribution\n  Features Based on Semantic Similarity of Large Language Models","summary":"  The use of children's drawings to examining their conceptual understanding\nhas been proven to be an effective method, but there are two major problems\nwith previous research: 1. The content of the drawings heavily relies on the\ntask, and the ecological validity of the conclusions is low; 2. The\ninterpretation of drawings relies too much on the subjective feelings of the\nresearchers. To address this issue, this study uses the Large Language Model\n(LLM) to identify 1420 children's scientific drawings (covering 9 scientific\nthemes/concepts), and uses the word2vec algorithm to calculate their semantic\nsimilarity. The study explores whether there are consistent drawing\nrepresentations for children on the same theme, and attempts to establish a\nnorm for children's scientific drawings, providing a baseline reference for\nfollow-up children's drawing research. The results show that the representation\nof most drawings has consistency, manifested as most semantic similarity\ngreater than 0.8. At the same time, it was found that the consistency of the\nrepresentation is independent of the accuracy (of LLM's recognition),\nindicating the existence of consistency bias. In the subsequent exploration of\ninfluencing factors, we used Kendall rank correlation coefficient to\ninvestigate the effects of Sample Size, Abstract Degree, and Focus Points on\ndrawings, and used word frequency statistics to explore whether children\nrepresented abstract themes/concepts by reproducing what was taught in class.\n","authors":["Yi Zhang","Fan Wei","Jingyi Li","Yan Wang","Yanyan Yu","Jianli Chen","Zipo Cai","Xinyu Liu","Wei Wang","Peng Wang","Zhong Wang"],"pdf_url":"https://arxiv.org/pdf/2502.15348v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.15343v1","updated":"2025-02-21T09:58:54Z","published":"2025-02-21T09:58:54Z","title":"Tokenization is Sensitive to Language Variation","summary":"  Variation in language is ubiquitous and often systematically linked to\nregional, social, and contextual factors. Tokenizers split texts into smaller\nunits and might behave differently for less common linguistic forms. This might\naffect downstream LLM performance differently on two types of tasks: Tasks\nwhere the model should be robust to language variation (e.g., for semantic\ntasks like NLI, labels do not depend on whether a text uses British or American\nspelling) and tasks where the model should be sensitive to language variation\n(e.g., for form-based tasks like authorship verification, labels depend on\nwhether a text uses British or American spelling). We pre-train BERT base\nmodels for the popular Byte-Pair Encoding algorithm to investigate how key\nalgorithmic design choices impact downstream models' performances: fitting\ncorpus, pre-tokenizer and vocabulary size. We find that the best tokenizer\nvaries on the two task types -- with the pre-tokenizer having the biggest\nimpact on performance. Further, we introduce a new approach to estimate\ntokenizer impact on downstream LLM performance, showing significant improvement\nover techniques like R\\'enyi efficiency. We encourage more work on language\nvariation and its relation to tokenizers and thus LLM performance.\n","authors":["Anna Wegmann","Dong Nguyen","David Jurgens"],"pdf_url":"https://arxiv.org/pdf/2502.15343v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.11701v2","updated":"2025-02-21T09:48:58Z","published":"2024-10-15T15:39:37Z","title":"Magnifier Prompt: Tackling Multimodal Hallucination via Extremely Simple\n  Instructions","summary":"  Hallucinations in multimodal large language models (MLLMs) hinder their\npractical applications. To address this, we propose a Magnifier Prompt\n(MagPrompt), a simple yet effective method to tackle hallucinations in MLLMs\nvia extremely simple instructions. MagPrompt is based on the following two key\nprinciples, which guide the design of various effective prompts, demonstrating\nrobustness: (1) MLLMs should focus more on the image. (2) When there are\nconflicts between the image and the model's inner knowledge, MLLMs should\nprioritize the image. MagPrompt is training-free and can be applied to\nopen-source and closed-source models, such as GPT-4o and Gemini-pro. It\nperforms well across many datasets and its effectiveness is comparable or even\nbetter than more complex methods like VCD. Furthermore, our prompt design\nprinciples and experimental analyses provide valuable insights into multimodal\nhallucination.\n","authors":["Yuhan Fu","Ruobing Xie","Jiazhen Liu","Bangxiang Lan","Xingwu Sun","Zhanhui Kang","Xirong Li"],"pdf_url":"https://arxiv.org/pdf/2410.11701v2.pdf","comment":"The proposed method does not work for up-to-date MLLMs."},{"id":"http://arxiv.org/abs/2502.15335v1","updated":"2025-02-21T09:39:27Z","published":"2025-02-21T09:39:27Z","title":"Stepwise Informativeness Search for Improving LLM Reasoning","summary":"  Advances in Large Language Models (LLMs) have significantly improved\nmulti-step reasoning through generating free-text rationales. However, recent\nstudies show that LLMs tend to lose focus over the middle of long contexts.\nThis raises concerns that as reasoning progresses, LLMs may overlook\ninformation in earlier steps when decoding subsequent steps, leading to\ngenerate unreliable and redundant rationales. To address this, we propose\nguiding LLMs to generate more accurate and concise step-by-step rationales by\n(1) proactively referencing information from underutilized prior steps, and (2)\nminimizing redundant information between new and existing steps. We introduce\nstepwise informativeness search, an inference-time tree search framework\nincorporating two selection heuristics: grounding-guided selection which\nprioritizes steps paying higher attention over underutilized steps; and\nnovelty-guided selection which encourages steps with novel conclusions. During\nrationale generation, we use a self-grounding strategy that prompts LLMs to\nexplicitly reference relevant prior steps to provide premises before deduction\nat each step. Experimental results on four reasoning datasets demonstrate that\nour approach improves reasoning accuracy by generating higher-quality\nrationales with reduced errors and redundancy.\n","authors":["Siyuan Wang","Enda Zhao","Zhongyu Wei","Xiang Ren"],"pdf_url":"https://arxiv.org/pdf/2502.15335v1.pdf","comment":"Preprint"},{"id":"http://arxiv.org/abs/2502.15332v1","updated":"2025-02-21T09:34:34Z","published":"2025-02-21T09:34:34Z","title":"Detecting Future-related Contexts of Entity Mentions","summary":"  The ability to automatically identify whether an entity is referenced in a\nfuture context can have multiple applications including decision making,\nplanning and trend forecasting. This paper focuses on detecting implicit future\nreferences in entity-centric texts, addressing the growing need for automated\ntemporal analysis in information processing. We first present a novel dataset\nof 19,540 sentences built around popular entities sourced from Wikipedia, which\nconsists of future-related and non-future-related contexts in which those\nentities appear. As a second contribution, we evaluate the performance of\nseveral Language Models including also Large Language Models (LLMs) on the task\nof distinguishing future-oriented content in the absence of explicit temporal\nreferences.\n","authors":["Puneet Prashar","Krishna Mohan Shukla","Adam Jatowt"],"pdf_url":"https://arxiv.org/pdf/2502.15332v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17359v2","updated":"2025-02-21T09:32:54Z","published":"2024-03-26T03:51:01Z","title":"Chain-of-Action: Faithful and Multimodal Question Answering through\n  Large Language Models","summary":"  We present a Chain-of-Action (CoA) framework for multimodal and\nretrieval-augmented Question-Answering (QA). Compared to the literature, CoA\novercomes two major challenges of current QA applications: (i) unfaithful\nhallucination that is inconsistent with real-time or domain facts and (ii) weak\nreasoning performance over compositional information. Our key contribution is a\nnovel reasoning-retrieval mechanism that decomposes a complex question into a\nreasoning chain via systematic prompting and pre-designed actions.\nMethodologically, we propose three types of domain-adaptable `Plug-and-Play'\nactions for retrieving real-time information from heterogeneous sources. We\nalso propose a multi-reference faith score (MRFS) to verify and resolve\nconflicts in the answers. Empirically, we exploit both public benchmarks and a\nWeb3 case study to demonstrate the capability of CoA over other methods.\n","authors":["Zhenyu Pan","Haozheng Luo","Manling Li","Han Liu"],"pdf_url":"https://arxiv.org/pdf/2403.17359v2.pdf","comment":"International Conference on Learning Representations (ICLR) 2025"},{"id":"http://arxiv.org/abs/2502.15304v1","updated":"2025-02-21T08:55:21Z","published":"2025-02-21T08:55:21Z","title":"SVDq: 1.25-bit and 410x Key Cache Compression for LLM Attention","summary":"  For the efficient inference of Large Language Models (LLMs), the effective\ncompression of key-value (KV) cache is essential. Three main types of KV cache\ncompression techniques, namely sparsity, channel compression, and quantization,\nhave been identified. This study presents SVDq, a Singular Value Decomposition\n(SVD) - based mixed precision quantization method for K cache. Initially, K\ncache is transformed into latent channels using SVD basis representations.\nSince the values in latent channels decay rapidly and become negligible after\nonly a few latent channels, our method then incorporates importance-aware\nquantization and compression for latent channels. This enables the effective\nallocation of higher precision to more significant channels. Theoretically, we\nprove that SVDq results in quantization errors (x0.1 or even lower) that are\nmuch lower than those of per-channel key quantization in the original space.\nOur findings based on RULER and LongBench benchmarks demonstrate that SVDq can\nachieve an equivalent key cache precision as low as 1.25-bit. When combined\nwith key sparsity, it can reach a key compression ratio of up to 410x for\nattention computation, all while maintaining comparable model performance.\nNotably, our method is nearly lossless for LongBench datasets. This indicates\nthat SVDq enables high-precision low-bit quantization, providing a more\nefficient solution for KV cache compression in LLMs.\n","authors":["Hong Yankun","Li Xing","Zhen Hui-Ling","Yu Xianzhi","Liu Wulong","Yuan Mingxuan"],"pdf_url":"https://arxiv.org/pdf/2502.15304v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.08920v2","updated":"2025-02-21T08:20:29Z","published":"2024-12-12T04:06:54Z","title":"From Text to Trajectory: Exploring Complex Constraint Representation and\n  Decomposition in Safe Reinforcement Learning","summary":"  Safe reinforcement learning (RL) requires the agent to finish a given task\nwhile obeying specific constraints. Giving constraints in natural language form\nhas great potential for practical scenarios due to its flexible transfer\ncapability and accessibility. Previous safe RL methods with natural language\nconstraints typically need to design cost functions manually for each\nconstraint, which requires domain expertise and lacks flexibility. In this\npaper, we harness the dual role of text in this task, using it not only to\nprovide constraint but also as a training signal. We introduce the\nTrajectory-level Textual Constraints Translator (TTCT) to replace the manually\ndesigned cost function. Our empirical results demonstrate that TTCT effectively\ncomprehends textual constraint and trajectory, and the policies trained by TTCT\ncan achieve a lower violation rate than the standard cost function. Extra\nstudies are conducted to demonstrate that the TTCT has zero-shot transfer\ncapability to adapt to constraint-shift environments.\n","authors":["Pusen Dong","Tianchen Zhu","Yue Qiu","Haoyi Zhou","Jianxin Li"],"pdf_url":"https://arxiv.org/pdf/2412.08920v2.pdf","comment":"Accepted by NeurIPS 2024"},{"id":"http://arxiv.org/abs/2502.15277v1","updated":"2025-02-21T08:07:53Z","published":"2025-02-21T08:07:53Z","title":"Analyzing the Inner Workings of Transformers in Compositional\n  Generalization","summary":"  The compositional generalization abilities of neural models have been sought\nafter for human-like linguistic competence. The popular method to evaluate such\nabilities is to assess the models' input-output behavior. However, that does\nnot reveal the internal mechanisms, and the underlying competence of such\nmodels in compositional generalization remains unclear. To address this\nproblem, we explore the inner workings of a Transformer model by finding an\nexisting subnetwork that contributes to the generalization performance and by\nperforming causal analyses on how the model utilizes syntactic features. We\nfind that the model depends on syntactic features to output the correct answer,\nbut that the subnetwork with much better generalization performance than the\nwhole model relies on a non-compositional algorithm in addition to the\nsyntactic features. We also show that the subnetwork improves its\ngeneralization performance relatively slowly during the training compared to\nthe in-distribution one, and the non-compositional solution is acquired in the\nearly stages of the training.\n","authors":["Ryoma Kumon","Hitomi Yanaka"],"pdf_url":"https://arxiv.org/pdf/2502.15277v1.pdf","comment":"Accepted to NAACL 2025 main"},{"id":"http://arxiv.org/abs/2411.00750v2","updated":"2025-02-21T08:00:10Z","published":"2024-11-01T17:18:45Z","title":"Mitigating Tail Narrowing in LLM Self-Improvement via Socratic-Guided\n  Sampling","summary":"  Self-improvement methods enable large language models (LLMs) to generate\nsolutions themselves and iteratively train on filtered, high-quality\nrationales. This process proves effective and reduces the reliance on human\nsupervision in LLMs' reasoning, but the performance soon plateaus. We delve\ninto the process and find that models tend to over-sample on easy queries and\nunder-sample on queries they have yet to master. As iterations proceed, this\nimbalance in sampling is exacerbated, leading to a long-tail distribution where\nsolutions to difficult queries almost diminish. This phenomenon limits the\nperformance gain of self-improving models. A straightforward solution is\nbrute-force sampling to balance the distribution, which significantly raises\ncomputational costs. In this paper, we introduce Guided Self-Improvement (GSI),\na strategy aimed at improving the efficiency of sampling challenging\nheavy-tailed data. It leverages Socratic-style guidance signals to help LLM\nreasoning with complex queries, reducing the exploration effort and minimizing\ncomputational overhead. Experiments on four models across diverse mathematical\ntasks show that GSI strikes a balance between performance and efficiency, while\nalso being effective on held-out tasks.\n","authors":["Yiwen Ding","Zhiheng Xi","Wei He","Zhuoyuan Li","Yitao Zhai","Xiaowei Shi","Xunliang Cai","Tao Gui","Qi Zhang","Xuanjing Huang"],"pdf_url":"https://arxiv.org/pdf/2411.00750v2.pdf","comment":"Accepted to NAACL 2025 Main Conference. Codes are publicly available\n  at https://github.com/Yiwen-Ding/Guided-Self-Improvement"},{"id":"http://arxiv.org/abs/2501.03936v3","updated":"2025-02-21T07:52:39Z","published":"2025-01-07T16:53:01Z","title":"PPTAgent: Generating and Evaluating Presentations Beyond Text-to-Slides","summary":"  Automatically generating presentations from documents is a challenging task\nthat requires accommodating content quality, visual appeal, and structural\ncoherence. Existing methods primarily focus on improving and evaluating the\ncontent quality in isolation, overlooking visual appeal and structural\ncoherence, which limits their practical applicability. To address these\nlimitations, we propose PPTAgent, which comprehensively improves presentation\ngeneration through a two-stage, edit-based approach inspired by human\nworkflows. PPTAgent first analyzes reference presentations to extract\nslide-level functional types and content schemas, then drafts an outline and\niteratively generates editing actions based on selected reference slides to\ncreate new slides. To comprehensively evaluate the quality of generated\npresentations, we further introduce PPTEval, an evaluation framework that\nassesses presentations across three dimensions: Content, Design, and Coherence.\nResults demonstrate that PPTAgent significantly outperforms existing automatic\npresentation generation methods across all three dimensions.\n","authors":["Hao Zheng","Xinyan Guan","Hao Kong","Jia Zheng","Weixiang Zhou","Hongyu Lin","Yaojie Lu","Ben He","Xianpei Han","Le Sun"],"pdf_url":"https://arxiv.org/pdf/2501.03936v3.pdf","comment":"8 pages, 23 figures, see https://github.com/icip-cas/PPTAgent for\n  details"},{"id":"http://arxiv.org/abs/2502.15266v1","updated":"2025-02-21T07:48:54Z","published":"2025-02-21T07:48:54Z","title":"A Training-free LLM-based Approach to General Chinese Character Error\n  Correction","summary":"  Chinese spelling correction (CSC) is a crucial task that aims to correct\ncharacter errors in Chinese text. While conventional CSC focuses on character\nsubstitution errors caused by mistyping, two other common types of character\nerrors, missing and redundant characters, have received less attention. These\nerrors are often excluded from CSC datasets during the annotation process or\nignored during evaluation, even when they have been annotated. This issue\nlimits the practicality of the CSC task. To address this issue, we introduce\nthe task of General Chinese Character Error Correction (C2EC), which focuses on\nall three types of character errors. We construct a high-quality C2EC benchmark\nby combining and manually verifying data from CCTC and Lemon datasets. We\nextend the training-free prompt-free CSC method to C2EC by using Levenshtein\ndistance for handling length changes and leveraging an additional prompt-based\nlarge language model (LLM) to improve performance. Experiments show that our\nmethod enables a 14B-parameter LLM to be on par with models nearly 50 times\nlarger on both conventional CSC and C2EC tasks, without any fine-tuning.\n","authors":["Houquan Zhou","Bo Zhang","Zhenghua Li","Ming Yan","Min Zhang"],"pdf_url":"https://arxiv.org/pdf/2502.15266v1.pdf","comment":"25 pages, 12 figures"},{"id":"http://arxiv.org/abs/2502.15264v1","updated":"2025-02-21T07:47:50Z","published":"2025-02-21T07:47:50Z","title":"Retrieval-Augmented Speech Recognition Approach for Domain Challenges","summary":"  Speech recognition systems often face challenges due to domain mismatch,\nparticularly in real-world applications where domain-specific data is\nunavailable because of data accessibility and confidentiality constraints.\nInspired by Retrieval-Augmented Generation (RAG) techniques for large language\nmodels (LLMs), this paper introduces a LLM-based retrieval-augmented speech\nrecognition method that incorporates domain-specific textual data at the\ninference stage to enhance recognition performance. Rather than relying on\ndomain-specific textual data during the training phase, our model is trained to\nlearn how to utilize textual information provided in prompts for LLM decoder to\nimprove speech recognition performance. Benefiting from the advantages of the\nRAG retrieval mechanism, our approach efficiently accesses locally available\ndomain-specific documents, ensuring a convenient and effective process for\nsolving domain mismatch problems. Experiments conducted on the CSJ database\ndemonstrate that the proposed method significantly improves speech recognition\naccuracy and achieves state-of-the-art results on the CSJ dataset, even without\nrelying on the full training data.\n","authors":["Peng Shen","Xugang Lu","Hisashi Kawai"],"pdf_url":"https://arxiv.org/pdf/2502.15264v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.15261v1","updated":"2025-02-21T07:42:33Z","published":"2025-02-21T07:42:33Z","title":"Corrections Meet Explanations: A Unified Framework for Explainable\n  Grammatical Error Correction","summary":"  Grammatical Error Correction (GEC) faces a critical challenge concerning\nexplainability, notably when GEC systems are designed for language learners.\nExisting research predominantly focuses on explaining grammatical errors\nextracted in advance, thus neglecting the relationship between explanations and\ncorrections. To address this gap, we introduce EXGEC, a unified explainable GEC\nframework that integrates explanation and correction tasks in a generative\nmanner, advocating that these tasks mutually reinforce each other. Experiments\nhave been conducted on EXPECT, a recent human-labeled dataset for explainable\nGEC, comprising around 20k samples. Moreover, we detect significant noise\nwithin EXPECT, potentially compromising model training and evaluation.\nTherefore, we introduce an alternative dataset named EXPECT-denoised, ensuring\na more objective framework for training and evaluation. Results on various NLP\nmodels (BART, T5, and Llama3) show that EXGEC models surpass single-task\nbaselines in both tasks, demonstrating the effectiveness of our approach.\n","authors":["Jingheng Ye","Shang Qin","Yinghui Li","Hai-Tao Zheng","Shen Wang","Qingsong Wen"],"pdf_url":"https://arxiv.org/pdf/2502.15261v1.pdf","comment":"19 pages, 2 figures, and 9 tables"},{"id":"http://arxiv.org/abs/2502.13544v2","updated":"2025-02-21T07:23:35Z","published":"2025-02-19T08:52:45Z","title":"From Sub-Ability Diagnosis to Human-Aligned Generation: Bridging the Gap\n  for Text Length Control via MARKERGEN","summary":"  Despite the rapid progress of large language models (LLMs), their\nlength-controllable text generation (LCTG) ability remains below expectations,\nposing a major limitation for practical applications. Existing methods mainly\nfocus on end-to-end training to reinforce adherence to length constraints.\nHowever, the lack of decomposition and targeted enhancement of LCTG\nsub-abilities restricts further progress. To bridge this gap, we conduct a\nbottom-up decomposition of LCTG sub-abilities with human patterns as reference\nand perform a detailed error analysis. On this basis, we propose MarkerGen, a\nsimple-yet-effective plug-and-play approach that:(1) mitigates LLM fundamental\ndeficiencies via external tool integration;(2) conducts explicit length\nmodeling with dynamically inserted markers;(3) employs a three-stage generation\nscheme to better align length constraints while maintaining content quality.\nComprehensive experiments demonstrate that MarkerGen significantly improves\nLCTG across various settings, exhibiting outstanding effectiveness and\ngeneralizability.\n","authors":["Peiwen Yuan","Chuyi Tan","Shaoxiong Feng","Yiwei Li","Xinglin Wang","Yueqi Zhang","Jiayi Shi","Boyuan Pan","Yao Hu","Kan Li"],"pdf_url":"https://arxiv.org/pdf/2502.13544v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.15260v1","updated":"2025-02-21T07:23:23Z","published":"2025-02-21T07:23:23Z","title":"LightMamba: Efficient Mamba Acceleration on FPGA with Quantization and\n  Hardware Co-design","summary":"  State space models (SSMs) like Mamba have recently attracted much attention.\nCompared to Transformer-based large language models (LLMs), Mamba achieves\nlinear computation complexity with the sequence length and demonstrates\nsuperior performance. However, Mamba is hard to accelerate due to the scattered\nactivation outliers and the complex computation dependency, rendering existing\nLLM accelerators inefficient. In this paper, we propose LightMamba that\nco-designs the quantization algorithm and FPGA accelerator architecture for\nefficient Mamba inference. We first propose an FPGA-friendly post-training\nquantization algorithm that features rotation-assisted quantization and\npower-of-two SSM quantization to reduce the majority of computation to 4-bit.\nWe further design an FPGA accelerator that partially unrolls the Mamba\ncomputation to balance the efficiency and hardware costs. Through computation\nreordering as well as fine-grained tiling and fusion, the hardware utilization\nand memory efficiency of the accelerator get drastically improved. We implement\nLightMamba on Xilinx Versal VCK190 FPGA and achieve 4.65x to 6.06x higher\nenergy efficiency over the GPU baseline. When evaluated on Alveo U280 FPGA,\nLightMamba reaches 93 tokens/s, which is 1.43x that of the GPU baseline.\n","authors":["Renjie Wei","Songqiang Xu","Linfeng Zhong","Zebin Yang","Qingyu Guo","Yuan Wang","Runsheng Wang","Meng Li"],"pdf_url":"https://arxiv.org/pdf/2502.15260v1.pdf","comment":"Accepted by DATE 2025"},{"id":"http://arxiv.org/abs/2502.14671v2","updated":"2025-02-21T07:09:09Z","published":"2025-02-20T16:05:45Z","title":"Explanations of Deep Language Models Explain Language Representations in\n  the Brain","summary":"  Recent advances in artificial intelligence have given rise to large language\nmodels (LLMs) that not only achieve human-like performance but also share\ncomputational principles with the brain's language processing mechanisms. While\nprevious research has primarily focused on aligning LLMs' internal\nrepresentations with neural activity, we introduce a novel approach that\nleverages explainable AI (XAI) methods to forge deeper connections between the\ntwo domains. Using attribution methods, we quantified how preceding words\ncontribute to an LLM's next-word predictions and employed these explanations to\npredict fMRI recordings from participants listening to the same narratives. Our\nfindings demonstrate that attribution methods robustly predict brain activity\nacross the language network, surpassing traditional internal representations in\nearly language areas. This alignment is hierarchical: early-layer explanations\ncorrespond to the initial stages of language processing in the brain, while\nlater layers align with more advanced stages. Moreover, the layers more\ninfluential on LLM next-word prediction$\\unicode{x2014}$those with higher\nattribution scores$\\unicode{x2014}$exhibited stronger alignment with neural\nactivity. This work establishes a bidirectional bridge between AI and\nneuroscience. First, we demonstrate that attribution methods offer a powerful\nlens for investigating the neural mechanisms of language comprehension,\nrevealing how meaning emerges from preceding context. Second, we propose using\nbrain alignment as a metric to evaluate the validity of attribution methods,\nproviding a framework for assessing their biological plausibility.\n","authors":["Maryam Rahimi","Yadollah Yaghoobzadeh","Mohammad Reza Daliri"],"pdf_url":"https://arxiv.org/pdf/2502.14671v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.01830v3","updated":"2025-02-21T06:33:26Z","published":"2024-02-02T18:49:26Z","title":"PiCO: Peer Review in LLMs based on the Consistency Optimization","summary":"  Existing large language models (LLMs) evaluation methods typically focus on\ntesting the performance on some closed-environment and domain-specific\nbenchmarks with human annotations. In this paper, we explore a novel\nunsupervised evaluation direction, utilizing peer-review mechanisms to measure\nLLMs automatically. In this setting, both open-source and closed-source LLMs\nlie in the same environment, capable of answering unlabeled questions and\nevaluating each other, where each LLM's response score is jointly determined by\nother anonymous ones. To obtain the ability hierarchy among these models, we\nassign each LLM a learnable capability parameter to adjust the final ranking.\nWe formalize it as a constrained optimization problem, intending to maximize\nthe consistency of each LLM's capabilities and scores. The key assumption\nbehind is that high-level LLM can evaluate others' answers more accurately than\nlow-level ones, while higher-level LLM can also achieve higher response scores.\nMoreover, we propose three metrics called PEN, CIN, and LIS to evaluate the gap\nin aligning human rankings. We perform experiments on multiple datasets with\nthese metrics, validating the effectiveness of the proposed approach.\n","authors":["Kun-Peng Ning","Shuo Yang","Yu-Yang Liu","Jia-Yu Yao","Zhen-Hui Liu","Yong-Hong Tian","Yibing Song","Li Yuan"],"pdf_url":"https://arxiv.org/pdf/2402.01830v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.01504v2","updated":"2025-02-21T06:33:16Z","published":"2024-10-02T12:57:12Z","title":"PersonaMath: Boosting Mathematical Reasoning via Persona-Driven Data\n  Augmentation","summary":"  While closed-source Large Language Models (LLMs) demonstrate strong\nmathematical problem-solving abilities, open-source models still face\nchallenges with such tasks. To bridge this gap, we propose a data augmentation\napproach and introduce PersonaMathQA, a dataset derived from MATH and GSM8K, on\nwhich we train the PersonaMath models. Our approach consists of two stages: the\nfirst stage focuses on learning from Persona Diversification, and the second\nstage emphasizes learning from Reflection. In the first stage, we regenerate\ndetailed chain-of-thought (CoT) solutions as instructions using a closed-source\nLLM and introduce a persona-driven data augmentation technique. This technique\ninnovatively classifies personas based on occupations, significantly enhancing\nthe dataset's diversity and quality. In the second stage, we incorporate\nreflection to fully leverage more challenging and valuable questions.\nEvaluation of our PersonaMath models on MATH and GSM8K reveals that the\nPersonaMath-7B model (based on Qwen2.5-7B) achieves an accuracy of 61.2% on\nMATH and 87.8% on GSM8K, surpassing all baseline methods and achieving\nstate-of-the-art performance. Notably, our dataset contains only 128.9K data\npoints-merely 32.6% of MetaMathQA and 49.5% of MathInstruct-yet our model\noutperforms these baselines, demonstrating the high quality and diversity of\nour dataset, which enables more efficient model training. We open-source the\nPersonaMathQA dataset, PersonaMath models, and our code for public usage.\n","authors":["Jing Luo","Longze Chen","Run Luo","Liang Zhu","Chang Ao","Jiaming Li","Yukun Chen","Xin Cheng","Wen Yang","Jiayuan Su","Ahmadreza Argha","Hamid Alinejad-Rokny","Chengming Li","Shiwen Ni","Min Yang"],"pdf_url":"https://arxiv.org/pdf/2410.01504v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2306.03268v3","updated":"2025-02-21T06:19:39Z","published":"2023-06-05T21:38:30Z","title":"Skill over Scale: The Case for Medium, Domain-Specific Models for SE","summary":"  Recent advancements in AI have sparked a trend in constructing large,\ngeneralist language models that handle a multitude of tasks, including many\ncode-related ones. While these models are expensive to train and are often\nclosed-source, they have enjoyed broad adoption because they tend to outperform\nsmaller, domain-specific models of code. In this work, we argue that this is\nnot a foregone conclusion. We show that modestly sized domain-specific models\ncan outperform much larger ones on code labeling tasks, provided they are\ntrained to the same standards. Concretely, we focus on StackOverflow (SO),\nwhich offers large volumes of aligned code and text data. We align established\nbest-practices for pre-training large language models with properties of SO as\na data source, especially using a large context window (2,048 tokens), coupled\nwith a powerful toolkit (Megatron-LM) to train two models: SOBertBase (125M\nparameters) and SOBertLarge (762M parameters), at a budget of just $374 and\n$1600 each. We compare the performance of our models with a prior\ndomain-specific model which did not adopt many of these practices\n(BERTOverflow), as well two general-purpose BERT models and two models in\nOpenAI's GPT series (GPT-3.5 and GPT-4). We study four labeling tasks: question\nquality prediction, closed question prediction, NER and obsoletion prediction.\nThe final task is a new benchmark we introduce, on which we additionally\ncompare SOBert with a fine-tuned CodeLlama and StackLlama (models with 10x more\nparameters than SOBertLarge). Our models consistently outperform all baselines.\nIn contrast, BertOverflow is outperformed by generalist models in most tasks.\nThese results demonstrate that pre-training both extensively and properly on\nin-domain data can yield a powerful and affordable alternative to leveraging\nclosed-source general-purpose models. Both models are released to the public on\nHugging Face.\n","authors":["Manisha Mukherjee","Vincent J. Hellendoorn"],"pdf_url":"https://arxiv.org/pdf/2306.03268v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.15233v1","updated":"2025-02-21T06:15:53Z","published":"2025-02-21T06:15:53Z","title":"A General Pseudonymization Framework for Cloud-Based LLMs: Replacing\n  Privacy Information in Controlled Text Generation","summary":"  An increasing number of companies have begun providing services that leverage\ncloud-based large language models (LLMs), such as ChatGPT. However, this\ndevelopment raises substantial privacy concerns, as users' prompts are\ntransmitted to and processed by the model providers. Among the various privacy\nprotection methods for LLMs, those implemented during the pre-training and\nfine-tuning phrases fail to mitigate the privacy risks associated with the\nremote use of cloud-based LLMs by users. On the other hand, methods applied\nduring the inference phrase are primarily effective in scenarios where the\nLLM's inference does not rely on privacy-sensitive information. In this paper,\nwe outline the process of remote user interaction with LLMs and, for the first\ntime, propose a detailed definition of a general pseudonymization framework\napplicable to cloud-based LLMs. The experimental results demonstrate that the\nproposed framework strikes an optimal balance between privacy protection and\nutility. The code for our method is available to the public at\nhttps://github.com/Mebymeby/Pseudonymization-Framework.\n","authors":["Shilong Hou","Ruilin Shang","Zi Long","Xianghua Fu","Yin Chen"],"pdf_url":"https://arxiv.org/pdf/2502.15233v1.pdf","comment":"under review"},{"id":"http://arxiv.org/abs/2502.11681v3","updated":"2025-02-21T06:14:33Z","published":"2025-02-17T11:16:19Z","title":"RIDE: Enhancing Large Language Model Alignment through Restyled\n  In-Context Learning Demonstration Exemplars","summary":"  Alignment tuning is crucial for ensuring large language models (LLMs) behave\nethically and helpfully. Current alignment approaches require high-quality\nannotations and significant training resources. This paper proposes a low-cost,\ntuning-free method using in-context learning (ICL) to enhance LLM alignment.\nThrough an analysis of high-quality ICL demos, we identified style as a key\nfactor influencing LLM alignment capabilities and explicitly restyled ICL\nexemplars based on this stylistic framework. Additionally, we combined the\nrestyled demos to achieve a balance between the two conflicting aspects of LLM\nalignment--factuality and safety. We packaged the restyled examples as prompts\nto trigger few-shot learning, improving LLM alignment. Compared to the best\nbaseline approach, with an average score of 5.00 as the maximum, our method\nachieves a maximum 0.10 increase on the Alpaca task (from 4.50 to 4.60), a 0.22\nenhancement on the Just-eval benchmark (from 4.34 to 4.56), and a maximum\nimprovement of 0.32 (from 3.53 to 3.85) on the MT-Bench dataset. We release the\ncode and data at https://github.com/AnonymousCode-ComputerScience/RIDE.\n","authors":["Yuncheng Hua","Lizhen Qu","Zhuang Li","Hao Xue","Flora D. Salim","Gholamreza Haffari"],"pdf_url":"https://arxiv.org/pdf/2502.11681v3.pdf","comment":"38 pages, 2 figures, 20 tables; The paper is under review in ARR"},{"id":"http://arxiv.org/abs/2502.11517v2","updated":"2025-02-21T06:14:19Z","published":"2025-02-17T07:39:16Z","title":"Learning to Keep a Promise: Scaling Language Model Decoding Parallelism\n  with Learned Asynchronous Decoding","summary":"  Decoding with autoregressive large language models (LLMs) traditionally\noccurs sequentially, generating one token after another. An emerging line of\nwork explored parallel decoding by identifying and simultaneously generating\nsemantically independent chunks of LLM responses. However, these techniques\nrely on hand-crafted heuristics tied to syntactic structures like lists and\nparagraphs, making them rigid and imprecise. We present PASTA, a learning-based\nsystem that teaches LLMs to identify semantic independence and express parallel\ndecoding opportunities in their own responses. At its core are PASTA-LANG and\nits interpreter: PASTA-LANG is an annotation language that enables LLMs to\nexpress semantic independence in their own responses; the language interpreter\nacts on these annotations to orchestrate parallel decoding on-the-fly at\ninference time. Through a two-stage finetuning process, we train LLMs to\ngenerate PASTA-LANG annotations that optimize both response quality and\ndecoding speed. Evaluation on AlpacaEval, an instruction following benchmark,\nshows that our approach Pareto-dominates existing methods in terms of decoding\nspeed and response quality; our results demonstrate geometric mean speedups\nranging from 1.21x to 1.93x with corresponding quality changes of +2.2% to\n-7.1%, measured by length-controlled win rates against sequential decoding\nbaseline.\n","authors":["Tian Jin","Ellie Y. Cheng","Zack Ankner","Nikunj Saunshi","Blake M. Elias","Amir Yazdanbakhsh","Jonathan Ragan-Kelley","Suvinay Subramanian","Michael Carbin"],"pdf_url":"https://arxiv.org/pdf/2502.11517v2.pdf","comment":"15 pages"},{"id":"http://arxiv.org/abs/2501.14649v2","updated":"2025-02-21T06:12:55Z","published":"2025-01-24T17:15:09Z","title":"Investigating the (De)Composition Capabilities of Large Language Models\n  in Natural-to-Formal Language Conversion","summary":"  To achieve generalized and robust natural-to-formal language conversion\n(N2F), large language models (LLMs) need to have strong capabilities of\ndecomposition and composition in N2F when faced with an unfamiliar formal\nlanguage and be able to cope with compositional gaps and counter-intuitive\nsymbolic names. To investigate whether LLMs have this set of basic capabilities\nin N2F, we propose the DEDC framework. This framework semi-automatically\nperforms sample and task construction, allowing decoupled evaluation of the set\nof decomposition and composition capabilities of LLMs in N2F. Based on this\nframework, we evaluate and analyze the most advanced LLMs, and the main\nfindings include that: (1) the LLMs are deficient in both decomposition and\ncomposition; (2) the LLMs show a wide coverage of error types that can be\nattributed to deficiencies in natural language understanding and the learning\nand use of symbolic systems; (3) compositional gaps and counter-intuitive\nsymbolic names both affect the decomposition and composition of the LLMs. Our\nwork provides a new perspective for investigating the basic capabilities of\ndecomposition and composition of LLMs in N2F. The detailed analysis of\ndeficiencies and attributions can help subsequent improvements of LLMs.\n","authors":["Ziyao Xu","Houfeng Wang"],"pdf_url":"https://arxiv.org/pdf/2501.14649v2.pdf","comment":"Accepted at NAACL 2025 main conference"},{"id":"http://arxiv.org/abs/2502.15226v1","updated":"2025-02-21T05:42:22Z","published":"2025-02-21T05:42:22Z","title":"Understand User Opinions of Large Language Models via LLM-Powered\n  In-the-Moment User Experience Interviews","summary":"  Which large language model (LLM) is better? Every evaluation tells a story,\nbut what do users really think about current LLMs? This paper presents CLUE, an\nLLM-powered interviewer that conducts in-the-moment user experience interviews,\nright after users interacted with LLMs, and automatically gathers insights\nabout user opinions from massive interview logs. We conduct a study with\nthousands of users to understand user opinions on mainstream LLMs, recruiting\nusers to first chat with a target LLM and then interviewed by CLUE. Our\nexperiments demonstrate that CLUE captures interesting user opinions, for\nexample, the bipolar views on the displayed reasoning process of DeepSeek-R1\nand demands for information freshness and multi-modality. Our collected\nchat-and-interview logs will be released.\n","authors":["Mengqiao Liu","Tevin Wang","Cassandra A. Cohen","Sarah Li","Chenyan Xiong"],"pdf_url":"https://arxiv.org/pdf/2502.15226v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.15223v1","updated":"2025-02-21T05:35:08Z","published":"2025-02-21T05:35:08Z","title":"A BERT Based Hybrid Recommendation System For Academic Collaboration","summary":"  Universities serve as a hub for academic collaboration, promoting the\nexchange of diverse ideas and perspectives among students and faculty through\ninterdisciplinary dialogue. However, as universities expand in size,\nconventional networking approaches via student chapters, class groups, and\nfaculty committees become cumbersome. To address this challenge, an\nacademia-specific profile recommendation system is proposed to connect\nlike-minded stakeholders within any university community. This study evaluates\nthree techniques: Term Frequency-Inverse Document Frequency (TF-IDF),\nBidirectional Encoder Representations from Transformers (BERT), and a hybrid\napproach to generate effective recommendations. Due to the unlabelled nature of\nthe dataset, Affinity Propagation cluster-based relabelling is performed to\nunderstand the grouping of similar profiles. The hybrid model demonstrated\nsuperior performance, evidenced by its similarity score, Silhouette score,\nDavies-Bouldin index, and Normalized Discounted Cumulative Gain (NDCG),\nachieving an optimal balance between diversity and relevance in\nrecommendations. Furthermore, the optimal model has been implemented as a\nmobile application, which dynamically suggests relevant profiles based on\nusers' skills and collaboration interests, incorporating contextual\nunderstanding. The potential impact of this application is significant, as it\npromises to enhance networking opportunities within large academic institutions\nthrough the deployment of intelligent recommendation systems.\n","authors":["Sangeetha N","Harish Thangaraj","Varun Vashisht","Eshaan Joshi","Kanishka Verma","Diya Katariya"],"pdf_url":"https://arxiv.org/pdf/2502.15223v1.pdf","comment":"International Conference on Intelligent Systems and Security - 2024"},{"id":"http://arxiv.org/abs/2401.06824v5","updated":"2025-02-21T05:17:52Z","published":"2024-01-12T00:50:04Z","title":"Revisiting Jailbreaking for Large Language Models: A Representation\n  Engineering Perspective","summary":"  The recent surge in jailbreaking attacks has revealed significant\nvulnerabilities in Large Language Models (LLMs) when exposed to malicious\ninputs. While various defense strategies have been proposed to mitigate these\nthreats, there has been limited research into the underlying mechanisms that\nmake LLMs vulnerable to such attacks. In this study, we suggest that the\nself-safeguarding capability of LLMs is linked to specific activity patterns\nwithin their representation space. Although these patterns have little impact\non the semantic content of the generated text, they play a crucial role in\nshaping LLM behavior under jailbreaking attacks. Our findings demonstrate that\nthese patterns can be detected with just a few pairs of contrastive queries.\nExtensive experimentation shows that the robustness of LLMs against\njailbreaking can be manipulated by weakening or strengthening these patterns.\nFurther visual analysis provides additional evidence for our conclusions,\nproviding new insights into the jailbreaking phenomenon. These findings\nhighlight the importance of addressing the potential misuse of open-source LLMs\nwithin the community.\n","authors":["Tianlong Li","Zhenghua Wang","Wenhao Liu","Muling Wu","Shihan Dou","Changze Lv","Xiaohua Wang","Xiaoqing Zheng","Xuanjing Huang"],"pdf_url":"https://arxiv.org/pdf/2401.06824v5.pdf","comment":"Accepted by COLING 2025"},{"id":"http://arxiv.org/abs/2502.15214v1","updated":"2025-02-21T05:01:30Z","published":"2025-02-21T05:01:30Z","title":"The Evolving Landscape of LLM- and VLM-Integrated Reinforcement Learning","summary":"  Reinforcement learning (RL) has shown impressive results in sequential\ndecision-making tasks. Meanwhile, Large Language Models (LLMs) and\nVision-Language Models (VLMs) have emerged, exhibiting impressive capabilities\nin multimodal understanding and reasoning. These advances have led to a surge\nof research integrating LLMs and VLMs into RL. In this survey, we review\nrepresentative works in which LLMs and VLMs are used to overcome key challenges\nin RL, such as lack of prior knowledge, long-horizon planning, and reward\ndesign. We present a taxonomy that categorizes these LLM/VLM-assisted RL\napproaches into three roles: agent, planner, and reward. We conclude by\nexploring open problems, including grounding, bias mitigation, improved\nrepresentations, and action advice. By consolidating existing research and\nidentifying future directions, this survey establishes a framework for\nintegrating LLMs and VLMs into RL, advancing approaches that unify natural\nlanguage and visual understanding with sequential decision-making.\n","authors":["Sheila Schoepp","Masoud Jafaripour","Yingyue Cao","Tianpei Yang","Fatemeh Abdollahi","Shadan Golestan","Zahin Sufiyan","Osmar R. Zaiane","Matthew E. Taylor"],"pdf_url":"https://arxiv.org/pdf/2502.15214v1.pdf","comment":"9 pages, 4 figures"},{"id":"http://arxiv.org/abs/2501.11301v3","updated":"2025-02-21T04:58:38Z","published":"2025-01-20T07:05:15Z","title":"Question-to-Question Retrieval for Hallucination-Free Knowledge Access:\n  An Approach for Wikipedia and Wikidata Question Answering","summary":"  This paper introduces an approach to question answering over knowledge bases\nlike Wikipedia and Wikidata by performing \"question-to-question\" matching and\nretrieval from a dense vector embedding store. Instead of embedding document\ncontent, we generate a comprehensive set of questions for each logical content\nunit using an instruction-tuned LLM. These questions are vector-embedded and\nstored, mapping to the corresponding content. Vector embedding of user queries\nare then matched against this question vector store. The highest similarity\nscore leads to direct retrieval of the associated article content, eliminating\nthe need for answer generation. Our method achieves high cosine similarity ( >\n0.9 ) for relevant question pairs, enabling highly precise retrieval. This\napproach offers several advantages including computational efficiency, rapid\nresponse times, and increased scalability. We demonstrate its effectiveness on\nWikipedia and Wikidata, including multimedia content through structured fact\nretrieval from Wikidata, opening up new pathways for multimodal question\nanswering.\n","authors":["Santhosh Thottingal"],"pdf_url":"https://arxiv.org/pdf/2501.11301v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.15208v1","updated":"2025-02-21T04:46:57Z","published":"2025-02-21T04:46:57Z","title":"Unveiling Attractor Cycles in Large Language Models: A Dynamical Systems\n  View of Successive Paraphrasing","summary":"  Dynamical systems theory provides a framework for analyzing iterative\nprocesses and evolution over time. Within such systems, repetitive\ntransformations can lead to stable configurations, known as attractors,\nincluding fixed points and limit cycles. Applying this perspective to large\nlanguage models (LLMs), which iteratively map input text to output text,\nprovides a principled approach to characterizing long-term behaviors.\nSuccessive paraphrasing serves as a compelling testbed for exploring such\ndynamics, as paraphrases re-express the same underlying meaning with linguistic\nvariation. Although LLMs are expected to explore a diverse set of paraphrases\nin the text space, our study reveals that successive paraphrasing converges to\nstable periodic states, such as 2-period attractor cycles, limiting linguistic\ndiversity. This phenomenon is attributed to the self-reinforcing nature of\nLLMs, as they iteratively favour and amplify certain textual forms over others.\nThis pattern persists with increasing generation randomness or alternating\nprompts and LLMs. These findings underscore inherent constraints in LLM\ngenerative capability, while offering a novel dynamical systems perspective for\nstudying their expressive potential.\n","authors":["Zhilin Wang","Yafu Li","Jianhao Yan","Yu Cheng","Yue Zhang"],"pdf_url":"https://arxiv.org/pdf/2502.15208v1.pdf","comment":"9 pages"},{"id":"http://arxiv.org/abs/2408.04556v4","updated":"2025-02-21T04:43:43Z","published":"2024-08-08T16:13:26Z","title":"BA-LoRA: Bias-Alleviating Low-Rank Adaptation to Mitigate Catastrophic\n  Inheritance in Large Language Models","summary":"  Large language models (LLMs) have demonstrated remarkable proficiency across\nvarious natural language processing (NLP) tasks. However, adapting LLMs to\ndownstream applications requires computationally intensive and memory-demanding\nfine-tuning procedures. To alleviate these burdens, parameter-efficient\nfine-tuning (PEFT) techniques have emerged as a promising approach to tailor\nLLMs with minimal computational overhead. While PEFT methods offer substantial\nadvantages, they do not fully address the pervasive issue of bias propagation\nfrom pre-training data. This work introduces Bias-Alleviating Low-Rank\nAdaptation (BA-LoRA), a novel PEFT method designed to counteract bias\ninheritance. BA-LoRA incorporates three distinct regularization terms: (1) a\nconsistency regularizer, (2) a diversity regularizer, and (3) a singular value\ndecomposition regularizer. These regularizers aim to enhance the models'\nconsistency, diversity, and generalization capabilities during fine-tuning. We\nconduct extensive experiments on natural language understanding (NLU) and\nnatural language generation (NLG) tasks using prominent LLMs such as LLaMA,\nMistral, and Gemma. The results demonstrate that BA-LoRA outperforms LoRA and\nits state-of-the-art variants. Moreover, our method effectively mitigates the\nadverse effects of pre-training bias, leading to more reliable and robust model\noutputs. The code is available at https://github.com/cyp-jlu-ai/BA-LoRA.\n","authors":["Yupeng Chang","Yi Chang","Yuan Wu"],"pdf_url":"https://arxiv.org/pdf/2408.04556v4.pdf","comment":"25 pages"},{"id":"http://arxiv.org/abs/2502.15197v1","updated":"2025-02-21T04:19:24Z","published":"2025-02-21T04:19:24Z","title":"TETRIS: Optimal Draft Token Selection for Batch Speculative Decoding","summary":"  We propose TETRIS, a novel method that optimizes the total throughput of\nbatch speculative decoding in multi-request settings. Unlike existing methods\nthat optimize for a single request or a group of requests as a whole, TETRIS\nactively selects the most promising draft tokens (for every request in a batch)\nto be accepted when verified in parallel, resulting in fewer rejected tokens\nand hence less wasted computing resources. Such an effective resource\nutilization to achieve fast inference in large language models (LLMs) is\nespecially important to service providers with limited inference capacity.\nCompared to baseline speculative decoding, TETRIS yields a consistently higher\nacceptance rate and more effective utilization of the limited inference\ncapacity. We show theoretically and empirically that TETRIS outperforms\nbaseline speculative decoding and existing methods that dynamically select\ndraft tokens, leading to a more efficient batch inference in LLMs.\n","authors":["Zhaoxuan Wu","Zijian Zhou","Arun Verma","Alok Prakash","Daniela Rus","Bryan Kian Hsiang Low"],"pdf_url":"https://arxiv.org/pdf/2502.15197v1.pdf","comment":"15 pages, 10 figures, 5 tables"},{"id":"http://arxiv.org/abs/2502.07575v2","updated":"2025-02-21T04:10:54Z","published":"2025-02-11T14:17:29Z","title":"Towards Efficient and Multifaceted Computer-assisted Pronunciation\n  Training Leveraging Hierarchical Selective State Space Model and Decoupled\n  Cross-entropy Loss","summary":"  Prior efforts in building computer-assisted pronunciation training (CAPT)\nsystems often treat automatic pronunciation assessment (APA) and\nmispronunciation detection and diagnosis (MDD) as separate fronts: the former\naims to provide multiple pronunciation aspect scores across diverse linguistic\nlevels, while the latter focuses instead on pinpointing the precise phonetic\npronunciation errors made by non-native language learners. However, it is\ngenerally expected that a full-fledged CAPT system should perform both\nfunctionalities simultaneously and efficiently. In response to this surging\ndemand, we in this work first propose HMamba, a novel CAPT approach that\nseamlessly integrates APA and MDD tasks in parallel. In addition, we introduce\na novel loss function, decoupled cross-entropy loss (deXent), specifically\ntailored for MDD to facilitate better-supervised learning for detecting\nmispronounced phones, thereby enhancing overall performance. A comprehensive\nset of empirical results on the speechocean762 benchmark dataset demonstrates\nthe effectiveness of our approach on APA. Notably, our proposed approach also\nyields a considerable improvement in MDD performance over a strong baseline,\nachieving an F1-score of 63.85%. Our codes are made available at\nhttps://github.com/Fuann/hmamba\n","authors":["Fu-An Chao","Berlin Chen"],"pdf_url":"https://arxiv.org/pdf/2502.07575v2.pdf","comment":"Accepted to NAACL 2025 main conference"},{"id":"http://arxiv.org/abs/2412.04937v2","updated":"2025-02-21T04:08:06Z","published":"2024-12-06T10:45:54Z","title":"Who Speaks Next? Multi-party AI Discussion Leveraging the Systematics of\n  Turn-taking in Murder Mystery Games","summary":"  Multi-agent systems utilizing large language models (LLMs) have shown great\npromise in achieving natural dialogue. However, smooth dialogue control and\nautonomous decision making among agents still remain challenges. In this study,\nwe focus on conversational norms such as adjacency pairs and turn-taking found\nin conversation analysis and propose a new framework called \"Murder Mystery\nAgents\" that applies these norms to AI agents' dialogue control. As an\nevaluation target, we employed the \"Murder Mystery\" game, a reasoning-type\ntable-top role-playing game that requires complex social reasoning and\ninformation manipulation. In this game, players need to unravel the truth of\nthe case based on fragmentary information through cooperation and bargaining.\nThe proposed framework integrates next speaker selection based on adjacency\npairs and a self-selection mechanism that takes agents' internal states into\naccount to achieve more natural and strategic dialogue. To verify the\neffectiveness of this new approach, we analyzed utterances that led to dialogue\nbreakdowns and conducted automatic evaluation using LLMs, as well as human\nevaluation using evaluation criteria developed for the Murder Mystery game.\nExperimental results showed that the implementation of the next speaker\nselection mechanism significantly reduced dialogue breakdowns and improved the\nability of agents to share information and perform logical reasoning. The\nresults of this study demonstrate that the systematics of turn-taking in human\nconversation are also effective in controlling dialogue among AI agents, and\nprovide design guidelines for more advanced multi-agent dialogue systems.\n","authors":["Ryota Nonomura","Hiroki Mori"],"pdf_url":"https://arxiv.org/pdf/2412.04937v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.21276v3","updated":"2025-02-21T04:00:21Z","published":"2024-07-31T01:51:24Z","title":"Knowledge Pyramid Construction for Multi-Level Retrieval-Augmented\n  Generation","summary":"  This paper addresses the need for improved precision in existing\nknowledge-enhanced question-answering frameworks, specifically\nRetrieval-Augmented Generation (RAG) methods that primarily focus on enhancing\nrecall. We propose a multi-layer knowledge pyramid approach within the RAG\nframework to achieve a better balance between precision and recall. The\nknowledge pyramid consists of three layers: Ontologies, Knowledge Graphs (KGs),\nand chunk-based raw text. We employ cross-layer augmentation techniques for\ncomprehensive knowledge coverage and dynamic updates of the Ontology schema and\ninstances. To ensure compactness, we utilize cross-layer filtering methods for\nknowledge condensation in KGs. Our approach, named PolyRAG, follows a waterfall\nmodel for retrieval, starting from the top of the pyramid and progressing down\nuntil a confident answer is obtained. We introduce two benchmarks for\ndomain-specific knowledge retrieval, one in the academic domain and the other\nin the financial domain. The effectiveness of the methods has been validated\nthrough comprehensive experiments by outperforming 19 SOTA methods. An\nencouraging observation is that the proposed method has augmented the GPT-4,\nproviding 395% F1 gain by improving its performance from 0.1636 to 0.8109.\n","authors":["Rubing Chen","Xulu Zhang","Jiaxin Wu","Wenqi Fan","Xiao-Yong Wei","Qing Li"],"pdf_url":"https://arxiv.org/pdf/2407.21276v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.01509v4","updated":"2025-02-21T03:49:13Z","published":"2024-07-01T17:53:35Z","title":"MIA-Bench: Towards Better Instruction Following Evaluation of Multimodal\n  LLMs","summary":"  We introduce MIA-Bench, a new benchmark designed to evaluate multimodal large\nlanguage models (MLLMs) on their ability to strictly adhere to complex\ninstructions. Our benchmark comprises a diverse set of 400 image-prompt pairs,\neach crafted to challenge the models' compliance with layered instructions in\ngenerating accurate responses that satisfy specific requested patterns.\nEvaluation results from a wide array of state-of-the-art MLLMs reveal\nsignificant variations in performance, highlighting areas for improvement in\ninstruction fidelity. Additionally, we create extra training data and explore\nsupervised fine-tuning to enhance the models' ability to strictly follow\ninstructions without compromising performance on other tasks. We hope this\nbenchmark not only serves as a tool for measuring MLLM adherence to\ninstructions, but also guides future developments in MLLM training methods.\n","authors":["Yusu Qian","Hanrong Ye","Jean-Philippe Fauconnier","Peter Grasch","Yinfei Yang","Zhe Gan"],"pdf_url":"https://arxiv.org/pdf/2407.01509v4.pdf","comment":"Accepted at ICLR 2025"},{"id":"http://arxiv.org/abs/2502.15189v1","updated":"2025-02-21T03:41:43Z","published":"2025-02-21T03:41:43Z","title":"Scale-Free Graph-Language Models","summary":"  Graph-language models (GLMs) have demonstrated great potential in graph-based\nsemi-supervised learning. A typical GLM consists of two key stages: graph\ngeneration and text embedding, which are usually implemented by inferring a\nlatent graph and finetuning a language model (LM), respectively. However, the\nformer often relies on artificial assumptions about the underlying edge\ndistribution, while the latter requires extensive data annotations. To tackle\nthese challenges, this paper introduces a novel GLM that integrates graph\ngeneration and text embedding within a unified framework. Specifically, for\ngraph generation, we leverage an inherent characteristic of real edge\ndistribution--the scale-free property--as a structural prior. We unexpectedly\nfind that this natural property can be effectively approximated by a simple\nk-nearest neighbor (KNN) graph. For text embedding, we develop a graph-based\npseudo-labeler that utilizes scale-free graphs to provide complementary\nsupervision for improved LM finetuning. Extensive experiments on representative\ndatasets validate our findings on the scale-free structural approximation of\nKNN graphs and demonstrate the effectiveness of integrating graph generation\nand text embedding with a real structural prior. Our code is available at\nhttps://github.com/Jianglin954/SFGL.\n","authors":["Jianglin Lu","Yixuan Liu","Yitian Zhang","Yun Fu"],"pdf_url":"https://arxiv.org/pdf/2502.15189v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.20694v4","updated":"2025-02-21T03:28:49Z","published":"2024-12-30T04:05:22Z","title":"QUBE: Enhancing Automatic Heuristic Design via Quality-Uncertainty\n  Balanced Evolution","summary":"  Solving NP-hard problems traditionally relies on heuristics, yet manually\ndesigning effective heuristics for complex problems remains a significant\nchallenge. While recent advancements like FunSearch have shown that large\nlanguage models (LLMs) can be integrated into evolutionary algorithms (EAs) for\nheuristic design, their potential is hindered by limitations in balancing\nexploitation and exploration. We introduce Quality-Uncertainty Balanced\nEvolution (QUBE), a novel approach that enhances LLM+EA methods by redefining\nthe priority criterion within the FunSearch framework. QUBE employs the\nQuality-Uncertainty Trade-off Criterion (QUTC), based on our proposed\nUncertainty-Inclusive Quality metric, to evaluate and guide the evolutionary\nprocess. Through extensive experiments on challenging NP-complete problems,\nQUBE demonstrates significant performance improvements over FunSearch and\nbaseline methods. Our code are available at\nhttps://github.com/zzjchen/QUBE_code.\n","authors":["Zijie Chen","Zhanchao Zhou","Yu Lu","Renjun Xu","Lili Pan","Zhenzhong Lan"],"pdf_url":"https://arxiv.org/pdf/2412.20694v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.14693v2","updated":"2025-02-21T03:19:47Z","published":"2025-02-20T16:19:09Z","title":"I-MCTS: Enhancing Agentic AutoML via Introspective Monte Carlo Tree\n  Search","summary":"  Recent advancements in large language models (LLMs) have shown remarkable\npotential in automating machine learning tasks. However, existing LLM-based\nagents often struggle with low-diversity and suboptimal code generation. While\nrecent work has introduced Monte Carlo Tree Search (MCTS) to address these\nissues, limitations persist in the quality and diversity of thoughts generated,\nas well as in the scalar value feedback mechanisms used for node selection. In\nthis study, we introduce Introspective Monte Carlo Tree Search (I-MCTS), a\nnovel approach that iteratively expands tree nodes through an introspective\nprocess that meticulously analyzes solutions and results from parent and\nsibling nodes. This facilitates a continuous refinement of the node in the\nsearch tree, thereby enhancing the overall decision-making process.\nFurthermore, we integrate a Large Language Model (LLM)-based value model to\nfacilitate direct evaluation of each node's solution prior to conducting\ncomprehensive computational rollouts. A hybrid rewarding mechanism is\nimplemented to seamlessly transition the Q-value from LLM-estimated scores to\nactual performance scores. This allows higher-quality nodes to be traversed\nearlier. Applied to the various ML tasks, our approach demonstrates a 6%\nabsolute improvement in performance compared to the strong open-source AutoML\nagents, showcasing its effectiveness in enhancing agentic AutoML systems.\nResource available at https://github.com/jokieleung/I-MCTS\n","authors":["Zujie Liang","Feng Wei","Wujiang Xu","Lin Chen","Yuxi Qian","Xinhui Wu"],"pdf_url":"https://arxiv.org/pdf/2502.14693v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.15172v1","updated":"2025-02-21T03:13:44Z","published":"2025-02-21T03:13:44Z","title":"BP-GPT: Auditory Neural Decoding Using fMRI-prompted LLM","summary":"  Decoding language information from brain signals represents a vital research\narea within brain-computer interfaces, particularly in the context of\ndeciphering the semantic information from the fMRI signal. Although existing\nwork uses LLM to achieve this goal, their method does not use an end-to-end\napproach and avoids the LLM in the mapping of fMRI-to-text, leaving space for\nthe exploration of the LLM in auditory decoding. In this paper, we introduce a\nnovel method, the Brain Prompt GPT (BP-GPT). By using the brain representation\nthat is extracted from the fMRI as a prompt, our method can utilize GPT-2 to\ndecode fMRI signals into stimulus text. Further, we introduce the text prompt\nand align the fMRI prompt to it. By introducing the text prompt, our BP-GPT can\nextract a more robust brain prompt and promote the decoding of pre-trained LLM.\nWe evaluate our BP-GPT on the open-source auditory semantic decoding dataset\nand achieve a significant improvement up to 4.61 on METEOR and 2.43 on\nBERTScore across all the subjects compared to the state-of-the-art method. The\nexperimental results demonstrate that using brain representation as a prompt to\nfurther drive LLM for auditory neural decoding is feasible and effective. The\ncode is available at https://github.com/1994cxy/BP-GPT.\n","authors":["Xiaoyu Chen","Changde Du","Che Liu","Yizhe Wang","Huiguang He"],"pdf_url":"https://arxiv.org/pdf/2502.15172v1.pdf","comment":"arXiv admin note: substantial text overlap with arXiv:2405.07840"},{"id":"http://arxiv.org/abs/2502.09673v2","updated":"2025-02-21T03:12:17Z","published":"2025-02-13T06:37:28Z","title":"Are Smarter LLMs Safer? Exploring Safety-Reasoning Trade-offs in\n  Prompting and Fine-Tuning","summary":"  Large Language Models (LLMs) have demonstrated remarkable success across\nvarious NLP benchmarks. However, excelling in complex tasks that require\nnuanced reasoning and precise decision-making demands more than raw language\nproficiency--LLMs must reason, i.e., think logically, draw from past\nexperiences, and synthesize information to reach conclusions and take action.\nTo enhance reasoning abilities, approaches such as prompting and fine-tuning\nhave been widely explored. While these methods have led to clear improvements\nin reasoning, their impact on LLM safety remains less understood. In this work,\nwe investigate the interplay between reasoning and safety in LLMs. We highlight\nthe latent safety risks that arise as reasoning capabilities improve, shedding\nlight on previously overlooked vulnerabilities. At the same time, we explore\nhow reasoning itself can be leveraged to enhance safety, uncovering potential\nmitigation strategies. By examining both the risks and opportunities in\nreasoning-driven LLM safety, our study provides valuable insights for\ndeveloping models that are not only more capable but also more trustworthy in\nreal-world deployments.\n","authors":["Ang Li","Yichuan Mo","Mingjie Li","Yifei Wang","Yisen Wang"],"pdf_url":"https://arxiv.org/pdf/2502.09673v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.15168v1","updated":"2025-02-21T03:11:41Z","published":"2025-02-21T03:11:41Z","title":"mStyleDistance: Multilingual Style Embeddings and their Evaluation","summary":"  Style embeddings are useful for stylistic analysis and style transfer;\nhowever, only English style embeddings have been made available. We introduce\nMultilingual StyleDistance (mStyleDistance), a multilingual style embedding\nmodel trained using synthetic data and contrastive learning. We train the model\non data from nine languages and create a multilingual STEL-or-Content benchmark\n(Wegmann et al., 2022) that serves to assess the embeddings' quality. We also\nemploy our embeddings in an authorship verification task involving different\nlanguages. Our results show that mStyleDistance embeddings outperform existing\nmodels on these multilingual style benchmarks and generalize well to unseen\nfeatures and languages. We make our model publicly available at\nhttps://huggingface.co/StyleDistance/mstyledistance .\n","authors":["Justin Qiu","Jiacheng Zhu","Ajay Patel","Marianna Apidianaki","Chris Callison-Burch"],"pdf_url":"https://arxiv.org/pdf/2502.15168v1.pdf","comment":"arXiv admin note: substantial text overlap with arXiv:2410.12757"},{"id":"http://arxiv.org/abs/2502.14744v2","updated":"2025-02-21T03:09:12Z","published":"2025-02-20T17:14:34Z","title":"HiddenDetect: Detecting Jailbreak Attacks against Large Vision-Language\n  Models via Monitoring Hidden States","summary":"  The integration of additional modalities increases the susceptibility of\nlarge vision-language models (LVLMs) to safety risks, such as jailbreak\nattacks, compared to their language-only counterparts. While existing research\nprimarily focuses on post-hoc alignment techniques, the underlying safety\nmechanisms within LVLMs remain largely unexplored. In this work , we\ninvestigate whether LVLMs inherently encode safety-relevant signals within\ntheir internal activations during inference. Our findings reveal that LVLMs\nexhibit distinct activation patterns when processing unsafe prompts, which can\nbe leveraged to detect and mitigate adversarial inputs without requiring\nextensive fine-tuning. Building on this insight, we introduce HiddenDetect, a\nnovel tuning-free framework that harnesses internal model activations to\nenhance safety. Experimental results show that {HiddenDetect} surpasses\nstate-of-the-art methods in detecting jailbreak attacks against LVLMs. By\nutilizing intrinsic safety-aware patterns, our method provides an efficient and\nscalable solution for strengthening LVLM robustness against multimodal threats.\nOur code will be released publicly at\nhttps://github.com/leigest519/HiddenDetect.\n","authors":["Yilei Jiang","Xinyan Gao","Tianshuo Peng","Yingshui Tan","Xiaoyong Zhu","Bo Zheng","Xiangyu Yue"],"pdf_url":"https://arxiv.org/pdf/2502.14744v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.09121v4","updated":"2025-02-21T03:02:41Z","published":"2024-08-17T07:11:02Z","title":"Selective Prompt Anchoring for Code Generation","summary":"  Recent advances in large language models (LLMs) have transformed software\ndevelopment by automatically generating code from natural language. Yet\nchallenges remain in generating fully correct code that aligns with user\nintent. Our study reveals that LLMs tend to pay less attention to user prompts\nas more code tokens are generated. We hypothesize that this attention dilution\nissue is an important reason for code generation errors. To mitigate this\nissue, we propose Selective Prompt Anchoring (SPA) to guide code LLMs to pay\nmore attention to user intent when generating code. We evaluate SPA using six\nbase LLMs across six benchmarks. Our results demonstrate that SPA enhances\nPass@1 by up to 12.9%, consistently outperforming SOTA code generation methods\nin all settings. Our code is available at\nhttps://github.com/magic-YuanTian/Selective-Prompt-Anchoring.\n","authors":["Yuan Tian","Tianyi Zhang"],"pdf_url":"https://arxiv.org/pdf/2408.09121v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.13417v2","updated":"2025-02-21T02:51:18Z","published":"2025-02-19T04:25:11Z","title":"RLTHF: Targeted Human Feedback for LLM Alignment","summary":"  Fine-tuning large language models (LLMs) to align with user preferences is\nchallenging due to the high cost of quality human annotations in Reinforcement\nLearning from Human Feedback (RLHF) and the generalizability limitations of AI\nFeedback. To address these challenges, we propose RLTHF, a human-AI hybrid\nframework that combines LLM-based initial alignment with selective human\nannotations to achieve full-human annotation alignment with minimal effort.\nRLTHF identifies hard-to-annotate samples mislabeled by LLMs using a reward\nmodel's reward distribution and iteratively enhances alignment by integrating\nstrategic human corrections while leveraging LLM's correctly labeled samples.\nEvaluations on HH-RLHF and TL;DR datasets show that RLTHF reaches full-human\nannotation-level alignment with only 6-7% of the human annotation effort.\nFurthermore, models trained on RLTHF's curated datasets for downstream tasks\noutperform those trained on fully human-annotated datasets, underscoring the\neffectiveness of RLTHF's strategic data curation.\n","authors":["Yifei Xu","Tusher Chakraborty","Emre Kıcıman","Bibek Aryal","Eduardo Rodrigues","Srinagesh Sharma","Roberto Estevao","Maria Angels de Luis Balaguer","Jessica Wolk","Rafael Padilha","Leonardo Nunes","Shobana Balakrishnan","Songwu Lu","Ranveer Chandra"],"pdf_url":"https://arxiv.org/pdf/2502.13417v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.15155v1","updated":"2025-02-21T02:31:05Z","published":"2025-02-21T02:31:05Z","title":"Extreme Speech Classification in the Era of LLMs: Exploring Open-Source\n  and Proprietary Models","summary":"  In recent years, widespread internet adoption and the growth in userbase of\nvarious social media platforms have led to an increase in the proliferation of\nextreme speech online. While traditional language models have demonstrated\nproficiency in distinguishing between neutral text and non-neutral text (i.e.\nextreme speech), categorizing the diverse types of extreme speech presents\nsignificant challenges. The task of extreme speech classification is\nparticularly nuanced, as it requires a deep understanding of socio-cultural\ncontexts to accurately interpret the intent of the language used by the\nspeaker. Even human annotators often disagree on the appropriate classification\nof such content, emphasizing the complex and subjective nature of this task.\nThe use of human moderators also presents a scaling issue, necessitating the\nneed for automated systems for extreme speech classification. The recent launch\nof ChatGPT has drawn global attention to the potential applications of Large\nLanguage Models (LLMs) across a diverse variety of tasks. Trained on vast and\ndiverse corpora, and demonstrating the ability to effectively capture and\nencode contextual information, LLMs emerge as highly promising tools for\ntackling this specific task of extreme speech classification. In this paper, we\nleverage the Indian subset of the extreme speech dataset from Maronikolakis et\nal. (2022) to develop an effective classification framework using LLMs. We\nevaluate open-source Llama models against closed-source OpenAI models, finding\nthat while pre-trained LLMs show moderate efficacy, fine-tuning with\ndomain-specific data significantly enhances performance, highlighting their\nadaptability to linguistic and contextual nuances. Although GPT-based models\noutperform Llama models in zero-shot settings, the performance gap disappears\nafter fine-tuning.\n","authors":["Sarthak Mahajan","Nimmi Rangaswamy"],"pdf_url":"https://arxiv.org/pdf/2502.15155v1.pdf","comment":"Accepted to 7th International Conference on information systems and\n  management science (ISMS), 2024"},{"id":"http://arxiv.org/abs/2502.15153v1","updated":"2025-02-21T02:24:43Z","published":"2025-02-21T02:24:43Z","title":"Investigating the Adaptive Robustness with Knowledge Conflicts in\n  LLM-based Multi-Agent Systems","summary":"  Recent advances in Large Language Models (LLMs) have upgraded them from\nsophisticated text generators to autonomous agents capable of corporation and\ntool use in multi-agent systems (MASs). However, the robustness of these\nLLM-based MASs, especially under knowledge conflicts, remains unclear. In this\npaper, we design four comprehensive metrics to investigate the robustness of\nMASs when facing mild or task-critical knowledge conflicts. We first analyze\nmild knowledge conflicts introduced by heterogeneous agents and find that they\ndo not harm system robustness but instead improve collaborative\ndecision-making. Next, we investigate task-critical knowledge conflicts by\nsynthesizing knowledge conflicts and embedding them into one of the agents. Our\nresults show that these conflicts have surprisingly little to no impact on MAS\nrobustness. Furthermore, we observe that MASs demonstrate certain\nself-repairing capabilities by reducing their reliance on knowledge conflicts\nand adopting alternative solution paths to maintain stability. Finally, we\nconduct ablation studies on the knowledge conflict number, agent number, and\ninteraction rounds, finding that the self-repairing capability of MASs has\nintrinsic limits, and all findings hold consistently across various factors.\nOur code is publicly available at\nhttps://github.com/wbw625/MultiAgentRobustness.\n","authors":["Tianjie Ju","Bowen Wang","Hao Fei","Mong-Li Lee","Wynne Hsu","Yun Li","Qianren Wang","Pengzhou Cheng","Zongru Wu","Zhuosheng Zhang","Gongshen Liu"],"pdf_url":"https://arxiv.org/pdf/2502.15153v1.pdf","comment":"Working in progress"},{"id":"http://arxiv.org/abs/2502.14289v2","updated":"2025-02-21T02:12:59Z","published":"2025-02-20T06:05:29Z","title":"Drift: Decoding-time Personalized Alignments with Implicit User\n  Preferences","summary":"  Personalized alignments for individual users have been a long-standing goal\nin large language models (LLMs). We introduce Drift, a novel framework that\npersonalizes LLMs at decoding time with implicit user preferences. Traditional\nReinforcement Learning from Human Feedback (RLHF) requires thousands of\nannotated examples and expensive gradient updates. In contrast, Drift\npersonalizes LLMs in a training-free manner, using only a few dozen examples to\nsteer a frozen model through efficient preference modeling. Our approach models\nuser preferences as a composition of predefined, interpretable attributes and\naligns them at decoding time to enable personalized generation. Experiments on\nboth a synthetic persona dataset (Perspective) and a real human-annotated\ndataset (PRISM) demonstrate that Drift significantly outperforms RLHF baselines\nwhile using only 50-100 examples. Our results and analysis show that Drift is\nboth computationally efficient and interpretable.\n","authors":["Minbeom Kim","Kang-il Lee","Seongho Joo","Hwaran Lee","Kyomin Jung"],"pdf_url":"https://arxiv.org/pdf/2502.14289v2.pdf","comment":"19 pages, 6 figures"},{"id":"http://arxiv.org/abs/2404.08877v5","updated":"2025-02-21T02:06:51Z","published":"2024-04-13T02:36:40Z","title":"Aligning the Objective of LLM-based Program Repair","summary":"  Large language models (LLMs) have achieved decent results on automated\nprogram repair (APR). However, the next token prediction training objective of\ndecoder-only LLMs (e.g., GPT-4) is misaligned with the masked span prediction\nobjective of current infilling-style methods, which impedes LLMs from fully\nleveraging pre-trained knowledge for program repair. In addition, while some\nLLMs can locate and repair bugs in certain functions using the related\nartifacts (e.g., test cases), existing methods still depend on statement-level\nfault localization methods to provide a list of buggy hunks for repair. This\nrestriction hinders LLMs from exploring potential patches beyond the given\nlocations.\n  In this paper, we investigate a new approach to adapt LLMs to program repair.\nOur core insight is that LLM's APR capability can be greatly improved by simply\naligning the output to their training objective and allowing them to refine the\nwhole program without first identifying faulty statements. Based on this\ninsight, we designed D4C, a straightforward prompting framework for APR. D4C\ncan repair 180 bugs correctly in Defects4J, with each patch being sampled only\n10 times. This surpasses the SOTA APR methods with perfect fault localization\nby 10% and reduces the patch sampling number by 90%. Our findings reveal that\n(1) objective alignment is crucial for fully exploiting LLM's pre-trained\ncapability, and (2) replacing the traditional localize-buggy-hunks-then-repair\nworkflow with direct debugging is more effective for LLM-based APR methods.\nThus, we believe this paper introduces a new mindset for harnessing LLMs in\nAPR.\n","authors":["Junjielong Xu","Ying Fu","Shin Hwei Tan","Pinjia He"],"pdf_url":"https://arxiv.org/pdf/2404.08877v5.pdf","comment":"Accepted by ICSE'25"},{"id":"http://arxiv.org/abs/2502.15147v1","updated":"2025-02-21T02:03:08Z","published":"2025-02-21T02:03:08Z","title":"Latent Factor Models Meets Instructions:Goal-conditioned Latent Factor\n  Discovery without Task Supervision","summary":"  Instruction-following LLMs have recently allowed systems to discover hidden\nconcepts from a collection of unstructured documents based on a natural\nlanguage description of the purpose of the discovery (i.e., goal). Still, the\nquality of the discovered concepts remains mixed, as it depends heavily on\nLLM's reasoning ability and drops when the data is noisy or beyond LLM's\nknowledge. We present Instruct-LF, a goal-oriented latent factor discovery\nsystem that integrates LLM's instruction-following ability with statistical\nmodels to handle large, noisy datasets where LLM reasoning alone falls short.\n  Instruct-LF uses LLMs to propose fine-grained, goal-related properties from\ndocuments, estimates their presence across the dataset, and applies\ngradient-based optimization to uncover hidden factors, where each factor is\nrepresented by a cluster of co-occurring properties. We evaluate latent factors\nproduced by Instruct-LF on movie recommendation, text-world navigation, and\nlegal document categorization tasks. These interpretable representations\nimprove downstream task performance by 5-52% than the best baselines and were\npreferred 1.8 times as often as the best alternative, on average, in human\nevaluation.\n","authors":["Zhouhang Xie","Tushar Khot","Bhavana Dalvi Mishra","Harshit Surana","Julian McAuley","Peter Clark","Bodhisattwa Prasad Majumder"],"pdf_url":"https://arxiv.org/pdf/2502.15147v1.pdf","comment":"NAACL 2025"},{"id":"http://arxiv.org/abs/2410.00487v2","updated":"2025-02-21T01:56:47Z","published":"2024-10-01T08:18:17Z","title":"Self-Updatable Large Language Models by Integrating Context into Model\n  Parameters","summary":"  Despite significant advancements in large language models (LLMs), the rapid\nand frequent integration of small-scale experiences, such as interactions with\nsurrounding objects, remains a substantial challenge. Two critical factors in\nassimilating these experiences are (1) Efficacy: the ability to accurately\nremember recent events; (2) Retention: the capacity to recall long-past\nexperiences. Current methods either embed experiences within model parameters\nusing continual learning, model editing, or knowledge distillation techniques,\nwhich often struggle with rapid updates and complex interactions, or rely on\nexternal storage to achieve long-term retention, thereby increasing storage\nrequirements. In this paper, we propose SELF-PARAM (Self-Updatable Large\nLanguage Models with Parameter Integration). SELF-PARAM requires no extra\nparameters while ensuring near-optimal efficacy and long-term retention. Our\nmethod employs a training objective that minimizes the Kullback-Leibler (KL)\ndivergence between the predictions of an original model (with access to\ncontextual information) and a target model (without such access). By generating\ndiverse question-answer pairs related to the knowledge and minimizing the KL\ndivergence across this dataset, we update the target model to internalize the\nknowledge seamlessly within its parameters. Evaluations on question-answering\nand conversational recommendation tasks demonstrate that SELF-PARAM\nsignificantly outperforms existing methods, even when accounting for non-zero\nstorage requirements. This advancement paves the way for more efficient and\nscalable integration of experiences in large language models by embedding\nknowledge directly into model parameters.\n","authors":["Yu Wang","Xinshuang Liu","Xiusi Chen","Sean O'Brien","Junda Wu","Julian McAuley"],"pdf_url":"https://arxiv.org/pdf/2410.00487v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.15140v1","updated":"2025-02-21T01:43:32Z","published":"2025-02-21T01:43:32Z","title":"Do LLMs Make Mistakes Like Students? Exploring Natural Alignment between\n  Language Models and Human Error Patterns","summary":"  Large Language Models (LLMs) have demonstrated remarkable capabilities in\nvarious educational tasks, yet their alignment with human learning patterns,\nparticularly in predicting which incorrect options students are most likely to\nselect in multiple-choice questions (MCQs), remains underexplored. Our work\ninvestigates the relationship between LLM generation likelihood and student\nresponse distributions in MCQs with a specific focus on distractor selections.\nWe collect a comprehensive dataset of MCQs with real-world student response\ndistributions to explore two fundamental research questions: (1). RQ1 - Do the\ndistractors that students more frequently select correspond to those that LLMs\nassign higher generation likelihood to? (2). RQ2 - When an LLM selects a\nincorrect choice, does it choose the same distractor that most students pick?\nOur experiments reveals moderate correlations between LLM-assigned\nprobabilities and student selection patterns for distractors in MCQs.\nAdditionally, when LLMs make mistakes, they are more likley to select the same\nincorrect answers that commonly mislead students, which is a pattern consistent\nacross both small and large language models. Our work provides empirical\nevidence that despite LLMs' strong performance on generating educational\ncontent, there remains a gap between LLM's underlying reasoning process and\nhuman cognitive processes in identifying confusing distractors. Our findings\nalso have significant implications for educational assessment development. The\nsmaller language models could be efficiently utilized for automated distractor\ngeneration as they demonstrate similar patterns in identifying confusing answer\nchoices as larger language models. This observed alignment between LLMs and\nstudent misconception patterns opens new opportunities for generating\nhigh-quality distractors that complement traditional human-designed\ndistractors.\n","authors":["Naiming Liu","Shashank Sonkar","Richard G. Baraniuk"],"pdf_url":"https://arxiv.org/pdf/2502.15140v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.15134v1","updated":"2025-02-21T01:28:12Z","published":"2025-02-21T01:28:12Z","title":"Chain-of-Rank: Enhancing Large Language Models for Domain-Specific RAG\n  in Edge Device","summary":"  Retrieval-augmented generation (RAG) with large language models (LLMs) is\nespecially valuable in specialized domains, where precision is critical. To\nmore specialize the LLMs into a target domain, domain-specific RAG has recently\nbeen developed by allowing the LLM to access the target domain early via\nfinetuning. The domain-specific RAG makes more sense in resource-constrained\nenvironments like edge devices, as they should perform a specific task (e.g.\npersonalization) reliably using only small-scale LLMs. While the\ndomain-specific RAG is well-aligned with edge devices in this respect, it often\nrelies on widely-used reasoning techniques like chain-of-thought (CoT). The\nreasoning step is useful to understand the given external knowledge, and yet it\nis computationally expensive and difficult for small-scale LLMs to learn it.\nTackling this, we propose the Chain of Rank (CoR) which shifts the focus from\nintricate lengthy reasoning to simple ranking of the reliability of input\nexternal documents. Then, CoR reduces computational complexity while\nmaintaining high accuracy, making it particularly suited for\nresource-constrained environments. We attain the state-of-the-art (SOTA)\nresults in benchmarks, and analyze its efficacy.\n","authors":["Juntae Lee","Jihwan Bang","Seunghan Yang","Kyuhong Shim","Simyung Chang"],"pdf_url":"https://arxiv.org/pdf/2502.15134v1.pdf","comment":"NAACL 2025 (Findings)"},{"id":"http://arxiv.org/abs/2502.15132v1","updated":"2025-02-21T01:24:54Z","published":"2025-02-21T01:24:54Z","title":"CoT-ICL Lab: A Petri Dish for Studying Chain-of-Thought Learning from\n  In-Context Demonstrations","summary":"  We introduce CoT-ICL Lab, a framework and methodology to generate synthetic\ntokenized datasets and systematically study chain-of-thought (CoT) in-context\nlearning (ICL) in language models. CoT-ICL Lab allows fine grained control over\nthe complexity of in-context examples by decoupling (1) the causal structure\ninvolved in chain token generation from (2) the underlying token processing\nfunctions. We train decoder-only transformers (up to 700M parameters) on these\ndatasets and show that CoT accelerates the accuracy transition to higher values\nacross model sizes. In particular, we find that model depth is crucial for\nleveraging CoT with limited in-context examples, while more examples help\nshallow models match deeper model performance. Additionally, limiting the\ndiversity of token processing functions throughout training improves causal\nstructure learning via ICL. We also interpret these transitions by analyzing\ntransformer embeddings and attention maps. Overall, CoT-ICL Lab serves as a\nsimple yet powerful testbed for theoretical and empirical insights into ICL and\nCoT in language models.\n","authors":["Vignesh Kothapalli","Hamed Firooz","Maziar Sanjabi"],"pdf_url":"https://arxiv.org/pdf/2502.15132v1.pdf","comment":"22 pages, 27 figures, 3 tables"},{"id":"http://arxiv.org/abs/2501.14249v5","updated":"2025-02-21T01:12:25Z","published":"2025-01-24T05:27:46Z","title":"Humanity's Last Exam","summary":"  Benchmarks are important tools for tracking the rapid advancements in large\nlanguage model (LLM) capabilities. However, benchmarks are not keeping pace in\ndifficulty: LLMs now achieve over 90\\% accuracy on popular benchmarks like\nMMLU, limiting informed measurement of state-of-the-art LLM capabilities. In\nresponse, we introduce Humanity's Last Exam (HLE), a multi-modal benchmark at\nthe frontier of human knowledge, designed to be the final closed-ended academic\nbenchmark of its kind with broad subject coverage. HLE consists of 2,700\nquestions across dozens of subjects, including mathematics, humanities, and the\nnatural sciences. HLE is developed globally by subject-matter experts and\nconsists of multiple-choice and short-answer questions suitable for automated\ngrading. Each question has a known solution that is unambiguous and easily\nverifiable, but cannot be quickly answered via internet retrieval.\nState-of-the-art LLMs demonstrate low accuracy and calibration on HLE,\nhighlighting a significant gap between current LLM capabilities and the expert\nhuman frontier on closed-ended academic questions. To inform research and\npolicymaking upon a clear understanding of model capabilities, we publicly\nrelease HLE at https://lastexam.ai.\n","authors":["Long Phan","Alice Gatti","Ziwen Han","Nathaniel Li","Josephina Hu","Hugh Zhang","Chen Bo Calvin Zhang","Mohamed Shaaban","John Ling","Sean Shi","Michael Choi","Anish Agrawal","Arnav Chopra","Adam Khoja","Ryan Kim","Richard Ren","Jason Hausenloy","Oliver Zhang","Mantas Mazeika","Tung Nguyen","Daron Anderson","Imad Ali Shah","Mikhail Doroshenko","Alun Cennyth Stokes","Mobeen Mahmood","Jaeho Lee","Oleksandr Pokutnyi","Oleg Iskra","Jessica P. Wang","Robert Gerbicz","John-Clark Levin","Serguei Popov","Fiona Feng","Steven Y. Feng","Haoran Zhao","Michael Yu","Varun Gangal","Chelsea Zou","Zihan Wang","Mstyslav Kazakov","Geoff Galgon","Johannes Schmitt","Alvaro Sanchez","Yongki Lee","Will Yeadon","Scott Sauers","Marc Roth","Chidozie Agu","Søren Riis","Fabian Giska","Saiteja Utpala","Antrell Cheatom","Zachary Giboney","Gashaw M. Goshu","Sarah-Jane Crowson","Mohinder Maheshbhai Naiya","Noah Burns","Lennart Finke","Zerui Cheng","Hyunwoo Park","Francesco Fournier-Facio","Jennifer Zampese","John B. Wydallis","Ryan G. Hoerr","Mark Nandor","Tim Gehrunger","Jiaqi Cai","Ben McCarty","Jungbae Nam","Edwin Taylor","Jun Jin","Gautier Abou Loume","Hangrui Cao","Alexis C Garretson","Damien Sileo","Qiuyu Ren","Doru Cojoc","Pavel Arkhipov","Usman Qazi","Aras Bacho","Lianghui Li","Sumeet Motwani","Christian Schroeder de Witt","Alexei Kopylov","Johannes Veith","Eric Singer","Paolo Rissone","Jaehyeok Jin","Jack Wei Lun Shi","Chris G. Willcocks","Ameya Prabhu","Longke Tang","Kevin Zhou","Emily de Oliveira Santos","Andrey Pupasov Maksimov","Edward Vendrow","Kengo Zenitani","Joshua Robinson","Aleksandar Mikov","Julien Guillod","Yuqi Li","Ben Pageler","Joshua Vendrow","Vladyslav Kuchkin","Pierre Marion","Denis Efremov","Jayson Lynch","Kaiqu Liang","Andrew Gritsevskiy","Dakotah Martinez","Nick Crispino","Dimitri Zvonkine","Natanael Wildner Fraga","Saeed Soori","Ori Press","Henry Tang","Julian Salazar","Sean R. Green","Lina Brüssel","Moon Twayana","Aymeric Dieuleveut","T. Ryan Rogers","Wenjin Zhang","Ross Finocchio","Bikun Li","Jinzhou Yang","Arun Rao","Gabriel Loiseau","Mikhail Kalinin","Marco Lukas","Ciprian Manolescu","Nate Stambaugh","Subrata Mishra","Ariel Ghislain Kemogne Kamdoum","Tad Hogg","Alvin Jin","Carlo Bosio","Gongbo Sun","Brian P Coppola","Haline Heidinger","Rafael Sayous","Stefan Ivanov","Joseph M Cavanagh","Jiawei Shen","Joseph Marvin Imperial","Philippe Schwaller","Shaipranesh Senthilkuma","Andres M Bran","Andres Algaba","Brecht Verbeken","Kelsey Van den Houte","Lynn Van Der Sypt","David Noever","Lisa Schut","Ilia Sucholutsky","Evgenii Zheltonozhskii","Qiaochu Yuan","Derek Lim","Richard Stanley","Shankar Sivarajan","Tong Yang","John Maar","Julian Wykowski","Martí Oller","Jennifer Sandlin","Anmol Sahu","Cesare Giulio Ardito","Yuzheng Hu","Felipe Meneguitti Dias","Tobias Kreiman","Kaivalya Rawal","Tobias Garcia Vilchis","Yuexuan Zu","Martin Lackner","James Koppel","Jeremy Nguyen","Daniil S. Antonenko","Steffi Chern","Bingchen Zhao","Pierrot Arsene","Sergey Ivanov","Rafał Poświata","Chenguang Wang","Daofeng Li","Donato Crisostomi","Ali Dehghan","Andrea Achilleos","John Arnold Ambay","Benjamin Myklebust","Archan Sen","David Perrella","Nurdin Kaparov","Mark H Inlow","Allen Zang","Kalyan Ramakrishnan","Daniil Orel","Vladislav Poritski","Shalev Ben-David","Zachary Berger","Parker Whitfill","Michael Foster","Daniel Munro","Linh Ho","Dan Bar Hava","Aleksey Kuchkin","Robert Lauff","David Holmes","Frank Sommerhage","Anji Zhang","Richard Moat","Keith Schneider","Daniel Pyda","Zakayo Kazibwe","Mukhwinder Singh","Don Clarke","Dae Hyun Kim","Sara Fish","Veit Elser","Victor Efren Guadarrama Vilchis","Immo Klose","Christoph Demian","Ujjwala Anantheswaran","Adam Zweiger","Guglielmo Albani","Jeffery Li","Nicolas Daans","Maksim Radionov","Václav Rozhoň","Vincent Ginis","Ziqiao Ma","Christian Stump","Jacob Platnick","Volodymyr Nevirkovets","Luke Basler","Marco Piccardo","Niv Cohen","Virendra Singh","Josef Tkadlec","Paul Rosu","Alan Goldfarb","Piotr Padlewski","Stanislaw Barzowski","Kyle Montgomery","Aline Menezes","Arkil Patel","Zixuan Wang","Jamie Tucker-Foltz","Jack Stade","Declan Grabb","Tom Goertzen","Fereshteh Kazemi","Jeremiah Milbauer","Abhishek Shukla","Hossam Elgnainy","Yan Carlos Leyva Labrador","Hao He","Ling Zhang","Alan Givré","Hew Wolff","Gözdenur Demir","Muhammad Fayez Aziz","Younesse Kaddar","Ivar Ängquist","Yanxu Chen","Elliott Thornley","Robin Zhang","Jiayi Pan","Antonio Terpin","Niklas Muennighoff","Hailey Schoelkopf","Eric Zheng","Avishy Carmi","Jainam Shah","Ethan D. L. Brown","Kelin Zhu","Max Bartolo","Richard Wheeler","Andrew Ho","Shaul Barkan","Jiaqi Wang","Martin Stehberger","Egor Kretov","Peter Bradshaw","JP Heimonen","Kaustubh Sridhar","Zaki Hossain","Ido Akov","Yury Makarychev","Joanna Tam","Hieu Hoang","David M. Cunningham","Vladimir Goryachev","Demosthenes Patramanis","Michael Krause","Andrew Redenti","David Aldous","Jesyin Lai","Shannon Coleman","Jiangnan Xu","Sangwon Lee","Ilias Magoulas","Sandy Zhao","Ning Tang","Michael K. Cohen","Micah Carroll","Orr Paradise","Jan Hendrik Kirchner","Stefan Steinerberger","Maksym Ovchynnikov","Jason O. Matos","Adithya Shenoy","Michael Wang","Yuzhou Nie","Paolo Giordano","Philipp Petersen","Anna Sztyber-Betley","Paolo Faraboschi","Robin Riblet","Jonathan Crozier","Shiv Halasyamani","Antonella Pinto","Shreyas Verma","Prashant Joshi","Eli Meril","Zheng-Xin Yong","Allison Tee","Jérémy Andréoletti","Orion Weller","Raghav Singhal","Gang Zhang","Alexander Ivanov","Seri Khoury","Nils Gustafsson","Hamid Mostaghimi","Kunvar Thaman","Qijia Chen","Tran Quoc Khánh","Jacob Loader","Stefano Cavalleri","Hannah Szlyk","Zachary Brown","Himanshu Narayan","Jonathan Roberts","William Alley","Kunyang Sun","Ryan Stendall","Max Lamparth","Anka Reuel","Ting Wang","Hanmeng Xu","Pablo Hernández-Cámara","Freddie Martin","Thomas Preu","Tomek Korbak","Marcus Abramovitch","Dominic Williamson","Ida Bosio","Ziye Chen","Biró Bálint","Eve J. Y. Lo","Maria Inês S. Nunes","Yibo Jiang","M Saiful Bari","Peyman Kassani","Zihao Wang","Behzad Ansarinejad","Yewen Sun","Stephane Durand","Guillaume Douville","Daniel Tordera","George Balabanian","Earth Anderson","Lynna Kvistad","Alejandro José Moyano","Hsiaoyun Milliron","Ahmad Sakor","Murat Eron","Isaac C. McAlister","Andrew Favre D. O.","Shailesh Shah","Xiaoxiang Zhou","Firuz Kamalov","Ronald Clark","Sherwin Abdoli","Tim Santens","Harrison K Wang","Evan Chen","Alessandro Tomasiello","G. Bruno De Luca","Shi-Zhuo Looi","Vinh-Kha Le","Noam Kolt","Niels Mündler","Avi Semler","Emma Rodman","Jacob Drori","Carl J Fossum","Luk Gloor","Milind Jagota","Ronak Pradeep","Honglu Fan","Tej Shah","Jonathan Eicher","Michael Chen","Kushal Thaman","William Merrill","Moritz Firsching","Carter Harris","Stefan Ciobâcă","Jason Gross","Rohan Pandey","Ilya Gusev","Adam Jones","Shashank Agnihotri","Pavel Zhelnov","Siranut Usawasutsakorn","Mohammadreza Mofayezi","Alexander Piperski","Marc Carauleanu","David K. Zhang","Kostiantyn Dobarskyi","Dylan Ler","Roman Leventov","Ignat Soroko","Thorben Jansen","Scott Creighton","Pascal Lauer","Joshua Duersch","Vage Taamazyan","Dario Bezzi","Wiktor Morak","Wenjie Ma","William Held","Tran Đuc Huy","Ruicheng Xian","Armel Randy Zebaze","Mohanad Mohamed","Julian Noah Leser","Michelle X Yuan","Laila Yacar","Johannes Lengler","Katarzyna Olszewska","Hossein Shahrtash","Edson Oliveira","Joseph W. Jackson","Daniel Espinosa Gonzalez","Andy Zou","Muthu Chidambaram","Timothy Manik","Hector Haffenden","Dashiell Stander","Ali Dasouqi","Alexander Shen","Emilien Duc","Bita Golshani","David Stap","Mikalai Uzhou","Alina Borisovna Zhidkovskaya","Lukas Lewark","Miguel Orbegozo Rodriguez","Mátyás Vincze","Dustin Wehr","Colin Tang","Shaun Phillips","Fortuna Samuele","Jiang Muzhen","Fredrik Ekström","Angela Hammon","Oam Patel","Faraz Farhidi","George Medley","Forough Mohammadzadeh","Madellene Peñaflor","Haile Kassahun","Alena Friedrich","Claire Sparrow","Rayner Hernandez Perez","Taom Sakal","Omkar Dhamane","Ali Khajegili Mirabadi","Eric Hallman","Kenchi Okutsu","Mike Battaglia","Mohammad Maghsoudimehrabani","Alon Amit","Dave Hulbert","Roberto Pereira","Simon Weber"," Handoko","Anton Peristyy","Stephen Malina","Samuel Albanie","Will Cai","Mustafa Mehkary","Rami Aly","Frank Reidegeld","Anna-Katharina Dick","Cary Friday","Jasdeep Sidhu","Hassan Shapourian","Wanyoung Kim","Mariana Costa","Hubeyb Gurdogan","Brian Weber","Harsh Kumar","Tong Jiang","Arunim Agarwal","Chiara Ceconello","Warren S. Vaz","Chao Zhuang","Haon Park","Andrew R. Tawfeek","Daattavya Aggarwal","Michael Kirchhof","Linjie Dai","Evan Kim","Johan Ferret","Yuzhou Wang","Minghao Yan","Krzysztof Burdzy","Lixin Zhang","Antonio Franca","Diana T. Pham","Kang Yong Loh","Joshua Robinson","Abram Jackson","Shreen Gul","Gunjan Chhablani","Zhehang Du","Adrian Cosma","Jesus Colino","Colin White","Jacob Votava","Vladimir Vinnikov","Ethan Delaney","Petr Spelda","Vit Stritecky","Syed M. Shahid","Jean-Christophe Mourrat","Lavr Vetoshkin","Koen Sponselee","Renas Bacho","Florencia de la Rosa","Xiuyu Li","Guillaume Malod","Leon Lang","Julien Laurendeau","Dmitry Kazakov","Fatimah Adesanya","Julien Portier","Lawrence Hollom","Victor Souza","Yuchen Anna Zhou","Julien Degorre","Yiğit Yalın","Gbenga Daniel Obikoya","Luca Arnaboldi"," Rai","Filippo Bigi","M. C. Boscá","Oleg Shumar","Kaniuar Bacho","Pierre Clavier","Gabriel Recchia","Mara Popescu","Nikita Shulga","Ngefor Mildred Tanwie","Denis Peskoff","Thomas C. H. Lux","Ben Rank","Colin Ni","Matthew Brooks","Alesia Yakimchyk"," Huanxu"," Liu","Olle Häggström","Emil Verkama","Hans Gundlach","Leonor Brito-Santana","Brian Amaro","Vivek Vajipey","Rynaa Grover","Yiyang Fan","Gabriel Poesia Reis e Silva","Linwei Xin","Yosi Kratish","Jakub Łucki","Wen-Ding Li","Sivakanth Gopi","Andrea Caciolai","Justin Xu","Kevin Joseph Scaria","Freddie Vargus","Farzad Habibi"," Long"," Lian","Emanuele Rodolà","Jules Robins","Vincent Cheng","Tony Fruhauff","Brad Raynor","Hao Qi","Xi Jiang","Ben Segev","Jingxuan Fan","Sarah Martinson","Erik Y. Wang","Kaylie Hausknecht","Michael P. Brenner","Mao Mao","Xinyu Zhang","David Avagian","Eshawn Jessica Scipio","Alon Ragoler","Justin Tan","Blake Sims","Rebeka Plecnik","Aaron Kirtland","Omer Faruk Bodur","D. P. Shinde","Zahra Adoul","Mohamed Zekry","Ali Karakoc","Tania C. B. Santos","Samir Shamseldeen","Loukmane Karim","Anna Liakhovitskaia","Nate Resman","Nicholas Farina","Juan Carlos Gonzalez","Gabe Maayan","Sarah Hoback","Rodrigo De Oliveira Pena","Glen Sherman","Elizabeth Kelley","Hodjat Mariji","Rasoul Pouriamanesh","Wentao Wu","Sandra Mendoza","Ismail Alarab","Joshua Cole","Danyelle Ferreira","Bryan Johnson","Mohammad Safdari","Liangti Dai","Siriphan Arthornthurasuk","Alexey Pronin","Jing Fan","Angel Ramirez-Trinidad","Ashley Cartwright","Daphiny Pottmaier","Omid Taheri","David Outevsky","Stanley Stepanic","Samuel Perry","Luke Askew","Raúl Adrián Huerta Rodríguez","Ali M. R. Minissi","Sam Ali","Ricardo Lorena","Krishnamurthy Iyer","Arshad Anil Fasiludeen","Sk Md Salauddin","Murat Islam","Juan Gonzalez","Josh Ducey","Maja Somrak","Vasilios Mavroudis","Eric Vergo","Juehang Qin","Benjámin Borbás","Eric Chu","Jack Lindsey","Anil Radhakrishnan","Antoine Jallon","I. M. J. McInnis","Pawan Kumar","Laxman Prasad Goswami","Daniel Bugas","Nasser Heydari","Ferenc Jeanplong","Archimedes Apronti","Abdallah Galal","Ng Ze-An","Ankit Singh","Joan of Arc Xavier","Kanu Priya Agarwal","Mohammed Berkani","Benedito Alves de Oliveira Junior","Dmitry Malishev","Nicolas Remy","Taylor D. Hartman","Tim Tarver","Stephen Mensah","Javier Gimenez","Roselynn Grace Montecillo","Russell Campbell","Asankhaya Sharma","Khalida Meer","Xavier Alapont","Deepakkumar Patil","Rajat Maheshwari","Abdelkader Dendane","Priti Shukla","Sergei Bogdanov","Sören Möller","Muhammad Rehan Siddiqi","Prajvi Saxena","Himanshu Gupta","Innocent Enyekwe","Ragavendran P V","Zienab EL-Wasif","Aleksandr Maksapetyan","Vivien Rossbach","Chris Harjadi","Mohsen Bahaloohoreh","Song Bian","John Lai","Justine Leon Uro","Greg Bateman","Mohamed Sayed","Ahmed Menshawy","Darling Duclosel","Yashaswini Jain","Ashley Aaron","Murat Tiryakioglu","Sheeshram Siddh","Keith Krenek","Alex Hoover","Joseph McGowan","Tejal Patwardhan","Summer Yue","Alexandr Wang","Dan Hendrycks"],"pdf_url":"https://arxiv.org/pdf/2501.14249v5.pdf","comment":"27 pages, 6 figures"},{"id":"http://arxiv.org/abs/2502.15120v1","updated":"2025-02-21T00:48:32Z","published":"2025-02-21T00:48:32Z","title":"Unveiling Reasoning Thresholds in Language Models: Scaling, Fine-Tuning,\n  and Interpretability through Attention Maps","summary":"  This study investigates the in-context learning capabilities of various\ndecoder-only transformer-based language models with different model sizes and\ntraining data, including GPT2, SmolLM2, OpenELM, TinyLlama, Stable LM, and\nGemma 2. We identify a critical parameter threshold (~1.6 billion), beyond\nwhich reasoning performance improves significantly in tasks such as commonsense\nreasoning in multiple-choice question answering and deductive reasoning.\nSpecifically, models above this threshold achieve better success rates in\nchain-of-thought (CoT) prompting for deductive reasoning tasks, especially\nthose requiring longer reasoning chains, such as proof by contradiction and\ndisjunction elimination. To address limitations in sub-threshold models, we\ndemonstrate that fine-tuning with task-specific exemplars substantially\nenhances reasoning performance, enabling accurate CoT generation even without\nadditional exemplars in the prompt for tasks with shorter reasoning chains.\nFinally, our analysis of attention maps reveals that models capable of\ngenerating correct CoTs exhibit higher token-level attention scores on\nsubsequent correct tokens and the correct parts of speech, providing\ninterpretability insights into reasoning processes. These findings collectively\nadvance understanding of reasoning capabilities in decoder-only\ntransformer-based models. The code is available at:\nhttps://github.com/AnnonymousForPapers/CoT_Reasoning_Test.\n","authors":["Yen-Che Hsiao","Abhishek Dutta"],"pdf_url":"https://arxiv.org/pdf/2502.15120v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.06583v5","updated":"2025-02-21T00:45:49Z","published":"2024-08-13T02:43:19Z","title":"A Structure-aware Generative Model for Biomedical Event Extraction","summary":"  Biomedical Event Extraction (BEE) is a challenging task that involves\nmodeling complex relationships between fine-grained entities in biomedical\ntext. BEE has traditionally been formulated as a classification problem. With\nrecent advancements in large language models (LLMs), generation-based models\nthat cast event extraction as a sequence generation problem have attracted\nattention in the NLP research community. However, current generative models\noften overlook cross-instance information in complex event structures, such as\nnested and overlapping events, which constitute over 20% of events in benchmark\ndatasets. In this paper, we propose GenBEE, an event structure-aware generative\nmodel that captures complex event structures in biomedical text for biomedical\nevent extraction. GenBEE constructs event prompts that distill knowledge from\nLLMs to incorporate both label semantics and argument dependency relationships.\nIn addition, GenBEE generates prefixes with event structural prompts to\nincorporate structural features to improve the model's overall performance. We\nhave evaluated the proposed GenBEE model on three widely used BEE benchmark\ndatasets, namely MLEE, GE11, and PHEE. Experimental results show that GenBEE\nhas achieved state-of-the-art performance on the MLEE and GE11 datasets, and\nachieved competitive results when compared to the state-of-the-art\nclassification-based models on the PHEE dataset.\n","authors":["Haohan Yuan","Siu Cheung Hui","Haopeng Zhang"],"pdf_url":"https://arxiv.org/pdf/2408.06583v5.pdf","comment":"8 pages, 4 figures, 6 tables"},{"id":"http://arxiv.org/abs/2502.15109v1","updated":"2025-02-21T00:05:40Z","published":"2025-02-21T00:05:40Z","title":"Social Genome: Grounded Social Reasoning Abilities of Multimodal Models","summary":"  Social reasoning abilities are crucial for AI systems to effectively\ninterpret and respond to multimodal human communication and interaction within\nsocial contexts. We introduce Social Genome, the first benchmark for\nfine-grained, grounded social reasoning abilities of multimodal models. Social\nGenome contains 272 videos of interactions and 1,486 human-annotated reasoning\ntraces related to inferences about these interactions. These traces contain\n5,777 reasoning steps that reference evidence from visual cues, verbal cues,\nvocal cues, and external knowledge (contextual knowledge external to videos).\nSocial Genome is also the first modeling challenge to study external knowledge\nin social reasoning. Social Genome computes metrics to holistically evaluate\nsemantic and structural qualities of model-generated social reasoning traces.\nWe demonstrate the utility of Social Genome through experiments with\nstate-of-the-art models, identifying performance gaps and opportunities for\nfuture research to improve the grounded social reasoning abilities of\nmultimodal models.\n","authors":["Leena Mathur","Marian Qian","Paul Pu Liang","Louis-Philippe Morency"],"pdf_url":"https://arxiv.org/pdf/2502.15109v1.pdf","comment":"Under Review, 22 pages"}],"Computer Vision and Pattern Recognition":[{"id":"http://arxiv.org/abs/2502.15682v1","updated":"2025-02-21T18:59:57Z","published":"2025-02-21T18:59:57Z","title":"ELIP: Enhanced Visual-Language Foundation Models for Image Retrieval","summary":"  The objective in this paper is to improve the performance of text-to-image\nretrieval. To this end, we introduce a new framework that can boost the\nperformance of large-scale pre-trained vision-language models, so that they can\nbe used for text-to-image re-ranking. The approach, Enhanced Language-Image\nPre-training (ELIP), uses the text query to predict a set of visual prompts to\ncondition the ViT image encoding. ELIP can easily be applied to the commonly\nused CLIP/SigLIP and the state-of-the-art BLIP-2 architectures. To train the\narchitecture with limited computing resources, we develop a 'student friendly'\nbest practice involving global hard sample mining, and selection and curation\nof a large-scale dataset. On the evaluation side, we set up two new\nout-of-distribution benchmarks, Occluded COCO and ImageNet-R, to assess the\nzero-shot generalisation of the models to different domains. Benefiting from\nthe novel architecture and data curation, experiments show our enhanced network\nsignificantly boosts CLIP/SigLIP performance and outperforms the\nstate-of-the-art BLIP-2 model on text-to-image retrieval.\n","authors":["Guanqi Zhan","Yuanpei Liu","Kai Han","Weidi Xie","Andrew Zisserman"],"pdf_url":"https://arxiv.org/pdf/2502.15682v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.04459v2","updated":"2025-02-21T18:59:52Z","published":"2024-12-05T18:59:11Z","title":"Sparse Voxels Rasterization: Real-time High-fidelity Radiance Field\n  Rendering","summary":"  We propose an efficient radiance field rendering algorithm that incorporates\na rasterization process on adaptive sparse voxels without neural networks or 3D\nGaussians. There are two key contributions coupled with the proposed system.\nThe first is to adaptively and explicitly allocate sparse voxels to different\nlevels of detail within scenes, faithfully reproducing scene details with\n$65536^3$ grid resolution while achieving high rendering frame rates. Second,\nwe customize a rasterizer for efficient adaptive sparse voxels rendering. We\nrender voxels in the correct depth order by using ray direction-dependent\nMorton ordering, which avoids the well-known popping artifact found in Gaussian\nsplatting. Our method improves the previous neural-free voxel model by over 4db\nPSNR and more than 10x FPS speedup, achieving state-of-the-art comparable\nnovel-view synthesis results. Additionally, our voxel representation is\nseamlessly compatible with grid-based 3D processing techniques such as Volume\nFusion, Voxel Pooling, and Marching Cubes, enabling a wide range of future\nextensions and applications.\n","authors":["Cheng Sun","Jaesung Choe","Charles Loop","Wei-Chiu Ma","Yu-Chiang Frank Wang"],"pdf_url":"https://arxiv.org/pdf/2412.04459v2.pdf","comment":"Project page at https://svraster.github.io/ Code at\n  https://github.com/NVlabs/svraster"},{"id":"http://arxiv.org/abs/2502.15681v1","updated":"2025-02-21T18:59:20Z","published":"2025-02-21T18:59:20Z","title":"One-step Diffusion Models with $f$-Divergence Distribution Matching","summary":"  Sampling from diffusion models involves a slow iterative process that hinders\ntheir practical deployment, especially for interactive applications. To\naccelerate generation speed, recent approaches distill a multi-step diffusion\nmodel into a single-step student generator via variational score distillation,\nwhich matches the distribution of samples generated by the student to the\nteacher's distribution. However, these approaches use the reverse\nKullback-Leibler (KL) divergence for distribution matching which is known to be\nmode seeking. In this paper, we generalize the distribution matching approach\nusing a novel $f$-divergence minimization framework, termed $f$-distill, that\ncovers different divergences with different trade-offs in terms of mode\ncoverage and training variance. We derive the gradient of the $f$-divergence\nbetween the teacher and student distributions and show that it is expressed as\nthe product of their score differences and a weighting function determined by\ntheir density ratio. This weighting function naturally emphasizes samples with\nhigher density in the teacher distribution, when using a less mode-seeking\ndivergence. We observe that the popular variational score distillation approach\nusing the reverse-KL divergence is a special case within our framework.\nEmpirically, we demonstrate that alternative $f$-divergences, such as\nforward-KL and Jensen-Shannon divergences, outperform the current best\nvariational score distillation methods across image generation tasks. In\nparticular, when using Jensen-Shannon divergence, $f$-distill achieves current\nstate-of-the-art one-step generation performance on ImageNet64 and zero-shot\ntext-to-image generation on MS-COCO. Project page:\nhttps://research.nvidia.com/labs/genair/f-distill\n","authors":["Yilun Xu","Weili Nie","Arash Vahdat"],"pdf_url":"https://arxiv.org/pdf/2502.15681v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.15679v1","updated":"2025-02-21T18:58:57Z","published":"2025-02-21T18:58:57Z","title":"BOSS: Benchmark for Observation Space Shift in Long-Horizon Task","summary":"  Robotics has long sought to develop visual-servoing robots capable of\ncompleting previously unseen long-horizon tasks. Hierarchical approaches offer\na pathway for achieving this goal by executing skill combinations arranged by a\ntask planner, with each visuomotor skill pre-trained using a specific imitation\nlearning (IL) algorithm. However, even in simple long-horizon tasks like skill\nchaining, hierarchical approaches often struggle due to a problem we identify\nas Observation Space Shift (OSS), where the sequential execution of preceding\nskills causes shifts in the observation space, disrupting the performance of\nsubsequent individually trained skill policies. To validate OSS and evaluate\nits impact on long-horizon tasks, we introduce BOSS (a Benchmark for\nObservation Space Shift). BOSS comprises three distinct challenges: \"Single\nPredicate Shift\", \"Accumulated Predicate Shift\", and \"Skill Chaining\", each\ndesigned to assess a different aspect of OSS's negative effect. We evaluated\nseveral recent popular IL algorithms on BOSS, including three Behavioral\nCloning methods and the Visual Language Action model OpenVLA. Even on the\nsimplest challenge, we observed average performance drops of 67%, 35%, 34%, and\n54%, respectively, when comparing skill performance with and without OSS.\nAdditionally, we investigate a potential solution to OSS that scales up the\ntraining data for each skill with a larger and more visually diverse set of\ndemonstrations, with our results showing it is not sufficient to resolve OSS.\nThe project page is: https://boss-benchmark.github.io/\n","authors":["Yue Yang","Linfeng Zhao","Mingyu Ding","Gedas Bertasius","Daniel Szafir"],"pdf_url":"https://arxiv.org/pdf/2502.15679v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.15672v1","updated":"2025-02-21T18:56:02Z","published":"2025-02-21T18:56:02Z","title":"VaViM and VaVAM: Autonomous Driving through Video Generative Modeling","summary":"  We explore the potential of large-scale generative video models for\nautonomous driving, introducing an open-source auto-regressive video model\n(VaViM) and its companion video-action model (VaVAM) to investigate how video\npre-training transfers to real-world driving. VaViM is a simple auto-regressive\nvideo model that predicts frames using spatio-temporal token sequences. We show\nthat it captures the semantics and dynamics of driving scenes. VaVAM, the\nvideo-action model, leverages the learned representations of VaViM to generate\ndriving trajectories through imitation learning. Together, the models form a\ncomplete perception-to-action pipeline. We evaluate our models in open- and\nclosed-loop driving scenarios, revealing that video-based pre-training holds\npromise for autonomous driving. Key insights include the semantic richness of\nthe learned representations, the benefits of scaling for video synthesis, and\nthe complex relationship between model size, data, and safety metrics in\nclosed-loop evaluations. We release code and model weights at\nhttps://github.com/valeoai/VideoActionModel\n","authors":["Florent Bartoccioni","Elias Ramzi","Victor Besnier","Shashanka Venkataramanan","Tuan-Hung Vu","Yihong Xu","Loick Chambon","Spyros Gidaris","Serkan Odabas","David Hurych","Renaud Marlet","Alexandre Boulch","Mickael Chen","Éloi Zablocki","Andrei Bursuc","Eduardo Valle","Matthieu Cord"],"pdf_url":"https://arxiv.org/pdf/2502.15672v1.pdf","comment":"Code and model: https://github.com/valeoai/VideoActionModel, project\n  page: https://valeoai.github.io/vavim-vavam/"},{"id":"http://arxiv.org/abs/2502.15648v1","updated":"2025-02-21T18:15:11Z","published":"2025-02-21T18:15:11Z","title":"Logit Disagreement: OoD Detection with Bayesian Neural Networks","summary":"  Bayesian neural networks (BNNs), which estimate the full posterior\ndistribution over model parameters, are well-known for their role in\nuncertainty quantification and its promising application in out-of-distribution\ndetection (OoD). Amongst other uncertainty measures, BNNs provide a\nstate-of-the art estimation of predictive entropy (total uncertainty) which can\nbe decomposed as the sum of mutual information and expected entropy. In the\ncontext of OoD detection the estimation of predictive uncertainty in the form\nof the predictive entropy score confounds aleatoric and epistemic uncertainty,\nthe latter being hypothesized to be high for OoD points. Despite these\njustifications, the mutual information score has been shown to perform worse\nthan predictive entropy. Taking inspiration from Bayesian variational\nautoencoder (BVAE) literature, this work proposes to measure the disagreement\nbetween a corrected version of the pre-softmax quantities, otherwise known as\nlogits, as an estimate of epistemic uncertainty for Bayesian NNs under mean\nfield variational inference. The three proposed epistemic uncertainty scores\ndemonstrate marked improvements over mutual information on a range of OoD\nexperiments, with equal performance otherwise. Moreover, the epistemic\nuncertainty scores perform on par with the Bayesian benchmark predictive\nentropy on a range of MNIST and CIFAR10 experiments.\n","authors":["Kevin Raina"],"pdf_url":"https://arxiv.org/pdf/2502.15648v1.pdf","comment":"Presented at ECCV 2024 Workshop: 3rd Workshop on Uncertainty\n  Quantification for Computer Vision"},{"id":"http://arxiv.org/abs/2502.15633v1","updated":"2025-02-21T18:02:31Z","published":"2025-02-21T18:02:31Z","title":"RGB-Only Gaussian Splatting SLAM for Unbounded Outdoor Scenes","summary":"  3D Gaussian Splatting (3DGS) has become a popular solution in SLAM, as it can\nproduce high-fidelity novel views. However, previous GS-based methods primarily\ntarget indoor scenes and rely on RGB-D sensors or pre-trained depth estimation\nmodels, hence underperforming in outdoor scenarios. To address this issue, we\npropose a RGB-only gaussian splatting SLAM method for unbounded outdoor\nscenes--OpenGS-SLAM. Technically, we first employ a pointmap regression network\nto generate consistent pointmaps between frames for pose estimation. Compared\nto commonly used depth maps, pointmaps include spatial relationships and scene\ngeometry across multiple views, enabling robust camera pose estimation. Then,\nwe propose integrating the estimated camera poses with 3DGS rendering as an\nend-to-end differentiable pipeline. Our method achieves simultaneous\noptimization of camera poses and 3DGS scene parameters, significantly enhancing\nsystem tracking accuracy. Specifically, we also design an adaptive scale mapper\nfor the pointmap regression network, which provides more accurate pointmap\nmapping to the 3DGS map representation. Our experiments on the Waymo dataset\ndemonstrate that OpenGS-SLAM reduces tracking error to 9.8\\% of previous 3DGS\nmethods, and achieves state-of-the-art results in novel view synthesis. Project\nPage: https://3dagentworld.github.io/opengs-slam/\n","authors":["Sicheng Yu","Chong Cheng","Yifan Zhou","Xiaojun Yang","Hao Wang"],"pdf_url":"https://arxiv.org/pdf/2502.15633v1.pdf","comment":"ICRA 2025"},{"id":"http://arxiv.org/abs/2502.15632v1","updated":"2025-02-21T18:00:40Z","published":"2025-02-21T18:00:40Z","title":"Continual Person Identification using Footstep-Induced Floor Vibrations\n  on Heterogeneous Floor Structures","summary":"  Person identification is important for smart buildings to provide\npersonalized services such as health monitoring, activity tracking, and\npersonnel management. However, previous person identification relies on\npre-collected data from everyone, which is impractical in many buildings and\npublic facilities in which visitors are typically expected. This calls for a\ncontinual person identification system that gradually learns people's\nidentities on the fly. Existing studies use cameras to achieve this goal, but\nthey require direct line-of-sight and also have raised privacy concerns in\npublic. Other modalities such as wearables and pressure mats are limited by the\nrequirement of device-carrying or dense deployment. Thus, prior studies\nintroduced footstep-induced structural vibration sensing, which is\nnon-intrusive and perceived as more privacy-friendly. However, this approach\nhas a significant challenge: the high variability of vibration data due to\nstructural heterogeneity and human gait variations, which makes online person\nidentification algorithms perform poorly. In this paper, we characterize the\nvariability in footstep-induced structural vibration data for accurate online\nperson identification. To achieve this, we quantify and decompose different\nsources of variability and then design a feature transformation function to\nreduce the variability within each person's data to make different people's\ndata more separable. We evaluate our approach through field experiments with 20\npeople. The results show a 70% variability reduction and a 90% accuracy for\nonline person identification.\n","authors":["Yiwen Dong","Hae Young Noh"],"pdf_url":"https://arxiv.org/pdf/2502.15632v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.13514v2","updated":"2025-02-21T17:51:09Z","published":"2025-01-23T10:01:33Z","title":"Self-Supervised Diffusion MRI Denoising via Iterative and Stable\n  Refinement","summary":"  Magnetic Resonance Imaging (MRI), including diffusion MRI (dMRI), serves as a\n``microscope'' for anatomical structures and routinely mitigates the influence\nof low signal-to-noise ratio scans by compromising temporal or spatial\nresolution. However, these compromises fail to meet clinical demands for both\nefficiency and precision. Consequently, denoising is a vital preprocessing\nstep, particularly for dMRI, where clean data is unavailable. In this paper, we\nintroduce Di-Fusion, a fully self-supervised denoising method that leverages\nthe latter diffusion steps and an adaptive sampling process. Unlike previous\napproaches, our single-stage framework achieves efficient and stable training\nwithout extra noise model training and offers adaptive and controllable results\nin the sampling process. Our thorough experiments on real and simulated data\ndemonstrate that Di-Fusion achieves state-of-the-art performance in\nmicrostructure modeling, tractography tracking, and other downstream tasks.\nCode is available at https://github.com/FouierL/Di-Fusion.\n","authors":["Chenxu Wu","Qingpeng Kong","Zihang Jiang","S. Kevin Zhou"],"pdf_url":"https://arxiv.org/pdf/2501.13514v2.pdf","comment":"39pages, 34figures"},{"id":"http://arxiv.org/abs/2502.09838v3","updated":"2025-02-21T17:39:29Z","published":"2025-02-14T00:42:36Z","title":"HealthGPT: A Medical Large Vision-Language Model for Unifying\n  Comprehension and Generation via Heterogeneous Knowledge Adaptation","summary":"  We present HealthGPT, a powerful Medical Large Vision-Language Model\n(Med-LVLM) that integrates medical visual comprehension and generation\ncapabilities within a unified autoregressive paradigm. Our bootstrapping\nphilosophy is to progressively adapt heterogeneous comprehension and generation\nknowledge to pre-trained large language models (LLMs). This is achieved through\na novel heterogeneous low-rank adaptation (H-LoRA) technique, which is\ncomplemented by a tailored hierarchical visual perception approach and a\nthree-stage learning strategy. To effectively learn the HealthGPT, we devise a\ncomprehensive medical domain-specific comprehension and generation dataset\ncalled VL-Health. Experimental results demonstrate exceptional performance and\nscalability of HealthGPT in medical visual unified tasks. Our project can be\naccessed at https://github.com/DCDmllm/HealthGPT.\n","authors":["Tianwei Lin","Wenqiao Zhang","Sijing Li","Yuqian Yuan","Binhe Yu","Haoyuan Li","Wanggui He","Hao Jiang","Mengze Li","Xiaohui Song","Siliang Tang","Jun Xiao","Hui Lin","Yueting Zhuang","Beng Chin Ooi"],"pdf_url":"https://arxiv.org/pdf/2502.09838v3.pdf","comment":"Comments: added project page"},{"id":"http://arxiv.org/abs/2502.15601v1","updated":"2025-02-21T17:18:30Z","published":"2025-02-21T17:18:30Z","title":"WorldCraft: Photo-Realistic 3D World Creation and Customization via LLM\n  Agents","summary":"  Constructing photorealistic virtual worlds has applications across various\nfields, but it often requires the extensive labor of highly trained\nprofessionals to operate conventional 3D modeling software. To democratize this\nprocess, we introduce WorldCraft, a system where large language model (LLM)\nagents leverage procedural generation to create indoor and outdoor scenes\npopulated with objects, allowing users to control individual object attributes\nand the scene layout using intuitive natural language commands. In our\nframework, a coordinator agent manages the overall process and works with two\nspecialized LLM agents to complete the scene creation: ForgeIt, which\nintegrates an ever-growing manual through auto-verification to enable precise\ncustomization of individual objects, and ArrangeIt, which formulates\nhierarchical optimization problems to achieve a layout that balances ergonomic\nand aesthetic considerations. Additionally, our pipeline incorporates a\ntrajectory control agent, allowing users to animate the scene and operate the\ncamera through natural language interactions. Our system is also compatible\nwith off-the-shelf deep 3D generators to enrich scene assets. Through\nevaluations and comparisons with state-of-the-art methods, we demonstrate the\nversatility of WorldCraft, ranging from single-object customization to\nintricate, large-scale interior and exterior scene designs. This system\nempowers non-professionals to bring their creative visions to life.\n","authors":["Xinhang Liu","Chi-Keung Tang","Yu-Wing Tai"],"pdf_url":"https://arxiv.org/pdf/2502.15601v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.21121v2","updated":"2025-02-21T17:05:11Z","published":"2024-07-30T18:24:46Z","title":"Tuning the Frequencies: Robust Training for Sinusoidal Neural Networks","summary":"  Sinusoidal neural networks have been shown effective as implicit neural\nrepresentations (INRs) of low-dimensional signals, due to their smoothness and\nhigh representation capacity. However, initializing and training them remain\nempirical tasks which lack on deeper understanding to guide the learning\nprocess. To fill this gap, our work introduces a theoretical framework that\nexplains the capacity property of sinusoidal networks and offers robust control\nmechanisms for initialization and training. Our analysis is based on a novel\namplitude-phase expansion of the sinusoidal multilayer perceptron, showing how\nits layer compositions produce a large number of new frequencies expressed as\ninteger combinations of the input frequencies. This relationship can be\ndirectly used to initialize the input neurons, as a form of spectral sampling,\nand to bound the network's spectrum while training. Our method, referred to as\nTUNER (TUNing sinusoidal nEtwoRks), greatly improves the stability and\nconvergence of sinusoidal INR training, leading to detailed reconstructions,\nwhile preventing overfitting.\n","authors":["Tiago Novello","Diana Aldana","Andre Araujo","Luiz Velho"],"pdf_url":"https://arxiv.org/pdf/2407.21121v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.04385v2","updated":"2025-02-21T16:39:21Z","published":"2025-02-05T19:41:06Z","title":"TexLiDAR: Automated Text Understanding for Panoramic LiDAR Data","summary":"  Efforts to connect LiDAR data with text, such as LidarCLIP, have primarily\nfocused on embedding 3D point clouds into CLIP text-image space. However, these\napproaches rely on 3D point clouds, which present challenges in encoding\nefficiency and neural network processing. With the advent of advanced LiDAR\nsensors like Ouster OS1, which, in addition to 3D point clouds, produce fixed\nresolution depth, signal, and ambient panoramic 2D images, new opportunities\nemerge for LiDAR based tasks. In this work, we propose an alternative approach\nto connect LiDAR data with text by leveraging 2D imagery generated by the OS1\nsensor instead of 3D point clouds. Using the Florence 2 large model in a\nzero-shot setting, we perform image captioning and object detection. Our\nexperiments demonstrate that Florence 2 generates more informative captions and\nachieves superior performance in object detection tasks compared to existing\nmethods like CLIP. By combining advanced LiDAR sensor data with a large\npre-trained model, our approach provides a robust and accurate solution for\nchallenging detection scenarios, including real-time applications requiring\nhigh accuracy and robustness.\n","authors":["Naor Cohen","Roy Orfaig","Ben-Zion Bobrovsky"],"pdf_url":"https://arxiv.org/pdf/2502.04385v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.15563v1","updated":"2025-02-21T16:24:10Z","published":"2025-02-21T16:24:10Z","title":"Bridging vision language model (VLM) evaluation gaps with a framework\n  for scalable and cost-effective benchmark generation","summary":"  Reliable evaluation of AI models is critical for scientific progress and\npractical application. While existing VLM benchmarks provide general insights\ninto model capabilities, their heterogeneous designs and limited focus on a few\nimaging domains pose significant challenges for both cross-domain performance\ncomparison and targeted domain-specific evaluation. To address this, we propose\nthree key contributions: (1) a framework for the resource-efficient creation of\ndomain-specific VLM benchmarks enabled by task augmentation for creating\nmultiple diverse tasks from a single existing task, (2) the release of new VLM\nbenchmarks for seven domains, created according to the same homogeneous\nprotocol and including 162,946 thoroughly human-validated answers, and (3) an\nextensive benchmarking of 22 state-of-the-art VLMs on a total of 37,171 tasks,\nrevealing performance variances across domains and tasks, thereby supporting\nthe need for tailored VLM benchmarks. Adoption of our methodology will pave the\nway for the resource-efficient domain-specific selection of models and guide\nfuture research efforts toward addressing core open questions.\n","authors":["Tim Rädsch","Leon Mayer","Simon Pavicic","A. Emre Kavur","Marcel Knopp","Barış Öztürk","Klaus Maier-Hein","Paul F. Jaeger","Fabian Isensee","Annika Reinke","Lena Maier-Hein"],"pdf_url":"https://arxiv.org/pdf/2502.15563v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.03765v2","updated":"2025-02-21T16:12:29Z","published":"2024-08-19T22:45:46Z","title":"AI and Entrepreneurship: Facial Recognition Technology Detects\n  Entrepreneurs, Outperforming Human Experts","summary":"  Occupational outcomes like entrepreneurship are generally considered personal\ninformation that individuals should have the autonomy to disclose. With the\nadvancing capability of artificial intelligence (AI) to infer private details\nfrom widely available human-centric data (e.g., social media), it is crucial to\ninvestigate whether AI can accurately extract private occupational information\nfrom such data. In this study, we demonstrate that deep neural networks can\nclassify individuals as entrepreneurs with high accuracy based on facial images\nsourced from Crunchbase, a premier source for entrepreneurship data. Utilizing\na dataset comprising facial images of 40,728 individuals, including both\nentrepreneurs and non-entrepreneurs, we train a Convolutional Neural Network\n(CNN) using a contrastive learning approach based on pairs of facial images\n(one entrepreneur and one non-entrepreneur per pair). While human experts\n(n=650) and trained participants (n=133) were unable to classify entrepreneurs\nwith accuracy above chance levels (>50%), our AI model achieved a\nclassification accuracy of 79.51%. Several robustness tests indicate that this\nhigh level of accuracy is maintained under various conditions. These results\nindicate privacy risks for entrepreneurs.\n","authors":["Martin Obschonka","Christian Fisch","Tharindu Fernando","Clinton Fookes"],"pdf_url":"https://arxiv.org/pdf/2409.03765v2.pdf","comment":"46 pages, 2 tables, 11 figures"},{"id":"http://arxiv.org/abs/2408.13252v2","updated":"2025-02-21T16:06:37Z","published":"2024-08-23T17:50:23Z","title":"LayerPano3D: Layered 3D Panorama for Hyper-Immersive Scene Generation","summary":"  3D immersive scene generation is a challenging yet critical task in computer\nvision and graphics. A desired virtual 3D scene should 1) exhibit\nomnidirectional view consistency, and 2) allow for free exploration in complex\nscene hierarchies. Existing methods either rely on successive scene expansion\nvia inpainting or employ panorama representation to represent large FOV scene\nenvironments. However, the generated scene suffers from semantic drift during\nexpansion and is unable to handle occlusion among scene hierarchies. To tackle\nthese challenges, we introduce Layerpano3D, a novel framework for full-view,\nexplorable panoramic 3D scene generation from a single text prompt. Our key\ninsight is to decompose a reference 2D panorama into multiple layers at\ndifferent depth levels, where each layer reveals the unseen space from the\nreference views via diffusion prior. Layerpano3D comprises multiple dedicated\ndesigns: 1) We introduce a new panorama dataset Upright360, comprising 9k\nhigh-quality and upright panorama images, and finetune the advanced Flux model\non Upright360 for high-quality, upright and consistent panorama generation. 2)\nWe pioneer the Layered 3D Panorama as underlying representation to manage\ncomplex scene hierarchies and lift it into 3D Gaussians to splat detailed\n360-degree omnidirectional scenes with unconstrained viewing paths. Extensive\nexperiments demonstrate that our framework generates state-of-the-art 3D\npanoramic scene in both full view consistency and immersive exploratory\nexperience. We believe that Layerpano3D holds promise for advancing 3D\npanoramic scene creation with numerous applications.\n","authors":["Shuai Yang","Jing Tan","Mengchen Zhang","Tong Wu","Yixuan Li","Gordon Wetzstein","Ziwei Liu","Dahua Lin"],"pdf_url":"https://arxiv.org/pdf/2408.13252v2.pdf","comment":"Project page: https://ys-imtech.github.io/projects/LayerPano3D/"},{"id":"http://arxiv.org/abs/2502.12080v2","updated":"2025-02-21T16:03:54Z","published":"2025-02-17T17:55:27Z","title":"HumanGif: Single-View Human Diffusion with Generative Prior","summary":"  Previous 3D human creation methods have made significant progress in\nsynthesizing view-consistent and temporally aligned results from sparse-view\nimages or monocular videos. However, it remains challenging to produce\nperpetually realistic, view-consistent, and temporally coherent human avatars\nfrom a single image, as limited information is available in the single-view\ninput setting. Motivated by the success of 2D character animation, we propose\nHumanGif, a single-view human diffusion model with generative prior.\nSpecifically, we formulate the single-view-based 3D human novel view and pose\nsynthesis as a single-view-conditioned human diffusion process, utilizing\ngenerative priors from foundational diffusion models to complement the missing\ninformation. To ensure fine-grained and consistent novel view and pose\nsynthesis, we introduce a Human NeRF module in HumanGif to learn spatially\naligned features from the input image, implicitly capturing the relative camera\nand human pose transformation. Furthermore, we introduce an image-level loss\nduring optimization to bridge the gap between latent and image spaces in\ndiffusion models. Extensive experiments on RenderPeople and DNA-Rendering\ndatasets demonstrate that HumanGif achieves the best perceptual performance,\nwith better generalizability for novel view and pose synthesis.\n","authors":["Shoukang Hu","Takuya Narihira","Kazumi Fukuda","Ryosuke Sawata","Takashi Shibuya","Yuki Mitsufuji"],"pdf_url":"https://arxiv.org/pdf/2502.12080v2.pdf","comment":"Project page: https://skhu101.github.io/HumanGif/"},{"id":"http://arxiv.org/abs/2502.15545v1","updated":"2025-02-21T15:51:49Z","published":"2025-02-21T15:51:49Z","title":"Estimating Vehicle Speed on Roadways Using RNNs and Transformers: A\n  Video-based Approach","summary":"  This project explores the application of advanced machine learning models,\nspecifically Long Short-Term Memory (LSTM), Gated Recurrent Units (GRU), and\nTransformers, to the task of vehicle speed estimation using video data.\nTraditional methods of speed estimation, such as radar and manual systems, are\noften constrained by high costs, limited coverage, and potential disruptions.\nIn contrast, leveraging existing surveillance infrastructure and cutting-edge\nneural network architectures presents a non-intrusive, scalable solution. Our\napproach utilizes LSTM and GRU to effectively manage long-term dependencies\nwithin the temporal sequence of video frames, while Transformers are employed\nto harness their self-attention mechanisms, enabling the processing of entire\nsequences in parallel and focusing on the most informative segments of the\ndata. This study demonstrates that both LSTM and GRU outperform basic Recurrent\nNeural Networks (RNNs) due to their advanced gating mechanisms. Furthermore,\nincreasing the sequence length of input data consistently improves model\naccuracy, highlighting the importance of contextual information in dynamic\nenvironments. Transformers, in particular, show exceptional adaptability and\nrobustness across varied sequence lengths and complexities, making them highly\nsuitable for real-time applications in diverse traffic conditions. The findings\nsuggest that integrating these sophisticated neural network models can\nsignificantly enhance the accuracy and reliability of automated speed detection\nsystems, thus promising to revolutionize traffic management and road safety.\n","authors":["Sai Krishna Reddy Mareddy","Dhanush Upplapati","Dhanush Kumar Antharam"],"pdf_url":"https://arxiv.org/pdf/2502.15545v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.15516v1","updated":"2025-02-21T15:14:30Z","published":"2025-02-21T15:14:30Z","title":"Depth-aware Fusion Method based on Image and 4D Radar Spectrum for 3D\n  Object Detection","summary":"  Safety and reliability are crucial for the public acceptance of autonomous\ndriving. To ensure accurate and reliable environmental perception, intelligent\nvehicles must exhibit accuracy and robustness in various environments.\nMillimeter-wave radar, known for its high penetration capability, can operate\neffectively in adverse weather conditions such as rain, snow, and fog.\nTraditional 3D millimeter-wave radars can only provide range, Doppler, and\nazimuth information for objects. Although the recent emergence of 4D\nmillimeter-wave radars has added elevation resolution, the radar point clouds\nremain sparse due to Constant False Alarm Rate (CFAR) operations. In contrast,\ncameras offer rich semantic details but are sensitive to lighting and weather\nconditions. Hence, this paper leverages these two highly complementary and\ncost-effective sensors, 4D millimeter-wave radar and camera. By integrating 4D\nradar spectra with depth-aware camera images and employing attention\nmechanisms, we fuse texture-rich images with depth-rich radar data in the\nBird's Eye View (BEV) perspective, enhancing 3D object detection. Additionally,\nwe propose using GAN-based networks to generate depth images from radar spectra\nin the absence of depth sensors, further improving detection accuracy.\n","authors":["Yue Sun","Yeqiang Qian","Chunxiang Wang","Ming Yang"],"pdf_url":"https://arxiv.org/pdf/2502.15516v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.05749v3","updated":"2025-02-21T15:01:36Z","published":"2025-02-09T02:43:57Z","title":"UniDB: A Unified Diffusion Bridge Framework via Stochastic Optimal\n  Control","summary":"  Recent advances in diffusion bridge models leverage Doob's $h$-transform to\nestablish fixed endpoints between distributions, demonstrating promising\nresults in image translation and restoration tasks. However, these approaches\nfrequently produce blurred or excessively smoothed image details and lack a\ncomprehensive theoretical foundation to explain these shortcomings. To address\nthese limitations, we propose UniDB, a unified framework for diffusion bridges\nbased on Stochastic Optimal Control (SOC). UniDB formulates the problem through\nan SOC-based optimization and derives a closed-form solution for the optimal\ncontroller, thereby unifying and generalizing existing diffusion bridge models.\nWe demonstrate that existing diffusion bridges employing Doob's $h$-transform\nconstitute a special case of our framework, emerging when the terminal penalty\ncoefficient in the SOC cost function tends to infinity. By incorporating a\ntunable terminal penalty coefficient, UniDB achieves an optimal balance between\ncontrol costs and terminal penalties, substantially improving detail\npreservation and output quality. Notably, UniDB seamlessly integrates with\nexisting diffusion bridge models, requiring only minimal code modifications.\nExtensive experiments across diverse image restoration tasks validate the\nsuperiority and adaptability of the proposed framework. Our code is available\nat https://github.com/UniDB-SOC/UniDB/.\n","authors":["Kaizhen Zhu","Mokai Pan","Yuexin Ma","Yanwei Fu","Jingyi Yu","Jingya Wang","Ye Shi"],"pdf_url":"https://arxiv.org/pdf/2502.05749v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.15488v1","updated":"2025-02-21T14:26:23Z","published":"2025-02-21T14:26:23Z","title":"Q-PETR: Quant-aware Position Embedding Transformation for Multi-View 3D\n  Object Detection","summary":"  PETR-based methods have dominated benchmarks in 3D perception and are\nincreasingly becoming a key component in modern autonomous driving systems.\nHowever, their quantization performance significantly degrades when INT8\ninference is required, with a degradation of 58.2% in mAP and 36.9% in NDS on\nthe NuScenes dataset. To address this issue, we propose a quantization-aware\nposition embedding transformation for multi-view 3D object detection, termed\nQ-PETR. Q-PETR offers a quantizationfriendly and deployment-friendly\narchitecture while preserving the original performance of PETR. It\nsubstantially narrows the accuracy gap between INT8 and FP32 inference for\nPETR-series methods. Without bells and whistles, our approach reduces the mAP\nand NDS drop to within 1% under standard 8-bit per-tensor post-training\nquantization. Furthermore, our method exceeds the performance of the original\nPETR in terms of floating-point precision. Extensive experiments across a\nvariety of PETR-series models demonstrate its broad generalization.\n","authors":["Jiangyong Yu","Changyong Shu","Dawei Yang","Zichen Yu","Xing Hu","Yan Chen"],"pdf_url":"https://arxiv.org/pdf/2502.15488v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.15484v1","updated":"2025-02-21T14:16:06Z","published":"2025-02-21T14:16:06Z","title":"Confidence-Based Annotation Of Brain Tumours In Ultrasound","summary":"  Purpose: An investigation of the challenge of annotating discrete\nsegmentations of brain tumours in ultrasound, with a focus on the issue of\naleatoric uncertainty along the tumour margin, particularly for diffuse\ntumours. A segmentation protocol and method is proposed that incorporates this\nmargin-related uncertainty while minimising the interobserver variance through\nreduced subjectivity, thereby diminishing annotator epistemic uncertainty.\nApproach: A sparse confidence method for annotation is proposed, based on a\nprotocol designed using computer vision and radiology theory. Results: Output\nannotations using the proposed method are compared with the corresponding\nprofessional discrete annotation variance between the observers. A linear\nrelationship was measured within the tumour margin region, with a Pearson\ncorrelation of 0.8. The downstream application was explored, comparing training\nusing confidence annotations as soft labels with using the best discrete\nannotations as hard labels. In all evaluation folds, the Brier score was\nsuperior for the soft-label trained network. Conclusion: A formal framework was\nconstructed to demonstrate the infeasibility of discrete annotation of brain\ntumours in B-mode ultrasound. Subsequently, a method for sparse\nconfidence-based annotation is proposed and evaluated. Keywords: Brain tumours,\nultrasound, confidence, annotation.\n","authors":["Alistair Weld","Luke Dixon","Alfie Roddan","Giulio Anichini","Sophie Camp","Stamatia Giannarou"],"pdf_url":"https://arxiv.org/pdf/2502.15484v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.15480v1","updated":"2025-02-21T14:05:25Z","published":"2025-02-21T14:05:25Z","title":"On Neural BRDFs: A Thorough Comparison of State-of-the-Art Approaches","summary":"  The bidirectional reflectance distribution function (BRDF) is an essential\ntool to capture the complex interaction of light and matter. Recently, several\nworks have employed neural methods for BRDF modeling, following various\nstrategies, ranging from utilizing existing parametric models to purely neural\nparametrizations. While all methods yield impressive results, a comprehensive\ncomparison of the different approaches is missing in the literature. In this\nwork, we present a thorough evaluation of several approaches, including results\nfor qualitative and quantitative reconstruction quality and an analysis of\nreciprocity and energy conservation. Moreover, we propose two extensions that\ncan be added to existing approaches: A novel additive combination strategy for\nneural BRDFs that split the reflectance into a diffuse and a specular part, and\nan input mapping that ensures reciprocity exactly by construction, while\nprevious approaches only ensure it by soft constraints.\n","authors":["Florian Hofherr","Bjoern Haefner","Daniel Cremers"],"pdf_url":"https://arxiv.org/pdf/2502.15480v1.pdf","comment":"Published in IEEE/CVF Winter Conference on Applications of Computer\n  Vision (WACV) 2025"},{"id":"http://arxiv.org/abs/2502.15478v1","updated":"2025-02-21T14:04:30Z","published":"2025-02-21T14:04:30Z","title":"CondiQuant: Condition Number Based Low-Bit Quantization for Image\n  Super-Resolution","summary":"  Low-bit model quantization for image super-resolution (SR) is a longstanding\ntask that is renowned for its surprising compression and acceleration ability.\nHowever, accuracy degradation is inevitable when compressing the full-precision\n(FP) model to ultra-low bit widths (2~4 bits). Experimentally, we observe that\nthe degradation of quantization is mainly attributed to the quantization of\nactivation instead of model weights. In numerical analysis, the condition\nnumber of weights could measure how much the output value can change for a\nsmall change in the input argument, inherently reflecting the quantization\nerror. Therefore, we propose CondiQuant, a condition number based low-bit\npost-training quantization for image super-resolution. Specifically, we\nformulate the quantization error as the condition number of weight metrics. By\ndecoupling the representation ability and the quantization sensitivity, we\ndesign an efficient proximal gradient descent algorithm to iteratively minimize\nthe condition number and maintain the output still. With comprehensive\nexperiments, we demonstrate that CondiQuant outperforms existing\nstate-of-the-art post-training quantization methods in accuracy without\ncomputation overhead and gains the theoretically optimal compression ratio in\nmodel parameters. Our code and model are released at\nhttps://github.com/Kai-Liu001/CondiQuant.\n","authors":["Kai Liu","Dehui Wang","Zhiteng Li","Zheng Chen","Yong Guo","Wenbo Li","Linghe Kong","Yulun Zhang"],"pdf_url":"https://arxiv.org/pdf/2502.15478v1.pdf","comment":"10 pages, 5 figures. Code and models are released at\n  https://github.com/Kai-Liu001/CondiQuant"},{"id":"http://arxiv.org/abs/2502.15472v1","updated":"2025-02-21T13:55:41Z","published":"2025-02-21T13:55:41Z","title":"Aligning Task- and Reconstruction-Oriented Communications for Edge\n  Intelligence","summary":"  Existing communication systems aim to reconstruct the information at the\nreceiver side, and are known as reconstruction-oriented communications. This\napproach often falls short in meeting the real-time, task-specific demands of\nmodern AI-driven applications such as autonomous driving and semantic\nsegmentation. As a new design principle, task-oriented communications have been\ndeveloped. However, it typically requires joint optimization of encoder,\ndecoder, and modified inference neural networks, resulting in extensive\ncross-system redesigns and compatibility issues. This paper proposes a novel\ncommunication framework that aligns reconstruction-oriented and task-oriented\ncommunications for edge intelligence. The idea is to extend the Information\nBottleneck (IB) theory to optimize data transmission by minimizing\ntask-relevant loss function, while maintaining the structure of the original\ndata by an information reshaper. Such an approach integrates task-oriented\ncommunications with reconstruction-oriented communications, where a variational\napproach is designed to handle the intractability of mutual information in\nhigh-dimensional neural network features. We also introduce a joint\nsource-channel coding (JSCC) modulation scheme compatible with classical\nmodulation techniques, enabling the deployment of AI technologies within\nexisting digital infrastructures. The proposed framework is particularly\neffective in edge-based autonomous driving scenarios. Our evaluation in the Car\nLearning to Act (CARLA) simulator demonstrates that the proposed framework\nsignificantly reduces bits per service by 99.19% compared to existing methods,\nsuch as JPEG, JPEG2000, and BPG, without compromising the effectiveness of task\nexecution.\n","authors":["Yufeng Diao","Yichi Zhang","Changyang She","Philip Guodong Zhao","Emma Liying Li"],"pdf_url":"https://arxiv.org/pdf/2502.15472v1.pdf","comment":"Accepted for publication in IEEE Journal on Selected Areas in\n  Communications (JSAC)"},{"id":"http://arxiv.org/abs/2412.09945v2","updated":"2025-02-21T13:50:09Z","published":"2024-12-13T08:10:47Z","title":"Going Beyond Feature Similarity: Effective Dataset distillation based on\n  Class-aware Conditional Mutual Information","summary":"  Dataset distillation (DD) aims to minimize the time and memory consumption\nneeded for training deep neural networks on large datasets, by creating a\nsmaller synthetic dataset that has similar performance to that of the full real\ndataset. However, current dataset distillation methods often result in\nsynthetic datasets that are excessively difficult for networks to learn from,\ndue to the compression of a substantial amount of information from the original\ndata through metrics measuring feature similarity, e,g., distribution matching\n(DM). In this work, we introduce conditional mutual information (CMI) to assess\nthe class-aware complexity of a dataset and propose a novel method by\nminimizing CMI. Specifically, we minimize the distillation loss while\nconstraining the class-aware complexity of the synthetic dataset by minimizing\nits empirical CMI from the feature space of pre-trained networks,\nsimultaneously. Conducting on a thorough set of experiments, we show that our\nmethod can serve as a general regularization method to existing DD methods and\nimprove the performance and training efficiency.\n","authors":["Xinhao Zhong","Bin Chen","Hao Fang","Xulin Gu","Shu-Tao Xia","En-Hui Yang"],"pdf_url":"https://arxiv.org/pdf/2412.09945v2.pdf","comment":"Accepted to ICLR 2025"},{"id":"http://arxiv.org/abs/2502.15462v1","updated":"2025-02-21T13:41:38Z","published":"2025-02-21T13:41:38Z","title":"Game State and Spatio-temporal Action Detection in Soccer using Graph\n  Neural Networks and 3D Convolutional Networks","summary":"  Soccer analytics rely on two data sources: the player positions on the pitch\nand the sequences of events they perform. With around 2000 ball events per\ngame, their precise and exhaustive annotation based on a monocular video stream\nremains a tedious and costly manual task. While state-of-the-art\nspatio-temporal action detection methods show promise for automating this task,\nthey lack contextual understanding of the game. Assuming professional players'\nbehaviors are interdependent, we hypothesize that incorporating surrounding\nplayers' information such as positions, velocity and team membership can\nenhance purely visual predictions. We propose a spatio-temporal action\ndetection approach that combines visual and game state information via Graph\nNeural Networks trained end-to-end with state-of-the-art 3D CNNs, demonstrating\nimproved metrics through game state integration.\n","authors":["Jeremie Ochin","Guillaume Devineau","Bogdan Stanciulescu","Sotiris Manitsaris"],"pdf_url":"https://arxiv.org/pdf/2502.15462v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.15457v1","updated":"2025-02-21T13:31:58Z","published":"2025-02-21T13:31:58Z","title":"Memory Helps, but Confabulation Misleads: Understanding Streaming Events\n  in Videos with MLLMs","summary":"  Multimodal large language models (MLLMs) have demonstrated strong performance\nin understanding videos holistically, yet their ability to process streaming\nvideos-videos are treated as a sequence of visual events-remains underexplored.\nIntuitively, leveraging past events as memory can enrich contextual and\ntemporal understanding of the current event. In this paper, we show that\nleveraging memories as contexts helps MLLMs better understand video events.\nHowever, because such memories rely on predictions of preceding events, they\nmay contain misinformation, leading to confabulation and degraded performance.\nTo address this, we propose a confabulation-aware memory modification method\nthat mitigates confabulated memory for memory-enhanced event understanding.\n","authors":["Gengyuan Zhang","Mingcong Ding","Tong Liu","Yao Zhang","Volker Tresp"],"pdf_url":"https://arxiv.org/pdf/2502.15457v1.pdf","comment":"Short paper (5 pages)"},{"id":"http://arxiv.org/abs/2502.15448v1","updated":"2025-02-21T13:22:29Z","published":"2025-02-21T13:22:29Z","title":"MVIP -- A Dataset and Methods for Application Oriented Multi-View and\n  Multi-Modal Industrial Part Recognition","summary":"  We present MVIP, a novel dataset for multi-modal and multi-view\napplication-oriented industrial part recognition. Here we are the first to\ncombine a calibrated RGBD multi-view dataset with additional object context\nsuch as physical properties, natural language, and super-classes. The current\nportfolio of available datasets offers a wide range of representations to\ndesign and benchmark related methods. In contrast to existing classification\nchallenges, industrial recognition applications offer controlled multi-modal\nenvironments but at the same time have different problems than traditional\n2D/3D classification challenges. Frequently, industrial applications must deal\nwith a small amount or increased number of training data, visually similar\nparts, and varying object sizes, while requiring a robust near 100% top 5\naccuracy under cost and time constraints. Current methods tackle such\nchallenges individually, but direct adoption of these methods within industrial\napplications is complex and requires further research. Our main goal with MVIP\nis to study and push transferability of various state-of-the-art methods within\nrelated downstream tasks towards an efficient deployment of industrial\nclassifiers. Additionally, we intend to push with MVIP research regarding\nseveral modality fusion topics, (automated) synthetic data generation, and\ncomplex data sampling -- combined in a single application-oriented benchmark.\n","authors":["Paul Koch","Marian Schlüter","Jörg Krüger"],"pdf_url":"https://arxiv.org/pdf/2502.15448v1.pdf","comment":"Accepted to IMPROVE 2025"},{"id":"http://arxiv.org/abs/2502.15438v1","updated":"2025-02-21T13:07:45Z","published":"2025-02-21T13:07:45Z","title":"LEAP: Enhancing Vision-Based Occupancy Networks with Lightweight\n  Spatio-Temporal Correlation","summary":"  Vision-based occupancy networks provide an end-to-end solution for\nreconstructing the surrounding environment using semantic occupied voxels\nderived from multi-view images. This technique relies on effectively learning\nthe correlation between pixel-level visual information and voxels. Despite\nrecent advancements, occupancy results still suffer from limited accuracy due\nto occlusions and sparse visual cues. To address this, we propose a Lightweight\nSpatio-Temporal Correlation (LEAP)} method, which significantly enhances the\nperformance of existing occupancy networks with minimal computational overhead.\nLEAP can be seamlessly integrated into various baseline networks, enabling a\nplug-and-play application. LEAP operates in three stages: 1) it tokenizes\ninformation from recent baseline and motion features into a shared, compact\nlatent space; 2) it establishes full correlation through a tri-stream fusion\narchitecture; 3) it generates occupancy results that strengthen the baseline's\noutput. Extensive experiments demonstrate the efficiency and effectiveness of\nour method, outperforming the latest baseline models. The source code and\nseveral demos are available in the supplementary material.\n","authors":["Fengcheng Yu","Haoran Xu","Canming Xia","Guang Tan"],"pdf_url":"https://arxiv.org/pdf/2502.15438v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.01946v3","updated":"2025-02-21T12:56:17Z","published":"2025-02-04T02:41:00Z","title":"HeRCULES: Heterogeneous Radar Dataset in Complex Urban Environment for\n  Multi-session Radar SLAM","summary":"  Recently, radars have been widely featured in robotics for their robustness\nin challenging weather conditions. Two commonly used radar types are spinning\nradars and phased-array radars, each offering distinct sensor characteristics.\nExisting datasets typically feature only a single type of radar, leading to the\ndevelopment of algorithms limited to that specific kind. In this work, we\nhighlight that combining different radar types offers complementary advantages,\nwhich can be leveraged through a heterogeneous radar dataset. Moreover, this\nnew dataset fosters research in multi-session and multi-robot scenarios where\nrobots are equipped with different types of radars. In this context, we\nintroduce the HeRCULES dataset, a comprehensive, multi-modal dataset with\nheterogeneous radars, FMCW LiDAR, IMU, GPS, and cameras. This is the first\ndataset to integrate 4D radar and spinning radar alongside FMCW LiDAR, offering\nunparalleled localization, mapping, and place recognition capabilities. The\ndataset covers diverse weather and lighting conditions and a range of urban\ntraffic scenarios, enabling a comprehensive analysis across various\nenvironments. The sequence paths with multiple revisits and ground truth pose\nfor each sensor enhance its suitability for place recognition research. We\nexpect the HeRCULES dataset to facilitate odometry, mapping, place recognition,\nand sensor fusion research. The dataset and development tools are available at\nhttps://sites.google.com/view/herculesdataset.\n","authors":["Hanjun Kim","Minwoo Jung","Chiyun Noh","Sangwoo Jung","Hyunho Song","Wooseong Yang","Hyesu Jang","Ayoung Kim"],"pdf_url":"https://arxiv.org/pdf/2502.01946v3.pdf","comment":"2025 IEEE International Conference on Robotics and Automation (ICRA\n  2025)"},{"id":"http://arxiv.org/abs/2403.17465v4","updated":"2025-02-21T12:51:57Z","published":"2024-03-26T07:55:16Z","title":"LaRE$^2$: Latent Reconstruction Error Based Method for\n  Diffusion-Generated Image Detection","summary":"  The evolution of Diffusion Models has dramatically improved image generation\nquality, making it increasingly difficult to differentiate between real and\ngenerated images. This development, while impressive, also raises significant\nprivacy and security concerns. In response to this, we propose a novel Latent\nREconstruction error guided feature REfinement method (LaRE^2) for detecting\nthe diffusion-generated images. We come up with the Latent Reconstruction Error\n(LaRE), the first reconstruction-error based feature in the latent space for\ngenerated image detection. LaRE surpasses existing methods in terms of feature\nextraction efficiency while preserving crucial cues required to differentiate\nbetween the real and the fake. To exploit LaRE, we propose an Error-Guided\nfeature REfinement module (EGRE), which can refine the image feature guided by\nLaRE to enhance the discriminativeness of the feature. Our EGRE utilizes an\nalign-then-refine mechanism, which effectively refines the image feature for\ngenerated-image detection from both spatial and channel perspectives. Extensive\nexperiments on the large-scale GenImage benchmark demonstrate the superiority\nof our LaRE^2, which surpasses the best SoTA method by up to 11.9%/12.1%\naverage ACC/AP across 8 different image generators. LaRE also surpasses\nexisting methods in terms of feature extraction cost, delivering an impressive\nspeed enhancement of 8 times. Code is available.\n","authors":["Yunpeng Luo","Junlong Du","Ke Yan","Shouhong Ding"],"pdf_url":"https://arxiv.org/pdf/2403.17465v4.pdf","comment":"CVPR 2024. Code is available at https://github.com/luo3300612/LaRE"},{"id":"http://arxiv.org/abs/2502.15424v1","updated":"2025-02-21T12:49:35Z","published":"2025-02-21T12:49:35Z","title":"Anatomy-Informed Deep Learning and Radiomics for Automated Neurofibroma\n  Segmentation in Whole-Body MRI","summary":"  Neurofibromatosis Type 1 is a genetic disorder characterized by the\ndevelopment of neurofibromas (NFs), which exhibit significant variability in\nsize, morphology, and anatomical location. Accurate and automated segmentation\nof these tumors in whole-body magnetic resonance imaging (WB-MRI) is crucial to\nassess tumor burden and monitor disease progression. In this study, we present\nand analyze a fully automated pipeline for NF segmentation in fat-suppressed\nT2-weighted WB-MRI, consisting of three stages: anatomy segmentation, NF\nsegmentation, and tumor candidate classification. In the first stage, we use\nthe MRSegmentator model to generate an anatomy segmentation mask, extended with\na high-risk zone for NFs. This mask is concatenated with the input image as\nanatomical context information for NF segmentation. The second stage employs an\nensemble of 3D anisotropic anatomy-informed U-Nets to produce an NF\nsegmentation confidence mask. In the final stage, tumor candidates are\nextracted from the confidence mask and classified based on radiomic features,\ndistinguishing tumors from non-tumor regions and reducing false positives. We\nevaluate the proposed pipeline on three test sets representing different\nconditions: in-domain data (test set 1), varying imaging protocols and field\nstrength (test set 2), and low tumor burden cases (test set 3). Experimental\nresults show a 68% improvement in per-scan Dice Similarity Coefficient (DSC), a\n21% increase in per-tumor DSC, and a two-fold improvement in F1 score for tumor\ndetection in high tumor burden cases by integrating anatomy information. The\nmethod is integrated into the 3D Slicer platform for practical clinical use,\nwith the code publicly accessible.\n","authors":["Georgii Kolokolnikov","Marie-Lena Schmalhofer","Lennart Well","Said Farschtschi","Victor-Felix Mautner","Inka Ristow","Rene Werner"],"pdf_url":"https://arxiv.org/pdf/2502.15424v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.15422v1","updated":"2025-02-21T12:46:40Z","published":"2025-02-21T12:46:40Z","title":"Evaluating Multimodal Generative AI with Korean Educational Standards","summary":"  This paper presents the Korean National Educational Test Benchmark (KoNET), a\nnew benchmark designed to evaluate Multimodal Generative AI Systems using\nKorean national educational tests. KoNET comprises four exams: the Korean\nElementary General Educational Development Test (KoEGED), Middle (KoMGED), High\n(KoHGED), and College Scholastic Ability Test (KoCSAT). These exams are\nrenowned for their rigorous standards and diverse questions, facilitating a\ncomprehensive analysis of AI performance across different educational levels.\nBy focusing on Korean, KoNET provides insights into model performance in\nless-explored languages. We assess a range of models - open-source,\nopen-access, and closed APIs - by examining difficulties, subject diversity,\nand human error rates. The code and dataset builder will be made fully\nopen-sourced at https://github.com/naver-ai/KoNET.\n","authors":["Sanghee Park","Geewook Kim"],"pdf_url":"https://arxiv.org/pdf/2502.15422v1.pdf","comment":"18 pages; To appear at NAACL 2025 Main Conference (Project page:\n  https://github.com/naver-ai/KoNET )"},{"id":"http://arxiv.org/abs/2401.13329v3","updated":"2025-02-21T12:30:11Z","published":"2024-01-24T09:45:40Z","title":"Generative Video Diffusion for Unseen Novel Semantic Video Moment\n  Retrieval","summary":"  Video moment retrieval (VMR) aims to locate the most likely video moment(s)\ncorresponding to a text query in untrimmed videos. Training of existing methods\nis limited by the lack of diverse and generalisable VMR datasets, hindering\ntheir ability to generalise moment-text associations to queries containing\nnovel semantic concepts (unseen both visually and textually in a training\nsource domain). For model generalisation to novel semantics, existing methods\nrely heavily on assuming to have access to both video and text sentence pairs\nfrom a target domain in addition to the source domain pair-wise training data.\nThis is neither practical nor scalable. In this work, we introduce a more\ngeneralisable approach by assuming only text sentences describing new semantics\nare available in model training without having seen any videos from a target\ndomain. To that end, we propose a Fine-grained Video Editing framework, termed\nFVE, that explores generative video diffusion to facilitate fine-grained video\nediting from the seen source concepts to the unseen target sentences consisting\nof new concepts. This enables generative hypotheses of unseen video moments\ncorresponding to the novel concepts in the target domain. This fine-grained\ngenerative video diffusion retains the original video structure and subject\nspecifics from the source domain while introducing semantic distinctions of\nunseen novel vocabularies in the target domain. A critical challenge is how to\nenable this generative fine-grained diffusion process to be meaningful in\noptimising VMR, more than just synthesising visually pleasing videos. We solve\nthis problem by introducing a hybrid selection mechanism that integrates three\nquantitative metrics to selectively incorporate synthetic video moments (novel\nvideo hypotheses) as enlarged additions to the original source training data,\nwhilst minimising potential ...\n","authors":["Dezhao Luo","Shaogang Gong","Jiabo Huang","Hailin Jin","Yang Liu"],"pdf_url":"https://arxiv.org/pdf/2401.13329v3.pdf","comment":"AAAI-25"},{"id":"http://arxiv.org/abs/2502.15398v1","updated":"2025-02-21T11:52:56Z","published":"2025-02-21T11:52:56Z","title":"Enhancing Vehicle Make and Model Recognition with 3D Attention Modules","summary":"  Vehicle make and model recognition (VMMR) is a crucial component of the\nIntelligent Transport System, garnering significant attention in recent years.\nVMMR has been widely utilized for detecting suspicious vehicles, monitoring\nurban traffic, and autonomous driving systems. The complexity of VMMR arises\nfrom the subtle visual distinctions among vehicle models and the wide variety\nof classes produced by manufacturers. Convolutional Neural Networks (CNNs), a\nprominent type of deep learning model, have been extensively employed in\nvarious computer vision tasks, including VMMR, yielding remarkable results. As\nVMMR is a fine-grained classification problem, it primarily faces inter-class\nsimilarity and intra-class variation challenges. In this study, we implement an\nattention module to address these challenges and enhance the model's focus on\ncritical areas containing distinguishing features. This module, which does not\nincrease the parameters of the original model, generates three-dimensional\n(3-D) attention weights to refine the feature map. Our proposed model\nintegrates the attention module into two different locations within the middle\nsection of a convolutional model, where the feature maps from these sections\noffer sufficient information about the input frames without being overly\ndetailed or overly coarse. The performance of our proposed model, along with\nstate-of-the-art (SOTA) convolutional and transformer-based models, was\nevaluated using the Stanford Cars dataset. Our proposed model achieved the\nhighest accuracy, 90.69\\%, among the compared models.\n","authors":["Narges Semiromizadeh","Omid Nejati Manzari","Shahriar B. Shokouhi","Sattar Mirzakuchaki"],"pdf_url":"https://arxiv.org/pdf/2502.15398v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.15393v1","updated":"2025-02-21T11:40:23Z","published":"2025-02-21T11:40:23Z","title":"LongCaptioning: Unlocking the Power of Long Caption Generation in Large\n  Multimodal Models","summary":"  Large multimodal models (LMMs) have shown remarkable performance in video\nunderstanding tasks and can even process videos longer than one hour. However,\ndespite their ability to handle long inputs, generating outputs with\ncorresponding levels of richness remains a challenge. In this paper, we explore\nthe issue of long outputs in LMMs using video captioning as a proxy task, and\nwe find that open-source LMMs struggle to consistently generate outputs\nexceeding about 300 words. Through controlled experiments, we find that the\nscarcity of paired examples with long-captions during training is the primary\nfactor limiting the model's output length. However, manually annotating\nlong-caption examples is time-consuming and expensive. To address this, we\npropose the LongCaption-Agent, a framework that synthesizes long caption data\nby aggregating multi-level descriptions. Using LongCaption-Agent, we curated a\nnew long-caption dataset, LongCaption-10K. We also develop LongCaption-Bench, a\nbenchmark designed to comprehensively evaluate the quality of long captions\ngenerated by LMMs. By incorporating LongCaption-10K into training, we enable\nLMMs to generate captions exceeding 1,000 words, while maintaining high output\nquality. In LongCaption-Bench, our 8B parameter model achieved state-of-the-art\nperformance, even surpassing larger proprietary models. We will release the\ndataset and code after publication.\n","authors":["Hongchen Wei","Zhihong Tan","Yaosi Hu","Changwen Chen","Zhenzhong Chen"],"pdf_url":"https://arxiv.org/pdf/2502.15393v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.15392v1","updated":"2025-02-21T11:38:40Z","published":"2025-02-21T11:38:40Z","title":"Chitrarth: Bridging Vision and Language for a Billion People","summary":"  Recent multimodal foundation models are primarily trained on English or high\nresource European language data, which hinders their applicability to other\nmedium and low-resource languages. To address this limitation, we introduce\nChitrarth (Chitra: Image; Artha: Meaning), an inclusive Vision-Language Model\n(VLM), specifically targeting the rich linguistic diversity and visual\nreasoning across 10 prominent Indian languages. Our model effectively\nintegrates a state-of-the-art (SOTA) multilingual Large Language Model (LLM)\nwith a vision module, primarily trained on multilingual image-text data.\nFurthermore, we also introduce BharatBench, a comprehensive framework for\nevaluating VLMs across various Indian languages, ultimately contributing to\nmore diverse and effective AI systems. Our model achieves SOTA results for\nbenchmarks across low resource languages while retaining its efficiency in\nEnglish. Through our research, we aim to set new benchmarks in\nmultilingual-multimodal capabilities, offering substantial improvements over\nexisting models and establishing a foundation to facilitate future advancements\nin this arena.\n","authors":["Shaharukh Khan","Ayush Tarun","Abhinav Ravi","Ali Faraz","Akshat Patidar","Praveen Kumar Pokala","Anagha Bhangare","Raja Kolla","Chandra Khatri","Shubham Agarwal"],"pdf_url":"https://arxiv.org/pdf/2502.15392v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.05075v3","updated":"2025-02-21T11:35:26Z","published":"2024-08-09T14:04:21Z","title":"DeepInteraction++: Multi-Modality Interaction for Autonomous Driving","summary":"  Existing top-performance autonomous driving systems typically rely on the\nmulti-modal fusion strategy for reliable scene understanding. This design is\nhowever fundamentally restricted due to overlooking the modality-specific\nstrengths and finally hampering the model performance. To address this\nlimitation, in this work, we introduce a novel modality interaction strategy\nthat allows individual per-modality representations to be learned and\nmaintained throughout, enabling their unique characteristics to be exploited\nduring the whole perception pipeline. To demonstrate the effectiveness of the\nproposed strategy, we design DeepInteraction++, a multi-modal interaction\nframework characterized by a multi-modal representational interaction encoder\nand a multi-modal predictive interaction decoder. Specifically, the encoder is\nimplemented as a dual-stream Transformer with specialized attention operation\nfor information exchange and integration between separate modality-specific\nrepresentations. Our multi-modal representational learning incorporates both\nobject-centric, precise sampling-based feature alignment and global dense\ninformation spreading, essential for the more challenging planning task. The\ndecoder is designed to iteratively refine the predictions by alternately\naggregating information from separate representations in a unified\nmodality-agnostic manner, realizing multi-modal predictive interaction.\nExtensive experiments demonstrate the superior performance of the proposed\nframework on both 3D object detection and end-to-end autonomous driving tasks.\nOur code is available at https://github.com/fudan-zvg/DeepInteraction.\n","authors":["Zeyu Yang","Nan Song","Wei Li","Xiatian Zhu","Li Zhang","Philip H. S. Torr"],"pdf_url":"https://arxiv.org/pdf/2408.05075v3.pdf","comment":"Journal extension of NeurIPS 2022. arXiv admin note: text overlap\n  with arXiv:2208.11112"},{"id":"http://arxiv.org/abs/2502.15389v1","updated":"2025-02-21T11:26:21Z","published":"2025-02-21T11:26:21Z","title":"The Role of Background Information in Reducing Object Hallucination in\n  Vision-Language Models: Insights from Cutoff API Prompting","summary":"  Vision-Language Models (VLMs) occasionally generate outputs that contradict\ninput images, constraining their reliability in real-world applications. While\nvisual prompting is reported to suppress hallucinations by augmenting prompts\nwith relevant area inside an image, the effectiveness in terms of the area\nremains uncertain. This study analyzes success and failure cases of\nAttention-driven visual prompting in object hallucination, revealing that\npreserving background context is crucial for mitigating object hallucination.\n","authors":["Masayo Tomita","Katsuhiko Hayashi","Tomoyuki Kaneko"],"pdf_url":"https://arxiv.org/pdf/2502.15389v1.pdf","comment":"Under review"},{"id":"http://arxiv.org/abs/2406.13844v3","updated":"2025-02-21T11:20:47Z","published":"2024-06-19T21:11:46Z","title":"A large-scale multicenter breast cancer DCE-MRI benchmark dataset with\n  expert segmentations","summary":"  Artificial Intelligence (AI) research in breast cancer Magnetic Resonance\nImaging (MRI) faces challenges due to limited expert-labeled segmentations. To\naddress this, we present a multicenter dataset of 1506 pre-treatment\nT1-weighted dynamic contrast-enhanced MRI cases, including expert annotations\nof primary tumors and non-mass-enhanced regions. The dataset integrates imaging\ndata from four collections in The Cancer Imaging Archive (TCIA), where only 163\ncases with expert segmentations were initially available. To facilitate the\nannotation process, a deep learning model was trained to produce preliminary\nsegmentations for the remaining cases. These were subsequently corrected and\nverified by 16 breast cancer experts (averaging 9 years of experience),\ncreating a fully annotated dataset. Additionally, the dataset includes 49\nharmonized clinical and demographic variables, as well as pre-trained weights\nfor a baseline nnU-Net model trained on the annotated data. This resource\naddresses a critical gap in publicly available breast cancer datasets, enabling\nthe development, validation, and benchmarking of advanced deep learning models,\nthus driving progress in breast cancer diagnostics, treatment response\nprediction, and personalized care.\n","authors":["Lidia Garrucho","Kaisar Kushibar","Claire-Anne Reidel","Smriti Joshi","Richard Osuala","Apostolia Tsirikoglou","Maciej Bobowicz","Javier del Riego","Alessandro Catanese","Katarzyna Gwoździewicz","Maria-Laura Cosaka","Pasant M. Abo-Elhoda","Sara W. Tantawy","Shorouq S. Sakrana","Norhan O. Shawky-Abdelfatah","Amr Muhammad Abdo-Salem","Androniki Kozana","Eugen Divjak","Gordana Ivanac","Katerina Nikiforaki","Michail E. Klontzas","Rosa García-Dosdá","Meltem Gulsun-Akpinar","Oğuz Lafcı","Ritse Mann","Carlos Martín-Isla","Fred Prior","Kostas Marias","Martijn P. A. Starmans","Fredrik Strand","Oliver Díaz","Laura Igual","Karim Lekadir"],"pdf_url":"https://arxiv.org/pdf/2406.13844v3.pdf","comment":"15 paes, 7 figures, 3 tables"},{"id":"http://arxiv.org/abs/2312.04931v2","updated":"2025-02-21T11:10:15Z","published":"2023-12-08T09:48:36Z","title":"Long Video Understanding with Learnable Retrieval in Video-Language\n  Models","summary":"  The remarkable natural language understanding, reasoning, and generation\ncapabilities of large language models (LLMs) have made them attractive for\napplication to video understanding, utilizing video tokens as contextual input.\nHowever, employing LLMs for long video understanding presents significant\nchallenges. The extensive number of video tokens leads to considerable\ncomputational costs for LLMs while using aggregated tokens results in loss of\nvision details. Moreover, the presence of abundant question-irrelevant tokens\nintroduces noise to the video reasoning process. To address these issues, we\nintroduce a simple yet effective learnable retrieval-based video-language model\n(R-VLM) for efficient long video understanding. Specifically, given a question\n(query) and a long video, our model identifies and selects the most relevant K\nvideo chunks and uses their associated visual tokens to serve as context for\nthe LLM inference. This effectively reduces the number of video tokens,\neliminates noise interference, and enhances system performance. We achieve this\nby incorporating a learnable lightweight MLP block to facilitate the efficient\nretrieval of question-relevant chunks, through the end-to-end training of our\nvideo-language model with a proposed soft matching loss. Our experimental\nresults on multiple zero-shot video question answering datasets validate the\neffectiveness of our framework for comprehending long videos.\n","authors":["Jiaqi Xu","Cuiling Lan","Wenxuan Xie","Xuejin Chen","Yan Lu"],"pdf_url":"https://arxiv.org/pdf/2312.04931v2.pdf","comment":"14 pages, 8 figures"},{"id":"http://arxiv.org/abs/2502.15381v1","updated":"2025-02-21T11:05:30Z","published":"2025-02-21T11:05:30Z","title":"MOVE: A Mixture-of-Vision-Encoders Approach for Domain-Focused\n  Vision-Language Processing","summary":"  Multimodal language models (MLMs) integrate visual and textual information by\ncoupling a vision encoder with a large language model through the specific\nadapter. While existing approaches commonly rely on a single pre-trained vision\nencoder, there is a great variability of specialized encoders that can boost\nmodel's performance in distinct domains. In this work, we propose MOVE (Mixture\nof Vision Encoders) a simple yet effective approach to leverage multiple\npre-trained encoders for specialized multimodal tasks. MOVE automatically\nroutes inputs to the most appropriate encoder among candidates such as Unichat,\nInternViT, and Texify, thereby enhancing performance across a diverse set of\nbenchmarks, including ChartQA, MMBench, and MMMU. Experimental results\ndemonstrate that MOVE achieves competitive accuracy without incurring the\ncomplexities of image slicing for high-resolution images.\n","authors":["Matvey Skripkin","Elizaveta Goncharova","Dmitrii Tarasov","Andrey Kuznetsov"],"pdf_url":"https://arxiv.org/pdf/2502.15381v1.pdf","comment":"10 pages, 6 figures, 4 tables"},{"id":"http://arxiv.org/abs/2407.06606v2","updated":"2025-02-21T10:52:07Z","published":"2024-07-09T07:15:56Z","title":"Tailored Design of Audio-Visual Speech Recognition Models using\n  Branchformers","summary":"  Recent advances in Audio-Visual Speech Recognition (AVSR) have led to\nunprecedented achievements in the field, improving the robustness of this type\nof system in adverse, noisy environments. In most cases, this task has been\naddressed through the design of models composed of two independent encoders,\neach dedicated to a specific modality. However, while recent works have\nexplored unified audio-visual encoders, determining the optimal cross-modal\narchitecture remains an ongoing challenge. Furthermore, such approaches often\nrely on models comprising vast amounts of parameters and high computational\ncost training processes. In this paper, we aim to bridge this research gap by\nintroducing a novel audio-visual framework. Our proposed method constitutes, to\nthe best of our knowledge, the first attempt to harness the flexibility and\ninterpretability offered by encoder architectures, such as the Branchformer, in\nthe design of parameter-efficient AVSR systems. To be more precise, the\nproposed framework consists of two steps: first, estimating audio- and\nvideo-only systems, and then designing a tailored audio-visual unified encoder\nbased on the layer-level branch scores provided by the modality-specific\nmodels. Extensive experiments on English and Spanish AVSR benchmarks covering\nmultiple data conditions and scenarios demonstrated the effectiveness of our\nproposed method. Even when trained on a moderate scale of data, our models\nachieve competitive word error rates (WER) of approximately 2.5\\% for English\nand surpass existing approaches for Spanish, establishing a new benchmark with\nan average WER of around 9.1\\%. These results reflect how our tailored AVSR\nsystem is able to reach state-of-the-art recognition rates while significantly\nreducing the model complexity w.r.t. the prevalent approach in the field. Code\nand pre-trained models are available at\nhttps://github.com/david-gimeno/tailored-avsr.\n","authors":["David Gimeno-Gómez","Carlos-D. Martínez-Hinarejos"],"pdf_url":"https://arxiv.org/pdf/2407.06606v2.pdf","comment":"Submitted and under review for the Computer Speech and Language\n  journal of Elsevier"},{"id":"http://arxiv.org/abs/2502.15370v1","updated":"2025-02-21T10:42:04Z","published":"2025-02-21T10:42:04Z","title":"Weakly Supervised Video Scene Graph Generation via Natural Language\n  Supervision","summary":"  Existing Video Scene Graph Generation (VidSGG) studies are trained in a fully\nsupervised manner, which requires all frames in a video to be annotated,\nthereby incurring high annotation cost compared to Image Scene Graph Generation\n(ImgSGG). Although the annotation cost of VidSGG can be alleviated by adopting\na weakly supervised approach commonly used for ImgSGG (WS-ImgSGG) that uses\nimage captions, there are two key reasons that hinder such a naive adoption: 1)\nTemporality within video captions, i.e., unlike image captions, video captions\ninclude temporal markers (e.g., before, while, then, after) that indicate time\nrelated details, and 2) Variability in action duration, i.e., unlike human\nactions in image captions, human actions in video captions unfold over varying\nduration. To address these issues, we propose a Natural Language-based Video\nScene Graph Generation (NL-VSGG) framework that only utilizes the readily\navailable video captions for training a VidSGG model. NL-VSGG consists of two\nkey modules: Temporality-aware Caption Segmentation (TCS) module and Action\nDuration Variability-aware caption-frame alignment (ADV) module. Specifically,\nTCS segments the video captions into multiple sentences in a temporal order\nbased on a Large Language Model (LLM), and ADV aligns each segmented sentence\nwith appropriate frames considering the variability in action duration. Our\napproach leads to a significant enhancement in performance compared to simply\napplying the WS-ImgSGG pipeline to VidSGG on the Action Genome dataset. As a\nfurther benefit of utilizing the video captions as weak supervision, we show\nthat the VidSGG model trained by NL-VSGG is able to predict a broader range of\naction classes that are not included in the training data, which makes our\nframework practical in reality.\n","authors":["Kibum Kim","Kanghoon Yoon","Yeonjun In","Jaehyeong Jeon","Jinyoung Moon","Donghyun Kim","Chanyoung Park"],"pdf_url":"https://arxiv.org/pdf/2502.15370v1.pdf","comment":"10 pages, ICLR 2025"},{"id":"http://arxiv.org/abs/2305.10465v2","updated":"2025-02-21T10:36:52Z","published":"2023-05-17T12:31:48Z","title":"Towards Robust Probabilistic Modeling on SO(3) via Rotation Laplace\n  Distribution","summary":"  Estimating the 3DoF rotation from a single RGB image is an important yet\nchallenging problem. As a popular approach, probabilistic rotation modeling\nadditionally carries prediction uncertainty information, compared to\nsingle-prediction rotation regression. For modeling probabilistic distribution\nover SO(3), it is natural to use Gaussian-like Bingham distribution and matrix\nFisher, however they are shown to be sensitive to outlier predictions, e.g.\n$180^\\circ$ error and thus are unlikely to converge with optimal performance.\nIn this paper, we draw inspiration from multivariate Laplace distribution and\npropose a novel rotation Laplace distribution on SO(3). Our rotation Laplace\ndistribution is robust to the disturbance of outliers and enforces much\ngradient to the low-error region that it can improve. In addition, we show that\nour method also exhibits robustness to small noises and thus tolerates\nimperfect annotations. With this benefit, we demonstrate its advantages in\nsemi-supervised rotation regression, where the pseudo labels are noisy. To\nfurther capture the multi-modal rotation solution space for symmetric objects,\nwe extend our distribution to rotation Laplace mixture model and demonstrate\nits effectiveness. Our extensive experiments show that our proposed\ndistribution and the mixture model achieve state-of-the-art performance in all\nthe rotation regression experiments over both probabilistic and\nnon-probabilistic baselines.\n","authors":["Yingda Yin","Jiangran Lyu","Yang Wang","Haoran Liu","He Wang","Baoquan Chen"],"pdf_url":"https://arxiv.org/pdf/2305.10465v2.pdf","comment":"TPAMI 2025. ICLR 2023 spotlight. arXiv admin note: substantial text\n  overlap with arXiv:2303.01743"},{"id":"http://arxiv.org/abs/2403.18490v2","updated":"2025-02-21T10:22:15Z","published":"2024-03-27T12:05:22Z","title":"I2CKD : Intra- and Inter-Class Knowledge Distillation for Semantic\n  Segmentation","summary":"  This paper proposes a new knowledge distillation method tailored for image\nsemantic segmentation, termed Intra- and Inter-Class Knowledge Distillation\n(I2CKD). The focus of this method is on capturing and transferring knowledge\nbetween the intermediate layers of teacher (cumbersome model) and student\n(compact model). For knowledge extraction, we exploit class prototypes derived\nfrom feature maps. To facilitate knowledge transfer, we employ a triplet loss\nin order to minimize intra-class variances and maximize inter-class variances\nbetween teacher and student prototypes. Consequently, I2CKD enables the student\nto better mimic the feature representation of the teacher for each class,\nthereby enhancing the segmentation performance of the compact network.\nExtensive experiments on three segmentation datasets, i.e., Cityscapes, Pascal\nVOC and CamVid, using various teacher-student network pairs demonstrate the\neffectiveness of the proposed method.\n","authors":["Ayoub Karine","Thibault Napoléon","Maher Jridi"],"pdf_url":"https://arxiv.org/pdf/2403.18490v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.15363v1","updated":"2025-02-21T10:22:08Z","published":"2025-02-21T10:22:08Z","title":"M2LADS Demo: A System for Generating Multimodal Learning Analytics\n  Dashboards","summary":"  We present a demonstration of a web-based system called M2LADS (\"System for\nGenerating Multimodal Learning Analytics Dashboards\"), designed to integrate,\nsynchronize, visualize, and analyze multimodal data recorded during\ncomputer-based learning sessions with biosensors. This system presents a range\nof biometric and behavioral data on web-based dashboards, providing detailed\ninsights into various physiological and activity-based metrics. The multimodal\ndata visualized include electroencephalogram (EEG) data for assessing attention\nand brain activity, heart rate metrics, eye-tracking data to measure visual\nattention, webcam video recordings, and activity logs of the monitored tasks.\nM2LADS aims to assist data scientists in two key ways: (1) by providing a\ncomprehensive view of participants' experiences, displaying all data\ncategorized by the activities in which participants are engaged, and (2) by\nsynchronizing all biosignals and videos, facilitating easier data relabeling if\nany activity information contains errors.\n","authors":["Alvaro Becerra","Roberto Daza","Ruth Cobos","Aythami Morales","Julian Fierrez"],"pdf_url":"https://arxiv.org/pdf/2502.15363v1.pdf","comment":"Published in the Workshop on Innovation and Responsibility in\n  AI-Supported Education (iRAISE25) at AAAI 2025"},{"id":"http://arxiv.org/abs/2501.18936v3","updated":"2025-02-21T10:05:20Z","published":"2025-01-31T07:41:06Z","title":"Adaptive Prompt: Unlocking the Power of Visual Prompt Tuning","summary":"  Visual Prompt Tuning (VPT) has recently emerged as a powerful method for\nadapting pre-trained vision models to downstream tasks. By introducing\nlearnable prompt tokens as task-specific instructions, VPT effectively guides\npre-trained transformer models with minimal overhead. Despite its empirical\nsuccess, a comprehensive theoretical understanding of VPT remains an active\narea of research. Building on recent insights into the connection between\nmixture of experts and prompt-based approaches, we identify a key limitation in\nVPT: the restricted functional expressiveness in prompt formulation. To address\nthis limitation, we propose Visual Adaptive Prompt Tuning (VAPT), a new\ngeneration of prompts that redefines prompts as adaptive functions of the\ninput. Our theoretical analysis shows that this simple yet intuitive approach\nachieves optimal sample efficiency. Empirical results on VTAB-1K and FGVC\nfurther demonstrate VAPT's effectiveness, with performance gains of 7.34% and\n1.04% over fully fine-tuning baselines, respectively. Notably, VAPT also\nsurpasses VPT by a substantial margin while using fewer parameters. These\nresults highlight both the effectiveness and efficiency of our method and pave\nthe way for future research to explore the potential of adaptive prompts. Our\ncode is publicly available at https://github.com/Minhchuyentoancbn/VAPT\n","authors":["Minh Le","Anh Nguyen","Huy Nguyen","Chau Nguyen","Nhat Ho"],"pdf_url":"https://arxiv.org/pdf/2501.18936v3.pdf","comment":"57 pages, 10 figures, 18 tables"},{"id":"http://arxiv.org/abs/2502.14377v2","updated":"2025-02-21T10:02:02Z","published":"2025-02-20T09:10:05Z","title":"RelaCtrl: Relevance-Guided Efficient Control for Diffusion Transformers","summary":"  The Diffusion Transformer plays a pivotal role in advancing text-to-image and\ntext-to-video generation, owing primarily to its inherent scalability. However,\nexisting controlled diffusion transformer methods incur significant parameter\nand computational overheads and suffer from inefficient resource allocation due\nto their failure to account for the varying relevance of control information\nacross different transformer layers. To address this, we propose the\nRelevance-Guided Efficient Controllable Generation framework, RelaCtrl,\nenabling efficient and resource-optimized integration of control signals into\nthe Diffusion Transformer. First, we evaluate the relevance of each layer in\nthe Diffusion Transformer to the control information by assessing the\n\"ControlNet Relevance Score\"-i.e., the impact of skipping each control layer on\nboth the quality of generation and the control effectiveness during inference.\nBased on the strength of the relevance, we then tailor the positioning,\nparameter scale, and modeling capacity of the control layers to reduce\nunnecessary parameters and redundant computations. Additionally, to further\nimprove efficiency, we replace the self-attention and FFN in the commonly used\ncopy block with the carefully designed Two-Dimensional Shuffle Mixer (TDSM),\nenabling efficient implementation of both the token mixer and channel mixer.\nBoth qualitative and quantitative experimental results demonstrate that our\napproach achieves superior performance with only 15% of the parameters and\ncomputational complexity compared to PixArt-delta.\n","authors":["Ke Cao","Jing Wang","Ao Ma","Jiasong Feng","Zhanjie Zhang","Xuanhua He","Shanyuan Liu","Bo Cheng","Dawei Leng","Yuhui Yin","Jie Zhang"],"pdf_url":"https://arxiv.org/pdf/2502.14377v2.pdf","comment":"Homepage: https://360cvgroup.github.io/RelaCtrl/ Github:\n  https://github.com/360CVGroup/RelaCtrl"},{"id":"http://arxiv.org/abs/2410.11701v2","updated":"2025-02-21T09:48:58Z","published":"2024-10-15T15:39:37Z","title":"Magnifier Prompt: Tackling Multimodal Hallucination via Extremely Simple\n  Instructions","summary":"  Hallucinations in multimodal large language models (MLLMs) hinder their\npractical applications. To address this, we propose a Magnifier Prompt\n(MagPrompt), a simple yet effective method to tackle hallucinations in MLLMs\nvia extremely simple instructions. MagPrompt is based on the following two key\nprinciples, which guide the design of various effective prompts, demonstrating\nrobustness: (1) MLLMs should focus more on the image. (2) When there are\nconflicts between the image and the model's inner knowledge, MLLMs should\nprioritize the image. MagPrompt is training-free and can be applied to\nopen-source and closed-source models, such as GPT-4o and Gemini-pro. It\nperforms well across many datasets and its effectiveness is comparable or even\nbetter than more complex methods like VCD. Furthermore, our prompt design\nprinciples and experimental analyses provide valuable insights into multimodal\nhallucination.\n","authors":["Yuhan Fu","Ruobing Xie","Jiazhen Liu","Bangxiang Lan","Xingwu Sun","Zhanhui Kang","Xirong Li"],"pdf_url":"https://arxiv.org/pdf/2410.11701v2.pdf","comment":"The proposed method does not work for up-to-date MLLMs."},{"id":"http://arxiv.org/abs/2502.15322v1","updated":"2025-02-21T09:22:23Z","published":"2025-02-21T09:22:23Z","title":"SentiFormer: Metadata Enhanced Transformer for Image Sentiment Analysis","summary":"  As more and more internet users post images online to express their daily\nemotions, image sentiment analysis has attracted increasing attention.\nRecently, researchers generally tend to design different neural networks to\nextract visual features from images for sentiment analysis. Despite the\nsignificant progress, metadata, the data (e.g., text descriptions and keyword\ntags) for describing the image, has not been sufficiently explored in this\ntask. In this paper, we propose a novel Metadata Enhanced Transformer for\nsentiment analysis (SentiFormer) to fuse multiple metadata and the\ncorresponding image into a unified framework. Specifically, we first obtain\nmultiple metadata of the image and unify the representations of diverse data.\nTo adaptively learn the appropriate weights for each metadata, we then design\nan adaptive relevance learning module to highlight more effective information\nwhile suppressing weaker ones. Moreover, we further develop a cross-modal\nfusion module to fuse the adaptively learned representations and make the final\nprediction. Extensive experiments on three publicly available datasets\ndemonstrate the superiority and rationality of our proposed method.\n","authors":["Bin Feng","Shulan Ruan","Mingzheng Yang","Dongxuan Han","Huijie Liu","Kai Zhang","Qi Liu"],"pdf_url":"https://arxiv.org/pdf/2502.15322v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.14487v2","updated":"2025-02-21T09:05:35Z","published":"2025-02-20T12:09:30Z","title":"Temporal Misalignment in ANN-SNN Conversion and Its Mitigation via\n  Probabilistic Spiking Neurons","summary":"  Spiking Neural Networks (SNNs) offer a more energy-efficient alternative to\nArtificial Neural Networks (ANNs) by mimicking biological neural principles,\nestablishing them as a promising approach to mitigate the increasing energy\ndemands of large-scale neural models. However, fully harnessing the\ncapabilities of SNNs remains challenging due to their discrete signal\nprocessing and temporal dynamics. ANN-SNN conversion has emerged as a practical\napproach, enabling SNNs to achieve competitive performance on complex machine\nlearning tasks. In this work, we identify a phenomenon in the ANN-SNN\nconversion framework, termed temporal misalignment, in which random spike\nrearrangement across SNN layers leads to performance improvements. Based on\nthis observation, we introduce biologically plausible two-phase probabilistic\n(TPP) spiking neurons, further enhancing the conversion process. We demonstrate\nthe advantages of our proposed method both theoretically and empirically\nthrough comprehensive experiments on CIFAR-10/100, CIFAR10-DVS, and ImageNet\nacross a variety of architectures, achieving state-of-the-art results.\n","authors":["Velibor Bojković","Xiaofeng Wu","Bin Gu"],"pdf_url":"https://arxiv.org/pdf/2502.14487v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.15311v1","updated":"2025-02-21T09:05:29Z","published":"2025-02-21T09:05:29Z","title":"Research advances on fish feeding behavior recognition and intensity\n  quantification methods in aquaculture","summary":"  As a key part of aquaculture management, fish feeding behavior recognition\nand intensity quantification has been a hot area of great concern to\nresearchers, and it plays a crucial role in monitoring fish health, guiding\nbaiting work and improving aquaculture efficiency. In order to better carry out\nthe related work in the future, this paper firstly reviews the research\nadvances of fish feeding behavior recognition and intensity quantification\nmethods based on computer vision, acoustics and sensors in a single modality.\nThen the application of the current emerging multimodal fusion in fish feeding\nbehavior recognition and intensity quantification methods is expounded.\nFinally, the advantages and disadvantages of various techniques are compared\nand analyzed, and the future research directions are envisioned.\n","authors":["Shulong Zhang","Daoliang Li","Jiayin Zhao","Mingyuan Yao","Yingyi Chen","Yukang Huo","Xiao Liu","Haihua Wang"],"pdf_url":"https://arxiv.org/pdf/2502.15311v1.pdf","comment":"22 pages, 4 figures,"},{"id":"http://arxiv.org/abs/2502.15307v1","updated":"2025-02-21T09:03:05Z","published":"2025-02-21T09:03:05Z","title":"Road Traffic Sign Recognition method using Siamese network Combining\n  Efficient-CNN based Encoder","summary":"  Traffic signs recognition (TSR) plays an essential role in assistant driving\nand intelligent transportation system. However, the noise of complex\nenvironment may lead to motion-blur or occlusion problems, which raise the\ntough challenge to real-time recognition with high accuracy and robust. In this\narticle, we propose IECES-network which with improved encoders and Siamese net.\nThe three-stage approach of our method includes Efficient-CNN based encoders,\nSiamese backbone and the fully-connected layers. We firstly use convolutional\nencoders to extract and encode the traffic sign features of augmented training\nsamples and standard images. Then, we design the Siamese neural network with\nEfficient-CNN based encoder and contrastive loss function, which can be trained\nto improve the robustness of TSR problem when facing the samples of motion-blur\nand occlusion by computing the distance between inputs and templates.\nAdditionally, the template branch of the proposed network can be stopped when\nexecuting the recognition tasks after training to raise the process speed of\nour real-time model, and alleviate the computational resource and parameter\nscale. Finally, we recombined the feature code and a fully-connected layer with\nSoftMax function to classify the codes of samples and recognize the category of\ntraffic signs. The results of experiments on the Tsinghua-Tencent 100K dataset\nand the German Traffic Sign Recognition Benchmark dataset demonstrate the\nperformance of the proposed IECESnetwork. Compared with other state-of-the-art\nmethods, in the case of motion-blur and occluded environment, the proposed\nmethod achieves competitive performance precision-recall and accuracy metric\naverage is 88.1%, 86.43% and 86.1% with a 2.9M lightweight scale, respectively.\nMoreover, processing time of our model is 0.1s per frame, of which the speed is\nincreased by 1.5 times compared with existing methods.\n","authors":["Zhenghao Xi","Yuchao Shao","Yang Zheng","Xiang Liu","Yaqi Liu","Yitong Cai"],"pdf_url":"https://arxiv.org/pdf/2502.15307v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.15302v1","updated":"2025-02-21T08:50:39Z","published":"2025-02-21T08:50:39Z","title":"A Novel Riemannian Sparse Representation Learning Network for\n  Polarimetric SAR Image Classification","summary":"  Deep learning is an effective end-to-end method for Polarimetric Synthetic\nAperture Radar(PolSAR) image classification, but it lacks the guidance of\nrelated mathematical principle and is essentially a black-box model. In\naddition, existing deep models learn features in Euclidean space, where PolSAR\ncomplex matrix is commonly converted into a complex-valued vector as the\nnetwork input, distorting matrix structure and channel relationship. However,\nthe complex covariance matrix is Hermitian positive definite (HPD), and resides\non a Riemannian manifold instead of a Euclidean one. Existing methods cannot\nmeasure the geometric distance of HPD matrices and easily cause some\nmisclassifications due to inappropriate Euclidean measures. To address these\nissues, we propose a novel Riemannian Sparse Representation Learning Network\n(SRSR CNN) for PolSAR images. Firstly, a superpixel-based Riemannian Sparse\nRepresentation (SRSR) model is designed to learn the sparse features with\nRiemannian metric. Then, the optimization procedure of the SRSR model is\ninferred and further unfolded into an SRSRnet, which can automatically learn\nthe sparse coefficients and dictionary atoms. Furthermore, to learn contextual\nhigh-level features, a CNN-enhanced module is added to improve classification\nperformance. The proposed network is a Sparse Representation (SR) guided deep\nlearning model, which can directly utilize the covariance matrix as the network\ninput, and utilize Riemannian metric to learn geometric structure and sparse\nfeatures of complex matrices in Riemannian space. Experiments on three real\nPolSAR datasets demonstrate that the proposed method surpasses state-of-the-art\ntechniques in ensuring accurate edge details and correct region homogeneity for\nclassification.\n","authors":["Junfei Shi","Mengmeng Nie","Weisi Lin","Haiyan Jin","Junhuai Li","Rui Wang"],"pdf_url":"https://arxiv.org/pdf/2502.15302v1.pdf","comment":"13 pages, 9 figures"},{"id":"http://arxiv.org/abs/2502.15286v1","updated":"2025-02-21T08:26:42Z","published":"2025-02-21T08:26:42Z","title":"Soybean pod and seed counting in both outdoor fields and indoor\n  laboratories using unions of deep neural networks","summary":"  Automatic counting soybean pods and seeds in outdoor fields allows for rapid\nyield estimation before harvesting, while indoor laboratory counting offers\ngreater accuracy. Both methods can significantly accelerate the breeding\nprocess. However, it remains challenging for accurately counting pods and seeds\nin outdoor fields, and there are still no accurate enough tools for counting\npods and seeds in laboratories. In this study, we developed efficient deep\nlearning models for counting soybean pods and seeds in both outdoor fields and\nindoor laboratories. For outdoor fields, annotating not only visible seeds but\nalso occluded seeds makes YOLO have the ability to estimate the number of\nsoybean seeds that are occluded. Moreover, we enhanced YOLO architecture by\nintegrating it with HQ-SAM (YOLO-SAM), and domain adaptation techniques\n(YOLO-DA), to improve model robustness and generalization across soybean images\ntaken in outdoor fields. Testing on soybean images from the outdoor field, we\nachieved a mean absolute error (MAE) of 6.13 for pod counting and 10.05 for\nseed counting. For the indoor setting, we utilized Mask-RCNN supplemented with\na Swin Transformer module (Mask-RCNN-Swin), models were trained exclusively on\nsynthetic training images generated from a small set of labeled data. This\napproach resulted in near-perfect accuracy, with an MAE of 1.07 for pod\ncounting and 1.33 for seed counting across actual laboratory images from two\ndistinct studies.\n","authors":["Tianyou Jiang","Mingshun Shao","Tianyi Zhang","Xiaoyu Liu","Qun Yu"],"pdf_url":"https://arxiv.org/pdf/2502.15286v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.14795v2","updated":"2025-02-21T08:09:14Z","published":"2025-02-20T18:17:11Z","title":"Humanoid-VLA: Towards Universal Humanoid Control with Visual Integration","summary":"  This paper addresses the limitations of current humanoid robot control\nframeworks, which primarily rely on reactive mechanisms and lack autonomous\ninteraction capabilities due to data scarcity. We propose Humanoid-VLA, a novel\nframework that integrates language understanding, egocentric scene perception,\nand motion control, enabling universal humanoid control. Humanoid-VLA begins\nwith language-motion pre-alignment using non-egocentric human motion datasets\npaired with textual descriptions, allowing the model to learn universal motion\npatterns and action semantics. We then incorporate egocentric visual context\nthrough a parameter efficient video-conditioned fine-tuning, enabling\ncontext-aware motion generation. Furthermore, we introduce a self-supervised\ndata augmentation strategy that automatically generates pseudoannotations\ndirectly derived from motion data. This process converts raw motion sequences\ninto informative question-answer pairs, facilitating the effective use of\nlarge-scale unlabeled video data. Built upon whole-body control architectures,\nextensive experiments show that Humanoid-VLA achieves object interaction and\nenvironment exploration tasks with enhanced contextual awareness, demonstrating\na more human-like capacity for adaptive and intelligent engagement.\n","authors":["Pengxiang Ding","Jianfei Ma","Xinyang Tong","Binghong Zou","Xinxin Luo","Yiguo Fan","Ting Wang","Hongchao Lu","Panzhong Mo","Jinxin Liu","Yuefan Wang","Huaicheng Zhou","Wenshuo Feng","Jiacheng Liu","Siteng Huang","Donglin Wang"],"pdf_url":"https://arxiv.org/pdf/2502.14795v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.15278v1","updated":"2025-02-21T08:09:07Z","published":"2025-02-21T08:09:07Z","title":"CopyJudge: Automated Copyright Infringement Identification and\n  Mitigation in Text-to-Image Diffusion Models","summary":"  Assessing whether AI-generated images are substantially similar to\ncopyrighted works is a crucial step in resolving copyright disputes. In this\npaper, we propose CopyJudge, an automated copyright infringement identification\nframework that leverages large vision-language models (LVLMs) to simulate\npractical court processes for determining substantial similarity between\ncopyrighted images and those generated by text-to-image diffusion models.\nSpecifically, we employ an abstraction-filtration-comparison test framework\nwith multi-LVLM debate to assess the likelihood of infringement and provide\ndetailed judgment rationales. Based on the judgments, we further introduce a\ngeneral LVLM-based mitigation strategy that automatically optimizes infringing\nprompts by avoiding sensitive expressions while preserving the non-infringing\ncontent. Besides, our approach can be enhanced by exploring non-infringing\nnoise vectors within the diffusion latent space via reinforcement learning,\neven without modifying the original prompts. Experimental results show that our\nidentification method achieves comparable state-of-the-art performance, while\noffering superior generalization and interpretability across various forms of\ninfringement, and that our mitigation method could more effectively mitigate\nmemorization and IP infringement without losing non-infringing expressions.\n","authors":["Shunchang Liu","Zhuan Shi","Lingjuan Lyu","Yaochu Jin","Boi Faltings"],"pdf_url":"https://arxiv.org/pdf/2502.15278v1.pdf","comment":"17pages, 8 figures"},{"id":"http://arxiv.org/abs/2501.01045v3","updated":"2025-02-21T08:00:02Z","published":"2025-01-02T04:10:17Z","title":"ZeroFlow: Overcoming Catastrophic Forgetting is Easier than You Think","summary":"  Backpropagation provides a generalized configuration for overcoming\ncatastrophic forgetting. Like, SGD and Adam are commonly used for weight\nupdates in continual learning and continual pre-training. In practice,\npermission to access gradient information is not always granted (the gradient\nban), such as black-box APIs, hardware limitations, and non-differentiable\nsystems. To bridge this gap, we introduce the first benchmark ZeroFlow to\nevaluate gradient-free optimization algorithms for overcoming forgetting. This\nbenchmark examines a suite of forward pass methods across multiple methods,\nforgetting scenarios, and datasets. We find that forward passes alone are\nenough to overcome forgetting. Our findings reveal new optimization principles\nthat highlight the potential of forward-pass in mitigating forgetting, managing\ntask conflicts, and reducing memory demands, alongside novel enhancements that\nfurther mitigate forgetting with just one forward pass. This work provides\nessential insights and tools for advancing forward pass methods to overcome\nforgetting.\n","authors":["Tao Feng","Wei Li","Didi Zhu","Hangjie Yuan","Wendi Zheng","Dan Zhang","Jie Tang"],"pdf_url":"https://arxiv.org/pdf/2501.01045v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.15271v1","updated":"2025-02-21T07:54:00Z","published":"2025-02-21T07:54:00Z","title":"Omnidirectional Image Quality Captioning: A Large-scale Database and A\n  New Model","summary":"  The fast growing application of omnidirectional images calls for effective\napproaches for omnidirectional image quality assessment (OIQA). Existing OIQA\nmethods have been developed and tested on homogeneously distorted\nomnidirectional images, but it is hard to transfer their success directly to\nthe heterogeneously distorted omnidirectional images. In this paper, we conduct\nthe largest study so far on OIQA, where we establish a large-scale database\ncalled OIQ-10K containing 10,000 omnidirectional images with both homogeneous\nand heterogeneous distortions. A comprehensive psychophysical study is\nelaborated to collect human opinions for each omnidirectional image, together\nwith the spatial distributions (within local regions or globally) of\ndistortions, and the head and eye movements of the subjects. Furthermore, we\npropose a novel multitask-derived adaptive feature-tailoring OIQA model named\nIQCaption360, which is capable of generating a quality caption for an\nomnidirectional image in a manner of textual template. Extensive experiments\ndemonstrate the effectiveness of IQCaption360, which outperforms\nstate-of-the-art methods by a significant margin on the proposed OIQ-10K\ndatabase. The OIQ-10K database and the related source codes are available at\nhttps://github.com/WenJuing/IQCaption360.\n","authors":["Jiebin Yan","Ziwen Tan","Yuming Fang","Junjie Chen","Wenhui Jiang","Zhou Wang"],"pdf_url":"https://arxiv.org/pdf/2502.15271v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.07277v3","updated":"2025-02-21T07:51:06Z","published":"2024-12-10T08:07:19Z","title":"Backdoor Attacks against No-Reference Image Quality Assessment Models\n  via a Scalable Trigger","summary":"  No-Reference Image Quality Assessment (NR-IQA), responsible for assessing the\nquality of a single input image without using any reference, plays a critical\nrole in evaluating and optimizing computer vision systems, e.g., low-light\nenhancement. Recent research indicates that NR-IQA models are susceptible to\nadversarial attacks, which can significantly alter predicted scores with\nvisually imperceptible perturbations. Despite revealing vulnerabilities, these\nattack methods have limitations, including high computational demands,\nuntargeted manipulation, limited practical utility in white-box scenarios, and\nreduced effectiveness in black-box scenarios. To address these challenges, we\nshift our focus to another significant threat and present a novel\npoisoning-based backdoor attack against NR-IQA (BAIQA), allowing the attacker\nto manipulate the IQA model's output to any desired target value by simply\nadjusting a scaling coefficient $\\alpha$ for the trigger. We propose to inject\nthe trigger in the discrete cosine transform (DCT) domain to improve the local\ninvariance of the trigger for countering trigger diminishment in NR-IQA models\ndue to widely adopted data augmentations. Furthermore, the universal\nadversarial perturbations (UAP) in the DCT space are designed as the trigger,\nto increase IQA model susceptibility to manipulation and improve attack\neffectiveness. In addition to the heuristic method for poison-label BAIQA\n(P-BAIQA), we explore the design of clean-label BAIQA (C-BAIQA), focusing on\n$\\alpha$ sampling and image data refinement, driven by theoretical insights we\nreveal. Extensive experiments on diverse datasets and various NR-IQA models\ndemonstrate the effectiveness of our attacks. Code can be found at\nhttps://github.com/yuyi-sd/BAIQA.\n","authors":["Yi Yu","Song Xia","Xun Lin","Wenhan Yang","Shijian Lu","Yap-peng Tan","Alex Kot"],"pdf_url":"https://arxiv.org/pdf/2412.07277v3.pdf","comment":"Accept by AAAI 2025 (Also fix the typo mistakes in line 9 of the\n  Algorithm 2 in the AAAI camera-ready version)"},{"id":"http://arxiv.org/abs/2502.14420v2","updated":"2025-02-21T07:28:36Z","published":"2025-02-20T10:16:18Z","title":"ChatVLA: Unified Multimodal Understanding and Robot Control with\n  Vision-Language-Action Model","summary":"  Humans possess a unified cognitive ability to perceive, comprehend, and\ninteract with the physical world. Why can't large language models replicate\nthis holistic understanding? Through a systematic analysis of existing training\nparadigms in vision-language-action models (VLA), we identify two key\nchallenges: spurious forgetting, where robot training overwrites crucial\nvisual-text alignments, and task interference, where competing control and\nunderstanding tasks degrade performance when trained jointly. To overcome these\nlimitations, we propose ChatVLA, a novel framework featuring Phased Alignment\nTraining, which incrementally integrates multimodal data after initial control\nmastery, and a Mixture-of-Experts architecture to minimize task interference.\nChatVLA demonstrates competitive performance on visual question-answering\ndatasets and significantly surpasses state-of-the-art vision-language-action\n(VLA) methods on multimodal understanding benchmarks. Notably, it achieves a\nsix times higher performance on MMMU and scores 47.2% on MMStar with a more\nparameter-efficient design than ECoT. Furthermore, ChatVLA demonstrates\nsuperior performance on 25 real-world robot manipulation tasks compared to\nexisting VLA methods like OpenVLA. Our findings highlight the potential of our\nunified framework for achieving both robust multimodal understanding and\neffective robot control.\n","authors":["Zhongyi Zhou","Yichen Zhu","Minjie Zhu","Junjie Wen","Ning Liu","Zhiyuan Xu","Weibin Meng","Ran Cheng","Yaxin Peng","Chaomin Shen","Feifei Feng"],"pdf_url":"https://arxiv.org/pdf/2502.14420v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.15254v1","updated":"2025-02-21T07:13:38Z","published":"2025-02-21T07:13:38Z","title":"Quantum autoencoders for image classification","summary":"  Classical machine learning often struggles with complex, high-dimensional\ndata. Quantum machine learning offers a potential solution, promising more\nefficient processing. While the quantum convolutional neural network (QCNN), a\nhybrid quantum-classical algorithm, is suitable for current noisy\nintermediate-scale quantum-era hardware, its learning process relies heavily on\nclassical computation. Future large-scale, gate-based quantum computers could\nunlock the full potential of quantum effects in machine learning. In contrast\nto QCNNs, quantum autoencoders (QAEs) leverage classical optimization solely\nfor parameter tuning. Data compression and reconstruction are handled entirely\nwithin quantum circuits, enabling purely quantum-based feature extraction. This\nstudy introduces a novel image-classification approach using QAEs, achieving\nclassification without requiring additional qubits compared with conventional\nQAE implementations. The quantum circuit structure significantly impacts\nclassification accuracy. Unlike hybrid methods such as QCNN, QAE-based\nclassification emphasizes quantum computation. Our experiments demonstrate high\naccuracy in a four-class classification task, evaluating various quantum-gate\nconfigurations to understand the impact of different parameterized quantum\ncircuit (ansatz) structures on classification performance. Our results reveal\nthat specific ansatz structures achieve superior accuracy, and we provide an\nanalysis of their effectiveness. Moreover, the proposed approach achieves\nperformance comparable to that of conventional machine-learning methods while\nsignificantly reducing the number of parameters requiring optimization. These\nfindings indicate that QAEs can serve as efficient classification models with\nfewer parameters and highlight the potential of utilizing quantum circuits for\ncomplete end-to-end learning, a departure from hybrid approaches such as QCNN.\n","authors":["Hinako Asaoka","Kazue Kudo"],"pdf_url":"https://arxiv.org/pdf/2502.15254v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2308.14024v2","updated":"2025-02-21T07:13:37Z","published":"2023-08-27T07:25:51Z","title":"Balanced Representation Learning for Long-tailed Skeleton-based Action\n  Recognition","summary":"  Skeleton-based action recognition has recently made significant progress.\nHowever, data imbalance is still a great challenge in real-world scenarios. The\nperformance of current action recognition algorithms declines sharply when\ntraining data suffers from heavy class imbalance. The imbalanced data actually\ndegrades the representations learned by these methods and becomes the\nbottleneck for action recognition. How to learn unbiased representations from\nimbalanced action data is the key to long-tailed action recognition. In this\npaper, we propose a novel balanced representation learning method to address\nthe long-tailed problem in action recognition. Firstly, a spatial-temporal\naction exploration strategy is presented to expand the sample space\neffectively, generating more valuable samples in a rebalanced manner. Secondly,\nwe design a detached action-aware learning schedule to further mitigate the\nbias in the representation space. The schedule detaches the representation\nlearning of tail classes from training and proposes an action-aware loss to\nimpose more effective constraints. Additionally, a skip-modal representation is\nproposed to provide complementary structural information. The proposed method\nis validated on four skeleton datasets, NTU RGB+D 60, NTU RGB+D 120, NW-UCLA,\nand Kinetics. It not only achieves consistently large improvement compared to\nthe state-of-the-art (SOTA) methods, but also demonstrates a superior\ngeneralization capacity through extensive experiments. Our code is available at\nhttps://github.com/firework8/BRL.\n","authors":["Hongda Liu","Yunlong Wang","Min Ren","Junxing Hu","Zhengquan Luo","Guangqi Hou","Zhenan Sun"],"pdf_url":"https://arxiv.org/pdf/2308.14024v2.pdf","comment":"Accepted by Machine Intelligence Research\n  https://link.springer.com/article/10.1007/s11633-023-1487-8"},{"id":"http://arxiv.org/abs/2402.02112v4","updated":"2025-02-21T07:11:48Z","published":"2024-02-03T10:35:42Z","title":"S-NeRF++: Autonomous Driving Simulation via Neural Reconstruction and\n  Generation","summary":"  Autonomous driving simulation system plays a crucial role in enhancing\nself-driving data and simulating complex and rare traffic scenarios, ensuring\nnavigation safety. However, traditional simulation systems, which often heavily\nrely on manual modeling and 2D image editing, struggled with scaling to\nextensive scenes and generating realistic simulation data. In this study, we\npresent S-NeRF++, an innovative autonomous driving simulation system based on\nneural reconstruction. Trained on widely-used self-driving datasets such as\nnuScenes and Waymo, S-NeRF++ can generate a large number of realistic street\nscenes and foreground objects with high rendering quality as well as offering\nconsiderable flexibility in manipulation and simulation. Specifically, S-NeRF++\nis an enhanced neural radiance field for synthesizing large-scale scenes and\nmoving vehicles, with improved scene parameterization and camera pose learning.\nThe system effectively utilizes noisy and sparse LiDAR data to refine training\nand address depth outliers, ensuring high-quality reconstruction and novel-view\nrendering. It also provides a diverse foreground asset bank by reconstructing\nand generating different foreground vehicles to support comprehensive scenario\ncreation.Moreover, we have developed an advanced foreground-background fusion\npipeline that skillfully integrates illumination and shadow effects, further\nenhancing the realism of our simulations. With the high-quality simulated data\nprovided by our S-NeRF++, we found the perception methods enjoy performance\nboosts on several autonomous driving downstream tasks, further demonstrating\nour proposed simulator's effectiveness.\n","authors":["Yurui Chen","Junge Zhang","Ziyang Xie","Wenye Li","Feihu Zhang","Jiachen Lu","Li Zhang"],"pdf_url":"https://arxiv.org/pdf/2402.02112v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.03783v2","updated":"2025-02-21T07:03:17Z","published":"2025-02-06T05:11:29Z","title":"UltraBones100k: A reliable automated labeling method and large-scale\n  dataset for ultrasound-based bone surface extraction","summary":"  Ultrasound-based bone surface segmentation is crucial in computer-assisted\northopedic surgery. However, ultrasound images have limitations, including a\nlow signal-to-noise ratio, and acoustic shadowing, which make interpretation\ndifficult. Existing deep learning models for bone segmentation rely primarily\non costly manual labeling by experts, limiting dataset size and model\ngeneralizability. Additionally, the complexity of ultrasound physics and\nacoustic shadow makes the images difficult for humans to interpret, leading to\nincomplete labels in anechoic regions and limiting model performance. To\nadvance ultrasound bone segmentation and establish effective model benchmarks,\nlarger and higher-quality datasets are needed.\n  We propose a methodology for collecting ex-vivo ultrasound datasets with\nautomatically generated bone labels, including anechoic regions. The proposed\nlabels are derived by accurately superimposing tracked bone CT models onto the\ntracked ultrasound images. These initial labels are refined to account for\nultrasound physics. A clinical evaluation is conducted by an expert physician\nspecialized on orthopedic sonography to assess the quality of the generated\nbone labels. A neural network for bone segmentation is trained on the collected\ndataset and its predictions are compared to expert manual labels, evaluating\naccuracy, completeness, and F1-score.\n  We collected the largest known dataset of 100k ultrasound images of human\nlower limbs with bone labels, called UltraBones100k. A Wilcoxon signed-rank\ntest with Bonferroni correction confirmed that the bone alignment after our\nmethod significantly improved the quality of bone labeling (p < 0.001). The\nmodel trained on UltraBones100k consistently outperforms manual labeling in all\nmetrics, particularly in low-intensity regions (320% improvement in\ncompleteness at a distance threshold of 0.5 mm).\n","authors":["Luohong Wu","Nicola A. Cavalcanti","Matthias Seibold","Giuseppe Loggia","Lisa Reissner","Jonas Hein","Silvan Beeler","Arnd Viehöfer","Stephan Wirth","Lilian Calvet","Philipp Fürnstahl"],"pdf_url":"https://arxiv.org/pdf/2502.03783v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.15251v1","updated":"2025-02-21T07:02:05Z","published":"2025-02-21T07:02:05Z","title":"SiMHand: Mining Similar Hands for Large-Scale 3D Hand Pose Pre-training","summary":"  We present a framework for pre-training of 3D hand pose estimation from\nin-the-wild hand images sharing with similar hand characteristics, dubbed\nSimHand. Pre-training with large-scale images achieves promising results in\nvarious tasks, but prior methods for 3D hand pose pre-training have not fully\nutilized the potential of diverse hand images accessible from in-the-wild\nvideos. To facilitate scalable pre-training, we first prepare an extensive pool\nof hand images from in-the-wild videos and design our pre-training method with\ncontrastive learning. Specifically, we collect over 2.0M hand images from\nrecent human-centric videos, such as 100DOH and Ego4D. To extract\ndiscriminative information from these images, we focus on the similarity of\nhands: pairs of non-identical samples with similar hand poses. We then propose\na novel contrastive learning method that embeds similar hand pairs closer in\nthe feature space. Our method not only learns from similar samples but also\nadaptively weights the contrastive learning loss based on inter-sample\ndistance, leading to additional performance gains. Our experiments demonstrate\nthat our method outperforms conventional contrastive learning approaches that\nproduce positive pairs sorely from a single image with data augmentation. We\nachieve significant improvements over the state-of-the-art method (PeCLR) in\nvarious datasets, with gains of 15% on FreiHand, 10% on DexYCB, and 4% on\nAssemblyHands.\n  Our code is available at https://github.com/ut-vision/SiMHand.\n","authors":["Nie Lin","Takehiko Ohkawa","Yifei Huang","Mingfang Zhang","Minjie Cai","Ming Li","Ryosuke Furuta","Yoichi Sato"],"pdf_url":"https://arxiv.org/pdf/2502.15251v1.pdf","comment":"ICLR 2025. arXiv admin note: text overlap with arXiv:2409.09714"},{"id":"http://arxiv.org/abs/2407.21363v2","updated":"2025-02-21T06:56:59Z","published":"2024-07-31T06:20:21Z","title":"ESIQA: Perceptual Quality Assessment of Vision-Pro-based Egocentric\n  Spatial Images","summary":"  With the development of eXtended Reality (XR), photo capturing and display\ntechnology based on head-mounted displays (HMDs) have experienced significant\nadvancements and gained considerable attention. Egocentric spatial images and\nvideos are emerging as a compelling form of stereoscopic XR content. The\nassessment for the Quality of Experience (QoE) of XR content is important to\nensure a high-quality viewing experience. Different from traditional 2D images,\negocentric spatial images present challenges for perceptual quality assessment\ndue to their special shooting, processing methods, and stereoscopic\ncharacteristics. However, the corresponding image quality assessment (IQA)\nresearch for egocentric spatial images is still lacking. In this paper, we\nestablish the Egocentric Spatial Images Quality Assessment Database (ESIQAD),\nthe first IQA database dedicated for egocentric spatial images as far as we\nknow. Our ESIQAD includes 500 egocentric spatial images and the corresponding\nmean opinion scores (MOSs) under three display modes, including 2D display,\n3D-window display, and 3D-immersive display. Based on our ESIQAD, we propose a\nnovel mamba2-based multi-stage feature fusion model, termed ESIQAnet, which\npredicts the perceptual quality of egocentric spatial images under the three\ndisplay modes. Specifically, we first extract features from multiple visual\nstate space duality (VSSD) blocks, then apply cross attention to fuse binocular\nview information and use transposed attention to further refine the features.\nThe multi-stage features are finally concatenated and fed into a quality\nregression network to predict the quality score. Extensive experimental results\ndemonstrate that the ESIQAnet outperforms 22 state-of-the-art IQA models on the\nESIQAD under all three display modes. The database and code are available at\nhttps://github.com/IntMeGroup/ESIQA.\n","authors":["Xilei Zhu","Liu Yang","Huiyu Duan","Xiongkuo Min","Guangtao Zhai","Patrick Le Callet"],"pdf_url":"https://arxiv.org/pdf/2407.21363v2.pdf","comment":"9 pages, 12 figures"},{"id":"http://arxiv.org/abs/2311.14262v4","updated":"2025-02-21T06:54:39Z","published":"2023-11-24T03:19:17Z","title":"ZeroPS: High-quality Cross-modal Knowledge Transfer for Zero-Shot 3D\n  Part Segmentation","summary":"  Zero-shot 3D part segmentation is a challenging and fundamental task. In this\nwork, we propose a novel pipeline, ZeroPS, which achieves high-quality\nknowledge transfer from 2D pretrained foundation models (FMs), SAM and GLIP, to\n3D object point clouds. We aim to explore the natural relationship between\nmulti-view correspondence and the FMs' prompt mechanism and build bridges on\nit. In ZeroPS, the relationship manifests as follows: 1) lifting 2D to 3D by\nleveraging co-viewed regions and SAM's prompt mechanism, 2) relating 1D classes\nto 3D parts by leveraging 2D-3D view projection and GLIP's prompt mechanism,\nand 3) enhancing prediction performance by leveraging multi-view observations.\nExtensive evaluations on the PartNetE and AKBSeg benchmarks demonstrate that\nZeroPS significantly outperforms the SOTA method across zero-shot unlabeled and\ninstance segmentation tasks. ZeroPS does not require additional training or\nfine-tuning for the FMs. ZeroPS applies to both simulated and real-world data.\nIt is hardly affected by domain shift. The project page is available at\nhttps://luis2088.github.io/ZeroPS_page.\n","authors":["Yuheng Xue","Nenglun Chen","Jun Liu","Wenyun Sun"],"pdf_url":"https://arxiv.org/pdf/2311.14262v4.pdf","comment":"2025 International Conference on 3D Vision (3DV)"},{"id":"http://arxiv.org/abs/2408.07416v3","updated":"2025-02-21T06:43:11Z","published":"2024-08-14T09:50:02Z","title":"Rethinking Open-Vocabulary Segmentation of Radiance Fields in 3D Space","summary":"  Understanding the 3D semantics of a scene is a fundamental problem for\nvarious scenarios such as embodied agents. While NeRFs and 3DGS excel at\nnovel-view synthesis, previous methods for understanding their semantics have\nbeen limited to incomplete 3D understanding: their segmentation results are\nrendered as 2D masks that do not represent the entire 3D space. To address this\nlimitation, we redefine the problem to segment the 3D volume and propose the\nfollowing methods for better 3D understanding. We directly supervise the 3D\npoints to train the language embedding field, unlike previous methods that\nanchor supervision at 2D pixels. We transfer the learned language field to\n3DGS, achieving the first real-time rendering speed without sacrificing\ntraining time or accuracy. Lastly, we introduce a 3D querying and evaluation\nprotocol for assessing the reconstructed geometry and semantics together. Code,\ncheckpoints, and annotations are available at the project page.\n","authors":["Hyunjee Lee","Youngsik Yun","Jeongmin Bae","Seoha Kim","Youngjung Uh"],"pdf_url":"https://arxiv.org/pdf/2408.07416v3.pdf","comment":"AAAI 2025. Project page: https://hyunji12.github.io/Open3DRF"},{"id":"http://arxiv.org/abs/2303.12270v2","updated":"2025-02-21T06:11:50Z","published":"2023-03-22T02:36:13Z","title":"SCALES: Boost Binary Neural Network for Image Super-Resolution with\n  Efficient Scalings","summary":"  Deep neural networks for image super-resolution (SR) have demonstrated\nsuperior performance. However, the large memory and computation consumption\nhinders their deployment on resource-constrained devices. Binary neural\nnetworks (BNNs), which quantize the floating point weights and activations to\n1-bit can significantly reduce the cost. Although BNNs for image classification\nhave made great progress these days, existing BNNs for SR still suffer from a\nlarge performance gap between the FP SR networks. To this end, we observe the\nactivation distribution in SR networks and find much larger pixel-to-pixel,\nchannel-to-channel, layer-to-layer, and image-to-image variation in the\nactivation distribution than image classification networks. However, existing\nBNNs for SR fail to capture these variations that contain rich information for\nimage reconstruction, leading to inferior performance. To address this problem,\nwe propose SCALES, a binarization method for SR networks that consists of the\nlayer-wise scaling factor, the spatial re-scaling method, and the channel-wise\nre-scaling method, capturing the layer-wise, pixel-wise, and channel-wise\nvariations efficiently in an input-dependent manner. We evaluate our method\nacross different network architectures and datasets. For CNN-based SR networks,\nour binarization method SCALES outperforms the prior art method by 0.2dB with\nfewer parameters and operations. With SCALES, we achieve the first accurate\nbinary Transformer-based SR network, improving PSNR by more than 1dB compared\nto the baseline method.\n","authors":["Renjie Wei","Zechun Liu","Yuchen Fan","Runsheng Wang","Ru Huang","Meng Li"],"pdf_url":"https://arxiv.org/pdf/2303.12270v2.pdf","comment":"Accpeted by DATE 2025"},{"id":"http://arxiv.org/abs/2502.15228v1","updated":"2025-02-21T05:59:41Z","published":"2025-02-21T05:59:41Z","title":"AutoMR: A Universal Time Series Motion Recognition Pipeline","summary":"  In this paper, we present an end-to-end automated motion recognition (AutoMR)\npipeline designed for multimodal datasets. The proposed framework seamlessly\nintegrates data preprocessing, model training, hyperparameter tuning, and\nevaluation, enabling robust performance across diverse scenarios. Our approach\naddresses two primary challenges: 1) variability in sensor data formats and\nparameters across datasets, which traditionally requires task-specific machine\nlearning implementations, and 2) the complexity and time consumption of\nhyperparameter tuning for optimal model performance. Our library features an\nall-in-one solution incorporating QuartzNet as the core model, automated\nhyperparameter tuning, and comprehensive metrics tracking. Extensive\nexperiments demonstrate its effectiveness on 10 diverse datasets, achieving\nstate-of-the-art performance. This work lays a solid foundation for deploying\nmotion-capture solutions across varied real-world applications.\n","authors":["Likun Zhang","Sicheng Yang","Zhuo Wang","Haining Liang","Junxiao Shen"],"pdf_url":"https://arxiv.org/pdf/2502.15228v1.pdf","comment":"5 figures"},{"id":"http://arxiv.org/abs/2502.11744v2","updated":"2025-02-21T05:54:10Z","published":"2025-02-17T12:34:42Z","title":"FUNCTO: Function-Centric One-Shot Imitation Learning for Tool\n  Manipulation","summary":"  Learning tool use from a single human demonstration video offers a highly\nintuitive and efficient approach to robot teaching. While humans can\neffortlessly generalize a demonstrated tool manipulation skill to diverse tools\nthat support the same function (e.g., pouring with a mug versus a teapot),\ncurrent one-shot imitation learning (OSIL) methods struggle to achieve this. A\nkey challenge lies in establishing functional correspondences between\ndemonstration and test tools, considering significant geometric variations\namong tools with the same function (i.e., intra-function variations). To\naddress this challenge, we propose FUNCTO (Function-Centric OSIL for Tool\nManipulation), an OSIL method that establishes function-centric correspondences\nwith a 3D functional keypoint representation, enabling robots to generalize\ntool manipulation skills from a single human demonstration video to novel tools\nwith the same function despite significant intra-function variations. With this\nformulation, we factorize FUNCTO into three stages: (1) functional keypoint\nextraction, (2) function-centric correspondence establishment, and (3)\nfunctional keypoint-based action planning. We evaluate FUNCTO against exiting\nmodular OSIL methods and end-to-end behavioral cloning methods through\nreal-robot experiments on diverse tool manipulation tasks. The results\ndemonstrate the superiority of FUNCTO when generalizing to novel tools with\nintra-function geometric variations. More details are available at\nhttps://sites.google.com/view/functo.\n","authors":["Chao Tang","Anxing Xiao","Yuhong Deng","Tianrun Hu","Wenlong Dong","Hanbo Zhang","David Hsu","Hong Zhang"],"pdf_url":"https://arxiv.org/pdf/2502.11744v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.14801v2","updated":"2025-02-21T05:33:06Z","published":"2025-02-20T18:22:44Z","title":"AVD2: Accident Video Diffusion for Accident Video Description","summary":"  Traffic accidents present complex challenges for autonomous driving, often\nfeaturing unpredictable scenarios that hinder accurate system interpretation\nand responses. Nonetheless, prevailing methodologies fall short in elucidating\nthe causes of accidents and proposing preventive measures due to the paucity of\ntraining data specific to accident scenarios. In this work, we introduce AVD2\n(Accident Video Diffusion for Accident Video Description), a novel framework\nthat enhances accident scene understanding by generating accident videos that\naligned with detailed natural language descriptions and reasoning, resulting in\nthe contributed EMM-AU (Enhanced Multi-Modal Accident Video Understanding)\ndataset. Empirical results reveal that the integration of the EMM-AU dataset\nestablishes state-of-the-art performance across both automated metrics and\nhuman evaluations, markedly advancing the domains of accident analysis and\nprevention. Project resources are available at https://an-answer-tree.github.io\n","authors":["Cheng Li","Keyuan Zhou","Tong Liu","Yu Wang","Mingqiao Zhuang","Huan-ang Gao","Bu Jin","Hao Zhao"],"pdf_url":"https://arxiv.org/pdf/2502.14801v2.pdf","comment":"ICRA 2025, Project Page: https://an-answer-tree.github.io/"},{"id":"http://arxiv.org/abs/2501.15464v2","updated":"2025-02-21T05:16:21Z","published":"2025-01-26T09:54:10Z","title":"TractoGPT: A GPT architecture for White Matter Segmentation","summary":"  White matter bundle segmentation is crucial for studying brain structural\nconnectivity, neurosurgical planning, and neurological disorders. White Matter\nSegmentation remains challenging due to structural similarity in streamlines,\nsubject variability, symmetry in 2 hemispheres, etc. To address these\nchallenges, we propose TractoGPT, a GPT-based architecture trained on\nstreamline, cluster, and fusion data representations separately. TractoGPT is a\nfully-automatic method that generalizes across datasets and retains shape\ninformation of the white matter bundles. Experiments also show that TractoGPT\noutperforms state-of-the-art methods on average DICE, Overlap and Overreach\nscores. We use TractoInferno and 105HCP datasets and validate generalization\nacross dataset.\n","authors":["Anoushkrit Goel","Simroop Singh","Ankita Joshi","Ranjeet Ranjan Jha","Chirag Ahuja","Aditya Nigam","Arnav Bhavsar"],"pdf_url":"https://arxiv.org/pdf/2501.15464v2.pdf","comment":"Accepted as a conference paper at 23rd IEEE International Symposium\n  on Biomedical Imaging 2025. IEEE holds the copyright for this publication"},{"id":"http://arxiv.org/abs/2410.09453v3","updated":"2025-02-21T04:50:45Z","published":"2024-10-12T09:16:09Z","title":"MMAD: A Comprehensive Benchmark for Multimodal Large Language Models in\n  Industrial Anomaly Detection","summary":"  In the field of industrial inspection, Multimodal Large Language Models\n(MLLMs) have a high potential to renew the paradigms in practical applications\ndue to their robust language capabilities and generalization abilities.\nHowever, despite their impressive problem-solving skills in many domains,\nMLLMs' ability in industrial anomaly detection has not been systematically\nstudied. To bridge this gap, we present MMAD, the first-ever full-spectrum\nMLLMs benchmark in industrial Anomaly Detection. We defined seven key subtasks\nof MLLMs in industrial inspection and designed a novel pipeline to generate the\nMMAD dataset with 39,672 questions for 8,366 industrial images. With MMAD, we\nhave conducted a comprehensive, quantitative evaluation of various\nstate-of-the-art MLLMs. The commercial models performed the best, with the\naverage accuracy of GPT-4o models reaching 74.9%. However, this result falls\nfar short of industrial requirements. Our analysis reveals that current MLLMs\nstill have significant room for improvement in answering questions related to\nindustrial anomalies and defects. We further explore two training-free\nperformance enhancement strategies to help models improve in industrial\nscenarios, highlighting their promising potential for future research.\n","authors":["Xi Jiang","Jian Li","Hanqiu Deng","Yong Liu","Bin-Bin Gao","Yifeng Zhou","Jialin Li","Chengjie Wang","Feng Zheng"],"pdf_url":"https://arxiv.org/pdf/2410.09453v3.pdf","comment":"Accepted by ICLR 2025. The code and data are available at\n  https://github.com/jam-cc/MMAD"},{"id":"http://arxiv.org/abs/2502.15204v1","updated":"2025-02-21T04:38:27Z","published":"2025-02-21T04:38:27Z","title":"Lung-DDPM: Semantic Layout-guided Diffusion Models for Thoracic CT Image\n  Synthesis","summary":"  With the rapid development of artificial intelligence (AI), AI-assisted\nmedical imaging analysis demonstrates remarkable performance in early lung\ncancer screening. However, the costly annotation process and privacy concerns\nlimit the construction of large-scale medical datasets, hampering the further\napplication of AI in healthcare. To address the data scarcity in lung cancer\nscreening, we propose Lung-DDPM, a thoracic CT image synthesis approach that\neffectively generates high-fidelity 3D synthetic CT images, which prove helpful\nin downstream lung nodule segmentation tasks. Our method is based on semantic\nlayout-guided denoising diffusion probabilistic models (DDPM), enabling\nanatomically reasonable, seamless, and consistent sample generation even from\nincomplete semantic layouts. Our results suggest that the proposed method\noutperforms other state-of-the-art (SOTA) generative models in image quality\nevaluation and downstream lung nodule segmentation tasks. Specifically,\nLung-DDPM achieved superior performance on our large validation cohort, with a\nFr\\'echet inception distance (FID) of 0.0047, maximum mean discrepancy (MMD) of\n0.0070, and mean squared error (MSE) of 0.0024. These results were 7.4$\\times$,\n3.1$\\times$, and 29.5$\\times$ better than the second-best competitors,\nrespectively. Furthermore, the lung nodule segmentation model, trained on a\ndataset combining real and Lung-DDPM-generated synthetic samples, attained a\ndice coefficient (Dice) of 0.3914 and sensitivity of 0.4393. This represents\n8.8\\% and 18.6\\% improvements in DICE and sensitivity compared to the model\ntrained solely on real samples. The experimental results highlight Lung-DDPM's\npotential for a broader range of medical imaging applications, such as general\ntumor segmentation, cancer survival estimation, and risk prediction.\n","authors":["Yifan Jiang","Yannick Lemaréchal","Josée Bafaro","Jessica Abi-Rjeile","Philippe Joubert","Philippe Després","Venkata Manem"],"pdf_url":"https://arxiv.org/pdf/2502.15204v1.pdf","comment":"The code and pretrained models are available at\n  https://github.com/Manem-Lab/Lung-DDPM"},{"id":"http://arxiv.org/abs/2502.15203v1","updated":"2025-02-21T04:37:18Z","published":"2025-02-21T04:37:18Z","title":"FlipConcept: Tuning-Free Multi-Concept Personalization for Text-to-Image\n  Generation","summary":"  Recently, methods that integrate multiple personalized concepts into a single\nimage have garnered significant attention in the field of text-to-image (T2I)\ngeneration. However, existing methods experience performance degradation in\ncomplex scenes with multiple objects due to distortions in non-personalized\nregions. To address this issue, we propose FlipConcept, a novel approach that\nseamlessly integrates multiple personalized concepts into a single image\nwithout requiring additional tuning. We introduce guided appearance attention\nto accurately mimic the appearance of a personalized concept as intended.\nAdditionally, we introduce mask-guided noise mixing to protect non-personalized\nregions during editing. Lastly, we apply background dilution to minimize\nattribute leakage, which is the undesired blending of personalized concept\nattributes with other objects in the image. In our experiments, we demonstrate\nthat the proposed method, despite not requiring tuning, outperforms existing\nmodels in both single and multiple personalized concept inference.\n","authors":["Young Beom Woo","Sun Eung Kim"],"pdf_url":"https://arxiv.org/pdf/2502.15203v1.pdf","comment":"9 pages, 4 figures"},{"id":"http://arxiv.org/abs/2501.19164v2","updated":"2025-02-21T04:25:31Z","published":"2025-01-31T14:31:00Z","title":"Poison as Cure: Visual Noise for Mitigating Object Hallucinations in\n  LVMs","summary":"  Large vision-language models (LVMs) extend large language models (LLMs) with\nvisual perception capabilities, enabling them to process and interpret visual\ninformation. A major challenge compromising their reliability is object\nhallucination that LVMs may generate plausible but factually inaccurate\ninformation. We propose a novel visual adversarial perturbation (VAP) method to\nmitigate this hallucination issue. VAP alleviates LVM hallucination by applying\nstrategically optimized visual noise without altering the base model. Our\napproach formulates hallucination suppression as an optimization problem,\nleveraging adversarial strategies to generate beneficial visual perturbations\nthat enhance the model's factual grounding and reduce parametric knowledge\nbias. Extensive experimental results demonstrate that our method consistently\nreduces object hallucinations across 8 state-of-the-art LVMs, validating its\nefficacy across diverse evaluations.\n","authors":["Kejia Zhang","Keda Tao","Jiasheng Tang","Huan Wang"],"pdf_url":"https://arxiv.org/pdf/2501.19164v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.15199v1","updated":"2025-02-21T04:25:19Z","published":"2025-02-21T04:25:19Z","title":"UrbanSAM: Learning Invariance-Inspired Adapters for Segment Anything\n  Models in Urban Construction","summary":"  Object extraction and segmentation from remote sensing (RS) images is a\ncritical yet challenging task in urban environment monitoring. Urban morphology\nis inherently complex, with irregular objects of diverse shapes and varying\nscales. These challenges are amplified by heterogeneity and scale disparities\nacross RS data sources, including sensors, platforms, and modalities, making\naccurate object segmentation particularly demanding. While the Segment Anything\nModel (SAM) has shown significant potential in segmenting complex scenes, its\nperformance in handling form-varying objects remains limited due to\nmanual-interactive prompting. To this end, we propose UrbanSAM, a customized\nversion of SAM specifically designed to analyze complex urban environments\nwhile tackling scaling effects from remotely sensed observations. Inspired by\nmulti-resolution analysis (MRA) theory, UrbanSAM incorporates a novel learnable\nprompter equipped with a Uscaling-Adapter that adheres to the invariance\ncriterion, enabling the model to capture multiscale contextual information of\nobjects and adapt to arbitrary scale variations with theoretical guarantees.\nFurthermore, features from the Uscaling-Adapter and the trunk encoder are\naligned through a masked cross-attention operation, allowing the trunk encoder\nto inherit the adapter's multiscale aggregation capability. This synergy\nenhances the segmentation performance, resulting in more powerful and accurate\noutputs, supported by the learned adapter. Extensive experimental results\ndemonstrate the flexibility and superior segmentation performance of the\nproposed UrbanSAM on a global-scale dataset, encompassing scale-varying urban\nobjects such as buildings, roads, and water.\n","authors":["Chenyu Li","Danfeng Hong","Bing Zhang","Yuxuan Li","Gustau Camps-Valls","Xiao Xiang Zhu","Jocelyn Chanussot"],"pdf_url":"https://arxiv.org/pdf/2502.15199v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.19201v2","updated":"2025-02-21T04:05:29Z","published":"2024-04-30T01:59:25Z","title":"Exploring Quasi-Global Solutions to Compound Lens Based Computational\n  Imaging Systems","summary":"  Recently, joint design approaches that simultaneously optimize optical\nsystems and downstream algorithms through data-driven learning have\ndemonstrated superior performance over traditional separate design approaches.\nHowever, current joint design approaches heavily rely on the manual\nidentification of initial lenses, posing challenges and limitations,\nparticularly for compound lens systems with multiple potential starting points.\nIn this work, we present Quasi-Global Search Optics (QGSO) to automatically\ndesign compound lens based computational imaging systems through two parts: (i)\nFused Optimization Method for Automatic Optical Design (OptiFusion), which\nsearches for diverse initial optical systems under certain design\nspecifications; and (ii) Efficient Physic-aware Joint Optimization (EPJO),\nwhich conducts parallel joint optimization of initial optical systems and image\nreconstruction networks with the consideration of physical constraints,\nculminating in the selection of the optimal solution in all search results.\nExtensive experimental results illustrate that QGSO serves as a transformative\nend-to-end lens design paradigm for superior global search ability, which\nautomatically provides compound lens based computational imaging systems with\nhigher imaging quality compared to existing paradigms. The source code will be\nmade publicly available at https://github.com/LiGpy/QGSO.\n","authors":["Yao Gao","Qi Jiang","Shaohua Gao","Lei Sun","Kailun Yang","Kaiwei Wang"],"pdf_url":"https://arxiv.org/pdf/2404.19201v2.pdf","comment":"Accepted to IEEE Transactions on Computational Imaging (TCI). The\n  source code will be made publicly available at https://github.com/LiGpy/QGSO"},{"id":"http://arxiv.org/abs/2407.01509v4","updated":"2025-02-21T03:49:13Z","published":"2024-07-01T17:53:35Z","title":"MIA-Bench: Towards Better Instruction Following Evaluation of Multimodal\n  LLMs","summary":"  We introduce MIA-Bench, a new benchmark designed to evaluate multimodal large\nlanguage models (MLLMs) on their ability to strictly adhere to complex\ninstructions. Our benchmark comprises a diverse set of 400 image-prompt pairs,\neach crafted to challenge the models' compliance with layered instructions in\ngenerating accurate responses that satisfy specific requested patterns.\nEvaluation results from a wide array of state-of-the-art MLLMs reveal\nsignificant variations in performance, highlighting areas for improvement in\ninstruction fidelity. Additionally, we create extra training data and explore\nsupervised fine-tuning to enhance the models' ability to strictly follow\ninstructions without compromising performance on other tasks. We hope this\nbenchmark not only serves as a tool for measuring MLLM adherence to\ninstructions, but also guides future developments in MLLM training methods.\n","authors":["Yusu Qian","Hanrong Ye","Jean-Philippe Fauconnier","Peter Grasch","Yinfei Yang","Zhe Gan"],"pdf_url":"https://arxiv.org/pdf/2407.01509v4.pdf","comment":"Accepted at ICLR 2025"},{"id":"http://arxiv.org/abs/2406.19407v6","updated":"2025-02-21T03:48:42Z","published":"2024-06-12T06:41:23Z","title":"YOLOv12 to Its Genesis: A Decadal and Comprehensive Review of The You\n  Only Look Once (YOLO) Series","summary":"  This review systematically examines the progression of the You Only Look Once\n(YOLO) object detection algorithms from YOLOv1 to the recently unveiled\nYOLOv12. Employing a reverse chronological analysis, this study examines the\nadvancements introduced by YOLO algorithms, beginning with YOLOv12 and\nprogressing through YOLO11 (or YOLOv11), YOLOv10, YOLOv9, YOLOv8, and\nsubsequent versions to explore each version's contributions to enhancing speed,\ndetection accuracy, and computational efficiency in real-time object detection.\nAdditionally, this study reviews the alternative versions derived from YOLO\narchitectural advancements of YOLO-NAS, YOLO-X, YOLO-R, DAMO-YOLO, and\nGold-YOLO. By detailing the incremental technological advancements in\nsubsequent YOLO versions, this review chronicles the evolution of YOLO, and\ndiscusses the challenges and limitations in each of the earlier versions. The\nevolution signifies a path towards integrating YOLO with multimodal,\ncontext-aware, and Artificial General Intelligence (AGI) systems for the next\nYOLO decade, promising significant implications for future developments in\nAI-driven applications. (Key terms: YOLOv12, YOLOv12 architecture, YOLOv11,\nYOLO11, YOLO Review, YOLOv14, YOLOv15, YOLO architecture, YOLOv12 architecture)\n","authors":["Ranjan Sapkota","Rizwan Qureshi","Marco Flores Calero","Chetan Badjugar","Upesh Nepal","Alwin Poulose","Peter Zeno","Uday Bhanu Prakash Vaddevolu","Sheheryar Khan","Maged Shoman","Hong Yan","Manoj Karkee"],"pdf_url":"https://arxiv.org/pdf/2406.19407v6.pdf","comment":"11 Figures, 7 Tables"},{"id":"http://arxiv.org/abs/2502.14584v2","updated":"2025-02-21T03:42:34Z","published":"2025-02-20T14:13:46Z","title":"Vision Foundation Models in Medical Image Analysis: Advances and\n  Challenges","summary":"  The rapid development of Vision Foundation Models (VFMs), particularly Vision\nTransformers (ViT) and Segment Anything Model (SAM), has sparked significant\nadvances in the field of medical image analysis. These models have demonstrated\nexceptional capabilities in capturing long-range dependencies and achieving\nhigh generalization in segmentation tasks. However, adapting these large models\nto medical image analysis presents several challenges, including domain\ndifferences between medical and natural images, the need for efficient model\nadaptation strategies, and the limitations of small-scale medical datasets.\nThis paper reviews the state-of-the-art research on the adaptation of VFMs to\nmedical image segmentation, focusing on the challenges of domain adaptation,\nmodel compression, and federated learning. We discuss the latest developments\nin adapter-based improvements, knowledge distillation techniques, and\nmulti-scale contextual feature modeling, and propose future directions to\novercome these bottlenecks. Our analysis highlights the potential of VFMs,\nalong with emerging methodologies such as federated learning and model\ncompression, to revolutionize medical image analysis and enhance clinical\napplications. The goal of this work is to provide a comprehensive overview of\ncurrent approaches and suggest key areas for future research that can drive the\nnext wave of innovation in medical image segmentation.\n","authors":["Pengchen Liang","Bin Pu","Haishan Huang","Yiwei Li","Hualiang Wang","Weibo Ma","Qing Chang"],"pdf_url":"https://arxiv.org/pdf/2502.14584v2.pdf","comment":"17 pages, 1 figure"},{"id":"http://arxiv.org/abs/2502.15188v1","updated":"2025-02-21T03:40:27Z","published":"2025-02-21T03:40:27Z","title":"Interleaved Block-based Learned Image Compression with Feature\n  Enhancement and Quantization Error Compensation","summary":"  In recent years, learned image compression (LIC) methods have achieved\nsignificant performance improvements. However, obtaining a more compact latent\nrepresentation and reducing the impact of quantization errors remain key\nchallenges in the field of LIC. To address these challenges, we propose a\nfeature extraction module, a feature refinement module, and a feature\nenhancement module. Our feature extraction module shuffles the pixels in the\nimage, splits the resulting image into sub-images, and extracts coarse features\nfrom the sub-images. Our feature refinement module stacks the coarse features\nand uses an attention refinement block composed of concatenated\nthree-dimensional convolution residual blocks to learn more compact latent\nfeatures by exploiting correlations across channels, within sub-images\n(intra-sub-image correlations), and across sub-images (inter-sub-image\ncorrelations). Our feature enhancement module reduces information loss in the\ndecoded features following quantization. We also propose a quantization error\ncompensation module that mitigates the quantization mismatch between training\nand testing. Our four modules can be readily integrated into state-of-the-art\nLIC methods. Experiments show that combining our modules with Tiny-LIC\noutperforms existing LIC methods and image compression standards in terms of\npeak signal-to-noise ratio (PSNR) and multi-scale structural similarity\n(MS-SSIM) on the Kodak dataset and the CLIC dataset.\n","authors":["Shiqi Jiang","Hui Yuan","Shuai Li","Raouf Hamzaoui","Xu Wang","Junyan Huo"],"pdf_url":"https://arxiv.org/pdf/2502.15188v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.15186v1","updated":"2025-02-21T03:37:58Z","published":"2025-02-21T03:37:58Z","title":"LUMINA-Net: Low-light Upgrade through Multi-stage Illumination and Noise\n  Adaptation Network for Image Enhancement","summary":"  Low-light image enhancement (LLIE) is a crucial task in computer vision aimed\nto enhance the visual fidelity of images captured under low-illumination\nconditions. Conventional methods frequently struggle to mitigate pervasive\nshortcomings such as noise, over-exposure, and color distortion thereby\nprecipitating a pronounced degradation in image quality. To address these\nchallenges, we propose LUMINA-Net an advanced deep learning framework designed\nspecifically by integrating multi-stage illumination and reflectance modules.\nFirst, the illumination module intelligently adjusts brightness and contrast\nlevels while meticulously preserving intricate textural details. Second, the\nreflectance module incorporates a noise reduction mechanism that leverages\nspatial attention and channel-wise feature refinement to mitigate noise\ncontamination. Through a comprehensive suite of experiments conducted on LOL\nand SICE datasets using PSNR, SSIM and LPIPS metrics, surpassing\nstate-of-the-art methodologies and showcasing its efficacy in low-light image\nenhancement.\n","authors":["Namrah Siddiqua","Kim Suneung"],"pdf_url":"https://arxiv.org/pdf/2502.15186v1.pdf","comment":"9 pages, 4 figures"},{"id":"http://arxiv.org/abs/2502.15184v1","updated":"2025-02-21T03:36:16Z","published":"2025-02-21T03:36:16Z","title":"Hierarchical Context Transformer for Multi-level Semantic Scene\n  Understanding","summary":"  A comprehensive and explicit understanding of surgical scenes plays a vital\nrole in developing context-aware computer-assisted systems in the operating\ntheatre. However, few works provide systematical analysis to enable\nhierarchical surgical scene understanding. In this work, we propose to\nrepresent the tasks set [phase recognition --> step recognition --> action and\ninstrument detection] as multi-level semantic scene understanding (MSSU). For\nthis target, we propose a novel hierarchical context transformer (HCT) network\nand thoroughly explore the relations across the different level tasks.\nSpecifically, a hierarchical relation aggregation module (HRAM) is designed to\nconcurrently relate entries inside multi-level interaction information and then\naugment task-specific features. To further boost the representation learning of\nthe different tasks, inter-task contrastive learning (ICL) is presented to\nguide the model to learn task-wise features via absorbing complementary\ninformation from other tasks. Furthermore, considering the computational costs\nof the transformer, we propose HCT+ to integrate the spatial and temporal\nadapter to access competitive performance on substantially fewer tunable\nparameters. Extensive experiments on our cataract dataset and a publicly\navailable endoscopic PSI-AVA dataset demonstrate the outstanding performance of\nour method, consistently exceeding the state-of-the-art methods by a large\nmargin. The code is available at https://github.com/Aurora-hao/HCT.\n","authors":["Luoying Hao","Yan Hu","Yang Yue","Li Wu","Huazhu Fu","Jinming Duan","Jiang Liu"],"pdf_url":"https://arxiv.org/pdf/2502.15184v1.pdf","comment":"This paper has been accepted by the IEEE TCSVT"},{"id":"http://arxiv.org/abs/2502.15180v1","updated":"2025-02-21T03:21:48Z","published":"2025-02-21T03:21:48Z","title":"OccProphet: Pushing Efficiency Frontier of Camera-Only 4D Occupancy\n  Forecasting with Observer-Forecaster-Refiner Framework","summary":"  Predicting variations in complex traffic environments is crucial for the\nsafety of autonomous driving. Recent advancements in occupancy forecasting have\nenabled forecasting future 3D occupied status in driving environments by\nobserving historical 2D images. However, high computational demands make\noccupancy forecasting less efficient during training and inference stages,\nhindering its feasibility for deployment on edge agents. In this paper, we\npropose a novel framework, i.e., OccProphet, to efficiently and effectively\nlearn occupancy forecasting with significantly lower computational requirements\nwhile improving forecasting accuracy. OccProphet comprises three lightweight\ncomponents: Observer, Forecaster, and Refiner. The Observer extracts\nspatio-temporal features from 3D multi-frame voxels using the proposed\nEfficient 4D Aggregation with Tripling-Attention Fusion, while the Forecaster\nand Refiner conditionally predict and refine future occupancy inferences.\nExperimental results on nuScenes, Lyft-Level5, and nuScenes-Occupancy datasets\ndemonstrate that OccProphet is both training- and inference-friendly.\nOccProphet reduces 58\\%$\\sim$78\\% of the computational cost with a 2.6$\\times$\nspeedup compared with the state-of-the-art Cam4DOcc. Moreover, it achieves\n4\\%$\\sim$18\\% relatively higher forecasting accuracy. Code and models are\npublicly available at https://github.com/JLChen-C/OccProphet.\n","authors":["Junliang Chen","Huaiyuan Xu","Yi Wang","Lap-Pui Chau"],"pdf_url":"https://arxiv.org/pdf/2502.15180v1.pdf","comment":"Accepted by ICLR2025"},{"id":"http://arxiv.org/abs/2502.15179v1","updated":"2025-02-21T03:20:10Z","published":"2025-02-21T03:20:10Z","title":"Nonlinear Dynamical Systems for Automatic Face Annotation in Head\n  Tracking and Pose Estimation","summary":"  Facial landmark tracking plays a vital role in applications such as facial\nrecognition, expression analysis, and medical diagnostics. In this paper, we\nconsider the performance of the Extended Kalman Filter (EKF) and Unscented\nKalman Filter (UKF) in tracking 3D facial motion in both deterministic and\nstochastic settings. We first analyze a noise-free environment where the state\ntransition is purely deterministic, demonstrating that UKF outperforms EKF by\nachieving lower mean squared error (MSE) due to its ability to capture\nhigher-order nonlinearities. However, when stochastic noise is introduced, EKF\nexhibits superior robustness, maintaining lower mean square error (MSE)\ncompared to UKF, which becomes more sensitive to measurement noise and\nocclusions. Our results highlight that UKF is preferable for high-precision\napplications in controlled environments, whereas EKF is better suited for\nreal-world scenarios with unpredictable noise. These findings provide practical\ninsights for selecting the appropriate filtering technique in 3D facial\ntracking applications, such as motion capture and facial recognition.\n","authors":["Thoa Thieu","Roderick Melnik"],"pdf_url":"https://arxiv.org/pdf/2502.15179v1.pdf","comment":"25 pages, 10 figures"},{"id":"http://arxiv.org/abs/2502.15176v1","updated":"2025-02-21T03:16:18Z","published":"2025-02-21T03:16:18Z","title":"Methods and Trends in Detecting Generated Images: A Comprehensive Review","summary":"  The proliferation of generative models, such as Generative Adversarial\nNetworks (GANs), Diffusion Models, and Variational Autoencoders (VAEs), has\nenabled the synthesis of high-quality multimedia data. However, these\nadvancements have also raised significant concerns regarding adversarial\nattacks, unethical usage, and societal harm. Recognizing these challenges,\nresearchers have increasingly focused on developing methodologies to detect\nsynthesized data effectively, aiming to mitigate potential risks. Prior reviews\nhave primarily focused on deepfake detection and often lack coverage of recent\nadvancements in synthetic image detection, particularly methods leveraging\nmultimodal frameworks for improved forensic analysis. To address this gap, the\npresent survey provides a comprehensive review of state-of-the-art methods for\ndetecting and classifying synthetic images generated by advanced generative AI\nmodels. This review systematically examines core detection methodologies,\nidentifies commonalities among approaches, and categorizes them into meaningful\ntaxonomies. Furthermore, given the crucial role of large-scale datasets in this\nfield, we present an overview of publicly available datasets that facilitate\nfurther research and benchmarking in synthetic data detection.\n","authors":["Arpan Mahara","Naphtali Rishe"],"pdf_url":"https://arxiv.org/pdf/2502.15176v1.pdf","comment":"30 pages, 4 Figures, 10 Tables"},{"id":"http://arxiv.org/abs/2502.15174v1","updated":"2025-02-21T03:15:16Z","published":"2025-02-21T03:15:16Z","title":"FD-LSCIC: Frequency Decomposition-based Learned Screen Content Image\n  Compression","summary":"  The learned image compression (LIC) methods have already surpassed\ntraditional techniques in compressing natural scene (NS) images. However,\ndirectly applying these methods to screen content (SC) images, which possess\ndistinct characteristics such as sharp edges, repetitive patterns, embedded\ntext and graphics, yields suboptimal results. This paper addresses three key\nchallenges in SC image compression: learning compact latent features, adapting\nquantization step sizes, and the lack of large SC datasets. To overcome these\nchallenges, we propose a novel compression method that employs a\nmulti-frequency two-stage octave residual block (MToRB) for feature extraction,\na cascaded triple-scale feature fusion residual block (CTSFRB) for multi-scale\nfeature integration and a multi-frequency context interaction module (MFCIM) to\nreduce inter-frequency correlations. Additionally, we introduce an adaptive\nquantization module that learns scaled uniform noise for each frequency\ncomponent, enabling flexible control over quantization granularity.\nFurthermore, we construct a large SC image compression dataset (SDU-SCICD10K),\nwhich includes over 10,000 images spanning basic SC images, computer-rendered\nimages, and mixed NS and SC images from both PC and mobile platforms.\nExperimental results demonstrate that our approach significantly improves SC\nimage compression performance, outperforming traditional standards and\nstate-of-the-art learning-based methods in terms of peak signal-to-noise ratio\n(PSNR) and multi-scale structural similarity (MS-SSIM).\n","authors":["Shiqi Jiang","Hui Yuan","Shuai Li","Huanqiang Zeng","Sam Kwong"],"pdf_url":"https://arxiv.org/pdf/2502.15174v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.11807v2","updated":"2025-02-21T03:06:07Z","published":"2024-12-16T14:18:01Z","title":"PhysAug: A Physical-guided and Frequency-based Data Augmentation for\n  Single-Domain Generalized Object Detection","summary":"  Single-Domain Generalized Object Detection~(S-DGOD) aims to train on a single\nsource domain for robust performance across a variety of unseen target domains\nby taking advantage of an object detector. Existing S-DGOD approaches often\nrely on data augmentation strategies, including a composition of visual\ntransformations, to enhance the detector's generalization ability. However, the\nabsence of real-world prior knowledge hinders data augmentation from\ncontributing to the diversity of training data distributions. To address this\nissue, we propose PhysAug, a novel physical model-based non-ideal imaging\ncondition data augmentation method, to enhance the adaptability of the S-DGOD\ntasks. Drawing upon the principles of atmospheric optics, we develop a\nuniversal perturbation model that serves as the foundation for our proposed\nPhysAug. Given that visual perturbations typically arise from the interaction\nof light with atmospheric particles, the image frequency spectrum is harnessed\nto simulate real-world variations during training. This approach fosters the\ndetector to learn domain-invariant representations, thereby enhancing its\nability to generalize across various settings. Without altering the network\narchitecture or loss function, our approach significantly outperforms the\nstate-of-the-art across various S-DGOD datasets. In particular, it achieves a\nsubstantial improvement of $7.3\\%$ and $7.2\\%$ over the baseline on DWD and\nCityscape-C, highlighting its enhanced generalizability in real-world settings.\n","authors":["Xiaoran Xu","Jiangang Yang","Wenhui Shi","Siyuan Ding","Luqing Luo","Jian Liu"],"pdf_url":"https://arxiv.org/pdf/2412.11807v2.pdf","comment":"Accepted to AAAI,2025"},{"id":"http://arxiv.org/abs/2502.15167v1","updated":"2025-02-21T03:05:45Z","published":"2025-02-21T03:05:45Z","title":"M3-AGIQA: Multimodal, Multi-Round, Multi-Aspect AI-Generated Image\n  Quality Assessment","summary":"  The rapid advancement of AI-generated image (AGI) models has introduced\nsignificant challenges in evaluating their quality, which requires considering\nmultiple dimensions such as perceptual quality, prompt correspondence, and\nauthenticity. To address these challenges, we propose M3-AGIQA, a comprehensive\nframework for AGI quality assessment that is Multimodal, Multi-Round, and\nMulti-Aspect. Our approach leverages the capabilities of Multimodal Large\nLanguage Models (MLLMs) as joint text and image encoders and distills advanced\ncaptioning capabilities from online MLLMs into a local model via Low-Rank\nAdaptation (LoRA) fine-tuning. The framework includes a structured multi-round\nevaluation mechanism, where intermediate image descriptions are generated to\nprovide deeper insights into the quality, correspondence, and authenticity\naspects. To align predictions with human perceptual judgments, a predictor\nconstructed by an xLSTM and a regression head is incorporated to process\nsequential logits and predict Mean Opinion Scores (MOSs). Extensive experiments\nconducted on multiple benchmark datasets demonstrate that M3-AGIQA achieves\nstate-of-the-art performance, effectively capturing nuanced aspects of AGI\nquality. Furthermore, cross-dataset validation confirms its strong\ngeneralizability. The code is available at\nhttps://github.com/strawhatboy/M3-AGIQA.\n","authors":["Chuan Cui","Kejiang Chen","Zhihua Wei","Wen Shen","Weiming Zhang","Nenghai Yu"],"pdf_url":"https://arxiv.org/pdf/2502.15167v1.pdf","comment":"14 pages, 5 figures. This work has been submitted to the IEEE for\n  possible publication"},{"id":"http://arxiv.org/abs/2502.15163v1","updated":"2025-02-21T02:59:18Z","published":"2025-02-21T02:59:18Z","title":"HOpenCls: Training Hyperspectral Image Open-Set Classifiers in Their\n  Living Environments","summary":"  Hyperspectral image (HSI) open-set classification is critical for HSI\nclassification models deployed in real-world environments, where classifiers\nmust simultaneously classify known classes and reject unknown classes. Recent\nmethods utilize auxiliary unknown classes data to improve classification\nperformance. However, the auxiliary unknown classes data is strongly assumed to\nbe completely separable from known classes and requires labor-intensive\nannotation. To address this limitation, this paper proposes a novel framework,\nHOpenCls, to leverage the unlabeled wild data-that is the mixture of known and\nunknown classes. Such wild data is abundant and can be collected freely during\ndeploying classifiers in their living environments. The key insight is\nreformulating the open-set HSI classification with unlabeled wild data as a\npositive-unlabeled (PU) learning problem. Specifically, the multi-label\nstrategy is introduced to bridge the PU learning and open-set HSI\nclassification, and then the proposed gradient contraction and gradient\nexpansion module to make this PU learning problem tractable from the\nobservation of abnormal gradient weights associated with wild data. Extensive\nexperiment results demonstrate that incorporating wild data has the potential\nto significantly enhance open-set HSI classification in complex real-world\nscenarios.\n","authors":["Hengwei Zhao","Xinyu Wang","Zhuo Zheng","Jingtao Li","Yanfei Zhong"],"pdf_url":"https://arxiv.org/pdf/2502.15163v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.14282v2","updated":"2025-02-21T02:54:09Z","published":"2025-02-20T05:41:55Z","title":"PC-Agent: A Hierarchical Multi-Agent Collaboration Framework for Complex\n  Task Automation on PC","summary":"  In the field of MLLM-based GUI agents, compared to smartphones, the PC\nscenario not only features a more complex interactive environment, but also\ninvolves more intricate intra- and inter-app workflows. To address these\nissues, we propose a hierarchical agent framework named PC-Agent. Specifically,\nfrom the perception perspective, we devise an Active Perception Module (APM) to\novercome the inadequate abilities of current MLLMs in perceiving screenshot\ncontent. From the decision-making perspective, to handle complex user\ninstructions and interdependent subtasks more effectively, we propose a\nhierarchical multi-agent collaboration architecture that decomposes\ndecision-making processes into Instruction-Subtask-Action levels. Within this\narchitecture, three agents (i.e., Manager, Progress and Decision) are set up\nfor instruction decomposition, progress tracking and step-by-step\ndecision-making respectively. Additionally, a Reflection agent is adopted to\nenable timely bottom-up error feedback and adjustment. We also introduce a new\nbenchmark PC-Eval with 25 real-world complex instructions. Empirical results on\nPC-Eval show that our PC-Agent achieves a 32% absolute improvement of task\nsuccess rate over previous state-of-the-art methods. The code is available at\nhttps://github.com/X-PLUG/MobileAgent/tree/main/PC-Agent.\n","authors":["Haowei Liu","Xi Zhang","Haiyang Xu","Yuyang Wanyan","Junyang Wang","Ming Yan","Ji Zhang","Chunfeng Yuan","Changsheng Xu","Weiming Hu","Fei Huang"],"pdf_url":"https://arxiv.org/pdf/2502.14282v2.pdf","comment":"14 pages, 7 figures"},{"id":"http://arxiv.org/abs/2502.15156v1","updated":"2025-02-21T02:34:57Z","published":"2025-02-21T02:34:57Z","title":"Optimized Pap Smear Image Enhancement: Hybrid PMD Filter-CLAHE Using\n  Spider Monkey Optimization","summary":"  Pap smear image quality is crucial for cervical cancer detection. This study\nintroduces an optimized hybrid approach that combines the Perona-Malik\nDiffusion (PMD) filter with contrast-limited adaptive histogram equalization\n(CLAHE) to enhance Pap smear image quality. The PMD filter reduces the image\nnoise, whereas CLAHE improves the image contrast. The hybrid method was\noptimized using spider monkey optimization (SMO PMD-CLAHE). BRISQUE and CEIQ\nare the new objective functions for the PMD filter and CLAHE optimization,\nrespectively. The simulations were conducted using the SIPaKMeD dataset. The\nresults indicate that SMO outperforms state-of-the-art methods in optimizing\nthe PMD filter and CLAHE. The proposed method achieved an average effective\nmeasure of enhancement (EME) of 5.45, root mean square (RMS) contrast of 60.45,\nMichelson's contrast (MC) of 0.995, and entropy of 6.80. This approach offers a\nnew perspective for improving Pap smear image quality.\n","authors":["Ach Khozaimi","Isnani Darti","Syaiful Anam","Wuryansari Muharini Kusumawinahyu"],"pdf_url":"https://arxiv.org/pdf/2502.15156v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.15152v1","updated":"2025-02-21T02:24:10Z","published":"2025-02-21T02:24:10Z","title":"Confidence-Weighted Boundary-Aware Learning for Semi-Supervised Semantic\n  Segmentation","summary":"  Semi-supervised semantic segmentation (SSSS) aims to improve segmentation\nperformance by utilising unlabeled data alongside limited labeled samples.\nExisting SSSS methods often face challenges such as coupling, where\nover-reliance on initial labeled data leads to suboptimal learning;\nconfirmation bias, where incorrect predictions reinforce themselves repeatedly;\nand boundary blur caused by insufficient boundary-awareness and ambiguous edge\ninformation. To address these issues, we propose CW-BASS, a novel framework for\nSSSS. In order to mitigate the impact of incorrect predictions, we assign\nconfidence weights to pseudo-labels. Additionally, we leverage\nboundary-delineation techniques, which, despite being extensively explored in\nweakly-supervised semantic segmentation (WSSS) remain under-explored in SSSS.\nSpecifically, our approach: (1) reduces coupling through a confidence-weighted\nloss function that adjusts the influence of pseudo-labels based on their\npredicted confidence scores, (2) mitigates confirmation bias with a dynamic\nthresholding mechanism that learns to filter out pseudo-labels based on model\nperformance, (3) resolves boundary blur with a boundary-aware module that\nenhances segmentation accuracy near object boundaries, and (4) reduces label\nnoise with a confidence decay strategy that progressively refines pseudo-labels\nduring training. Extensive experiments on the Pascal VOC 2012 and Cityscapes\ndemonstrate that our method achieves state-of-the-art performance. Moreover,\nusing only 1/8 or 12.5\\% of labeled data, our method achieves a mIoU of 75.81\non Pascal VOC 2012, highlighting its effectiveness in limited-label settings.\n","authors":["Ebenezer Tarubinga","Jenifer Kalafatovich Espinoza"],"pdf_url":"https://arxiv.org/pdf/2502.15152v1.pdf","comment":"9 pages, 5 figures"},{"id":"http://arxiv.org/abs/2409.04679v2","updated":"2025-02-21T01:37:55Z","published":"2024-09-07T02:16:19Z","title":"Neural Augmentation Based Panoramic High Dynamic Range Stitching","summary":"  Due to saturated regions of inputting low dynamic range (LDR) images and\nlarge intensity changes among the LDR images caused by different exposures, it\nis challenging to produce an information enriched panoramic LDR image without\nvisual artifacts for a high dynamic range (HDR) scene through stitching\nmultiple geometrically synchronized LDR images with different exposures and\npairwise overlapping fields of views (OFOVs). Fortunately, the stitching of\nsuch images is innately a perfect scenario for the fusion of a physics-driven\napproach and a data-driven approach due to their OFOVs. Based on this new\ninsight, a novel neural augmentation based panoramic HDR stitching algorithm is\nproposed in this paper. The physics-driven approach is built up using the\nOFOVs. Different exposed images of each view are initially generated by using\nthe physics-driven approach, are then refined by a data-driven approach, and\nare finally used to produce panoramic LDR images with different exposures. All\nthe panoramic LDR images with different exposures are combined together via a\nmulti-scale exposure fusion algorithm to produce the final panoramic LDR image.\nExperimental results demonstrate the proposed algorithm outperforms existing\npanoramic stitching algorithms.\n","authors":["Chaobing Zheng","Yilun Xu","Weihai Chen","Shiqian Wu","Sen Zhang","Zhengguo Li"],"pdf_url":"https://arxiv.org/pdf/2409.04679v2.pdf","comment":"11 pages"},{"id":"http://arxiv.org/abs/2502.15130v1","updated":"2025-02-21T01:22:01Z","published":"2025-02-21T01:22:01Z","title":"TransMamba: Fast Universal Architecture Adaption from Transformers to\n  Mamba","summary":"  Transformers have been favored in both uni-modal and multi-modal foundation\nmodels for their flexible scalability in attention modules. Consequently, a\nnumber of pre-trained Transformer models, e.g., LLaVA, CLIP, and DEIT, are\npublicly available. Recent research has introduced subquadratic architectures\nlike Mamba, which enables global awareness with linear complexity.\nNevertheless, training specialized subquadratic architectures from scratch for\ncertain tasks is both resource-intensive and time-consuming. As a motivator, we\nexplore cross-architecture training to transfer the ready knowledge in existing\nTransformer models to alternative architecture Mamba, termed TransMamba. Our\napproach employs a two-stage strategy to expedite training new Mamba models,\nensuring effectiveness in across uni-modal and cross-modal tasks. Concerning\narchitecture disparities, we project the intermediate features into an aligned\nlatent space before transferring knowledge. On top of that, a Weight Subcloning\nand Adaptive Bidirectional distillation method (WSAB) is introduced for\nknowledge transfer without limitations on varying layer counts. For cross-modal\nlearning, we propose a cross-Mamba module that integrates language awareness\ninto Mamba's visual features, enhancing the cross-modal interaction\ncapabilities of Mamba architecture. Despite using less than 75% of the training\ndata typically required for training from scratch, TransMamba boasts\nsubstantially stronger performance across various network architectures and\ndownstream tasks, including image classification, visual question answering,\nand text-video retrieval. The code will be publicly available.\n","authors":["Xiuwei Chen","Sihao Lin","Xiao Dong","Zisheng Chen","Meng Cao","Jianhua Han","Hang Xu","Xiaodan Liang"],"pdf_url":"https://arxiv.org/pdf/2502.15130v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.15128v1","updated":"2025-02-21T01:15:10Z","published":"2025-02-21T01:15:10Z","title":"DAM-Seg: Anatomically accurate cardiac segmentation using Dense\n  Associative Networks","summary":"  Deep learning-based cardiac segmentation has seen significant advancements\nover the years. Many studies have tackled the challenge of anatomically\nincorrect segmentation predictions by introducing auxiliary modules. These\nmodules either post-process segmentation outputs or enforce consistency between\nspecific points to ensure anatomical correctness. However, such approaches\noften increase network complexity, require separate training for these modules,\nand may lack robustness in scenarios with poor visibility. To address these\nlimitations, we propose a novel transformer-based architecture that leverages\ndense associative networks to learn and retain specific patterns inherent to\ncardiac inputs. Unlike traditional methods, our approach restricts the network\nto memorize a limited set of patterns. During forward propagation, a weighted\nsum of these patterns is used to enforce anatomical correctness in the output.\nSince these patterns are input-independent, the model demonstrates enhanced\nrobustness, even in cases with poor visibility. The proposed pipeline was\nevaluated on two publicly available datasets, CAMUS and CardiacNet.\nExperimental results indicate that our model consistently outperforms baseline\napproaches across all metrics, highlighting its effectiveness and reliability\nfor cardiac segmentation tasks.\n","authors":["Zahid Ullah","Jihie Kim"],"pdf_url":"https://arxiv.org/pdf/2502.15128v1.pdf","comment":"12 pages, 7 figures, 5 tables"},{"id":"http://arxiv.org/abs/2406.18113v4","updated":"2025-02-21T00:49:07Z","published":"2024-06-26T06:59:09Z","title":"Chrono: A Simple Blueprint for Representing Time in MLLMs","summary":"  The recent success of Large Language Models (LLMs) has prompted the extension\nto the multimodal domain developing image-text Multimodal LLMs (MLLMs) and then\nvideo-text models. In this work, we investigate the challenge of contextual and\ntemporal comprehension in video-language models by exploring the task of\ntemporal localization in videos. To address this problem, prior works have\ndeveloped complex task-specific architectures, novel modules to embed time into\nMLLMs, or leveraged additional input signals such as video transcripts to best\nencode contextual and temporal information. Interestingly, we find that most of\nthese efforts are surpassed by a much simpler design. We introduce Chrono, a\nuniversal sequence blueprint that can be applied to an image-text pretrained\nMLLM. Through extensive ablations across different MLLM architectures,\nfinetuning and zero-shot settings, and different datasets, we achieve a new\nSOTA in moment retrieval on the most widely used benchmarks Charades-STA,\nQVHighlights, ActivityNet Captions, and grounded video question answering on\nNeXT-GQA.\n","authors":["Boris Meinardus","Hector Garcia Rodriguez","Anil Batra","Anna Rohrbach","Marcus Rohrbach"],"pdf_url":"https://arxiv.org/pdf/2406.18113v4.pdf","comment":"Code: https://github.com/sudo-Boris/mr-Blip"},{"id":"http://arxiv.org/abs/2502.15119v1","updated":"2025-02-21T00:42:40Z","published":"2025-02-21T00:42:40Z","title":"CurricuVLM: Towards Safe Autonomous Driving via Personalized\n  Safety-Critical Curriculum Learning with Vision-Language Models","summary":"  Ensuring safety in autonomous driving systems remains a critical challenge,\nparticularly in handling rare but potentially catastrophic safety-critical\nscenarios. While existing research has explored generating safety-critical\nscenarios for autonomous vehicle (AV) testing, there is limited work on\neffectively incorporating these scenarios into policy learning to enhance\nsafety. Furthermore, developing training curricula that adapt to an AV's\nevolving behavioral patterns and performance bottlenecks remains largely\nunexplored. To address these challenges, we propose CurricuVLM, a novel\nframework that leverages Vision-Language Models (VLMs) to enable personalized\ncurriculum learning for autonomous driving agents. Our approach uniquely\nexploits VLMs' multimodal understanding capabilities to analyze agent behavior,\nidentify performance weaknesses, and dynamically generate tailored training\nscenarios for curriculum adaptation. Through comprehensive analysis of unsafe\ndriving situations with narrative descriptions, CurricuVLM performs in-depth\nreasoning to evaluate the AV's capabilities and identify critical behavioral\npatterns. The framework then synthesizes customized training scenarios\ntargeting these identified limitations, enabling effective and personalized\ncurriculum learning. Extensive experiments on the Waymo Open Motion Dataset\nshow that CurricuVLM outperforms state-of-the-art baselines across both regular\nand safety-critical scenarios, achieving superior performance in terms of\nnavigation success, driving efficiency, and safety metrics. Further analysis\nreveals that CurricuVLM serves as a general approach that can be integrated\nwith various RL algorithms to enhance autonomous driving systems. The code and\ndemo video are available at: https://zihaosheng.github.io/CurricuVLM/.\n","authors":["Zihao Sheng","Zilin Huang","Yansong Qu","Yue Leng","Sruthi Bhavanam","Sikai Chen"],"pdf_url":"https://arxiv.org/pdf/2502.15119v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.15107v1","updated":"2025-02-21T00:02:28Z","published":"2025-02-21T00:02:28Z","title":"Assessing a Single Student's Concentration on Learning Platforms: A\n  Machine Learning-Enhanced EEG-Based Framework","summary":"  This study introduces a specialized pipeline designed to classify the\nconcentration state of an individual student during online learning sessions by\ntraining a custom-tailored machine learning model. Detailed protocols for\nacquiring and preprocessing EEG data are outlined, along with the extraction of\nfifty statistical features from five EEG signal bands: alpha, beta, theta,\ndelta, and gamma. Following feature extraction, a thorough feature selection\nprocess was conducted to optimize the data inputs for a personalized analysis.\nThe study also explores the benefits of hyperparameter fine-tuning to enhance\nthe classification accuracy of the student's concentration state. EEG signals\nwere captured from the student using a Muse headband (Gen 2), equipped with\nfive electrodes (TP9, AF7, AF8, TP10, and a reference electrode NZ), during\nengagement with educational content on computer-based e-learning platforms.\nEmploying a random forest model customized to the student's data, we achieved\nremarkable classification performance, with test accuracies of 97.6% in the\ncomputer-based learning setting and 98% in the virtual reality setting. These\nresults underscore the effectiveness of our approach in delivering personalized\ninsights into student concentration during online educational activities.\n","authors":["Zewen Zhuo","Mohamad Najafi","Hazem Zein","Amine Nait-Ali"],"pdf_url":"https://arxiv.org/pdf/2502.15107v1.pdf","comment":null}],"Information Retrieval":[{"id":"http://arxiv.org/abs/2502.15623v1","updated":"2025-02-21T17:51:37Z","published":"2025-02-21T17:51:37Z","title":"Dynamic Knowledge Selector and Evaluator for recommendation with\n  Knowledge Graph","summary":"  In recent years recommendation systems typically employ the edge information\nprovided by knowledge graphs combined with the advantages of high-order\nconnectivity of graph networks in the recommendation field. However, this\nmethod is limited by the sparsity of labels, cannot learn the graph structure\nwell, and a large number of noisy entities in the knowledge graph will affect\nthe accuracy of the recommendation results. In order to alleviate the above\nproblems, we propose a dynamic knowledge-selecting and evaluating method guided\nby collaborative signals to distill information in the knowledge graph.\nSpecifically, we use a Chain Route Evaluator to evaluate the contributions of\ndifferent neighborhoods for the recommendation task and employ a Knowledge\nSelector strategy to filter the less informative knowledge before evaluating.\nWe conduct baseline model comparison and experimental ablation evaluations on\nthree public datasets. The experiments demonstrate that our proposed model\noutperforms current state-of-the-art baseline models, and each modules\neffectiveness in our model is demonstrated through ablation experiments.\n","authors":["Feng Xia","Zhifei Hu"],"pdf_url":"https://arxiv.org/pdf/2502.15623v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.15604v1","updated":"2025-02-21T17:19:39Z","published":"2025-02-21T17:19:39Z","title":"Cross-Format Retrieval-Augmented Generation in XR with LLMs for\n  Context-Aware Maintenance Assistance","summary":"  This paper presents a detailed evaluation of a Retrieval-Augmented Generation\n(RAG) system that integrates large language models (LLMs) to enhance\ninformation retrieval and instruction generation for maintenance personnel\nacross diverse data formats. We assessed the performance of eight LLMs,\nemphasizing key metrics such as response speed and accuracy, which were\nquantified using BLEU and METEOR scores. Our findings reveal that advanced\nmodels like GPT-4 and GPT-4o-mini significantly outperform their counterparts,\nparticularly when addressing complex queries requiring multi-format data\nintegration. The results validate the system's ability to deliver timely and\naccurate responses, highlighting the potential of RAG frameworks to optimize\nmaintenance operations. Future research will focus on refining retrieval\ntechniques for these models and enhancing response generation, particularly for\nintricate scenarios, ultimately improving the system's practical applicability\nin dynamic real-world environments.\n","authors":["Akos Nagy","Yannis Spyridis","Vasileios Argyriou"],"pdf_url":"https://arxiv.org/pdf/2502.15604v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.15589v1","updated":"2025-02-21T16:57:22Z","published":"2025-02-21T16:57:22Z","title":"LightThinker: Thinking Step-by-Step Compression","summary":"  Large language models (LLMs) have shown remarkable performance in complex\nreasoning tasks, but their efficiency is hindered by the substantial memory and\ncomputational costs associated with generating lengthy tokens. In this paper,\nwe propose LightThinker, a novel method that enables LLMs to dynamically\ncompress intermediate thoughts during reasoning. Inspired by human cognitive\nprocesses, LightThinker compresses verbose thought steps into compact\nrepresentations and discards the original reasoning chains, thereby\nsignificantly reducing the number of tokens stored in the context window. This\nis achieved by training the model on when and how to perform compression\nthrough data construction, mapping hidden states to condensed gist tokens, and\ncreating specialized attention masks. Additionally, we introduce the Dependency\n(Dep) metric to quantify the degree of compression by measuring the reliance on\nhistorical tokens during generation. Extensive experiments on four datasets and\ntwo models show that LightThinker reduces peak memory usage and inference time,\nwhile maintaining competitive accuracy. Our work provides a new direction for\nimproving the efficiency of LLMs in complex reasoning tasks without sacrificing\nperformance. Code will be released at https://github.com/zjunlp/LightThinker.\n","authors":["Jintian Zhang","Yuqi Zhu","Mengshu Sun","Yujie Luo","Shuofei Qiao","Lun Du","Da Zheng","Huajun Chen","Ningyu Zhang"],"pdf_url":"https://arxiv.org/pdf/2502.15589v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.15542v1","updated":"2025-02-21T15:50:14Z","published":"2025-02-21T15:50:14Z","title":"Bridging Domain Gaps between Pretrained Multimodal Models and\n  Recommendations","summary":"  With the explosive growth of multimodal content online, pre-trained\nvisual-language models have shown great potential for multimodal\nrecommendation. However, while these models achieve decent performance when\napplied in a frozen manner, surprisingly, due to significant domain gaps (e.g.,\nfeature distribution discrepancy and task objective misalignment) between\npre-training and personalized recommendation, adopting a joint training\napproach instead leads to performance worse than baseline. Existing approaches\neither rely on simple feature extraction or require computationally expensive\nfull model fine-tuning, struggling to balance effectiveness and efficiency. To\ntackle these challenges, we propose \\textbf{P}arameter-efficient\n\\textbf{T}uning for \\textbf{M}ultimodal \\textbf{Rec}ommendation\n(\\textbf{PTMRec}), a novel framework that bridges the domain gap between\npre-trained models and recommendation systems through a knowledge-guided\ndual-stage parameter-efficient training strategy. This framework not only\neliminates the need for costly additional pre-training but also flexibly\naccommodates various parameter-efficient tuning methods.\n","authors":["Wenyu Zhang","Jie Luo","Xinming Zhang","Yuan Fang"],"pdf_url":"https://arxiv.org/pdf/2502.15542v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.15526v1","updated":"2025-02-21T15:28:26Z","published":"2025-02-21T15:28:26Z","title":"Scaling Sparse and Dense Retrieval in Decoder-Only LLMs","summary":"  Scaling large language models (LLMs) has shown great potential for improving\nretrieval model performance; however, previous studies have mainly focused on\ndense retrieval trained with contrastive loss (CL), neglecting the scaling\nbehavior of other retrieval paradigms and optimization techniques, such as\nsparse retrieval and knowledge distillation (KD). In this work, we conduct a\nsystematic comparative study on how different retrieval paradigms (sparse vs.\ndense) and fine-tuning objectives (CL vs. KD vs. their combination) affect\nretrieval performance across different model scales. Using MSMARCO passages as\nthe training dataset, decoder-only LLMs (Llama-3 series: 1B, 3B, 8B), and a\nfixed compute budget, we evaluate various training configurations on both\nin-domain (MSMARCO, TREC DL) and out-of-domain (BEIR) benchmarks. Our key\nfindings reveal that: (1) Scaling behaviors emerge clearly only with CL, where\nlarger models achieve significant performance gains, whereas KD-trained models\nshow minimal improvement, performing similarly across the 1B, 3B, and 8B\nscales. (2) Sparse retrieval models consistently outperform dense retrieval\nacross both in-domain (MSMARCO, TREC DL) and out-of-domain (BEIR) benchmarks,\nand they demonstrate greater robustness to imperfect supervised signals. (3) We\nsuccessfully scale sparse retrieval models with the combination of CL and KD\nlosses at 8B scale, achieving state-of-the-art (SOTA) results in all evaluation\nsets.\n","authors":["Hansi Zeng","Julian Killingback","Hamed Zamani"],"pdf_url":"https://arxiv.org/pdf/2502.15526v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.13245v2","updated":"2025-02-21T15:20:30Z","published":"2025-02-18T19:18:01Z","title":"Range Retrieval with Graph-Based Indices","summary":"  Retrieving points based on proximity in a high-dimensional vector space is a\ncrucial step in information retrieval applications. The approximate nearest\nneighbor search (ANNS) problem, which identifies the $k$ nearest neighbors for\na query (approximately, since exactly is hard), has been extensively studied in\nrecent years. However, comparatively little attention has been paid to the\nrelated problem of finding all points within a given distance of a query, the\nrange retrieval problem, despite its applications in areas such as duplicate\ndetection, plagiarism checking, and facial recognition. In this paper, we\npresent a set of algorithms for range retrieval on graph-based vector indices,\nwhich are known to achieve excellent performance on ANNS queries. Since a range\nquery may have anywhere from no matching results to thousands of matching\nresults in the database, we introduce a set of range retrieval algorithms based\non modifications of the standard graph search that adapt to terminate quickly\non queries in the former group, and to put more resources into finding results\nfor the latter group. Due to the lack of existing benchmarks for range\nretrieval, we also undertake a comprehensive study of range characteristics of\nexisting embedding datasets, and select a suitable range retrieval radius for\neight existing datasets with up to 100 million points in addition to the one\nexisting benchmark. We test our algorithms on these datasets, and find up to\n100x improvement in query throughput over a naive baseline approach, with 5-10x\nimprovement on average, and strong performance up to 100 million data points.\n","authors":["Magdalen Dobson Manohar","Taekseung Kim","Guy E. Blelloch"],"pdf_url":"https://arxiv.org/pdf/2502.13245v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2306.04487v5","updated":"2025-02-21T12:35:19Z","published":"2023-06-07T14:57:21Z","title":"Vague Preference Policy Learning for Conversational Recommendation","summary":"  Conversational recommendation systems (CRS) commonly assume users have clear\npreferences, leading to potential over-filtering of relevant alternatives.\nHowever, users often exhibit vague, non-binary preferences. We introduce the\nVague Preference Multi-round Conversational Recommendation (VPMCR) scenario,\nemploying a soft estimation mechanism to accommodate users' vague and dynamic\npreferences while mitigating over-filtering. In VPMCR, we propose Vague\nPreference Policy Learning (VPPL), consisting of Ambiguity-aware Soft\nEstimation (ASE) and Dynamism-aware Policy Learning (DPL). ASE captures\npreference vagueness by estimating scores for clicked and non-clicked options,\nusing a choice-based approach and time-aware preference decay. DPL leverages\nASE's preference distribution to guide the conversation and adapt to preference\nchanges for recommendations or attribute queries. Extensive experiments\ndemonstrate VPPL's effectiveness within VPMCR, outperforming existing methods\nand setting a new benchmark. Our work advances CRS by accommodating users'\ninherent ambiguity and relative decision-making processes, improving real-world\napplicability.\n","authors":["Gangyi Zhang","Chongming Gao","Wenqiang Lei","Xiaojie Guo","Shijun Li","Hongshen Chen","Zhuozhi Ding","Sulong Xu","Lingfei Wu"],"pdf_url":"https://arxiv.org/pdf/2306.04487v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.13506v2","updated":"2025-02-21T10:18:26Z","published":"2025-02-19T07:50:59Z","title":"Reproducing NevIR: Negation in Neural Information Retrieval","summary":"  Negation is a fundamental aspect of human communication, yet it remains a\nchallenge for Language Models (LMs) in Information Retrieval (IR). Despite the\nheavy reliance of modern neural IR systems on LMs, little attention has been\ngiven to their handling of negation. In this study, we reproduce and extend the\nfindings of NevIR, a benchmark study that revealed most IR models perform at or\nbelow the level of random ranking when dealing with negation. We replicate\nNevIR's original experiments and evaluate newly developed state-of-the-art IR\nmodels. Our findings show that a recently emerging category - listwise Large\nLanguage Model (LLM) rerankers - outperforms other models but still\nunderperforms human performance. Additionally, we leverage ExcluIR, a benchmark\ndataset designed for exclusionary queries with extensive negation, to assess\nthe generalizability of negation understanding. Our findings suggest that\nfine-tuning on one dataset does not reliably improve performance on the other,\nindicating notable differences in their data distributions. Furthermore, we\nobserve that only cross-encoders and listwise LLM rerankers achieve reasonable\nperformance across both negation tasks.\n","authors":["Coen van den Elsen","Francien Barkhof","Thijmen Nijdam","Simon Lupart","Mohammad Alliannejadi"],"pdf_url":"https://arxiv.org/pdf/2502.13506v2.pdf","comment":"9 pages, 5 figures, under review at SIGIR 2025"},{"id":"http://arxiv.org/abs/2502.15355v1","updated":"2025-02-21T10:12:34Z","published":"2025-02-21T10:12:34Z","title":"A Universal Framework for Compressing Embeddings in CTR Prediction","summary":"  Accurate click-through rate (CTR) prediction is vital for online advertising\nand recommendation systems. Recent deep learning advancements have improved the\nability to capture feature interactions and understand user interests. However,\noptimizing the embedding layer often remains overlooked. Embedding tables,\nwhich represent categorical and sequential features, can become excessively\nlarge, surpassing GPU memory limits and necessitating storage in CPU memory.\nThis results in high memory consumption and increased latency due to frequent\nGPU-CPU data transfers. To tackle these challenges, we introduce a\nModel-agnostic Embedding Compression (MEC) framework that compresses embedding\ntables by quantizing pre-trained embeddings, without sacrificing recommendation\nquality. Our approach consists of two stages: first, we apply\npopularity-weighted regularization to balance code distribution between high-\nand low-frequency features. Then, we integrate a contrastive learning mechanism\nto ensure a uniform distribution of quantized codes, enhancing the\ndistinctiveness of embeddings. Experiments on three datasets reveal that our\nmethod reduces memory usage by over 50x while maintaining or improving\nrecommendation performance compared to existing models. The implementation code\nis accessible in our project repository https://github.com/USTC-StarTeam/MEC.\n","authors":["Kefan Wang","Hao Wang","Kenan Song","Wei Guo","Kai Cheng","Zhi Li","Yong Liu","Defu Lian","Enhong Chen"],"pdf_url":"https://arxiv.org/pdf/2502.15355v1.pdf","comment":"Accepted by DASFAA2025"},{"id":"http://arxiv.org/abs/2502.15332v1","updated":"2025-02-21T09:34:34Z","published":"2025-02-21T09:34:34Z","title":"Detecting Future-related Contexts of Entity Mentions","summary":"  The ability to automatically identify whether an entity is referenced in a\nfuture context can have multiple applications including decision making,\nplanning and trend forecasting. This paper focuses on detecting implicit future\nreferences in entity-centric texts, addressing the growing need for automated\ntemporal analysis in information processing. We first present a novel dataset\nof 19,540 sentences built around popular entities sourced from Wikipedia, which\nconsists of future-related and non-future-related contexts in which those\nentities appear. As a second contribution, we evaluate the performance of\nseveral Language Models including also Large Language Models (LLMs) on the task\nof distinguishing future-oriented content in the absence of explicit temporal\nreferences.\n","authors":["Puneet Prashar","Krishna Mohan Shukla","Adam Jatowt"],"pdf_url":"https://arxiv.org/pdf/2502.15332v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.15331v1","updated":"2025-02-21T09:34:31Z","published":"2025-02-21T09:34:31Z","title":"Lightweight yet Efficient: An External Attentive Graph Convolutional\n  Network with Positional Prompts for Sequential Recommendation","summary":"  Graph-based Sequential Recommender systems (GSRs) have gained significant\nresearch attention due to their ability to simultaneously handle user-item\ninteractions and sequential relationships between items. Current GSRs often\nutilize composite or in-depth structures for graph encoding (e.g., the Graph\nTransformer). Nevertheless, they have high computational complexity, hindering\nthe deployment on resource-constrained edge devices. Moreover, the relative\nposition encoding in Graph Transformer has difficulty in considering the\ncomplicated positional dependencies within sequence. To this end, we propose an\nExternal Attentive Graph convolutional network with Positional prompts for\nSequential recommendation, namely EA-GPS. Specifically, we first introduce an\nexternal attentive graph convolutional network that linearly measures the\nglobal associations among nodes via two external memory units. Then, we present\na positional prompt-based decoder that explicitly treats the absolute item\npositions as external prompts. By introducing length-adaptive sequential\nmasking and a soft attention network, such a decoder facilitates the model to\ncapture the long-term positional dependencies and contextual relationships\nwithin sequences. Extensive experimental results on five real-world datasets\ndemonstrate that the proposed EA-GPS outperforms the state-of-the-art methods.\nRemarkably, it achieves the superior performance while maintaining a smaller\nparameter size and lower training overhead. The implementation of this work is\npublicly available at https://github.com/ZZY-GraphMiningLab/EA-GPS.\n","authors":["Jinyu Zhang","Chao Li","Zhongying Zhao"],"pdf_url":"https://arxiv.org/pdf/2502.15331v1.pdf","comment":"26 pages, 8 figures, journal paper, accepted by TOIS at 20th\n  February, 2025"},{"id":"http://arxiv.org/abs/2105.11866v5","updated":"2025-02-21T09:17:00Z","published":"2021-05-25T12:10:54Z","title":"GraphFM: Graph Factorization Machines for Feature Interaction Modeling","summary":"  Factorization machine (FM) is a prevalent approach to modeling pairwise\n(second-order) feature interactions when dealing with high-dimensional sparse\ndata. However, on the one hand, FM fails to capture higher-order feature\ninteractions suffering from combinatorial expansion. On the other hand, taking\ninto account interactions between every pair of features may introduce noise\nand degrade prediction accuracy. To solve the problems, we propose a novel\napproach, Graph Factorization Machine (GraphFM), by naturally representing\nfeatures in the graph structure. In particular, we design a mechanism to select\nthe beneficial feature interactions and formulate them as edges between\nfeatures. Then the proposed model, which integrates the interaction function of\nFM into the feature aggregation strategy of Graph Neural Network (GNN), can\nmodel arbitrary-order feature interactions on the graph-structured features by\nstacking layers. Experimental results on several real-world datasets have\ndemonstrated the rationality and effectiveness of our proposed approach. The\ncode and data are available at\nhttps://github.com/CRIPAC-DIG/GraphCTR}{https://github.com/CRIPAC-DIG/GraphCTR\n","authors":["Shu Wu","Zekun Li","Yunyue Su","Zeyu Cui","Xiaoyu Zhang","Liang Wang"],"pdf_url":"https://arxiv.org/pdf/2105.11866v5.pdf","comment":"The code and data are available at\n  https://github.com/CRIPAC-DIG/GraphCTR"},{"id":"http://arxiv.org/abs/2502.15237v1","updated":"2025-02-21T06:22:12Z","published":"2025-02-21T06:22:12Z","title":"From Documents to Dialogue: Building KG-RAG Enhanced AI Assistants","summary":"  The Adobe Experience Platform AI Assistant is a conversational tool that\nenables organizations to interact seamlessly with proprietary enterprise data\nthrough a chatbot. However, due to access restrictions, Large Language Models\n(LLMs) cannot retrieve these internal documents, limiting their ability to\ngenerate accurate zero-shot responses. To overcome this limitation, we use a\nRetrieval-Augmented Generation (RAG) framework powered by a Knowledge Graph\n(KG) to retrieve relevant information from external knowledge sources, enabling\nLLMs to answer questions over private or previously unseen document\ncollections. In this paper, we propose a novel approach for building a\nhigh-quality, low-noise KG. We apply several techniques, including incremental\nentity resolution using seed concepts, similarity-based filtering to\ndeduplicate entries, assigning confidence scores to entity-relation pairs to\nfilter for high-confidence pairs, and linking facts to source documents for\nprovenance. Our KG-RAG system retrieves relevant tuples, which are added to the\nuser prompts context before being sent to the LLM generating the response. Our\nevaluation demonstrates that this approach significantly enhances response\nrelevance, reducing irrelevant answers by over 50% and increasing fully\nrelevant answers by 88% compared to the existing production system.\n","authors":["Manisha Mukherjee","Sungchul Kim","Xiang Chen","Dan Luo","Tong Yu","Tung Mai"],"pdf_url":"https://arxiv.org/pdf/2502.15237v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.18735v2","updated":"2025-02-21T06:11:41Z","published":"2024-12-25T01:47:39Z","title":"Automatic Self-supervised Learning for Social Recommendations","summary":"  In recent years, researchers have attempted to exploit social relations to\nimprove the performance in recommendation systems. Generally, most existing\nsocial recommendation methods heavily depends on substantial domain knowledge\nand expertise in primary recommendation tasks for designing useful auxiliary\ntasks. Meanwhile, Self-Supervised Learning (SSL) recently has received\nconsiderable attention in the field of recommendation, since it can provide\nself-supervision signals in assisting the improvement of target recommendation\nsystems by constructing self-supervised auxiliary tasks from raw data without\nhuman-annotated labels. Despite the great success, these SSL-based social\nrecommendations are insufficient to adaptively balance various self-supervised\nauxiliary tasks, since assigning equal weights on various auxiliary tasks can\nresult in sub-optimal recommendation performance, where different\nself-supervised auxiliary tasks may contribute differently to improving the\nprimary social recommendation across different datasets. To address this issue,\nin this work, we propose Adaptive Self-supervised Learning for Social\nRecommendations (AdasRec) by taking advantage of various self-supervised\nauxiliary tasks. More specifically, an adaptive weighting mechanism is proposed\nto learn adaptive weights for various self-supervised auxiliary tasks, so as to\nbalance the contribution of such self-supervised auxiliary tasks for enhancing\nrepresentation learning in social recommendations. The adaptive weighting\nmechanism is used to assign different weights on auxiliary tasks to achieve an\noverall weighting of the entire auxiliary tasks and ultimately assist the\nprimary recommendation task, achieved by a meta learning optimization problem\nwith an adaptive weighting network. Comprehensive experiments on various\nreal-world datasets are constructed to verify the effectiveness of our proposed\nmethod.\n","authors":["Xin He","Wenqi Fan","Mingchen Sun","Ying Wang","Xin Wang"],"pdf_url":"https://arxiv.org/pdf/2412.18735v2.pdf","comment":"13 pages, 4 figures"},{"id":"http://arxiv.org/abs/2502.15223v1","updated":"2025-02-21T05:35:08Z","published":"2025-02-21T05:35:08Z","title":"A BERT Based Hybrid Recommendation System For Academic Collaboration","summary":"  Universities serve as a hub for academic collaboration, promoting the\nexchange of diverse ideas and perspectives among students and faculty through\ninterdisciplinary dialogue. However, as universities expand in size,\nconventional networking approaches via student chapters, class groups, and\nfaculty committees become cumbersome. To address this challenge, an\nacademia-specific profile recommendation system is proposed to connect\nlike-minded stakeholders within any university community. This study evaluates\nthree techniques: Term Frequency-Inverse Document Frequency (TF-IDF),\nBidirectional Encoder Representations from Transformers (BERT), and a hybrid\napproach to generate effective recommendations. Due to the unlabelled nature of\nthe dataset, Affinity Propagation cluster-based relabelling is performed to\nunderstand the grouping of similar profiles. The hybrid model demonstrated\nsuperior performance, evidenced by its similarity score, Silhouette score,\nDavies-Bouldin index, and Normalized Discounted Cumulative Gain (NDCG),\nachieving an optimal balance between diversity and relevance in\nrecommendations. Furthermore, the optimal model has been implemented as a\nmobile application, which dynamically suggests relevant profiles based on\nusers' skills and collaboration interests, incorporating contextual\nunderstanding. The potential impact of this application is significant, as it\npromises to enhance networking opportunities within large academic institutions\nthrough the deployment of intelligent recommendation systems.\n","authors":["Sangeetha N","Harish Thangaraj","Varun Vashisht","Eshaan Joshi","Kanishka Verma","Diya Katariya"],"pdf_url":"https://arxiv.org/pdf/2502.15223v1.pdf","comment":"International Conference on Intelligent Systems and Security - 2024"},{"id":"http://arxiv.org/abs/2502.15202v1","updated":"2025-02-21T04:29:53Z","published":"2025-02-21T04:29:53Z","title":"GNN-Coder: Boosting Semantic Code Retrieval with Combined GNNs and\n  Transformer","summary":"  Code retrieval is a crucial component in modern software development,\nparticularly in large-scale projects. However, existing approaches relying on\nsequence-based models often fail to fully exploit the structural dependencies\ninherent in code, leading to suboptimal retrieval performance, particularly\nwith structurally complex code fragments. In this paper, we introduce\nGNN-Coder, a novel framework based on Graph Neural Network (GNN) to utilize\nAbstract Syntax Tree (AST). We make the first attempt to study how\nGNN-integrated Transformer can promote the development of semantic retrieval\ntasks by capturing the structural and semantic features of code. We further\npropose an innovative graph pooling method tailored for AST, utilizing the\nnumber of child nodes as a key feature to highlight the intrinsic topological\nrelationships within the AST. This design effectively integrates both\nsequential and hierarchical representations, enhancing the model's ability to\ncapture code structure and semantics. Additionally, we introduce the Mean\nAngular Margin (MAM), a novel metric for quantifying the uniformity of code\nembedding distributions, providing a standardized measure of feature\nseparability. The proposed method achieves a lower MAM, indicating a more\ndiscriminative feature representation. This underscores GNN-Coder's superior\nability to distinguish between code snippets, thereby enhancing retrieval\naccuracy. Experimental results show that GNN-Coder significantly boosts\nretrieval performance, with a 1\\%-10\\% improvement in MRR on the CSN dataset,\nand a notable 20\\% gain in zero-shot performance on the CosQA dataset.\n","authors":["Yufan Ye","Pu Pang","Ting Zhang","Hua Huang"],"pdf_url":"https://arxiv.org/pdf/2502.15202v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.13909v2","updated":"2025-02-21T03:12:46Z","published":"2025-02-19T17:41:09Z","title":"Lost in Sequence: Do Large Language Models Understand Sequential\n  Recommendation?","summary":"  Large Language Models (LLMs) have recently emerged as promising tools for\nrecommendation thanks to their advanced textual understanding ability and\ncontext-awareness. Despite the current practice of training and evaluating\nLLM-based recommendation (LLM4Rec) models under a sequential recommendation\nscenario, we found that whether these models understand the sequential\ninformation inherent in users' item interaction sequences has been largely\noverlooked. In this paper, we first demonstrate through a series of experiments\nthat existing LLM4Rec models do not fully capture sequential information both\nduring training and inference. Then, we propose a simple yet effective\nLLM-based sequential recommender, called LLM-SRec, a method that enhances the\nintegration of sequential information into LLMs by distilling the user\nrepresentations extracted from a pre-trained CF-SRec model into LLMs. Our\nextensive experiments show that LLM-SRec enhances LLMs' ability to understand\nusers' item interaction sequences, ultimately leading to improved\nrecommendation performance. Furthermore, unlike existing LLM4Rec models that\nrequire fine-tuning of LLMs, LLM-SRec achieves state-of-the-art performance by\ntraining only a few lightweight MLPs, highlighting its practicality in\nreal-world applications. Our code is available at\nhttps://github.com/Sein-Kim/LLM-SRec.\n","authors":["Sein Kim","Hongseok Kang","Kibum Kim","Jiwan Kim","Donghyun Kim","Minchul Yang","Kwangjin Oh","Julian McAuley","Chanyoung Park"],"pdf_url":"https://arxiv.org/pdf/2502.13909v2.pdf","comment":null}],"Machine Learning":[{"id":"http://arxiv.org/abs/2412.13148v3","updated":"2025-02-21T18:59:37Z","published":"2024-12-17T18:13:18Z","title":"SWAN: SGD with Normalization and Whitening Enables Stateless LLM\n  Training","summary":"  Adaptive optimizers such as Adam (Kingma & Ba, 2015) have been central to the\nsuccess of large language models. However, they often require to maintain\noptimizer states throughout training, which can result in memory requirements\nseveral times greater than the model footprint. This overhead imposes\nconstraints on scalability and computational efficiency. Stochastic Gradient\nDescent (SGD), in contrast, is a stateless optimizer, as it does not track\nstate variables during training. Consequently, it achieves optimal memory\nefficiency. However, its capability in LLM training is limited (Zhao et al.,\n2024b). In this work, we show that pre-processing SGD in a stateless manner can\nachieve the same performance as the Adam optimizer for LLM training, while\ndrastically reducing the memory cost. Specifically, we propose to pre-process\nthe instantaneous stochastic gradients using normalization and whitening. We\nshow that normalization stabilizes gradient distributions, and whitening\ncounteracts the local curvature of the loss landscape. This results in SWAN\n(SGD with Whitening And Normalization), a stochastic optimizer that eliminates\nthe need to store any optimizer states. Empirically, SWAN has the same memory\nfootprint as SGD, achieving $\\approx 50\\%$ reduction on total end-to-end memory\ncompared to Adam. In language modeling tasks, SWAN demonstrates comparable or\neven better performance than Adam: when pre-training the LLaMA model with 350M\nand 1.3B parameters, SWAN achieves a 2x speedup by reaching the same evaluation\nperplexity using half as many tokens.\n","authors":["Chao Ma","Wenbo Gong","Meyer Scetbon","Edward Meeds"],"pdf_url":"https://arxiv.org/pdf/2412.13148v3.pdf","comment":"In v2 we have revised the related work, added more comprehensive\n  citations, and clarified our key contributions"},{"id":"http://arxiv.org/abs/2502.15681v1","updated":"2025-02-21T18:59:20Z","published":"2025-02-21T18:59:20Z","title":"One-step Diffusion Models with $f$-Divergence Distribution Matching","summary":"  Sampling from diffusion models involves a slow iterative process that hinders\ntheir practical deployment, especially for interactive applications. To\naccelerate generation speed, recent approaches distill a multi-step diffusion\nmodel into a single-step student generator via variational score distillation,\nwhich matches the distribution of samples generated by the student to the\nteacher's distribution. However, these approaches use the reverse\nKullback-Leibler (KL) divergence for distribution matching which is known to be\nmode seeking. In this paper, we generalize the distribution matching approach\nusing a novel $f$-divergence minimization framework, termed $f$-distill, that\ncovers different divergences with different trade-offs in terms of mode\ncoverage and training variance. We derive the gradient of the $f$-divergence\nbetween the teacher and student distributions and show that it is expressed as\nthe product of their score differences and a weighting function determined by\ntheir density ratio. This weighting function naturally emphasizes samples with\nhigher density in the teacher distribution, when using a less mode-seeking\ndivergence. We observe that the popular variational score distillation approach\nusing the reverse-KL divergence is a special case within our framework.\nEmpirically, we demonstrate that alternative $f$-divergences, such as\nforward-KL and Jensen-Shannon divergences, outperform the current best\nvariational score distillation methods across image generation tasks. In\nparticular, when using Jensen-Shannon divergence, $f$-distill achieves current\nstate-of-the-art one-step generation performance on ImageNet64 and zero-shot\ntext-to-image generation on MS-COCO. Project page:\nhttps://research.nvidia.com/labs/genair/f-distill\n","authors":["Yilun Xu","Weili Nie","Arash Vahdat"],"pdf_url":"https://arxiv.org/pdf/2502.15681v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.15678v1","updated":"2025-02-21T18:58:30Z","published":"2025-02-21T18:58:30Z","title":"Testing the limits of fine-tuning to improve reasoning in vision\n  language models","summary":"  Pre-trained vision language models still fall short of human visual\ncognition. In an effort to improve visual cognition and align models with human\nbehavior, we introduce visual stimuli and human judgments on visual cognition\ntasks, allowing us to systematically evaluate performance across cognitive\ndomains under a consistent environment. We fine-tune models on ground truth\ndata for intuitive physics and causal reasoning and find that this improves\nmodel performance in the respective fine-tuning domain. Furthermore, it can\nimprove model alignment with human behavior. However, we find that fine-tuning\ndoes not contribute to robust human-like generalization to data with other\nvisual characteristics or to tasks in other cognitive domains.\n","authors":["Luca M. Schulze Buschoff","Konstantinos Voudouris","Elif Akata","Matthias Bethge","Joshua B. Tenenbaum","Eric Schulz"],"pdf_url":"https://arxiv.org/pdf/2502.15678v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.15677v1","updated":"2025-02-21T18:58:06Z","published":"2025-02-21T18:58:06Z","title":"FLEKE: Federated Locate-then-Edit Knowledge Editing","summary":"  Locate-then-Edit Knowledge Editing (LEKE) is a key technique for updating\nlarge language models (LLMs) without full retraining. However, existing methods\nassume a single-user setting and become inefficient in real-world multi-client\nscenarios, where decentralized organizations (e.g., hospitals, financial\ninstitutions) independently update overlapping knowledge, leading to redundant\nmediator knowledge vector (MKV) computations and privacy concerns. To address\nthese challenges, we introduce Federated Locate-then-Edit Knowledge Editing\n(FLEKE), a novel task that enables multiple clients to collaboratively perform\nLEKE while preserving privacy and reducing computational overhead. To achieve\nthis, we propose FedEdit, a two-stage framework that optimizes MKV selection\nand reuse. In the first stage, clients locally apply LEKE and upload the\ncomputed MKVs. In the second stage, rather than relying solely on server-based\nMKV sharing, FLEKE allows clients retrieve relevant MKVs based on cosine\nsimilarity, enabling knowledge re-edit and minimizing redundant computations.\nExperimental results on two benchmark datasets demonstrate that FedEdit retains\nover 96% of the performance of non-federated LEKE while significantly\noutperforming a FedAvg-based baseline by approximately twofold. Besides, we\nfind that MEMIT performs more consistently than PMET in the FLEKE task with our\nFedEdit framework. Our code is available at https://github.com/zongkaiz/FLEKE.\n","authors":["Zongkai Zhao","Guozeng Xu","Xiuhua Li","Kaiwen Wei","Jiang Zhong"],"pdf_url":"https://arxiv.org/pdf/2502.15677v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.18219v2","updated":"2025-02-21T18:53:06Z","published":"2024-09-26T18:55:52Z","title":"Packet Inspection Transformer: A Self-Supervised Journey to Unseen\n  Malware Detection with Few Samples","summary":"  As networks continue to expand and become more interconnected, the need for\nnovel malware detection methods becomes more pronounced. Traditional security\nmeasures are increasingly inadequate against the sophistication of modern cyber\nattacks. Deep Packet Inspection (DPI) has been pivotal in enhancing network\nsecurity, offering an in-depth analysis of network traffic that surpasses\nconventional monitoring techniques. DPI not only examines the metadata of\nnetwork packets, but also dives into the actual content being carried within\nthe packet payloads, providing a comprehensive view of the data flowing through\nnetworks. While the integration of advanced deep learning techniques with DPI\nhas introduced modern methodologies into malware detection and network traffic\nclassification, state-of-the-art supervised learning approaches are limited by\ntheir reliance on large amounts of annotated data and their inability to\ngeneralize to novel, unseen malware threats. To address these limitations, this\npaper leverages the recent advancements in self-supervised learning (SSL) and\nfew-shot learning (FSL). Our proposed self-supervised approach trains a\ntransformer via SSL to learn the embedding of packet content, including\npayload, from vast amounts of unlabeled data by masking portions of packets,\nleading to a learned representation that generalizes to various downstream\ntasks. Once the representation is extracted from the packets, they are used to\ntrain a malware detection algorithm. The representation obtained from the\ntransformer is then used to adapt the malware detector to novel types of\nattacks using few-shot learning approaches. Our experimental results\ndemonstrate that our method achieves classification accuracies of up to 94.76%\non the UNSW-NB15 dataset and 83.25% on the CIC-IoT23 dataset.\n","authors":["Kyle Stein","Arash Mahyari","Guillermo Francia III","Eman El-Sheikh"],"pdf_url":"https://arxiv.org/pdf/2409.18219v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.15666v1","updated":"2025-02-21T18:45:37Z","published":"2025-02-21T18:45:37Z","title":"Almost AI, Almost Human: The Challenge of Detecting AI-Polished Writing","summary":"  The growing use of large language models (LLMs) for text generation has led\nto widespread concerns about AI-generated content detection. However, an\noverlooked challenge is AI-polished text, where human-written content undergoes\nsubtle refinements using AI tools. This raises a critical question: should\nminimally polished text be classified as AI-generated? Misclassification can\nlead to false plagiarism accusations and misleading claims about AI prevalence\nin online content. In this study, we systematically evaluate eleven\nstate-of-the-art AI-text detectors using our AI-Polished-Text Evaluation\n(APT-Eval) dataset, which contains $11.7K$ samples refined at varying\nAI-involvement levels. Our findings reveal that detectors frequently\nmisclassify even minimally polished text as AI-generated, struggle to\ndifferentiate between degrees of AI involvement, and exhibit biases against\nolder and smaller models. These limitations highlight the urgent need for more\nnuanced detection methodologies.\n","authors":["Shoumik Saha","Soheil Feizi"],"pdf_url":"https://arxiv.org/pdf/2502.15666v1.pdf","comment":"17 pages, 17 figures"},{"id":"http://arxiv.org/abs/2502.15662v1","updated":"2025-02-21T18:38:00Z","published":"2025-02-21T18:38:00Z","title":"Automating Curriculum Learning for Reinforcement Learning using a\n  Skill-Based Bayesian Network","summary":"  A major challenge for reinforcement learning is automatically generating\ncurricula to reduce training time or improve performance in some target task.\nWe introduce SEBNs (Skill-Environment Bayesian Networks) which model a\nprobabilistic relationship between a set of skills, a set of goals that relate\nto the reward structure, and a set of environment features to predict policy\nperformance on (possibly unseen) tasks. We develop an algorithm that uses the\ninferred estimates of agent success from SEBN to weigh the possible next tasks\nby expected improvement. We evaluate the benefit of the resulting curriculum on\nthree environments: a discrete gridworld, continuous control, and simulated\nrobotics. The results show that curricula constructed using SEBN frequently\noutperform other baselines.\n","authors":["Vincent Hsiao","Mark Roberts","Laura M. Hiatt","George Konidaris","Dana Nau"],"pdf_url":"https://arxiv.org/pdf/2502.15662v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.15654v1","updated":"2025-02-21T18:22:36Z","published":"2025-02-21T18:22:36Z","title":"Machine-generated text detection prevents language model collapse","summary":"  As Large Language Models (LLMs) become increasingly prevalent, their\ngenerated outputs are proliferating across the web, risking a future where\nmachine-generated content dilutes human-authored text. Since web data is the\nprimary resource for LLM pretraining, future models will be trained on an\nunknown portion of synthetic data. This will lead to model collapse, a\ndegenerative process which causes models to reinforce their own errors and\nexperience a drop in model performance. In this study, we investigate the\nimpact of decoding strategy on model collapse, where we analyse the\ncharacteristics of the generated data during recursive training, its similarity\nto human references and the resulting model performance. Using the decoding\nstrategies that lead to the most significant model degradation, we tackle the\nquestion: how to avoid model collapse when the origin (human or synthetic) of\nthe training data is unknown. We design a novel methodology based on resampling\nthe data distribution using importance weights from our machine-generated text\ndetector. Our method is validated on two LLM variants (GPT-2 and SmolLM2) on\nthe open-ended text generation task, demonstrating that we can successfully\nprevent model collapse and when there is enough human-authored data in the\ntraining dataset, our method improves model performance.\n","authors":["George Drayson","Vasileios Lampos"],"pdf_url":"https://arxiv.org/pdf/2502.15654v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.15648v1","updated":"2025-02-21T18:15:11Z","published":"2025-02-21T18:15:11Z","title":"Logit Disagreement: OoD Detection with Bayesian Neural Networks","summary":"  Bayesian neural networks (BNNs), which estimate the full posterior\ndistribution over model parameters, are well-known for their role in\nuncertainty quantification and its promising application in out-of-distribution\ndetection (OoD). Amongst other uncertainty measures, BNNs provide a\nstate-of-the art estimation of predictive entropy (total uncertainty) which can\nbe decomposed as the sum of mutual information and expected entropy. In the\ncontext of OoD detection the estimation of predictive uncertainty in the form\nof the predictive entropy score confounds aleatoric and epistemic uncertainty,\nthe latter being hypothesized to be high for OoD points. Despite these\njustifications, the mutual information score has been shown to perform worse\nthan predictive entropy. Taking inspiration from Bayesian variational\nautoencoder (BVAE) literature, this work proposes to measure the disagreement\nbetween a corrected version of the pre-softmax quantities, otherwise known as\nlogits, as an estimate of epistemic uncertainty for Bayesian NNs under mean\nfield variational inference. The three proposed epistemic uncertainty scores\ndemonstrate marked improvements over mutual information on a range of OoD\nexperiments, with equal performance otherwise. Moreover, the epistemic\nuncertainty scores perform on par with the Bayesian benchmark predictive\nentropy on a range of MNIST and CIFAR10 experiments.\n","authors":["Kevin Raina"],"pdf_url":"https://arxiv.org/pdf/2502.15648v1.pdf","comment":"Presented at ECCV 2024 Workshop: 3rd Workshop on Uncertainty\n  Quantification for Computer Vision"},{"id":"http://arxiv.org/abs/2502.15646v1","updated":"2025-02-21T18:12:36Z","published":"2025-02-21T18:12:36Z","title":"Predicting gene essentiality and drug response from perturbation screens\n  in preclinical cancer models with LEAP: Layered Ensemble of Autoencoders and\n  Predictors","summary":"  Preclinical perturbation screens, where the effects of genetic, chemical, or\nenvironmental perturbations are systematically tested on disease models, hold\nsignificant promise for machine learning-enhanced drug discovery due to their\nscale and causal nature. Predictive models can infer perturbation responses for\npreviously untested disease models based on molecular profiles. These in silico\nlabels can expand databases and guide experimental prioritization.\n  However, modelling perturbation-specific effects and generating robust\nprediction performances across diverse biological contexts remain elusive. We\nintroduce LEAP (Layered Ensemble of Autoencoders and Predictors), a novel\nensemble framework to improve robustness and generalization. LEAP leverages\nmultiple DAMAE (Data Augmented Masked Autoencoder) representations and LASSO\nregressors. By combining diverse gene expression representation models learned\nfrom different random initializations, LEAP consistently outperforms\nstate-of-the-art approaches in predicting gene essentiality or drug responses\nin unseen cell lines, tissues and disease models. Notably, our results show\nthat ensembling representation models, rather than prediction models alone,\nyields superior predictive performance.\n  Beyond its performance gains, LEAP is computationally efficient, requires\nminimal hyperparameter tuning and can therefore be readily incorporated into\ndrug discovery pipelines to prioritize promising targets and support\nbiomarker-driven stratification. The code and datasets used in this work are\nmade publicly available.\n","authors":["Barbara Bodinier","Gaetan Dissez","Linus Bleistein","Antonin Dauvin"],"pdf_url":"https://arxiv.org/pdf/2502.15646v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.04628v3","updated":"2025-02-21T18:12:34Z","published":"2024-12-05T21:50:22Z","title":"SWEPO: Simultaneous Weighted Preference Optimization for Group\n  Contrastive Alignment","summary":"  Direct Preference Optimization (DPO) has proven effective in aligning large\nlanguage models with human preferences but is often constrained to pairwise\ncomparisons -- overlooking additional positive and negative responses that are\ncommonly available in real-world settings. We propose Simultaneous Weighted\nPreference Optimization (SWEPO), which incorporates multiple responses per\nquery and prioritizes those that deviate most from the average reward. This\ndeviation-based weighting focuses training on the most informative outliers,\nakin to a built-in curriculum. Theoretically, we prove that such\nmulti-preference sampling lowers alignment bias, bounding the expected\ndeviation from the true acceptable-response distribution at a rate of\n$\\mathcal{O}(\\tfrac{1}{\\sqrt{k}})$. Empirically, SWEPO outperforms\nstate-of-the-art baselines on the Ultra-Feedback dataset and demonstrates\nsubstantial improvements over DPO and InfoNCA, yielding boosts of up to $\\sim\n4$% on length-controlled win-rate on AlpacaEval.\n","authors":["Taneesh Gupta","Rahul Madhavan","Xuchao Zhang","Chetan Bansal","Saravan Rajmohan"],"pdf_url":"https://arxiv.org/pdf/2412.04628v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.15643v1","updated":"2025-02-21T18:10:56Z","published":"2025-02-21T18:10:56Z","title":"AutoTandemML: Active Learning Enhanced Tandem Neural Networks for\n  Inverse Design Problems","summary":"  Inverse design in science and engineering involves determining optimal design\nparameters that achieve desired performance outcomes, a process often hindered\nby the complexity and high dimensionality of design spaces, leading to\nsignificant computational costs. To tackle this challenge, we propose a novel\nhybrid approach that combines active learning with Tandem Neural Networks to\nenhance the efficiency and effectiveness of solving inverse design problems.\nActive learning allows to selectively sample the most informative data points,\nreducing the required dataset size without compromising accuracy. We\ninvestigate this approach using three benchmark problems: airfoil inverse\ndesign, photonic surface inverse design, and scalar boundary condition\nreconstruction in diffusion partial differential equations. We demonstrate that\nintegrating active learning with Tandem Neural Networks outperforms standard\napproaches across the benchmark suite, achieving better accuracy with fewer\ntraining samples.\n","authors":["Luka Grbcic","Juliane Müller","Wibe Albert de Jong"],"pdf_url":"https://arxiv.org/pdf/2502.15643v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.15642v1","updated":"2025-02-21T18:10:26Z","published":"2025-02-21T18:10:26Z","title":"Training Neural ODEs Using Fully Discretized Simultaneous Optimization","summary":"  Neural Ordinary Differential Equations (Neural ODEs) represent\ncontinuous-time dynamics with neural networks, offering advancements for\nmodeling and control tasks. However, training Neural ODEs requires solving\ndifferential equations at each epoch, leading to high computational costs. This\nwork investigates simultaneous optimization methods as a faster training\nalternative. In particular, we employ a collocation-based, fully discretized\nformulation and use IPOPT--a solver for large-scale nonlinear optimization--to\nsimultaneously optimize collocation coefficients and neural network parameters.\nUsing the Van der Pol Oscillator as a case study, we demonstrate faster\nconvergence compared to traditional training methods. Furthermore, we introduce\na decomposition framework utilizing Alternating Direction Method of Multipliers\n(ADMM) to effectively coordinate sub-models among data batches. Our results\nshow significant potential for (collocation-based) simultaneous Neural ODE\ntraining pipelines.\n","authors":["Mariia Shapovalova","Calvin Tsay"],"pdf_url":"https://arxiv.org/pdf/2502.15642v1.pdf","comment":"Accepted to the 14th IFAC Symposium on Dynamics and Control of\n  Process Systems, including Biosystems (DYCOPS 2025)"},{"id":"http://arxiv.org/abs/2502.15639v1","updated":"2025-02-21T18:09:54Z","published":"2025-02-21T18:09:54Z","title":"Steering into New Embedding Spaces: Analyzing Cross-Lingual Alignment\n  Induced by Model Interventions in Multilingual Language Models","summary":"  Aligned representations across languages is a desired property in\nmultilingual large language models (mLLMs), as alignment can improve\nperformance in cross-lingual tasks. Typically alignment requires fine-tuning a\nmodel, which is computationally expensive, and sizable language data, which\noften may not be available. A data-efficient alternative to fine-tuning is\nmodel interventions -- a method for manipulating model activations to steer\ngeneration into the desired direction. We analyze the effect of a popular\nintervention (finding experts) on the alignment of cross-lingual\nrepresentations in mLLMs. We identify the neurons to manipulate for a given\nlanguage and introspect the embedding space of mLLMs pre- and\npost-manipulation. We show that modifying the mLLM's activations changes its\nembedding space such that cross-lingual alignment is enhanced. Further, we show\nthat the changes to the embedding space translate into improved downstream\nperformance on retrieval tasks, with up to 2x improvements in top-1 accuracy on\ncross-lingual retrieval.\n","authors":["Anirudh Sundar","Sinead Williamson","Katherine Metcalf","Barry-John Theobald","Skyler Seto","Masha Fedzechkina"],"pdf_url":"https://arxiv.org/pdf/2502.15639v1.pdf","comment":"34 pages"},{"id":"http://arxiv.org/abs/2502.15637v1","updated":"2025-02-21T18:06:09Z","published":"2025-02-21T18:06:09Z","title":"Mantis: Lightweight Calibrated Foundation Model for User-Friendly Time\n  Series Classification","summary":"  In recent years, there has been increasing interest in developing foundation\nmodels for time series data that can generalize across diverse downstream\ntasks. While numerous forecasting-oriented foundation models have been\nintroduced, there is a notable scarcity of models tailored for time series\nclassification. To address this gap, we present Mantis, a new open-source\nfoundation model for time series classification based on the Vision Transformer\n(ViT) architecture that has been pre-trained using a contrastive learning\napproach. Our experimental results show that Mantis outperforms existing\nfoundation models both when the backbone is frozen and when fine-tuned, while\nachieving the lowest calibration error. In addition, we propose several\nadapters to handle the multivariate setting, reducing memory requirements and\nmodeling channel interdependence.\n","authors":["Vasilii Feofanov","Songkang Wen","Marius Alonso","Romain Ilbert","Hongbo Guo","Malik Tiomoko","Lujia Pan","Jianfeng Zhang","Ievgen Redko"],"pdf_url":"https://arxiv.org/pdf/2502.15637v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.03130v2","updated":"2025-02-21T18:04:49Z","published":"2025-01-06T16:48:30Z","title":"SpinSVAR: Estimating Structural Vector Autoregression Assuming Sparse\n  Input","summary":"  We introduce SpinSVAR, a novel method for estimating a structural vector\nautoregression (SVAR) from time-series data under sparse input assumption.\nUnlike prior approaches using Gaussian noise, we model the input as independent\nLaplacian variables, enforcing sparsity and yielding a maximum likelihood\nestimator (MLE) based on least absolute error regression. We provide\ntheoretical consistency guarantees for the MLE under mild assumptions. SpinSVAR\nis efficient: it can leverage GPU acceleration to scale to thousands of nodes.\nOn synthetic data with Laplacian or Bernoulli-uniform inputs, SpinSVAR\noutperforms state-of-the-art methods in accuracy and runtime. When applied to\nS&P 500 data, it clusters stocks by sectors and identifies significant\nstructural shocks linked to major price movements, demonstrating the viability\nof our sparse input assumption.\n","authors":["Panagiotis Misiakos","Markus Püschel"],"pdf_url":"https://arxiv.org/pdf/2501.03130v2.pdf","comment":"38 pages, 11 figures, conference preprint"},{"id":"http://arxiv.org/abs/2502.15634v1","updated":"2025-02-21T18:03:44Z","published":"2025-02-21T18:03:44Z","title":"Sparks of cognitive flexibility: self-guided context inference for\n  flexible stimulus-response mapping by attentional routing","summary":"  Flexible cognition demands discovering hidden rules to quickly adapt\nstimulus-response mappings. Standard neural networks struggle in tasks\nrequiring rapid, context-driven remapping. Recently, Hummos (2023) introduced a\nfast-and-slow learning algorithm to mitigate this shortfall, but its\nscalability to complex, image-computable tasks was unclear. Here, we propose\nthe Wisconsin Neural Network (WiNN), which expands on fast-and-slow learning\nfor real-world tasks demanding flexible rule-based behavior. WiNN employs a\npretrained convolutional neural network for vision, coupled with an adjustable\n\"context state\" that guides attention to relevant features. If WiNN produces an\nincorrect response, it first iteratively updates its context state to refocus\nattention on task-relevant cues, then performs minimal parameter updates to\nattention and readout layers. This strategy preserves generalizable\nrepresentations in the sensory network, reducing catastrophic forgetting. We\nevaluate WiNN on an image-based extension of the Wisconsin Card Sorting Task,\nrevealing several markers of cognitive flexibility: (i) WiNN autonomously\ninfers underlying rules, (ii) requires fewer examples to do so than control\nmodels reliant on large-scale parameter updates, (iii) can perform\ncontext-based rule inference solely via context-state adjustments-further\nenhanced by slow updates of attention and readout parameters, and (iv)\ngeneralizes to unseen compositional rules through context-state inference\nalone. By blending fast context inference with targeted attentional guidance,\nWiNN achieves \"sparks\" of flexibility. This approach offers a path toward\ncontext-sensitive models that retain knowledge while rapidly adapting to\ncomplex, rule-based tasks.\n","authors":["Rowan Sommers","Sushrut Thorat","Daniel Anthes","Tim C. Kietzmann"],"pdf_url":"https://arxiv.org/pdf/2502.15634v1.pdf","comment":"11 pages, 4 figures"},{"id":"http://arxiv.org/abs/2410.08288v2","updated":"2025-02-21T18:00:52Z","published":"2024-10-10T18:20:44Z","title":"Towards Foundation Models for Mixed Integer Linear Programming","summary":"  Mixed Integer Linear Programming (MILP) is essential for modeling complex\ndecision-making problems but faces challenges in computational tractability and\nrequires expert formulation. Current deep learning approaches for MILP focus on\nspecific problem classes and do not generalize to unseen classes. To address\nthis shortcoming, we take a foundation model training approach, where we train\na single deep learning model on a diverse set of MILP problems to generalize\nacross problem classes. As existing datasets for MILP lack diversity and\nvolume, we introduce MILP-Evolve, a novel LLM-based evolutionary framework that\nis capable of generating a large set of diverse MILP classes with an unlimited\namount of instances. We study our methodology on three key learning tasks that\ncapture diverse aspects of MILP: (1) integrality gap prediction, (2) learning\nto branch, and (3) a new task of aligning MILP instances with natural language\ndescriptions. Our empirical results show that models trained on the data\ngenerated by MILP-Evolve achieve significant improvements on unseen problems,\nincluding MIPLIB benchmarks. Our work highlights the potential of moving\ntowards a foundation model approach for MILP that can generalize to a broad\nrange of MILP applications. Our code and data are publicly available at\nhttps://github.com/microsoft/OptiGuide.\n","authors":["Sirui Li","Janardhan Kulkarni","Ishai Menache","Cathy Wu","Beibin Li"],"pdf_url":"https://arxiv.org/pdf/2410.08288v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.15631v1","updated":"2025-02-21T17:59:13Z","published":"2025-02-21T17:59:13Z","title":"The Relationship Between Reasoning and Performance in Large Language\n  Models -- o3 (mini) Thinks Harder, Not Longer","summary":"  Large language models have demonstrated remarkable progress in mathematical\nreasoning, leveraging chain-of-thought and test-time compute scaling. However,\nmany open questions remain regarding the interplay between reasoning token\nusage and accuracy gains. In particular, when comparing models across\ngenerations, it is unclear whether improved performance results from longer\nreasoning chains or more efficient reasoning. We systematically analyze\nchain-of-thought length across o1-mini and o3-mini variants on the Omni-MATH\nbenchmark, finding that o3-mini (m) achieves superior accuracy without\nrequiring longer reasoning chains than o1-mini. Moreover, we show that accuracy\ngenerally declines as reasoning chains grow across all models and compute\nsettings, even when controlling for difficulty of the questions. This accuracy\ndrop is significantly smaller in more proficient models, suggesting that new\ngenerations of reasoning models use test-time compute more effectively.\nFinally, we highlight that while o3-mini (h) achieves a marginal accuracy gain\nover o3-mini (m), it does so by allocating substantially more reasoning tokens\nacross all problems, even the ones that o3-mini (m) can already solve. These\nfindings provide new insights into the relationship between model capability\nand reasoning length, with implications for efficiency, scaling, and evaluation\nmethodologies.\n","authors":["Marthe Ballon","Andres Algaba","Vincent Ginis"],"pdf_url":"https://arxiv.org/pdf/2502.15631v1.pdf","comment":"19 pages, 11 figures"},{"id":"http://arxiv.org/abs/2502.15620v1","updated":"2025-02-21T17:44:05Z","published":"2025-02-21T17:44:05Z","title":"Paradigms of AI Evaluation: Mapping Goals, Methodologies and Culture","summary":"  Research in AI evaluation has grown increasingly complex and\nmultidisciplinary, attracting researchers with diverse backgrounds and\nobjectives. As a result, divergent evaluation paradigms have emerged, often\ndeveloping in isolation, adopting conflicting terminologies, and overlooking\neach other's contributions. This fragmentation has led to insular research\ntrajectories and communication barriers both among different paradigms and with\nthe general public, contributing to unmet expectations for deployed AI systems.\nTo help bridge this insularity, in this paper we survey recent work in the AI\nevaluation landscape and identify six main paradigms. We characterise major\nrecent contributions within each paradigm across key dimensions related to\ntheir goals, methodologies and research cultures. By clarifying the unique\ncombination of questions and approaches associated with each paradigm, we aim\nto increase awareness of the breadth of current evaluation approaches and\nfoster cross-pollination between different paradigms. We also identify\npotential gaps in the field to inspire future research directions.\n","authors":["John Burden","Marko Tešić","Lorenzo Pacchiardi","José Hernández-Orallo"],"pdf_url":"https://arxiv.org/pdf/2502.15620v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.11828v3","updated":"2025-02-21T17:42:56Z","published":"2024-07-16T15:16:10Z","title":"Vibravox: A Dataset of French Speech Captured with Body-conduction Audio\n  Sensors","summary":"  Vibravox is a dataset compliant with the General Data Protection Regulation\n(GDPR) containing audio recordings using five different body-conduction audio\nsensors : two in-ear microphones, two bone conduction vibration pickups and a\nlaryngophone. The dataset also includes audio data from an airborne microphone\nused as a reference. The Vibravox corpus contains 45 hours of speech samples\nand physiological sounds recorded by 188 participants under different acoustic\nconditions imposed by an high order ambisonics 3D spatializer. Annotations\nabout the recording conditions and linguistic transcriptions are also included\nin the corpus. We conducted a series of experiments on various speech-related\ntasks, including speech recognition, speech enhancement and speaker\nverification. These experiments were carried out using state-of-the-art models\nto evaluate and compare their performances on signals captured by the different\naudio sensors offered by the Vibravox dataset, with the aim of gaining a better\ngrasp of their individual characteristics.\n","authors":["Julien Hauret","Malo Olivier","Thomas Joubaud","Christophe Langrenne","Sarah Poirée","Véronique Zimpfer","Éric Bavu"],"pdf_url":"https://arxiv.org/pdf/2407.11828v3.pdf","comment":"23 pages, 42 figures"},{"id":"http://arxiv.org/abs/2410.23306v3","updated":"2025-02-21T17:42:34Z","published":"2024-10-26T14:27:17Z","title":"Securing Healthcare with Deep Learning: A CNN-Based Model for medical\n  IoT Threat Detection","summary":"  The increasing integration of the Internet of Medical Things (IoMT) into\nhealthcare systems has significantly enhanced patient care but has also\nintroduced critical cybersecurity challenges. This paper presents a novel\napproach based on Convolutional Neural Networks (CNNs) for detecting\ncyberattacks within IoMT environments. Unlike previous studies that\npredominantly utilized traditional machine learning (ML) models or simpler Deep\nNeural Networks (DNNs), the proposed model leverages the capabilities of CNNs\nto effectively analyze the temporal characteristics of network traffic data.\nTrained and evaluated on the CICIoMT2024 dataset, which comprises 18 distinct\ntypes of cyberattacks across a range of IoMT devices, the proposed CNN model\ndemonstrates superior performance compared to previous state-of-the-art\nmethods, achieving a perfect accuracy of 99% in binary, categorical, and\nmulticlass classification tasks. This performance surpasses that of\nconventional ML models such as Logistic Regression, AdaBoost, DNNs, and Random\nForests. These findings highlight the potential of CNNs to substantially\nimprove IoMT cybersecurity, thereby ensuring the protection and integrity of\nconnected healthcare systems.\n","authors":["Alireza Mohamadi","Hosna Ghahramani","Seyyed Amir Asghari","Mehdi Aminian"],"pdf_url":"https://arxiv.org/pdf/2410.23306v3.pdf","comment":"The final published version is available in IEEE Xplore:\n  https://doi.org/10.1109/ICIS64839.2024.10887510"},{"id":"http://arxiv.org/abs/2502.15618v1","updated":"2025-02-21T17:41:21Z","published":"2025-02-21T17:41:21Z","title":"Probe Pruning: Accelerating LLMs through Dynamic Pruning via\n  Model-Probing","summary":"  We introduce Probe Pruning (PP), a novel framework for online, dynamic,\nstructured pruning of Large Language Models (LLMs) applied in a batch-wise\nmanner. PP leverages the insight that not all samples and tokens contribute\nequally to the model's output, and probing a small portion of each batch\neffectively identifies crucial weights, enabling tailored dynamic pruning for\ndifferent batches. It comprises three main stages: probing, history-informed\npruning, and full inference. In the probing stage, PP selects a small yet\ncrucial set of hidden states, based on residual importance, to run a few model\nlayers ahead. During the history-informed pruning stage, PP strategically\nintegrates the probing states with historical states. Subsequently, it\nstructurally prunes weights based on the integrated states and the PP\nimportance score, a metric developed specifically to assess the importance of\neach weight channel in maintaining performance. In the final stage, full\ninference is conducted on the remaining weights. A major advantage of PP is its\ncompatibility with existing models, as it operates without requiring additional\nneural network modules or fine-tuning. Comprehensive evaluations of PP on\nLLaMA-2/3 and OPT models reveal that even minimal probing-using just 1.5% of\nFLOPs-can substantially enhance the efficiency of structured pruning of LLMs.\nFor instance, when evaluated on LLaMA-2-7B with WikiText2, PP achieves a 2.56\ntimes lower ratio of performance degradation per unit of runtime reduction\ncompared to the state-of-the-art method at a 40% pruning ratio. Our code is\navailable at https://github.com/Qi-Le1/Probe_Pruning.\n","authors":["Qi Le","Enmao Diao","Ziyan Wang","Xinran Wang","Jie Ding","Li Yang","Ali Anwar"],"pdf_url":"https://arxiv.org/pdf/2502.15618v1.pdf","comment":"ICLR 2025"},{"id":"http://arxiv.org/abs/2411.12135v2","updated":"2025-02-21T17:38:07Z","published":"2024-11-19T00:24:50Z","title":"Exact Risk Curves of signSGD in High-Dimensions: Quantifying\n  Preconditioning and Noise-Compression Effects","summary":"  In recent years, signSGD has garnered interest as both a practical optimizer\nas well as a simple model to understand adaptive optimizers like Adam. Though\nthere is a general consensus that signSGD acts to precondition optimization and\nreshapes noise, quantitatively understanding these effects in theoretically\nsolvable settings remains difficult. We present an analysis of signSGD in a\nhigh dimensional limit, and derive a limiting SDE and ODE to describe the risk.\nUsing this framework we quantify four effects of signSGD: effective learning\nrate, noise compression, diagonal preconditioning, and gradient noise\nreshaping. Our analysis is consistent with experimental observations but moves\nbeyond that by quantifying the dependence of these effects on the data and\nnoise distributions. We conclude with a conjecture on how these results might\nbe extended to Adam.\n","authors":["Ke Liang Xiao","Noah Marshall","Atish Agarwala","Elliot Paquette"],"pdf_url":"https://arxiv.org/pdf/2411.12135v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.15690v2","updated":"2025-02-21T17:33:00Z","published":"2025-01-26T22:35:24Z","title":"Refined climatologies of future precipitation over High Mountain Asia\n  using probabilistic ensemble learning","summary":"  High Mountain Asia holds the largest concentration of frozen water outside\nthe polar regions, serving as a crucial water source for more than 1.9 billion\npeople. In the face of climate change, precipitation represents the largest\nsource of uncertainty for hydrological modelling in this area. Future\nprecipitation predictions remain challenging due to complex orography, lack of\nin situ hydrological observations, and limitations in climate model resolution\nand parametrisation for this region. To address the uncertainty posed by these\nchallenges, climate models are often aggregated into multi-model ensembles.\nWhile multi-model ensembles are known to improve the predictive accuracy and\nanalysis of future climate projections, consensus regarding how models are\naggregated is lacking. In this study, we propose a probabilistic machine\nlearning framework to combine 13 regional climate models from the Coordinated\nRegional Downscaling Experiment (CORDEX) over High Mountain Asia. Our approach\naccounts for seasonal and spatial biases within the models, enabling the\nprediction of more faithful precipitation distributions. The framework is\nvalidated against gridded historical precipitation data and is used to generate\nprojections for the near future (2036$\\unicode{x2013}$2065) and far future\n(2066$\\unicode{x2013}$2095) under RCP4.5 and RCP8.5 scenarios.\n","authors":["Kenza Tazi","Sun Woo P. Kim","Marc Girona-Mata","Richard E. Turner"],"pdf_url":"https://arxiv.org/pdf/2501.15690v2.pdf","comment":"16 pages 8 figures (main text), 32 pages 14 figures (total)"},{"id":"http://arxiv.org/abs/2502.15610v1","updated":"2025-02-21T17:31:22Z","published":"2025-02-21T17:31:22Z","title":"PDeepPP:A Deep learning framework with Pretrained Protein language for\n  peptide classification","summary":"  Protein post-translational modifications (PTMs) and bioactive peptides (BPs)\nplay critical roles in various biological processes and have significant\ntherapeutic potential. However, identifying PTM sites and bioactive peptides\nthrough experimental methods is often labor-intensive, costly, and\ntime-consuming. As a result, computational tools, particularly those based on\ndeep learning, have become effective solutions for predicting PTM sites and\npeptide bioactivity. Despite progress in this field, existing methods still\nstruggle with the complexity of protein sequences and the challenge of\nrequiring high-quality predictions across diverse datasets.\n  To address these issues, we propose a deep learning framework that integrates\npretrained protein language models with a neural network combining transformer\nand CNN for peptide classification. By leveraging the ability of pretrained\nmodels to capture complex relationships within protein sequences, combined with\nthe predictive power of parallel networks, our approach improves feature\nextraction while enhancing prediction accuracy.\n  This framework was applied to multiple tasks involving PTM site and bioactive\npeptide prediction, utilizing large-scale datasets to enhance the model's\nrobustness. In the comparison across 33 tasks, the model achieved\nstate-of-the-art (SOTA) performance in 25 of them, surpassing existing methods\nand demonstrating its versatility across different datasets. Our results\nsuggest that this approach provides a scalable and effective solution for\nlarge-scale peptide discovery and PTM analysis, paving the way for more\nefficient peptide classification and functional annotation.\n","authors":["Jixiu Zhai","Tianchi Lu","Haitian Zhong","Ziyang Xu","Yuhuan Liu","Xueying Wang","Dan Huang"],"pdf_url":"https://arxiv.org/pdf/2502.15610v1.pdf","comment":"10 pages, 5 figures, submitted to arXiv"},{"id":"http://arxiv.org/abs/2502.15609v1","updated":"2025-02-21T17:31:00Z","published":"2025-02-21T17:31:00Z","title":"On the Robustness of Transformers against Context Hijacking for Linear\n  Classification","summary":"  Transformer-based Large Language Models (LLMs) have demonstrated powerful\nin-context learning capabilities. However, their predictions can be disrupted\nby factually correct context, a phenomenon known as context hijacking,\nrevealing a significant robustness issue. To understand this phenomenon\ntheoretically, we explore an in-context linear classification problem based on\nrecent advances in linear transformers. In our setup, context tokens are\ndesigned as factually correct query-answer pairs, where the queries are similar\nto the final query but have opposite labels. Then, we develop a general\ntheoretical analysis on the robustness of the linear transformers, which is\nformulated as a function of the model depth, training context lengths, and\nnumber of hijacking context tokens. A key finding is that a well-trained deeper\ntransformer can achieve higher robustness, which aligns with empirical\nobservations. We show that this improvement arises because deeper layers enable\nmore fine-grained optimization steps, effectively mitigating interference from\ncontext hijacking. This is also well supported by our numerical experiments.\nOur findings provide theoretical insights into the benefits of deeper\narchitectures and contribute to enhancing the understanding of transformer\narchitectures.\n","authors":["Tianle Li","Chenyang Zhang","Xingwu Chen","Yuan Cao","Difan Zou"],"pdf_url":"https://arxiv.org/pdf/2502.15609v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.15603v1","updated":"2025-02-21T17:19:23Z","published":"2025-02-21T17:19:23Z","title":"Do Multilingual LLMs Think In English?","summary":"  Large language models (LLMs) have multilingual capabilities and can solve\ntasks across various languages. However, we show that current LLMs make key\ndecisions in a representation space closest to English, regardless of their\ninput and output languages. Exploring the internal representations with a logit\nlens for sentences in French, German, Dutch, and Mandarin, we show that the LLM\nfirst emits representations close to English for semantically-loaded words\nbefore translating them into the target language. We further show that\nactivation steering in these LLMs is more effective when the steering vectors\nare computed in English rather than in the language of the inputs and outputs.\nThis suggests that multilingual LLMs perform key reasoning steps in a\nrepresentation that is heavily shaped by English in a way that is not\ntransparent to system users.\n","authors":["Lisa Schut","Yarin Gal","Sebastian Farquhar"],"pdf_url":"https://arxiv.org/pdf/2502.15603v1.pdf","comment":"Main paper 9 pages; including appendix 48 pages"},{"id":"http://arxiv.org/abs/2502.15602v1","updated":"2025-02-21T17:19:15Z","published":"2025-02-21T17:19:15Z","title":"KAD: No More FAD! An Effective and Efficient Evaluation Metric for Audio\n  Generation","summary":"  Although being widely adopted for evaluating generated audio signals, the\nFr\\'echet Audio Distance (FAD) suffers from significant limitations, including\nreliance on Gaussian assumptions, sensitivity to sample size, and high\ncomputational complexity. As an alternative, we introduce the Kernel Audio\nDistance (KAD), a novel, distribution-free, unbiased, and computationally\nefficient metric based on Maximum Mean Discrepancy (MMD). Through analysis and\nempirical validation, we demonstrate KAD's advantages: (1) faster convergence\nwith smaller sample sizes, enabling reliable evaluation with limited data; (2)\nlower computational cost, with scalable GPU acceleration; and (3) stronger\nalignment with human perceptual judgments. By leveraging advanced embeddings\nand characteristic kernels, KAD captures nuanced differences between real and\ngenerated audio. Open-sourced in the kadtk toolkit, KAD provides an efficient,\nreliable, and perceptually aligned benchmark for evaluating generative audio\nmodels.\n","authors":["Yoonjin Chung","Pilsun Eu","Junwon Lee","Keunwoo Choi","Juhan Nam","Ben Sangbae Chon"],"pdf_url":"https://arxiv.org/pdf/2502.15602v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.21121v2","updated":"2025-02-21T17:05:11Z","published":"2024-07-30T18:24:46Z","title":"Tuning the Frequencies: Robust Training for Sinusoidal Neural Networks","summary":"  Sinusoidal neural networks have been shown effective as implicit neural\nrepresentations (INRs) of low-dimensional signals, due to their smoothness and\nhigh representation capacity. However, initializing and training them remain\nempirical tasks which lack on deeper understanding to guide the learning\nprocess. To fill this gap, our work introduces a theoretical framework that\nexplains the capacity property of sinusoidal networks and offers robust control\nmechanisms for initialization and training. Our analysis is based on a novel\namplitude-phase expansion of the sinusoidal multilayer perceptron, showing how\nits layer compositions produce a large number of new frequencies expressed as\ninteger combinations of the input frequencies. This relationship can be\ndirectly used to initialize the input neurons, as a form of spectral sampling,\nand to bound the network's spectrum while training. Our method, referred to as\nTUNER (TUNing sinusoidal nEtwoRks), greatly improves the stability and\nconvergence of sinusoidal INR training, leading to detailed reconstructions,\nwhile preventing overfitting.\n","authors":["Tiago Novello","Diana Aldana","Andre Araujo","Luiz Velho"],"pdf_url":"https://arxiv.org/pdf/2407.21121v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.15589v1","updated":"2025-02-21T16:57:22Z","published":"2025-02-21T16:57:22Z","title":"LightThinker: Thinking Step-by-Step Compression","summary":"  Large language models (LLMs) have shown remarkable performance in complex\nreasoning tasks, but their efficiency is hindered by the substantial memory and\ncomputational costs associated with generating lengthy tokens. In this paper,\nwe propose LightThinker, a novel method that enables LLMs to dynamically\ncompress intermediate thoughts during reasoning. Inspired by human cognitive\nprocesses, LightThinker compresses verbose thought steps into compact\nrepresentations and discards the original reasoning chains, thereby\nsignificantly reducing the number of tokens stored in the context window. This\nis achieved by training the model on when and how to perform compression\nthrough data construction, mapping hidden states to condensed gist tokens, and\ncreating specialized attention masks. Additionally, we introduce the Dependency\n(Dep) metric to quantify the degree of compression by measuring the reliance on\nhistorical tokens during generation. Extensive experiments on four datasets and\ntwo models show that LightThinker reduces peak memory usage and inference time,\nwhile maintaining competitive accuracy. Our work provides a new direction for\nimproving the efficiency of LLMs in complex reasoning tasks without sacrificing\nperformance. Code will be released at https://github.com/zjunlp/LightThinker.\n","authors":["Jintian Zhang","Yuqi Zhu","Mengshu Sun","Yujie Luo","Shuofei Qiao","Lun Du","Da Zheng","Huajun Chen","Ningyu Zhang"],"pdf_url":"https://arxiv.org/pdf/2502.15589v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.15588v1","updated":"2025-02-21T16:56:15Z","published":"2025-02-21T16:56:15Z","title":"Improving the Scaling Laws of Synthetic Data with Deliberate Practice","summary":"  Inspired by the principle of deliberate practice in human learning, we\npropose Deliberate Practice for Synthetic Data Generation (DP), a novel\nframework that improves sample efficiency through dynamic synthetic data\ngeneration. Prior work has shown that scaling synthetic data is inherently\nchallenging, as naively adding new data leads to diminishing returns. To\naddress this, pruning has been identified as a key mechanism for improving\nscaling, enabling models to focus on the most informative synthetic samples.\nRather than generating a large dataset and pruning it afterward, DP efficiently\napproximates the direct generation of informative samples. We theoretically\nshow how training on challenging, informative examples improves scaling laws\nand empirically validate that DP achieves better scaling performance with\nsignificantly fewer training samples and iterations. On ImageNet-100, DP\ngenerates 3.4x fewer samples and requires six times fewer iterations, while on\nImageNet-1k, it generates 8x fewer samples with a 30 percent reduction in\niterations, all while achieving superior performance compared to prior work.\n","authors":["Reyhane Askari-Hemmat","Mohammad Pezeshki","Elvis Dohmatob","Florian Bordes","Pietro Astolfi","Melissa Hall","Jakob Verbeek","Michal Drozdzal","Adriana Romero-Soriano"],"pdf_url":"https://arxiv.org/pdf/2502.15588v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.00966v3","updated":"2025-02-21T16:42:44Z","published":"2024-12-01T21:06:08Z","title":"From Priest to Doctor: Domain Adaptation for Low-Resource Neural Machine\n  Translation","summary":"  Many of the world's languages have insufficient data to train high-performing\ngeneral neural machine translation (NMT) models, let alone domain-specific\nmodels, and often the only available parallel data are small amounts of\nreligious texts. Hence, domain adaptation (DA) is a crucial issue faced by\ncontemporary NMT and has, so far, been underexplored for low-resource\nlanguages. In this paper, we evaluate a set of methods from both low-resource\nNMT and DA in a realistic setting, in which we aim to translate between a\nhigh-resource and a low-resource language with access to only: a) parallel\nBible data, b) a bilingual dictionary, and c) a monolingual target-domain\ncorpus in the high-resource language. Our results show that the effectiveness\nof the tested methods varies, with the simplest one, DALI, being most\neffective. We follow up with a small human evaluation of DALI, which shows that\nthere is still a need for more careful investigation of how to accomplish DA\nfor low-resource NMT.\n","authors":["Ali Marashian","Enora Rice","Luke Gessler","Alexis Palmer","Katharina von der Wense"],"pdf_url":"https://arxiv.org/pdf/2412.00966v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.17506v2","updated":"2025-02-21T16:42:34Z","published":"2023-12-29T08:02:13Z","title":"A graph neural network-based model with Out-of-Distribution Robustness\n  for enhancing Antiretroviral Therapy Outcome Prediction for HIV-1","summary":"  Predicting the outcome of antiretroviral therapies (ART) for HIV-1 is a\npressing clinical challenge, especially when the ART includes drugs with\nlimited effectiveness data. This scarcity of data can arise either due to the\nintroduction of a new drug to the market or due to limited use in clinical\nsettings, resulting in clinical dataset with highly unbalanced therapy\nrepresentation. To tackle this issue, we introduce a novel joint fusion model,\nwhich combines features from a Fully Connected (FC) Neural Network and a Graph\nNeural Network (GNN) in a multi-modality fashion. Our model uses both tabular\ndata about genetic sequences and a knowledge base derived from Stanford\ndrug-resistance mutation tables, which serve as benchmark references for\ndeducing in-vivo treatment efficacy based on the viral genetic sequence. By\nleveraging this knowledge base structured as a graph, the GNN component enables\nour model to adapt to imbalanced data distributions and account for\nOut-of-Distribution (OoD) drugs. We evaluated these models' robustness against\nOoD drugs in the test set. Our comprehensive analysis demonstrates that the\nproposed model consistently outperforms the FC model. These results underscore\nthe advantage of integrating Stanford scores in the model, thereby enhancing\nits generalizability and robustness, but also extending its utility in\ncontributing in more informed clinical decisions with limited data\navailability. The source code is available at\nhttps://github.com/federicosiciliano/graph-ood-hiv\n","authors":["Giulia Di Teodoro","Federico Siciliano","Valerio Guarrasi","Anne-Mieke Vandamme","Valeria Ghisetti","Anders Sönnerborg","Maurizio Zazzi","Fabrizio Silvestri","Laura Palagi"],"pdf_url":"https://arxiv.org/pdf/2312.17506v2.pdf","comment":"32 pages, 2 figures"},{"id":"http://arxiv.org/abs/2502.15577v1","updated":"2025-02-21T16:38:45Z","published":"2025-02-21T16:38:45Z","title":"Context-Aware Doubly-Robust Semi-Supervised Learning","summary":"  The widespread adoption of artificial intelligence (AI) in next-generation\ncommunication systems is challenged by the heterogeneity of traffic and network\nconditions, which call for the use of highly contextual, site-specific, data. A\npromising solution is to rely not only on real-world data, but also on\nsynthetic pseudo-data generated by a network digital twin (NDT). However, the\neffectiveness of this approach hinges on the accuracy of the NDT, which can\nvary widely across different contexts. To address this problem, this paper\nintroduces context-aware doubly-robust (CDR) learning, a novel semi-supervised\nscheme that adapts its reliance on the pseudo-data to the different levels of\nfidelity of the NDT across contexts. CDR is evaluated on the task of downlink\nbeamforming, showing superior performance compared to previous state-of-the-art\nsemi-supervised approaches.\n","authors":["Clement Ruah","Houssem Sifaou","Osvaldo Simeone","Bashir Al-Hashimi"],"pdf_url":"https://arxiv.org/pdf/2502.15577v1.pdf","comment":"This work has been submitted to the IEEE for possible publication"},{"id":"http://arxiv.org/abs/2502.15575v1","updated":"2025-02-21T16:36:20Z","published":"2025-02-21T16:36:20Z","title":"Feature maps for the Laplacian kernel and its generalizations","summary":"  Recent applications of kernel methods in machine learning have seen a renewed\ninterest in the Laplacian kernel, due to its stability to the bandwidth\nhyperparameter in comparison to the Gaussian kernel, as well as its\nexpressivity being equivalent to that of the neural tangent kernel of deep\nfully connected networks. However, unlike the Gaussian kernel, the Laplacian\nkernel is not separable. This poses challenges for techniques to approximate\nit, especially via the random Fourier features (RFF) methodology and its\nvariants. In this work, we provide random features for the Laplacian kernel and\nits two generalizations: Mat\\'{e}rn kernel and the Exponential power kernel. We\nprovide efficiently implementable schemes to sample weight matrices so that\nrandom features approximate these kernels. These weight matrices have a weakly\ncoupled heavy-tailed randomness. Via numerical experiments on real datasets we\ndemonstrate the efficacy of these random feature maps.\n","authors":["Sudhendu Ahir","Parthe Pandit"],"pdf_url":"https://arxiv.org/pdf/2502.15575v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.15568v1","updated":"2025-02-21T16:30:53Z","published":"2025-02-21T16:30:53Z","title":"A Cautionary Tale About \"Neutrally\" Informative AI Tools Ahead of the\n  2025 Federal Elections in Germany","summary":"  In this study, we examine the reliability of AI-based Voting Advice\nApplications (VAAs) and large language models (LLMs) in providing objective\npolitical information. Our analysis is based upon a comparison with party\nresponses to 38 statements of the Wahl-O-Mat, a well-established German online\ntool that helps inform voters by comparing their views with political party\npositions. For the LLMs, we identify significant biases. They exhibit a strong\nalignment (over 75% on average) with left-wing parties and a substantially\nlower alignment with center-right (smaller 50%) and right-wing parties (around\n30%). Furthermore, for the VAAs, intended to objectively inform voters, we\nfound substantial deviations from the parties' stated positions in Wahl-O-Mat:\nWhile one VAA deviated in 25% of cases, another VAA showed deviations in more\nthan 50% of cases. For the latter, we even observed that simple prompt\ninjections led to severe hallucinations, including false claims such as\nnon-existent connections between political parties and right-wing extremist\nties.\n","authors":["Ina Dormuth","Sven Franke","Marlies Hafer","Tim Katzke","Alexander Marx","Emmanuel Müller","Daniel Neider","Markus Pauly","Jérôme Rutinowski"],"pdf_url":"https://arxiv.org/pdf/2502.15568v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.15567v1","updated":"2025-02-21T16:29:11Z","published":"2025-02-21T16:29:11Z","title":"Model Privacy: A Unified Framework to Understand Model Stealing Attacks\n  and Defenses","summary":"  The use of machine learning (ML) has become increasingly prevalent in various\ndomains, highlighting the importance of understanding and ensuring its safety.\nOne pressing concern is the vulnerability of ML applications to model stealing\nattacks. These attacks involve adversaries attempting to recover a learned\nmodel through limited query-response interactions, such as those found in\ncloud-based services or on-chip artificial intelligence interfaces. While\nexisting literature proposes various attack and defense strategies, these often\nlack a theoretical foundation and standardized evaluation criteria. In\nresponse, this work presents a framework called ``Model Privacy'', providing a\nfoundation for comprehensively analyzing model stealing attacks and defenses.\nWe establish a rigorous formulation for the threat model and objectives,\npropose methods to quantify the goodness of attack and defense strategies, and\nanalyze the fundamental tradeoffs between utility and privacy in ML models. Our\ndeveloped theory offers valuable insights into enhancing the security of ML\nmodels, especially highlighting the importance of the attack-specific structure\nof perturbations for effective defenses. We demonstrate the application of\nmodel privacy from the defender's perspective through various learning\nscenarios. Extensive experiments corroborate the insights and the effectiveness\nof defense mechanisms developed under the proposed framework.\n","authors":["Ganghua Wang","Yuhong Yang","Jie Ding"],"pdf_url":"https://arxiv.org/pdf/2502.15567v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.15563v1","updated":"2025-02-21T16:24:10Z","published":"2025-02-21T16:24:10Z","title":"Bridging vision language model (VLM) evaluation gaps with a framework\n  for scalable and cost-effective benchmark generation","summary":"  Reliable evaluation of AI models is critical for scientific progress and\npractical application. While existing VLM benchmarks provide general insights\ninto model capabilities, their heterogeneous designs and limited focus on a few\nimaging domains pose significant challenges for both cross-domain performance\ncomparison and targeted domain-specific evaluation. To address this, we propose\nthree key contributions: (1) a framework for the resource-efficient creation of\ndomain-specific VLM benchmarks enabled by task augmentation for creating\nmultiple diverse tasks from a single existing task, (2) the release of new VLM\nbenchmarks for seven domains, created according to the same homogeneous\nprotocol and including 162,946 thoroughly human-validated answers, and (3) an\nextensive benchmarking of 22 state-of-the-art VLMs on a total of 37,171 tasks,\nrevealing performance variances across domains and tasks, thereby supporting\nthe need for tailored VLM benchmarks. Adoption of our methodology will pave the\nway for the resource-efficient domain-specific selection of models and guide\nfuture research efforts toward addressing core open questions.\n","authors":["Tim Rädsch","Leon Mayer","Simon Pavicic","A. Emre Kavur","Marcel Knopp","Barış Öztürk","Klaus Maier-Hein","Paul F. Jaeger","Fabian Isensee","Annika Reinke","Lena Maier-Hein"],"pdf_url":"https://arxiv.org/pdf/2502.15563v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.14803v5","updated":"2025-02-21T16:23:59Z","published":"2024-10-18T18:19:56Z","title":"DistRL: An Asynchronous Distributed Reinforcement Learning Framework for\n  On-Device Control Agents","summary":"  On-device control agents, especially on mobile devices, are responsible for\noperating mobile devices to fulfill users' requests, enabling seamless and\nintuitive interactions. Integrating Multimodal Large Language Models (MLLMs)\ninto these agents enhances their ability to understand and execute complex\ncommands, thereby improving user experience. However, fine-tuning MLLMs for\non-device control presents significant challenges due to limited data\navailability and inefficient online training processes. This paper introduces\nDistRL, a novel framework designed to enhance the efficiency of online RL\nfine-tuning for mobile device control agents. DistRL employs centralized\ntraining and decentralized data acquisition to ensure efficient fine-tuning in\nthe context of dynamic online interactions. Additionally, the framework is\nbacked by our tailor-made RL algorithm, which effectively balances exploration\nwith the prioritized utilization of collected data to ensure stable and robust\ntraining. Our experiments show that, on average, DistRL delivers a 3X\nimprovement in training efficiency and enables training data collection 2.4X\nfaster than the leading synchronous multi-machine methods. Notably, after\ntraining, DistRL achieves a 20% relative improvement in success rate compared\nto state-of-the-art methods on general Android tasks from an open benchmark,\nsignificantly outperforming existing approaches while maintaining the same\ntraining time. These results validate DistRL as a scalable and efficient\nsolution, offering substantial improvements in both training efficiency and\nagent performance for real-world, in-the-wild device control tasks.\n","authors":["Taiyi Wang","Zhihao Wu","Jianheng Liu","Jianye Hao","Jun Wang","Kun Shao"],"pdf_url":"https://arxiv.org/pdf/2410.14803v5.pdf","comment":"Paper and Appendix, 26 pages"},{"id":"http://arxiv.org/abs/2502.15561v1","updated":"2025-02-21T16:22:11Z","published":"2025-02-21T16:22:11Z","title":"A Defensive Framework Against Adversarial Attacks on Machine\n  Learning-Based Network Intrusion Detection Systems","summary":"  As cyberattacks become increasingly sophisticated, advanced Network Intrusion\nDetection Systems (NIDS) are critical for modern network security. Traditional\nsignature-based NIDS are inadequate against zero-day and evolving attacks. In\nresponse, machine learning (ML)-based NIDS have emerged as promising solutions;\nhowever, they are vulnerable to adversarial evasion attacks that subtly\nmanipulate network traffic to bypass detection. To address this vulnerability,\nwe propose a novel defensive framework that enhances the robustness of ML-based\nNIDS by simultaneously integrating adversarial training, dataset balancing\ntechniques, advanced feature engineering, ensemble learning, and extensive\nmodel fine-tuning. We validate our framework using the NSL-KDD and UNSW-NB15\ndatasets. Experimental results show, on average, a 35% increase in detection\naccuracy and a 12.5% reduction in false positives compared to baseline models,\nparticularly under adversarial conditions. The proposed defense against\nadversarial attacks significantly advances the practical deployment of robust\nML-based NIDS in real-world networks.\n","authors":["Benyamin Tafreshian","Shengzhi Zhang"],"pdf_url":"https://arxiv.org/pdf/2502.15561v1.pdf","comment":"Accepted to IEEE AI+ TrustCom 2024"},{"id":"http://arxiv.org/abs/2410.01870v2","updated":"2025-02-21T16:07:00Z","published":"2024-10-02T17:29:23Z","title":"NEAT: Nonlinear Parameter-efficient Adaptation of Pre-trained Models","summary":"  Fine-tuning pre-trained models often yields state-of-the-art performance but\nis computationally expensive when updating all parameters. Parameter-efficient\nfine-tuning (PEFT) methods, such as Low-Rank Adaptation (LoRA), address this by\nfreezing pre-trained weights and introducing low-rank matrices. However,\nbecause LoRA relies on low-rank decomposition, it struggles to capture complex\nnonlinear dynamics and optimal optimization trajectories, resulting in a\nperformance gap relative to full fine-tuning and inefficient parameter\nutilization. To overcome these issues, we propose NEAT, a nonlinear PEFT\napproach that employs a lightweight neural network to learn a nonlinear\ntransformation of the pre-trained weights, thereby better approximating\ncumulative weight updates. Our theoretical analysis shows that NEAT achieves\ngreater efficiency than LoRA while maintaining equivalent expressivity.\nExtensive experiments on four benchmarks and over twenty datasets demonstrate\nthat NEAT significantly outperforms state-of-the-art baselines in both vision\nand text tasks.\n","authors":["Yibo Zhong","Haoxiang Jiang","Lincan Li","Ryumei Nakada","Tianci Liu","Linjun Zhang","Huaxiu Yao","Haoyu Wang"],"pdf_url":"https://arxiv.org/pdf/2410.01870v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.06613v2","updated":"2025-02-21T15:55:39Z","published":"2024-11-10T22:18:53Z","title":"Are Neuromorphic Architectures Inherently Privacy-preserving? An\n  Exploratory Study","summary":"  While machine learning (ML) models are becoming mainstream, especially in\nsensitive application areas, the risk of data leakage has become a growing\nconcern. Attacks like membership inference (MIA) have shown that trained models\ncan reveal sensitive data, jeopardizing confidentiality. While traditional\nArtificial Neural Networks (ANNs) dominate ML applications, neuromorphic\narchitectures, specifically Spiking Neural Networks (SNNs), are emerging as\npromising alternatives due to their low power consumption and event-driven\nprocessing, akin to biological neurons. Privacy in ANNs is well-studied;\nhowever, little work has explored the privacy-preserving properties of SNNs.\nThis paper examines whether SNNs inherently offer better privacy. Using MIAs,\nwe assess the privacy resilience of SNNs versus ANNs across diverse datasets.\nWe analyze the impact of learning algorithms (surrogate gradient and\nevolutionary), frameworks (snnTorch, TENNLab, LAVA), and parameters on SNN\nprivacy. Our findings show that SNNs consistently outperform ANNs in privacy\npreservation, with evolutionary algorithms offering additional resilience. For\ninstance, on CIFAR-10, SNNs achieve an AUC of 0.59, significantly lower than\nANNs' 0.82, and on CIFAR-100, SNNs maintain an AUC of 0.58 compared to ANNs'\n0.88. Additionally, we explore the privacy-utility trade-off with\nDifferentially Private Stochastic Gradient Descent (DPSGD), finding that SNNs\nsustain less accuracy loss than ANNs under similar privacy constraints.\n","authors":["Ayana Moshruba","Ihsen Alouani","Maryam Parsa"],"pdf_url":"https://arxiv.org/pdf/2411.06613v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.15545v1","updated":"2025-02-21T15:51:49Z","published":"2025-02-21T15:51:49Z","title":"Estimating Vehicle Speed on Roadways Using RNNs and Transformers: A\n  Video-based Approach","summary":"  This project explores the application of advanced machine learning models,\nspecifically Long Short-Term Memory (LSTM), Gated Recurrent Units (GRU), and\nTransformers, to the task of vehicle speed estimation using video data.\nTraditional methods of speed estimation, such as radar and manual systems, are\noften constrained by high costs, limited coverage, and potential disruptions.\nIn contrast, leveraging existing surveillance infrastructure and cutting-edge\nneural network architectures presents a non-intrusive, scalable solution. Our\napproach utilizes LSTM and GRU to effectively manage long-term dependencies\nwithin the temporal sequence of video frames, while Transformers are employed\nto harness their self-attention mechanisms, enabling the processing of entire\nsequences in parallel and focusing on the most informative segments of the\ndata. This study demonstrates that both LSTM and GRU outperform basic Recurrent\nNeural Networks (RNNs) due to their advanced gating mechanisms. Furthermore,\nincreasing the sequence length of input data consistently improves model\naccuracy, highlighting the importance of contextual information in dynamic\nenvironments. Transformers, in particular, show exceptional adaptability and\nrobustness across varied sequence lengths and complexities, making them highly\nsuitable for real-time applications in diverse traffic conditions. The findings\nsuggest that integrating these sophisticated neural network models can\nsignificantly enhance the accuracy and reliability of automated speed detection\nsystems, thus promising to revolutionize traffic management and road safety.\n","authors":["Sai Krishna Reddy Mareddy","Dhanush Upplapati","Dhanush Kumar Antharam"],"pdf_url":"https://arxiv.org/pdf/2502.15545v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.06300v2","updated":"2025-02-21T15:50:46Z","published":"2025-01-10T19:00:06Z","title":"Tensorization of neural networks for improved privacy and\n  interpretability","summary":"  We present a tensorization algorithm for constructing tensor train\nrepresentations of functions, drawing on sketching and cross interpolation\nideas. The method only requires black-box access to the target function and a\nsmall set of sample points defining the domain of interest. Thus, it is\nparticularly well-suited for machine learning models, where the domain of\ninterest is naturally defined by the training dataset. We show that this\napproach can be used to enhance the privacy and interpretability of neural\nnetwork models. Specifically, we apply our decomposition to (i) obfuscate\nneural networks whose parameters encode patterns tied to the training data\ndistribution, and (ii) estimate topological phases of matter that are easily\naccessible from the tensor train representation. Additionally, we show that\nthis tensorization can serve as an efficient initialization method for\noptimizing tensor trains in general settings, and that, for model compression,\nour algorithm achieves a superior trade-off between memory and time complexity\ncompared to conventional tensorization methods of neural networks.\n","authors":["José Ramón Pareja Monturiol","Alejandro Pozas-Kerstjens","David Pérez-García"],"pdf_url":"https://arxiv.org/pdf/2501.06300v2.pdf","comment":"40 pages, 9 figures. The code for the experiments is publicly\n  available at https://github.com/joserapa98/tensorization-nns. V2: Added code\n  repository"},{"id":"http://arxiv.org/abs/2502.07891v2","updated":"2025-02-21T15:50:24Z","published":"2025-02-11T19:00:58Z","title":"The Observational Partial Order of Causal Structures with Latent\n  Variables","summary":"  For two causal structures with the same set of visible variables, one is said\nto observationally dominate the other if the set of distributions over the\nvisible variables realizable by the first contains the set of distributions\nover the visible variables realizable by the second. Knowing such dominance\nrelations is useful for adjudicating between these structures given\nobservational data. We here consider the problem of determining the partial\norder of equivalence classes of causal structures with latent variables\nrelative to observational dominance. We provide a complete characterization of\nthe dominance order in the case of three visible variables, and a partial\ncharacterization in the case of four visible variables. Our techniques also\nhelp to identify which observational equivalence classes have a set of\nrealizable distributions that is characterized by nontrivial inequality\nconstraints, analogous to Bell inequalities and instrumental inequalities. We\nfind evidence that as one increases the number of visible variables, the\nequivalence classes satisfying nontrivial inequality constraints become\nubiquitous. (Because such classes are the ones for which there can be a\ndifference in the distributions that are quantumly and classically realizable,\nthis implies that the potential for quantum-classical gaps is also ubiquitous.)\nFurthermore, we find evidence that constraint-based causal discovery algorithms\nthat rely solely on conditional independence constraints have a significantly\nweaker distinguishing power among observational equivalence classes than\nalgorithms that go beyond these (i.e., algorithms that also leverage nested\nMarkov constraints and inequality constraints).\n","authors":["Marina Maciel Ansanelli","Elie Wolfe","Robert W. Spekkens"],"pdf_url":"https://arxiv.org/pdf/2502.07891v2.pdf","comment":"48 pages, 30 figures; acknowledgements added"},{"id":"http://arxiv.org/abs/2409.17179v2","updated":"2025-02-21T15:48:32Z","published":"2024-09-23T17:40:24Z","title":"Fully automatic extraction of morphological traits from the Web: utopia\n  or reality?","summary":"  Plant morphological traits, their observable characteristics, are fundamental\nto understand the role played by each species within their ecosystem. However,\ncompiling trait information for even a moderate number of species is a\ndemanding task that may take experts years to accomplish. At the same time,\nmassive amounts of information about species descriptions is available online\nin the form of text, although the lack of structure makes this source of data\nimpossible to use at scale. To overcome this, we propose to leverage recent\nadvances in large language models (LLMs) and devise a mechanism for gathering\nand processing information on plant traits in the form of unstructured textual\ndescriptions, without manual curation. We evaluate our approach by\nautomatically replicating three manually created species-trait matrices. Our\nmethod managed to find values for over half of all species-trait pairs, with an\nF1-score of over 75%. Our results suggest that large-scale creation of\nstructured trait databases from unstructured online text is currently feasible\nthanks to the information extraction capabilities of LLMs, being limited by the\navailability of textual descriptions covering all the traits of interest.\n","authors":["Diego Marcos","Robert van de Vlasakker","Ioannis N. Athanasiadis","Pierre Bonnet","Hervé Goeau","Alexis Joly","W. Daniel Kissling","César Leblanc","André S. J. van Proosdij","Konstantinos P. Panousis"],"pdf_url":"https://arxiv.org/pdf/2409.17179v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.15540v1","updated":"2025-02-21T15:43:31Z","published":"2025-02-21T15:43:31Z","title":"Generalization Guarantees for Representation Learning via Data-Dependent\n  Gaussian Mixture Priors","summary":"  We establish in-expectation and tail bounds on the generalization error of\nrepresentation learning type algorithms. The bounds are in terms of the\nrelative entropy between the distribution of the representations extracted from\nthe training and \"test'' datasets and a data-dependent symmetric prior, i.e.,\nthe Minimum Description Length (MDL) of the latent variables for the training\nand test datasets. Our bounds are shown to reflect the \"structure\" and\n\"simplicity'' of the encoder and significantly improve upon the few existing\nones for the studied model. We then use our in-expectation bound to devise a\nsuitable data-dependent regularizer; and we investigate thoroughly the\nimportant question of the selection of the prior. We propose a systematic\napproach to simultaneously learning a data-dependent Gaussian mixture prior and\nusing it as a regularizer. Interestingly, we show that a weighted attention\nmechanism emerges naturally in this procedure. Our experiments show that our\napproach outperforms the now popular Variational Information Bottleneck (VIB)\nmethod as well as the recent Category-Dependent VIB (CDVIB).\n","authors":["Milad Sefidgaran","Abdellatif Zaidi","Piotr Krasnowski"],"pdf_url":"https://arxiv.org/pdf/2502.15540v1.pdf","comment":"Accepted as a Spotlight Paper at ICLR 2025"},{"id":"http://arxiv.org/abs/2410.07014v2","updated":"2025-02-21T15:34:11Z","published":"2024-10-09T15:58:06Z","title":"Optimizing Estimators of Squared Calibration Errors in Classification","summary":"  In this work, we propose a mean-squared error-based risk that enables the\ncomparison and optimization of estimators of squared calibration errors in\npractical settings. Improving the calibration of classifiers is crucial for\nenhancing the trustworthiness and interpretability of machine learning models,\nespecially in sensitive decision-making scenarios. Although various calibration\n(error) estimators exist in the current literature, there is a lack of guidance\non selecting the appropriate estimator and tuning its hyperparameters. By\nleveraging the bilinear structure of squared calibration errors, we reformulate\ncalibration estimation as a regression problem with independent and identically\ndistributed (i.i.d.) input pairs. This reformulation allows us to quantify the\nperformance of different estimators even for the most challenging calibration\ncriterion, known as canonical calibration. Our approach advocates for a\ntraining-validation-testing pipeline when estimating a calibration error on an\nevaluation dataset. We demonstrate the effectiveness of our pipeline by\noptimizing existing calibration estimators and comparing them with novel kernel\nridge regression-based estimators on standard image classification tasks.\n","authors":["Sebastian G. Gruber","Francis Bach"],"pdf_url":"https://arxiv.org/pdf/2410.07014v2.pdf","comment":"Published at TMLR, see https://openreview.net/forum?id=BPDVZajOW5"},{"id":"http://arxiv.org/abs/2502.15522v1","updated":"2025-02-21T15:24:34Z","published":"2025-02-21T15:24:34Z","title":"Solving Inverse Problems with Deep Linear Neural Networks: Global\n  Convergence Guarantees for Gradient Descent with Weight Decay","summary":"  Machine learning methods are commonly used to solve inverse problems, wherein\nan unknown signal must be estimated from few measurements generated via a known\nacquisition procedure. In particular, neural networks perform well empirically\nbut have limited theoretical guarantees. In this work, we study an\nunderdetermined linear inverse problem that admits several possible solution\nmappings. A standard remedy (e.g., in compressed sensing) establishing\nuniqueness of the solution mapping is to assume knowledge of latent\nlow-dimensional structure in the source signal. We ask the following question:\ndo deep neural networks adapt to this low-dimensional structure when trained by\ngradient descent with weight decay regularization? We prove that mildly\noverparameterized deep linear networks trained in this manner converge to an\napproximate solution that accurately solves the inverse problem while\nimplicitly encoding latent subspace structure. To our knowledge, this is the\nfirst result to rigorously show that deep linear networks trained with weight\ndecay automatically adapt to latent subspace structure in the data under\npractical stepsize and weight initialization schemes. Our work highlights that\nregularization and overparameterization improve generalization, while\noverparameterization also accelerates convergence during training.\n","authors":["Hannah Laus","Suzanna Parkinson","Vasileios Charisopoulos","Felix Krahmer","Rebecca Willett"],"pdf_url":"https://arxiv.org/pdf/2502.15522v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.11164v2","updated":"2025-02-21T15:15:57Z","published":"2025-02-16T15:29:58Z","title":"Quantifying the Capability Boundary of DeepSeek Models: An\n  Application-Driven Performance Analysis","summary":"  DeepSeek-R1, known for its low training cost and exceptional reasoning\ncapabilities, has achieved state-of-the-art performance on various benchmarks.\nHowever, detailed evaluations from the perspective of real-world applications\nare lacking, making it challenging for users to select the most suitable\nDeepSeek models for their specific needs. To address this gap, we evaluate the\nDeepSeek-V3, DeepSeek-R1, DeepSeek-R1-Distill-Qwen series, and\nDeepSeek-R1-Distill-Llama series on the improved version A-Eval (A-Eval-2.0),\nan application-driven benchmark. By comparing original instruction-tuned models\nwith their distilled counterparts, we analyze how reasoning enhancements impact\nperformance across diverse practical tasks. Our results show that\nreasoning-enhanced models, while generally powerful, do not universally\noutperform across all tasks, with performance gains varying significantly\nacross tasks and models. To further assist users in model selection, we\nquantify the capability boundary of DeepSeek models through performance tier\nclassifications and intuitive line charts. Specific examples provide actionable\ninsights to help users select and deploy the most cost-effective DeepSeek\nmodels, ensuring optimal performance and resource efficiency in real-world\napplications. It should be noted that, despite our efforts to establish a\ncomprehensive, objective, and authoritative evaluation benchmark, the selection\nof test samples, characteristics of data distribution, and the setting of\nevaluation criteria may inevitably introduce certain biases into the evaluation\nresults. We will continuously optimize the evaluation benchmarks and\nperiodically update this paper to provide more comprehensive and accurate\nevaluation results. Please refer to the latest version of the paper for the\nmost recent results and conclusions.\n","authors":["Kaikai Zhao","Zhaoxiang Liu","Xuejiao Lei","Ning Wang","Jiaojiao Zhao","Zipeng Wang","Zhenhong Long","Peijun Yang","Minjie Hua","Chaoyang Ma","Wen Liu","Kai Wang","Shiguo Lian"],"pdf_url":"https://arxiv.org/pdf/2502.11164v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.04818v2","updated":"2025-02-21T15:15:21Z","published":"2025-02-07T10:42:01Z","title":"Harnessing omnipresent oscillator networks as computational resource","summary":"  Nature is pervaded with oscillatory behavior. In networks of coupled\noscillators patterns can arise when the system synchronizes to an external\ninput. Hence, these networks provide processing and memory of input. We present\na universal framework for harnessing oscillator networks as computational\nresource. This reservoir computing framework is introduced by the ubiquitous\nmodel for phase-locking, the Kuramoto model. We force the Kuramoto model by a\nnonlinear target-system, then after substituting the target-system with a\ntrained feedback-loop it emulates the target-system. Our results are two-fold.\nFirstly, the trained network inherits performance properties of the Kuramoto\nmodel, where all-to-all coupling is performed in linear time with respect to\nthe number of nodes and parameters for synchronization are abundant. Secondly,\nthe learning capabilities of the oscillator network can be explained using\nKuramoto model's order parameter. This work provides the foundation for\nutilizing nature's oscillator networks as a new class of information processing\nsystems.\n","authors":["Thomas Geert de Jong","Hirofumi Notsu","Kohei Nakajima"],"pdf_url":"https://arxiv.org/pdf/2502.04818v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.15512v1","updated":"2025-02-21T15:09:39Z","published":"2025-02-21T15:09:39Z","title":"SALSA-RL: Stability Analysis in the Latent Space of Actions for\n  Reinforcement Learning","summary":"  Modern deep reinforcement learning (DRL) methods have made significant\nadvances in handling continuous action spaces. However, real-world control\nsystems--especially those requiring precise and reliable performance--often\ndemand formal stability, and existing DRL approaches typically lack explicit\nmechanisms to ensure or analyze stability. To address this limitation, we\npropose SALSA-RL (Stability Analysis in the Latent Space of Actions), a novel\nRL framework that models control actions as dynamic, time-dependent variables\nevolving within a latent space. By employing a pre-trained encoder-decoder and\na state-dependent linear system, our approach enables both stability analysis\nand interpretability. We demonstrated that SALSA-RL can be deployed in a\nnon-invasive manner for assessing the local stability of actions from\npretrained RL agents without compromising on performance across diverse\nbenchmark environments. By enabling a more interpretable analysis of action\ngeneration, SALSA-RL provides a powerful tool for advancing the design,\nanalysis, and theoretical understanding of RL systems.\n","authors":["Xuyang Li","Romit Maulik"],"pdf_url":"https://arxiv.org/pdf/2502.15512v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.15507v1","updated":"2025-02-21T15:04:48Z","published":"2025-02-21T15:04:48Z","title":"Activation Steering in Neural Theorem Provers","summary":"  Large Language Models (LLMs) have shown promise in proving formal theorems\nusing proof assistants like Lean. However, current state of the art language\nmodels struggles to predict next step in proofs leading practitioners to use\ndifferent sampling techniques to improve LLMs capabilities. We observe that the\nLLM is capable of predicting the correct tactic; however, it faces challenges\nin ranking it appropriately within the set of candidate tactics, affecting the\noverall selection process. To overcome this hurdle, we use activation steering\nto guide LLMs responses to improve the generations at the time of inference.\nOur results suggest that activation steering offers a promising lightweight\nalternative to specialized fine-tuning for enhancing theorem proving\ncapabilities in LLMs, particularly valuable in resource-constrained\nenvironments.\n","authors":["Shashank Kirtania"],"pdf_url":"https://arxiv.org/pdf/2502.15507v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.03642v2","updated":"2025-02-21T14:54:05Z","published":"2024-06-05T22:35:17Z","title":"Is Free Self-Alignment Possible?","summary":"  Aligning pretrained language models (LMs) often requires large-scale\npreference data and substantial computational resources. These costs become\neven more prohibitive for multi-objective or pluralistic alignment. Is this\ntruly necessary? Can we perform efficient alignment using only internal model\ncapabilities, and without additional training? To answer this question, we\npropose AlignEZ, a novel approach that leverages (1) self-generated preference\ndata and (2) representation editing to achieve cost-effective, efficient\nalignment. By operating directly on learned representations, AlignEZ\nindependently targets different behavioral aspects without the overhead of\ntraditional alignment methods. Our experiments reveal that this cost-efficient\nprocedure improves performance across diverse tasks: up to 19.9% on general\nalignment and 1.9% on challenging mathematical reasoning tasks, even when\nstarting from a strong base model. AlignEZ can also align models to multiple\nobjectives simultaneously, granting fine-grained control over multiple\npreference axes. Finally, we show that AlignEZ can accelerate more expensive\nalignment procedures--such as DPO--even under limited availability of\nground-truth preference data.\n","authors":["Dyah Adila","Changho Shin","Yijing Zhang","Frederic Sala"],"pdf_url":"https://arxiv.org/pdf/2406.03642v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07780v2","updated":"2025-02-21T14:41:48Z","published":"2025-02-11T18:59:35Z","title":"DarwinLM: Evolutionary Structured Pruning of Large Language Models","summary":"  Large Language Models (LLMs) have achieved significant success across various\nNLP tasks. However, their massive computational costs limit their widespread\nuse, particularly in real-time applications. Structured pruning offers an\neffective solution by compressing models and directly providing end-to-end\nspeed improvements, regardless of the hardware environment. Meanwhile,\ndifferent components of the model exhibit varying sensitivities towards\npruning, calling for \\emph{non-uniform} model compression. However, a pruning\nmethod should not only identify a capable substructure, but also account for\npost-compression training. To this end, we propose \\sysname, a method for\n\\emph{training-aware} structured pruning. \\sysname builds upon an evolutionary\nsearch process, generating multiple offspring models in each generation through\nmutation, and selecting the fittest for survival. To assess the effect of\npost-training, we incorporate a lightweight, multistep training process within\nthe offspring population, progressively increasing the number of tokens and\neliminating poorly performing models in each selection stage. We validate our\nmethod through extensive experiments on Llama-2-7B, Llama-3.1-8B and\nQwen-2.5-14B-Instruct, achieving state-of-the-art performance for structured\npruning. For instance, \\sysname surpasses ShearedLlama while requiring\n$5\\times$ less training data during post-compression training. Code is at:\nhttps://github.com/IST-DASLab/DarwinLM\n","authors":["Shengkun Tang","Oliver Sieberling","Eldar Kurtic","Zhiqiang Shen","Dan Alistarh"],"pdf_url":"https://arxiv.org/pdf/2502.07780v2.pdf","comment":"Code: https://github.com/IST-DASLab/DarwinLM"},{"id":"http://arxiv.org/abs/2502.15496v1","updated":"2025-02-21T14:41:28Z","published":"2025-02-21T14:41:28Z","title":"Verification and Validation for Trustworthy Scientific Machine Learning","summary":"  Scientific machine learning (SciML) models are transforming many scientific\ndisciplines. However, the development of good modeling practices to increase\nthe trustworthiness of SciML has lagged behind its application, limiting its\npotential impact. The goal of this paper is to start a discussion on\nestablishing consensus-based good practices for predictive SciML. We identify\nkey challenges in applying existing computational science and engineering\nguidelines, such as verification and validation protocols, and provide\nrecommendations to address these challenges. Our discussion focuses on\npredictive SciML, which uses machine learning models to learn, improve, and\naccelerate numerical simulations of physical systems. While centered on\npredictive applications, our 16 recommendations aim to help researchers conduc\n","authors":["John D. Jakeman","Lorena A. Barba","Joaquim R. R. A. Martins","Thomas O'Leary-Roseberry"],"pdf_url":"https://arxiv.org/pdf/2502.15496v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.14790v2","updated":"2025-02-21T14:40:21Z","published":"2025-02-20T18:10:12Z","title":"An Adversarial Analysis of Thompson Sampling for Full-information Online\n  Learning: from Finite to Infinite Action Spaces","summary":"  We develop an analysis of Thompson sampling for online learning under full\nfeedback - also known as prediction with expert advice - where the learner's\nprior is defined over the space of an adversary's future actions, rather than\nthe space of experts. We show regret decomposes into regret the learner\nexpected a priori, plus a prior-robustness-type term we call excess regret. In\nthe classical finite-expert setting, this recovers optimal rates. As an initial\nstep towards practical online learning in settings with a\npotentially-uncountably-infinite number of experts, we show that Thompson\nsampling with a certain Gaussian process prior widely-used in the Bayesian\noptimization literature has a $\\mathcal{O}(\\beta\\sqrt{T\\log(1+\\lambda)})$ rate\nagainst a $\\beta$-bounded $\\lambda$-Lipschitz adversary.\n","authors":["Alexander Terenin","Jeffrey Negrea"],"pdf_url":"https://arxiv.org/pdf/2502.14790v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.03587v3","updated":"2025-02-21T14:36:42Z","published":"2025-02-05T20:09:28Z","title":"Stein Discrepancy for Unsupervised Domain Adaptation","summary":"  Unsupervised domain adaptation (UDA) leverages information from a labeled\nsource dataset to improve accuracy on a related but unlabeled target dataset. A\ncommon approach to UDA is aligning representations from the source and target\ndomains by minimizing the distance between their data distributions. Previous\nmethods have employed distances such as Wasserstein distance and maximum mean\ndiscrepancy. However, these approaches are less effective when the target data\nis significantly scarcer than the source data. Stein discrepancy is an\nasymmetric distance between distributions that relies on one distribution only\nthrough its score function. In this paper, we propose a novel UDA method that\nuses Stein discrepancy to measure the distance between source and target\ndomains. We develop a learning framework using both non-kernelized and\nkernelized Stein discrepancy. Theoretically, we derive an upper bound for the\ngeneralization error. Numerical experiments show that our method outperforms\nexisting methods using other domain discrepancy measures when only small\namounts of target data are available.\n","authors":["Anneke von Seeger","Dongmian Zou","Gilad Lerman"],"pdf_url":"https://arxiv.org/pdf/2502.03587v3.pdf","comment":"24 pages, 9 figures"},{"id":"http://arxiv.org/abs/2502.15491v1","updated":"2025-02-21T14:36:12Z","published":"2025-02-21T14:36:12Z","title":"Network Resource Optimization for ML-Based UAV Condition Monitoring with\n  Vibration Analysis","summary":"  As smart cities begin to materialize, the role of Unmanned Aerial Vehicles\n(UAVs) and their reliability becomes increasingly important. One aspect of\nreliability relates to Condition Monitoring (CM), where Machine Learning (ML)\nmodels are leveraged to identify abnormal and adverse conditions. Given the\nresource-constrained nature of next-generation edge networks, the utilization\nof precious network resources must be minimized. This work explores the\noptimization of network resources for ML-based UAV CM frameworks. The developed\nframework uses experimental data and varies the feature extraction aggregation\ninterval to optimize ML model selection. Additionally, by leveraging\ndimensionality reduction techniques, there is a 99.9% reduction in network\nresource consumption.\n","authors":["Alexandre Gemayel","Dimitrios Michael Manias","Abdallah Shami"],"pdf_url":"https://arxiv.org/pdf/2502.15491v1.pdf","comment":"Accepted for publication in IEEE Networking Letters"},{"id":"http://arxiv.org/abs/2501.02652v2","updated":"2025-02-21T14:32:16Z","published":"2025-01-05T20:37:34Z","title":"A View of the Certainty-Equivalence Method for PAC RL as an Application\n  of the Trajectory Tree Method","summary":"  Reinforcement learning (RL) enables an agent interacting with an unknown MDP\n$M$ to optimise its behaviour by observing transitions sampled from $M$. A\nnatural entity that emerges in the agent's reasoning is $\\widehat{M}$, the\nmaximum likelihood estimate of $M$ based on the observed transitions. The\nwell-known \\textit{certainty-equivalence} method (CEM) dictates that the agent\nupdate its behaviour to $\\widehat{\\pi}$, which is an optimal policy for\n$\\widehat{M}$. Not only is CEM intuitive, it has been shown to enjoy\nminimax-optimal sample complexity in some regions of the parameter space for\nPAC RL with a generative model~\\citep{Agarwal2020GenModel}.\n  A seemingly unrelated algorithm is the ``trajectory tree method''\n(TTM)~\\citep{Kearns+MN:1999}, originally developed for efficient decision-time\nplanning in large POMDPs. This paper presents a theoretical investigation that\nstems from the surprising finding that CEM may indeed be viewed as an\napplication of TTM. The qualitative benefits of this view are (1) new and\nsimple proofs of sample complexity upper bounds for CEM, in fact under a (2)\nweaker assumption on the rewards than is prevalent in the current literature.\nOur analysis applies to both non-stationary and stationary MDPs.\nQuantitatively, we obtain (3) improvements in the sample-complexity upper\nbounds for CEM both for non-stationary and stationary MDPs, in the regime that\nthe ``mistake probability'' $\\delta$ is small. Additionally, we show (4) a\nlower bound on the sample complexity for finite-horizon MDPs, which establishes\nthe minimax-optimality of our upper bound for non-stationary MDPs in the\nsmall-$\\delta$ regime.\n","authors":["Shivaram Kalyanakrishnan","Sheel Shah","Santhosh Kumar Guguloth"],"pdf_url":"https://arxiv.org/pdf/2501.02652v2.pdf","comment":"15 pages, excluding references and appendices. Total of 29 pages"},{"id":"http://arxiv.org/abs/2410.13577v2","updated":"2025-02-21T14:23:17Z","published":"2024-10-17T14:12:35Z","title":"Generalization Bounds via Meta-Learned Model Representations: PAC-Bayes\n  and Sample Compression Hypernetworks","summary":"  PAC-Bayesian and Sample Compress learning frameworks are instrumental for\nderiving tight (non-vacuous) generalization bounds for neural networks. We\nleverage these results in a meta-learning scheme, relying on a hypernetwork\nthat outputs the parameters of a downstream predictor from a dataset input. The\noriginality of our approach lies in the investigated hypernetwork architectures\nthat encode the dataset before decoding the parameters: (1) a PAC-Bayesian\nencoder that expresses a posterior distribution over a latent space, (2) a\nSample Compress encoder that selects a small sample of the dataset input along\nwith a message from a discrete set, and (3) a hybrid between both approaches\nmotivated by a new Sample Compress theorem handling continuous messages. The\nlatter theorem exploits the pivotal information transiting at the\nencoder-decoder junction to compute generalization guarantees for each\ndownstream predictor obtained by our meta-learning scheme.\n","authors":["Benjamin Leblanc","Mathieu Bazinet","Nathaniel D'Amours","Alexandre Drouin","Pascal Germain"],"pdf_url":"https://arxiv.org/pdf/2410.13577v2.pdf","comment":"Accepted at the NeurIPS 2024 workshop on Compression in Machine\n  Learning"},{"id":"http://arxiv.org/abs/2502.11910v2","updated":"2025-02-21T14:17:57Z","published":"2025-02-17T15:28:40Z","title":"Adversarial Alignment for LLMs Requires Simpler, Reproducible, and More\n  Measurable Objectives","summary":"  Misaligned research objectives have considerably hindered progress in\nadversarial robustness research over the past decade. For instance, an\nextensive focus on optimizing target metrics, while neglecting rigorous\nstandardized evaluation, has led researchers to pursue ad-hoc heuristic\ndefenses that were seemingly effective. Yet, most of these were exposed as\nflawed by subsequent evaluations, ultimately contributing little measurable\nprogress to the field. In this position paper, we illustrate that current\nresearch on the robustness of large language models (LLMs) risks repeating past\npatterns with potentially worsened real-world implications. To address this, we\nargue that realigned objectives are necessary for meaningful progress in\nadversarial alignment. To this end, we build on established cybersecurity\ntaxonomy to formally define differences between past and emerging threat models\nthat apply to LLMs. Using this framework, we illustrate that progress requires\ndisentangling adversarial alignment into addressable sub-problems and returning\nto core academic principles, such as measureability, reproducibility, and\ncomparability. Although the field presents significant challenges, the fresh\nstart on adversarial robustness offers the unique opportunity to build on past\nexperience while avoiding previous mistakes.\n","authors":["Leo Schwinn","Yan Scholten","Tom Wollschläger","Sophie Xhonneux","Stephen Casper","Stephan Günnemann","Gauthier Gidel"],"pdf_url":"https://arxiv.org/pdf/2502.11910v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.15483v1","updated":"2025-02-21T14:12:44Z","published":"2025-02-21T14:12:44Z","title":"MoMa: A Modular Deep Learning Framework for Material Property Prediction","summary":"  Deep learning methods for material property prediction have been widely\nexplored to advance materials discovery. However, the prevailing pre-train then\nfine-tune paradigm often fails to address the inherent diversity and disparity\nof material tasks. To overcome these challenges, we introduce MoMa, a Modular\nframework for Materials that first trains specialized modules across a wide\nrange of tasks and then adaptively composes synergistic modules tailored to\neach downstream scenario. Evaluation across 17 datasets demonstrates the\nsuperiority of MoMa, with a substantial 14% average improvement over the\nstrongest baseline. Few-shot and continual learning experiments further\nhighlight MoMa's potential for real-world applications. Pioneering a new\nparadigm of modular material learning, MoMa will be open-sourced to foster\nbroader community collaboration.\n","authors":["Botian Wang","Yawen Ouyang","Yaohui Li","Yiqun Wang","Haorui Cui","Jianbing Zhang","Xiaonan Wang","Wei-Ying Ma","Hao Zhou"],"pdf_url":"https://arxiv.org/pdf/2502.15483v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.14365v2","updated":"2025-02-21T14:11:33Z","published":"2025-02-20T08:42:30Z","title":"Is Q-learning an Ill-posed Problem?","summary":"  This paper investigates the instability of Q-learning in continuous\nenvironments, a challenge frequently encountered by practitioners.\nTraditionally, this instability is attributed to bootstrapping and regression\nmodel errors. Using a representative reinforcement learning benchmark, we\nsystematically examine the effects of bootstrapping and model inaccuracies by\nincrementally eliminating these potential error sources. Our findings reveal\nthat even in relatively simple benchmarks, the fundamental task of Q-learning -\niteratively learning a Q-function from policy-specific target values - can be\ninherently ill-posed and prone to failure. These insights cast doubt on the\nreliability of Q-learning as a universal solution for reinforcement learning\nproblems.\n","authors":["Philipp Wissmann","Daniel Hein","Steffen Udluft","Thomas Runkler"],"pdf_url":"https://arxiv.org/pdf/2502.14365v2.pdf","comment":"Accepted at ESANN 2025"},{"id":"http://arxiv.org/abs/2502.15476v1","updated":"2025-02-21T14:00:25Z","published":"2025-02-21T14:00:25Z","title":"Sheaf theory: from deep geometry to deep learning","summary":"  This paper provides an overview of the applications of sheaf theory in deep\nlearning, data science, and computer science in general. The primary text of\nthis work serves as a friendly introduction to applied and computational sheaf\ntheory accessible to those with modest mathematical familiarity. We describe\nintuitions and motivations underlying sheaf theory shared by both theoretical\nresearchers and practitioners, bridging classical mathematical theory and its\nmore recent implementations within signal processing and deep learning. We\nobserve that most notions commonly considered specific to cellular sheaves\ntranslate to sheaves on arbitrary posets, providing an interesting avenue for\nfurther generalization of these methods in applications, and we present a new\nalgorithm to compute sheaf cohomology on arbitrary finite posets in response.\nBy integrating classical theory with recent applications, this work reveals\ncertain blind spots in current machine learning practices. We conclude with a\nlist of problems related to sheaf-theoretic applications that we find\nmathematically insightful and practically instructive to solve. To ensure the\nexposition of sheaf theory is self-contained, a rigorous mathematical\nintroduction is provided in appendices which moves from an introduction of\ndiagrams and sheaves to the definition of derived functors, higher order\ncohomology, sheaf Laplacians, sheaf diffusion, and interconnections of these\nsubjects therein.\n","authors":["Anton Ayzenberg","Thomas Gebhart","German Magai","Grigory Solomadin"],"pdf_url":"https://arxiv.org/pdf/2502.15476v1.pdf","comment":"117 pages, 8 figures"},{"id":"http://arxiv.org/abs/2502.15475v1","updated":"2025-02-21T14:00:14Z","published":"2025-02-21T14:00:14Z","title":"Decoding for Punctured Convolutional and Turbo Codes: A Deep Learning\n  Solution for Protocols Compliance","summary":"  Neural network-based decoding methods have shown promise in enhancing error\ncorrection performance, but traditional approaches struggle with the challenges\nposed by punctured codes. In particular, these methods fail to address the\ncomplexities of variable code rates and the need for protocol compatibility.\nThis paper presents a unified Long Short-Term Memory (LSTM)-based decoding\narchitecture specifically designed to overcome these challenges. The proposed\nmethod unifies punctured convolutional and Turbo codes. A puncture embedding\nmechanism integrates puncturing patterns directly into the network, enabling\nseamless adaptation to varying code rates, while balanced bit error rate\ntraining ensures robustness across different code lengths, rates, and channels,\nmaintaining protocol flexibility. Extensive simulations in Additive White\nGaussian Noise and Rayleigh fading channels demonstrate that the proposed\napproach outperforms conventional decoding techniques, providing significant\nimprovements in decoding accuracy and robustness. These results underscore the\npotential of LSTM-based decoding as a promising solution for next-generation\nartificial intelligence powered communication systems.\n","authors":["Yongli Yan","Linglong Dai"],"pdf_url":"https://arxiv.org/pdf/2502.15475v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.15470v1","updated":"2025-02-21T13:52:31Z","published":"2025-02-21T13:52:31Z","title":"PAPI: Exploiting Dynamic Parallelism in Large Language Model Decoding\n  with a Processing-In-Memory-Enabled Computing System","summary":"  Large language models (LLMs) are widely used for natural language\nunderstanding and text generation. An LLM model relies on a time-consuming step\ncalled LLM decoding to generate output tokens. Several prior works focus on\nimproving the performance of LLM decoding using parallelism techniques, such as\nbatching and speculative decoding. State-of-the-art LLM decoding has both\ncompute-bound and memory-bound kernels. Some prior works statically identify\nand map these different kernels to a heterogeneous architecture consisting of\nboth processing-in-memory (PIM) units and computation-centric accelerators. We\nobserve that characteristics of LLM decoding kernels (e.g., whether or not a\nkernel is memory-bound) can change dynamically due to parameter changes to meet\nuser and/or system demands, making (1) static kernel mapping to PIM units and\ncomputation-centric accelerators suboptimal, and (2) one-size-fits-all approach\nof designing PIM units inefficient due to a large degree of heterogeneity even\nin memory-bound kernels.\n  In this paper, we aim to accelerate LLM decoding while considering the\ndynamically changing characteristics of the kernels involved. We propose PAPI\n(PArallel Decoding with PIM), a PIM-enabled heterogeneous architecture that\nexploits dynamic scheduling of compute-bound or memory-bound kernels to\nsuitable hardware units. PAPI has two key mechanisms: (1) online kernel\ncharacterization to dynamically schedule kernels to the most suitable hardware\nunits at runtime and (2) a PIM-enabled heterogeneous computing system that\nharmoniously orchestrates both computation-centric processing units and hybrid\nPIM units with different computing capabilities. Our experimental results on\nthree broadly-used LLMs show that PAPI achieves 1.8$\\times$ and 11.1$\\times$\nspeedups over a state-of-the-art heterogeneous LLM accelerator and a\nstate-of-the-art PIM-only LLM accelerator, respectively.\n","authors":["Yintao He","Haiyu Mao","Christina Giannoula","Mohammad Sadrosadati","Juan Gómez-Luna","Huawei Li","Xiaowei Li","Ying Wang","Onur Mutlu"],"pdf_url":"https://arxiv.org/pdf/2502.15470v1.pdf","comment":"To appear in ASPLOS 2025"},{"id":"http://arxiv.org/abs/2502.15466v1","updated":"2025-02-21T13:43:24Z","published":"2025-02-21T13:43:24Z","title":"Mitigating Data Scarcity in Time Series Analysis: A Foundation Model\n  with Series-Symbol Data Generation","summary":"  Foundation models for time series analysis (TSA) have attracted significant\nattention. However, challenges such as data scarcity and data imbalance\ncontinue to hinder their development. To address this, we consider modeling\ncomplex systems through symbolic expressions that serve as semantic descriptors\nof time series. Building on this concept, we introduce a series-symbol (S2)\ndual-modulity data generation mechanism, enabling the unrestricted creation of\nhigh-quality time series data paired with corresponding symbolic\nrepresentations. Leveraging the S2 dataset, we develop SymTime, a pre-trained\nfoundation model for TSA. SymTime demonstrates competitive performance across\nfive major TSA tasks when fine-tuned with downstream task, rivaling foundation\nmodels pre-trained on real-world datasets. This approach underscores the\npotential of dual-modality data generation and pretraining mechanisms in\novercoming data scarcity and enhancing task performance.\n","authors":["Wenxuan Wang","Kai Wu","Yujian Betterest Li","Dan Wang","Xiaoyu Zhang","Jing Liu"],"pdf_url":"https://arxiv.org/pdf/2502.15466v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2201.09698v3","updated":"2025-02-21T13:40:32Z","published":"2022-01-24T14:07:56Z","title":"Graph Neural Diffusion Networks for Semi-supervised Learning","summary":"  Graph Convolutional Networks (GCN) is a pioneering model for graph-based\nsemi-supervised learning. However, GCN does not perform well on\nsparsely-labeled graphs. Its two-layer version cannot effectively propagate the\nlabel information to the whole graph structure (i.e., the under-smoothing\nproblem) while its deep version over-smoothens and is hard to train (i.e., the\nover-smoothing problem). To solve these two issues, we propose a new graph\nneural network called GND-Nets (for Graph Neural Diffusion Networks) that\nexploits the local and global neighborhood information of a vertex in a single\nlayer. Exploiting the shallow network mitigates the over-smoothing problem\nwhile exploiting the local and global neighborhood information mitigates the\nunder-smoothing problem. The utilization of the local and global neighborhood\ninformation of a vertex is achieved by a new graph diffusion method called\nneural diffusions, which integrate neural networks into the conventional linear\nand nonlinear graph diffusions. The adoption of neural networks makes neural\ndiffusions adaptable to different datasets. Extensive experiments on various\nsparsely-labeled graphs verify the effectiveness and efficiency of GND-Nets\ncompared to state-of-the-art approaches.\n","authors":["Wei Ye","Zexi Huang","Yunqi Hong","Ambuj Singh"],"pdf_url":"https://arxiv.org/pdf/2201.09698v3.pdf","comment":"7 pages"},{"id":"http://arxiv.org/abs/2502.15455v1","updated":"2025-02-21T13:30:21Z","published":"2025-02-21T13:30:21Z","title":"R-LoRA: Random Initialization of Multi-Head LoRA for Multi-Task Learning","summary":"  Fine-tuning large language models (LLMs) is prohibitively expensive in terms\nof computational and memory costs. Low-rank Adaptation (LoRA), as one of the\nmost popular parameter-efficient fine-tuning (PEFT) methods, offers a\ncost-effective alternative by approximating the model changes $\\Delta W \\in\n\\mathbb{R}^{m \\times n}$ through the product of down-projection matrix $A \\in\n\\mathbb{R}^{m \\times r}$ and head matrix $B \\in \\mathbb{R}^{r \\times n}$, where\n$r \\ll \\min(m, n)$. In real-world scenarios, LLMs are fine-tuned on data from\nmultiple domains to perform tasks across various fields, embodying multi-task\nlearning (MTL). LoRA often underperforms in such complex scenarios. To enhance\nLoRA's capability in multi-task learning, we propose R-LoRA, which incorporates\nMulti-Head Randomization. Multi-Head Randomization diversifies the head\nmatrices through Multi-Head Random Initialization and Multi-Head Dropout,\nenabling more efficient learning of task-specific features while maintaining\nshared knowledge representation. Extensive experiments demonstrate that R-LoRA\nis better at capturing task-specific knowledge, thereby improving performance\nin multi-task scenarios. The code is available at\nhttps://github.com/jinda-liu/R-LoRA.\n","authors":["Jinda Liu","Yi Chang","Yuan Wu"],"pdf_url":"https://arxiv.org/pdf/2502.15455v1.pdf","comment":"9 pages, 10 figures"},{"id":"http://arxiv.org/abs/2502.15451v1","updated":"2025-02-21T13:25:00Z","published":"2025-02-21T13:25:00Z","title":"A fast convergence algorithm based on binary integer programming for\n  expert load balancing in MoE LLMs","summary":"  MoE (Mixture-of-Expert) architectures appear frequently in large language\nmodels, and the number of experts can be over one hundred recently. However,\nthe expert load imbalance problem always happens in MoE model pre-training,\nwhich will cause routing collapse or increased computational overhead. In order\nto balance loads on experts, we propose BIP-Based Balancing, an expert load\nbalancing algorithm based on binary integer programming (BIP). The algorithm\nmaintains an additional vector q that can help change the top-K order of s by\nsolving a binary integer programming with very small time costs. In simulation\nexperiments, we observe that BIP-Based Balancing make imbalance disappoint very\nfast, while the final sum of routine scores decreases very little. Our\nalgorithm achieves nearly perfect trade-off between expert load balance and\npre-training efficiency under the simulation view.\n","authors":["Yuan Sun"],"pdf_url":"https://arxiv.org/pdf/2502.15451v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.15437v1","updated":"2025-02-21T13:07:14Z","published":"2025-02-21T13:07:14Z","title":"Dimension-free bounds in high-dimensional linear regression via\n  error-in-operator approach","summary":"  We consider a problem of high-dimensional linear regression with random\ndesign. We suggest a novel approach referred to as error-in-operator which does\nnot estimate the design covariance $\\Sigma$ directly but incorporates it into\nempirical risk minimization. We provide an expansion of the excess prediction\nrisk and derive non-asymptotic dimension-free bounds on the leading term and\nthe remainder. This helps us to show that auxiliary variables do not increase\nthe effective dimension of the problem, provided that parameters of the\nprocedure are tuned properly. We also discuss computational aspects of our\nmethod and illustrate its performance with numerical experiments.\n","authors":["Fedor Noskov","Nikita Puchkin","Vladimir Spokoiny"],"pdf_url":"https://arxiv.org/pdf/2502.15437v1.pdf","comment":"100 pages"},{"id":"http://arxiv.org/abs/2502.15436v1","updated":"2025-02-21T13:05:19Z","published":"2025-02-21T13:05:19Z","title":"Fed-SB: A Silver Bullet for Extreme Communication Efficiency and\n  Performance in (Private) Federated LoRA Fine-Tuning","summary":"  Low-Rank Adaptation (LoRA) has become ubiquitous for efficiently fine-tuning\nfoundation models. However, federated fine-tuning using LoRA is challenging due\nto suboptimal updates arising from traditional federated averaging of\nindividual adapters. Existing solutions either incur prohibitively high\ncommunication cost that scales linearly with the number of clients or suffer\nfrom performance degradation due to limited expressivity. We introduce\nFederated Silver Bullet (Fed-SB), a novel approach for federated fine-tuning of\nLLMs using LoRA-SB, a recently proposed low-rank adaptation method. LoRA-SB\noptimally aligns the optimization trajectory with the ideal low-rank full\nfine-tuning projection by learning a small square matrix (R) between adapters B\nand A, keeping other components fixed. Direct averaging of R guarantees exact\nupdates, substantially reducing communication cost, which remains independent\nof the number of clients, and enables scalability. Fed-SB achieves\nstate-of-the-art performance across commonsense reasoning, arithmetic\nreasoning, and language inference tasks while reducing communication costs by\nup to 230x. In private settings, Fed-SB further improves performance by (1)\nreducing trainable parameters, thereby lowering the noise required for\ndifferential privacy and (2) avoiding noise amplification introduced by other\nmethods. Overall, Fed-SB establishes a new Pareto frontier in the tradeoff\nbetween communication and performance, offering an efficient and scalable\nsolution for both private and non-private federated fine-tuning. Our code is\npublicly available at https://github.com/CERT-Lab/fed-sb.\n","authors":["Raghav Singhal","Kaustubh Ponkshe","Rohit Vartak","Lav R. Varshney","Praneeth Vepakomma"],"pdf_url":"https://arxiv.org/pdf/2502.15436v1.pdf","comment":"Raghav Singhal and Kaustubh Ponkshe contributed equally to this work"},{"id":"http://arxiv.org/abs/2502.15435v1","updated":"2025-02-21T13:04:13Z","published":"2025-02-21T13:04:13Z","title":"Single-pass Detection of Jailbreaking Input in Large Language Models","summary":"  Defending aligned Large Language Models (LLMs) against jailbreaking attacks\nis a challenging problem, with existing approaches requiring multiple requests\nor even queries to auxiliary LLMs, making them computationally heavy. Instead,\nwe focus on detecting jailbreaking input in a single forward pass. Our method,\ncalled Single Pass Detection SPD, leverages the information carried by the\nlogits to predict whether the output sentence will be harmful. This allows us\nto defend in just one forward pass. SPD can not only detect attacks effectively\non open-source models, but also minimizes the misclassification of harmless\ninputs. Furthermore, we show that SPD remains effective even without complete\nlogit access in GPT-3.5 and GPT-4. We believe that our proposed method offers a\npromising approach to efficiently safeguard LLMs against adversarial attacks.\n","authors":["Leyla Naz Candogan","Yongtao Wu","Elias Abad Rocamora","Grigorios G. Chrysos","Volkan Cevher"],"pdf_url":"https://arxiv.org/pdf/2502.15435v1.pdf","comment":"Accepted in TMLR 2025"},{"id":"http://arxiv.org/abs/2502.15427v1","updated":"2025-02-21T12:54:25Z","published":"2025-02-21T12:54:25Z","title":"Adversarial Prompt Evaluation: Systematic Benchmarking of Guardrails\n  Against Prompt Input Attacks on LLMs","summary":"  As large language models (LLMs) become integrated into everyday applications,\nensuring their robustness and security is increasingly critical. In particular,\nLLMs can be manipulated into unsafe behaviour by prompts known as jailbreaks.\nThe variety of jailbreak styles is growing, necessitating the use of external\ndefences known as guardrails. While many jailbreak defences have been proposed,\nnot all defences are able to handle new out-of-distribution attacks due to the\nnarrow segment of jailbreaks used to align them. Moreover, the lack of\nsystematisation around defences has created significant gaps in their practical\napplication. In this work, we perform systematic benchmarking across 15\ndifferent defences, considering a broad swathe of malicious and benign\ndatasets. We find that there is significant performance variation depending on\nthe style of jailbreak a defence is subject to. Additionally, we show that\nbased on current datasets available for evaluation, simple baselines can\ndisplay competitive out-of-distribution performance compared to many\nstate-of-the-art defences. Code is available at\nhttps://github.com/IBM/Adversarial-Prompt-Evaluation.\n","authors":["Giulio Zizzo","Giandomenico Cornacchia","Kieran Fraser","Muhammad Zaid Hameed","Ambrish Rawat","Beat Buesser","Mark Purcell","Pin-Yu Chen","Prasanna Sattigeri","Kush Varshney"],"pdf_url":"https://arxiv.org/pdf/2502.15427v1.pdf","comment":"NeurIPS 2024, Safe Generative AI Workshop"},{"id":"http://arxiv.org/abs/2411.18220v3","updated":"2025-02-21T12:44:48Z","published":"2024-11-27T10:57:06Z","title":"R-MTLLMF: Resilient Multi-Task Large Language Model Fusion at the\n  Wireless Edge","summary":"  Multi-task large language models (MTLLMs) are important for many applications\nat the wireless edge, where users demand specialized models to handle multiple\ntasks efficiently. However, training MTLLMs is complex and exhaustive,\nparticularly when tasks are subject to change. Recently, the concept of model\nfusion via task vectors has emerged as an efficient approach for combining\nfine-tuning parameters to produce an MTLLM. In this paper, the problem of\nenabling edge users to collaboratively craft such MTLMs via tasks vectors is\nstudied, under the assumption of worst-case adversarial attacks. To this end,\nfirst the influence of adversarial noise to multi-task model fusion is\ninvestigated and a relationship between the so-called weight disentanglement\nerror and the mean squared error (MSE) is derived. Using hypothesis testing, it\nis directly shown that the MSE increases interference between task vectors,\nthereby rendering model fusion ineffective. Then, a novel resilient MTLLM\nfusion (R-MTLLMF) is proposed, which leverages insights about the LLM\narchitecture and fine-tuning process to safeguard task vector aggregation under\nadversarial noise by realigning the MTLLM. The proposed R-MTLLMF is then\ncompared for both worst-case and ideal transmission scenarios to study the\nimpact of the wireless channel. Extensive model fusion experiments with vision\nLLMs demonstrate R-MTLLMF's effectiveness, achieving close-to-baseline\nperformance across eight different tasks in ideal noise scenarios and\nsignificantly outperforming unprotected model fusion in worst-case scenarios.\nThe results further advocate for additional physical layer protection for a\nholistic approach to resilience, from both a wireless and LLM perspective.\n","authors":["Aladin Djuhera","Vlad C. Andrei","Mohsen Pourghasemian","Haris Gacanin","Holger Boche","Walid Saad"],"pdf_url":"https://arxiv.org/pdf/2411.18220v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.12835v2","updated":"2025-02-21T12:41:06Z","published":"2025-01-22T12:21:17Z","title":"Adaptive Retrieval Without Self-Knowledge? Bringing Uncertainty Back\n  Home","summary":"  Retrieval Augmented Generation (RAG) improves correctness of Question\nAnswering (QA) and addresses hallucinations in Large Language Models (LLMs),\nyet greatly increase computational costs. Besides, RAG is not always needed as\nmay introduce irrelevant information. Recent adaptive retrieval methods\nintegrate LLMs' intrinsic knowledge with external information appealing to LLM\nself-knowledge, but they often neglect efficiency evaluations and comparisons\nwith uncertainty estimation techniques. We bridge this gap by conducting a\ncomprehensive analysis of 35 adaptive retrieval methods, including 8 recent\napproaches and 27 uncertainty estimation techniques, across 6 datasets using 10\nmetrics for QA performance, self-knowledge, and efficiency. Our findings show\nthat uncertainty estimation techniques often outperform complex pipelines in\nterms of efficiency and self-knowledge, while maintaining comparable QA\nperformance.\n","authors":["Viktor Moskvoretskii","Maria Lysyuk","Mikhail Salnikov","Nikolay Ivanov","Sergey Pletenev","Daria Galimzianova","Nikita Krayko","Vasily Konovalov","Irina Nikishina","Alexander Panchenko"],"pdf_url":"https://arxiv.org/pdf/2501.12835v2.pdf","comment":"The code and data are at https://github.com/s-nlp/AdaRAGUE"},{"id":"http://arxiv.org/abs/2404.10255v3","updated":"2025-02-21T12:07:03Z","published":"2024-04-16T03:18:27Z","title":"Privacy-Enhanced Training-as-a-Service for On-Device Intelligence:\n  Concept, Architectural Scheme, and Open Problems","summary":"  On-device intelligence (ODI) enables artificial intelligence (AI)\napplications to run on end devices, providing real-time and customized AI\ninference without relying on remote servers. However, training models for\non-device deployment face significant challenges due to the decentralized and\nprivacy-sensitive nature of users' data, along with end-side constraints\nrelated to network connectivity, computation efficiency, etc. Existing training\nparadigms, such as cloud-based training, federated learning, and transfer\nlearning, fail to sufficiently address these practical constraints that are\nprevalent for devices. To overcome these challenges, we propose\nPrivacy-Enhanced Training-as-a-Service (PTaaS), a novel service computing\nparadigm that provides privacy-friendly, customized AI model training for end\ndevices. PTaaS outsources the core training process to remote and powerful\ncloud or edge servers, efficiently developing customized on-device models based\non uploaded anonymous queries, enhancing data privacy while reducing the\ncomputation load on individual devices. We explore the definition, goals, and\ndesign principles of PTaaS, alongside emerging technologies that support the\nPTaaS paradigm. An architectural scheme for PTaaS is also presented, followed\nby a series of open problems that set the stage for future research directions\nin the field of PTaaS.\n","authors":["Zhiyuan Wu","Sheng Sun","Yuwei Wang","Min Liu","Bo Gao","Tianliu He","Wen Wang"],"pdf_url":"https://arxiv.org/pdf/2404.10255v3.pdf","comment":"14 pages, 3 figures"},{"id":"http://arxiv.org/abs/2502.15403v1","updated":"2025-02-21T12:04:01Z","published":"2025-02-21T12:04:01Z","title":"Evaluate with the Inverse: Efficient Approximation of Latent Explanation\n  Quality Distribution","summary":"  Obtaining high-quality explanations of a model's output enables developers to\nidentify and correct biases, align the system's behavior with human values, and\nensure ethical compliance. Explainable Artificial Intelligence (XAI)\npractitioners rely on specific measures to gauge the quality of such\nexplanations. These measures assess key attributes, such as how closely an\nexplanation aligns with a model's decision process (faithfulness), how\naccurately it pinpoints the relevant input features (localization), and its\nconsistency across different cases (robustness). Despite providing valuable\ninformation, these measures do not fully address a critical practitioner's\nconcern: how does the quality of a given explanation compare to other potential\nexplanations? Traditionally, the quality of an explanation has been assessed by\ncomparing it to a randomly generated counterpart. This paper introduces an\nalternative: the Quality Gap Estimate (QGE). The QGE method offers a direct\ncomparison to what can be viewed as the `inverse' explanation, one that\nconceptually represents the antithesis of the original explanation. Our\nextensive testing across multiple model architectures, datasets, and\nestablished quality metrics demonstrates that the QGE method is superior to the\ntraditional approach. Furthermore, we show that QGE enhances the statistical\nreliability of these quality assessments. This advance represents a significant\nstep toward a more insightful evaluation of explanations that enables a more\neffective inspection of a model's behavior.\n","authors":["Carlos Eiras-Franco","Anna Hedström","Marina M. -C. Höhne"],"pdf_url":"https://arxiv.org/pdf/2502.15403v1.pdf","comment":"Accepted to AAAI 2025"},{"id":"http://arxiv.org/abs/2410.02320v3","updated":"2025-02-21T11:53:53Z","published":"2024-10-03T08:56:29Z","title":"Post-edits Are Preferences Too","summary":"  Preference Optimization (PO) techniques are currently one of the state of the\nart techniques for fine-tuning large language models (LLMs) on pairwise\npreference feedback from human annotators. However, in machine translation,\nthis sort of feedback can be difficult to solicit. Additionally, Kreutzer et\nal. (2018) have shown that, for machine translation, pairwise preferences are\nless reliable than other forms of human feedback, such as 5-point ratings.\n  We examine post-edits to see if they can be a source of reliable human\npreferences by construction. In PO, a human annotator is shown sequences $s_1$\nand $s_2$ and asked for a preference judgment, %$s_1 > s_2$; while for\npost-editing, editors create $s_1$ and know that it should be better than\n$s_2$. We attempt to use these implicit preferences for PO and show that it\nhelps the model move towards post-edit-like hypotheses and away from machine\ntranslation-like hypotheses. Furthermore, we show that best results are\nobtained by pre-training the model with supervised fine-tuning (SFT) on\npost-edits in order to promote post-edit-like hypotheses to the top output\nranks.\n","authors":["Nathaniel Berger","Miriam Exel","Matthias Huck","Stefan Riezler"],"pdf_url":"https://arxiv.org/pdf/2410.02320v3.pdf","comment":"To appear at the Ninth Conference on Machine Translation (WMT24)"},{"id":"http://arxiv.org/abs/2401.07085v3","updated":"2025-02-21T11:50:09Z","published":"2024-01-13T14:21:46Z","title":"Three Mechanisms of Feature Learning in a Linear Network","summary":"  Understanding the dynamics of neural networks in different width regimes is\ncrucial for improving their training and performance. We present an exact\nsolution for the learning dynamics of a one-hidden-layer linear network, with\none-dimensional data, across any finite width, uniquely exhibiting both kernel\nand feature learning phases. This study marks a technical advancement by\nenabling the analysis of the training trajectory from any initialization and a\ndetailed phase diagram under varying common hyperparameters such as width,\nlayer-wise learning rates, and scales of output and initialization. We identify\nthree novel prototype mechanisms specific to the feature learning regime: (1)\nlearning by alignment, (2) learning by disalignment, and (3) learning by\nrescaling, which contrast starkly with the dynamics observed in the kernel\nregime. Our theoretical findings are substantiated with empirical evidence\nshowing that these mechanisms also manifest in deep nonlinear networks handling\nreal-world tasks, enhancing our understanding of neural network training\ndynamics and guiding the design of more effective learning strategies.\n","authors":["Yizhou Xu","Liu Ziyin"],"pdf_url":"https://arxiv.org/pdf/2401.07085v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.18626v2","updated":"2025-02-21T11:46:46Z","published":"2024-10-24T10:35:02Z","title":"SAMG: Offline-to-Online Reinforcement Learning via\n  State-Action-Conditional Offline Model Guidance","summary":"  Offline-to-online (O2O) reinforcement learning (RL) pre-trains models on\noffline data and refines policies through online fine-tuning. However, existing\nO2O RL algorithms typically require maintaining the tedious offline datasets to\nmitigate the effects of out-of-distribution (OOD) data, which significantly\nlimits their efficiency in exploiting online samples. To address this\ndeficiency, we introduce a new paradigm for O2O RL called\nState-Action-Conditional Offline \\Model Guidance (SAMG). It freezes the\npre-trained offline critic to provide compact offline understanding for each\nstate-action sample, thus eliminating the need for retraining on offline data.\nThe frozen offline critic is incorporated with the online target critic\nweighted by a state-action-adaptive coefficient. This coefficient aims to\ncapture the offline degree of samples at the state-action level, and is updated\nadaptively during training. In practice, SAMG could be easily integrated with\nQ-function-based algorithms. Theoretical analysis shows good optimality and\nlower estimation error. Empirically, SAMG outperforms state-of-the-art O2O RL\nalgorithms on the D4RL benchmark.\n","authors":["Liyu Zhang","Haochi Wu","Xu Wan","Quan Kong","Ruilong Deng","Mingyang Sun"],"pdf_url":"https://arxiv.org/pdf/2410.18626v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.18356v2","updated":"2025-02-21T11:46:19Z","published":"2023-11-30T08:54:32Z","title":"Towards Comparable Active Learning","summary":"  Active Learning has received significant attention in the field of machine\nlearning for its potential in selecting the most informative samples for\nlabeling, thereby reducing data annotation costs. However, we show that the\nreported lifts in recent literature generalize poorly to other domains leading\nto an inconclusive landscape in Active Learning research. Furthermore, we\nhighlight overlooked problems for reproducing AL experiments that can lead to\nunfair comparisons and increased variance in the results. This paper addresses\nthese issues by providing an Active Learning framework for a fair comparison of\nalgorithms across different tasks and domains, as well as a fast and performant\noracle algorithm for evaluation. To the best of our knowledge, we propose the\nfirst AL benchmark that tests algorithms in 3 major domains: Tabular, Image,\nand Text. We report empirical results for 6 widely used algorithms on 7\nreal-world and 2 synthetic datasets and aggregate them into a domain-specific\nranking of AL algorithms.\n","authors":["Thorben Werner","Johannes Burchert","Lars Schmidt-Thieme"],"pdf_url":"https://arxiv.org/pdf/2311.18356v2.pdf","comment":"Deprecated version of the paper. Please use \"A Cross-Domain Benchmark\n  for Active Learning\" (arXiv:2408.00426) instead."},{"id":"http://arxiv.org/abs/2405.15773v2","updated":"2025-02-21T11:38:52Z","published":"2024-03-16T07:34:33Z","title":"Feature Aggregation with Latent Generative Replay for Federated\n  Continual Learning of Socially Appropriate Robot Behaviours","summary":"  It is critical for robots to explore Federated Learning (FL) settings where\nseveral robots, deployed in parallel, can learn independently while also\nsharing their learning with each other. This collaborative learning in\nreal-world environments requires social robots to adapt dynamically to changing\nand unpredictable situations and varying task settings. Our work contributes to\naddressing these challenges by exploring a simulated living room environment\nwhere robots need to learn the social appropriateness of their actions. First,\nwe propose Federated Root (FedRoot) averaging, a novel weight aggregation\nstrategy which disentangles feature learning across clients from individual\ntask-based learning. Second, to adapt to challenging environments, we extend\nFedRoot to Federated Latent Generative Replay (FedLGR), a novel Federated\nContinual Learning (FCL) strategy that uses FedRoot-based weight aggregation\nand embeds each client with a generator model for pseudo-rehearsal of learnt\nfeature embeddings to mitigate forgetting in a resource-efficient manner. Our\nresults show that FedRoot-based methods offer competitive performance while\nalso resulting in a sizeable reduction in resource consumption (up to 86% for\nCPU usage and up to 72% for GPU usage). Additionally, our results demonstrate\nthat FedRoot-based FCL methods outperform other methods while also offering an\nefficient solution (up to 84% CPU and 92% GPU usage reduction), with FedLGR\nproviding the best results across evaluations.\n","authors":["Nikhil Churamani","Saksham Checker","Fethiye Irmak Dogan","Hao-Tien Lewis Chiang","Hatice Gunes"],"pdf_url":"https://arxiv.org/pdf/2405.15773v2.pdf","comment":"8 pages, 4 figures, IEEE RA-L submission"},{"id":"http://arxiv.org/abs/2502.15376v1","updated":"2025-02-21T11:00:34Z","published":"2025-02-21T11:00:34Z","title":"Learning Chern Numbers of Topological Insulators with Gauge Equivariant\n  Neural Networks","summary":"  Equivariant network architectures are a well-established tool for predicting\ninvariant or equivariant quantities. However, almost all learning problems\nconsidered in this context feature a global symmetry, i.e. each point of the\nunderlying space is transformed with the same group element, as opposed to a\nlocal ``gauge'' symmetry, where each point is transformed with a different\ngroup element, exponentially enlarging the size of the symmetry group. Gauge\nequivariant networks have so far mainly been applied to problems in quantum\nchromodynamics. Here, we introduce a novel application domain for\ngauge-equivariant networks in the theory of topological condensed matter\nphysics. We use gauge equivariant networks to predict topological invariants\n(Chern numbers) of multiband topological insulators. The gauge symmetry of the\nnetwork guarantees that the predicted quantity is a topological invariant. We\nintroduce a novel gauge equivariant normalization layer to stabilize the\ntraining and prove a universal approximation theorem for our setup. We train on\nsamples with trivial Chern number only but show that our models generalize to\nsamples with non-trivial Chern number. We provide various ablations of our\nsetup. Our code is available at https://github.com/sitronsea/GENet/tree/main.\n","authors":["Longde Huang","Oleksandr Balabanov","Hampus Linander","Mats Granath","Daniel Persson","Jan E. Gerken"],"pdf_url":"https://arxiv.org/pdf/2502.15376v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.15374v1","updated":"2025-02-21T10:55:50Z","published":"2025-02-21T10:55:50Z","title":"Fréchet Cumulative Covariance Net for Deep Nonlinear Sufficient\n  Dimension Reduction with Random Objects","summary":"  Nonlinear sufficient dimension reduction\\citep{libing_generalSDR}, which\nconstructs nonlinear low-dimensional representations to summarize essential\nfeatures of high-dimensional data, is an important branch of representation\nlearning. However, most existing methods are not applicable when the response\nvariables are complex non-Euclidean random objects, which are frequently\nencountered in many recent statistical applications. In this paper, we\nintroduce a new statistical dependence measure termed Fr\\'echet Cumulative\nCovariance (FCCov) and develop a novel nonlinear SDR framework based on FCCov.\nOur approach is not only applicable to complex non-Euclidean data, but also\nexhibits robustness against outliers. We further incorporate Feedforward Neural\nNetworks (FNNs) and Convolutional Neural Networks (CNNs) to estimate nonlinear\nsufficient directions in the sample level. Theoretically, we prove that our\nmethod with squared Frobenius norm regularization achieves unbiasedness at the\n$\\sigma$-field level. Furthermore, we establish non-asymptotic convergence\nrates for our estimators based on FNNs and ResNet-type CNNs, which match the\nminimax rate of nonparametric regression up to logarithmic factors. Intensive\nsimulation studies verify the performance of our methods in both Euclidean and\nnon-Euclidean settings. We apply our method to facial expression recognition\ndatasets and the results underscore more realistic and broader applicability of\nour proposal.\n","authors":["Hang Yuan","Christina Dan Wang","Zhou Yu"],"pdf_url":"https://arxiv.org/pdf/2502.15374v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.11102v2","updated":"2025-02-21T10:54:36Z","published":"2025-02-16T12:38:37Z","title":"OptMATH: A Scalable Bidirectional Data Synthesis Framework for\n  Optimization Modeling","summary":"  Despite the rapid development of large language models (LLMs), a fundamental\nchallenge persists: the lack of high-quality optimization modeling datasets\nhampers LLMs' robust modeling of practical optimization problems from natural\nlanguage descriptions (NL). This data scarcity also contributes to the\ngeneralization difficulties experienced by learning-based methods. To address\nthese challenges, we propose a scalable framework for synthesizing a\nhigh-quality dataset, named OptMATH. Starting from curated seed data with\nmathematical formulations (MF), this framework automatically generates problem\ndata (PD) with controllable complexity. Then, a back-translation step is\nemployed to obtain NL. To verify the correspondence between the NL and the PD,\na forward modeling step followed by rejection sampling is used. The accepted\npairs constitute the training part of OptMATH. Then a collection of rejected\npairs is identified and further filtered. This collection serves as a new\nbenchmark for optimization modeling, containing difficult instances whose\nlengths are much longer than these of NL4OPT and MAMO. Through extensive\nexperiments, we demonstrate that models of various sizes (0.5B-32B parameters)\ntrained on OptMATH achieve superior results on multiple modeling benchmarks,\nthereby validating the effectiveness and scalability of our approach. Our\ndataset is publicly available at https://github.com/AuroraLHL/OptMATH.\n","authors":["Hongliang Lu","Zhonglin Xie","Yaoyu Wu","Can Ren","Yuxuan Chen","Zaiwen Wen"],"pdf_url":"https://arxiv.org/pdf/2502.11102v2.pdf","comment":"This paper has 36 pages, 18 figures, and two co-first authors:\n  Hongliang Lu and Zhonglin Xie"},{"id":"http://arxiv.org/abs/2502.15372v1","updated":"2025-02-21T10:47:46Z","published":"2025-02-21T10:47:46Z","title":"Efficient and Provable Algorithms for Covariate Shift","summary":"  Covariate shift, a widely used assumption in tackling {\\it distributional\nshift} (when training and test distributions differ), focuses on scenarios\nwhere the distribution of the labels conditioned on the feature vector is the\nsame, but the distribution of features in the training and test data are\ndifferent. Despite the significance and extensive work on covariate shift,\ntheoretical guarantees for algorithms in this domain remain sparse. In this\npaper, we distill the essence of the covariate shift problem and focus on\nestimating the average $\\mathbb{E}_{\\tilde{\\mathbf{x}}\\sim\np_{\\mathrm{test}}}\\mathbf{f}(\\tilde{\\mathbf{x}})$, of any unknown and bounded\nfunction $\\mathbf{f}$, given labeled training samples $(\\mathbf{x}_i,\n\\mathbf{f}(\\mathbf{x}_i))$, and unlabeled test samples $\\tilde{\\mathbf{x}}_i$;\nthis is a core subroutine for several widely studied learning problems. We give\nseveral efficient algorithms, with provable sample complexity and computational\nguarantees. Moreover, we provide the first rigorous analysis of algorithms in\nthis space when $\\mathbf{f}$ is unrestricted, laying the groundwork for\ndeveloping a solid theoretical foundation for covariate shift problems.\n","authors":["Deeksha Adil","Jarosław Błasiok"],"pdf_url":"https://arxiv.org/pdf/2502.15372v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.05884v2","updated":"2025-02-21T10:45:05Z","published":"2024-08-26T12:32:40Z","title":"Integrating the Expected Future in Load Forecasts with Contextually\n  Enhanced Transformer Models","summary":"  Accurate and reliable energy forecasting is essential for power grid\noperators who strive to minimize extreme forecasting errors that pose\nsignificant operational challenges and incur high intra-day trading costs.\nIncorporating planning information -- such as anticipated user behavior,\nscheduled events or timetables -- provides substantial contextual information\nto enhance forecast accuracy and reduce the occurrence of large forecasting\nerrors. Existing approaches, however, lack the flexibility to effectively\nintegrate both dynamic, forward-looking contextual inputs and historical data.\nIn this work, we conceptualize forecasting as a combined forecasting-regression\ntask, formulated as a sequence-to-sequence prediction problem, and introduce\ncontextually-enhanced transformer models designed to leverage all contextual\ninformation effectively. We demonstrate the effectiveness of our approach\nthrough a primary case study on nationwide railway energy consumption\nforecasting, where integrating contextual information into transformer models,\nparticularly timetable data, resulted in a significant average mean absolute\nerror reduction of 26.6%. An auxiliary case study on building energy\nforecasting, leveraging planned office occupancy data, further illustrates the\ngeneralizability of our method, showing an average reduction of 56.3% in mean\nabsolute error. Compared to other state-of-the-art methods, our approach\nconsistently outperforms existing models, underscoring the value of\ncontext-aware deep learning techniques in energy forecasting applications.\n","authors":["Raffael Theiler","Leandro Von Krannichfeldt","Giovanni Sansavini","Michael F. Howland","Olga Fink"],"pdf_url":"https://arxiv.org/pdf/2409.05884v2.pdf","comment":"39 pages, 8 figures and tables, journal paper"},{"id":"http://arxiv.org/abs/2502.15349v1","updated":"2025-02-21T10:06:41Z","published":"2025-02-21T10:06:41Z","title":"AttentionEngine: A Versatile Framework for Efficient Attention\n  Mechanisms on Diverse Hardware Platforms","summary":"  Transformers and large language models (LLMs) have revolutionized machine\nlearning, with attention mechanisms at the core of their success. As the\nlandscape of attention variants expands, so too do the challenges of optimizing\ntheir performance, particularly across different hardware platforms. Current\noptimization strategies are often narrowly focused, requiring extensive manual\nintervention to accommodate changes in model configurations or hardware\nenvironments. In this paper, we introduce AttentionEngine, a comprehensive\nframework designed to streamline the optimization of attention mechanisms\nacross heterogeneous hardware backends. By decomposing attention computation\ninto modular operations with customizable components, AttentionEngine enables\nflexible adaptation to diverse algorithmic requirements. The framework further\nautomates kernel optimization through a combination of programmable templates\nand a robust cross-platform scheduling strategy. Empirical results reveal\nperformance gains of up to 10x on configurations beyond the reach of existing\nmethods. AttentionEngine offers a scalable, efficient foundation for developing\nand deploying attention mechanisms with minimal manual tuning. Our code has\nbeen open-sourced and is available at\nhttps://github.com/microsoft/AttentionEngine.\n","authors":["Feiyang Chen","Yu Cheng","Lei Wang","Yuqing Xia","Ziming Miao","Lingxiao Ma","Fan Yang","Jilong Xue","Zhi Yang","Mao Yang","Haibo Chen"],"pdf_url":"https://arxiv.org/pdf/2502.15349v1.pdf","comment":"15 pages"},{"id":"http://arxiv.org/abs/2501.18936v3","updated":"2025-02-21T10:05:20Z","published":"2025-01-31T07:41:06Z","title":"Adaptive Prompt: Unlocking the Power of Visual Prompt Tuning","summary":"  Visual Prompt Tuning (VPT) has recently emerged as a powerful method for\nadapting pre-trained vision models to downstream tasks. By introducing\nlearnable prompt tokens as task-specific instructions, VPT effectively guides\npre-trained transformer models with minimal overhead. Despite its empirical\nsuccess, a comprehensive theoretical understanding of VPT remains an active\narea of research. Building on recent insights into the connection between\nmixture of experts and prompt-based approaches, we identify a key limitation in\nVPT: the restricted functional expressiveness in prompt formulation. To address\nthis limitation, we propose Visual Adaptive Prompt Tuning (VAPT), a new\ngeneration of prompts that redefines prompts as adaptive functions of the\ninput. Our theoretical analysis shows that this simple yet intuitive approach\nachieves optimal sample efficiency. Empirical results on VTAB-1K and FGVC\nfurther demonstrate VAPT's effectiveness, with performance gains of 7.34% and\n1.04% over fully fine-tuning baselines, respectively. Notably, VAPT also\nsurpasses VPT by a substantial margin while using fewer parameters. These\nresults highlight both the effectiveness and efficiency of our method and pave\nthe way for future research to explore the potential of adaptive prompts. Our\ncode is publicly available at https://github.com/Minhchuyentoancbn/VAPT\n","authors":["Minh Le","Anh Nguyen","Huy Nguyen","Chau Nguyen","Nhat Ho"],"pdf_url":"https://arxiv.org/pdf/2501.18936v3.pdf","comment":"57 pages, 10 figures, 18 tables"},{"id":"http://arxiv.org/abs/2502.15346v1","updated":"2025-02-21T10:00:43Z","published":"2025-02-21T10:00:43Z","title":"Drug-Target Interaction/Affinity Prediction: Deep Learning Models and\n  Advances Review","summary":"  Drug discovery remains a slow and expensive process that involves many steps,\nfrom detecting the target structure to obtaining approval from the Food and\nDrug Administration (FDA), and is often riddled with safety concerns. Accurate\nprediction of how drugs interact with their targets and the development of new\ndrugs by using better methods and technologies have immense potential to speed\nup this process, ultimately leading to faster delivery of life-saving\nmedications. Traditional methods used for drug-target interaction prediction\nshow limitations, particularly in capturing complex relationships between drugs\nand their targets. As an outcome, deep learning models have been presented to\novercome the challenges of interaction prediction through their precise and\nefficient end results. By outlining promising research avenues and models, each\nwith a different solution but similar to the problem, this paper aims to give\nresearchers a better idea of methods for even more accurate and efficient\nprediction of drug-target interaction, ultimately accelerating the development\nof more effective drugs. A total of 180 prediction methods for drug-target\ninteractions were analyzed throughout the period spanning 2016 to 2025 using\ndifferent frameworks based on machine learning, mainly deep learning and graph\nneural networks. Additionally, this paper discusses the novelty, architecture,\nand input representation of these models.\n","authors":["Ali Vefghi","Zahed Rahmati","Mohammad Akbari"],"pdf_url":"https://arxiv.org/pdf/2502.15346v1.pdf","comment":"64 pages, 7 figures, 10 tables"},{"id":"http://arxiv.org/abs/2502.15345v1","updated":"2025-02-21T09:59:46Z","published":"2025-02-21T09:59:46Z","title":"Efficiently Solving Discounted MDPs with Predictions on Transition\n  Matrices","summary":"  We study infinite-horizon Discounted Markov Decision Processes (DMDPs) under\na generative model. Motivated by the Algorithm with Advice framework\nMitzenmacher and Vassilvitskii 2022, we propose a novel framework to\ninvestigate how a prediction on the transition matrix can enhance the sample\nefficiency in solving DMDPs and improve sample complexity bounds. We focus on\nthe DMDPs with $N$ state-action pairs and discounted factor $\\gamma$. Firstly,\nwe provide an impossibility result that, without prior knowledge of the\nprediction accuracy, no sampling policy can compute an $\\epsilon$-optimal\npolicy with a sample complexity bound better than $\\tilde{O}((1-\\gamma)^{-3}\nN\\epsilon^{-2})$, which matches the state-of-the-art minimax sample complexity\nbound with no prediction. In complement, we propose an algorithm based on\nminimax optimization techniques that leverages the prediction on the transition\nmatrix. Our algorithm achieves a sample complexity bound depending on the\nprediction error, and the bound is uniformly better than\n$\\tilde{O}((1-\\gamma)^{-4} N \\epsilon^{-2})$, the previous best result derived\nfrom convex optimization methods. These theoretical findings are further\nsupported by our numerical experiments.\n","authors":["Lixing Lyu","Jiashuo Jiang","Wang Chi Cheung"],"pdf_url":"https://arxiv.org/pdf/2502.15345v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2206.05860v3","updated":"2025-02-21T09:48:01Z","published":"2022-06-13T00:02:23Z","title":"IGN : Implicit Generative Networks","summary":"  In this work, we build recent advances in distributional reinforcement\nlearning to give a state-of-art distributional variant of the model based on\nthe IQN. We achieve this by using the GAN model's generator and discriminator\nfunction with the quantile regression to approximate the full quantile value\nfor the state-action return distribution. We demonstrate improved performance\non our baseline dataset - 57 Atari 2600 games in the ALE. Also, we use our\nalgorithm to show the state-of-art training performance of risk-sensitive\npolicies in Atari games with the policy optimization and evaluation.\n","authors":["Haozheng Luo","Tianyi Wu","Colin Feiyu Han","Zhijun Yan"],"pdf_url":"https://arxiv.org/pdf/2206.05860v3.pdf","comment":"2022 21st IEEE International Conference on Machine Learning and\n  Applications (ICMLA)"},{"id":"http://arxiv.org/abs/2502.15338v1","updated":"2025-02-21T09:42:09Z","published":"2025-02-21T09:42:09Z","title":"Learning with Limited Shared Information in Multi-agent Multi-armed\n  Bandit","summary":"  Multi-agent multi-armed bandit (MAMAB) is a classic collaborative learning\nmodel and has gained much attention in recent years. However, existing studies\ndo not consider the case where an agent may refuse to share all her information\nwith others, e.g., when some of the data contains personal privacy. In this\npaper, we propose a novel limited shared information multi-agent multi-armed\nbandit (LSI-MAMAB) model in which each agent only shares the information that\nshe is willing to share, and propose the Balanced-ETC algorithm to help\nmultiple agents collaborate efficiently with limited shared information. Our\nanalysis shows that Balanced-ETC is asymptotically optimal and its average\nregret (on each agent) approaches a constant when there are sufficient agents\ninvolved. Moreover, to encourage agents to participate in this collaborative\nlearning, an incentive mechanism is proposed to make sure each agent can\nbenefit from the collaboration system. Finally, we present experimental results\nto validate our theoretical results.\n","authors":["Junning Shao","Siwei Wang","Zhixuan Fang"],"pdf_url":"https://arxiv.org/pdf/2502.15338v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.15334v1","updated":"2025-02-21T09:38:00Z","published":"2025-02-21T09:38:00Z","title":"Attention Eclipse: Manipulating Attention to Bypass LLM Safety-Alignment","summary":"  Recent research has shown that carefully crafted jailbreak inputs can induce\nlarge language models to produce harmful outputs, despite safety measures such\nas alignment. It is important to anticipate the range of potential Jailbreak\nattacks to guide effective defenses and accurate assessment of model safety. In\nthis paper, we present a new approach for generating highly effective Jailbreak\nattacks that manipulate the attention of the model to selectively strengthen or\nweaken attention among different parts of the prompt. By harnessing attention\nloss, we develop more effective jailbreak attacks, that are also transferrable.\nThe attacks amplify the success rate of existing Jailbreak algorithms including\nGCG, AutoDAN, and ReNeLLM, while lowering their generation cost (for example,\nthe amplified GCG attack achieves 91.2% ASR, vs. 67.9% for the original attack\non Llama2-7B/AdvBench, using less than a third of the generation time).\n","authors":["Pedram Zaree","Md Abdullah Al Mamun","Quazi Mishkatul Alam","Yue Dong","Ihsen Alouani","Nael Abu-Ghazaleh"],"pdf_url":"https://arxiv.org/pdf/2502.15334v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.15331v1","updated":"2025-02-21T09:34:31Z","published":"2025-02-21T09:34:31Z","title":"Lightweight yet Efficient: An External Attentive Graph Convolutional\n  Network with Positional Prompts for Sequential Recommendation","summary":"  Graph-based Sequential Recommender systems (GSRs) have gained significant\nresearch attention due to their ability to simultaneously handle user-item\ninteractions and sequential relationships between items. Current GSRs often\nutilize composite or in-depth structures for graph encoding (e.g., the Graph\nTransformer). Nevertheless, they have high computational complexity, hindering\nthe deployment on resource-constrained edge devices. Moreover, the relative\nposition encoding in Graph Transformer has difficulty in considering the\ncomplicated positional dependencies within sequence. To this end, we propose an\nExternal Attentive Graph convolutional network with Positional prompts for\nSequential recommendation, namely EA-GPS. Specifically, we first introduce an\nexternal attentive graph convolutional network that linearly measures the\nglobal associations among nodes via two external memory units. Then, we present\na positional prompt-based decoder that explicitly treats the absolute item\npositions as external prompts. By introducing length-adaptive sequential\nmasking and a soft attention network, such a decoder facilitates the model to\ncapture the long-term positional dependencies and contextual relationships\nwithin sequences. Extensive experimental results on five real-world datasets\ndemonstrate that the proposed EA-GPS outperforms the state-of-the-art methods.\nRemarkably, it achieves the superior performance while maintaining a smaller\nparameter size and lower training overhead. The implementation of this work is\npublicly available at https://github.com/ZZY-GraphMiningLab/EA-GPS.\n","authors":["Jinyu Zhang","Chao Li","Zhongying Zhao"],"pdf_url":"https://arxiv.org/pdf/2502.15331v1.pdf","comment":"26 pages, 8 figures, journal paper, accepted by TOIS at 20th\n  February, 2025"},{"id":"http://arxiv.org/abs/2105.11866v5","updated":"2025-02-21T09:17:00Z","published":"2021-05-25T12:10:54Z","title":"GraphFM: Graph Factorization Machines for Feature Interaction Modeling","summary":"  Factorization machine (FM) is a prevalent approach to modeling pairwise\n(second-order) feature interactions when dealing with high-dimensional sparse\ndata. However, on the one hand, FM fails to capture higher-order feature\ninteractions suffering from combinatorial expansion. On the other hand, taking\ninto account interactions between every pair of features may introduce noise\nand degrade prediction accuracy. To solve the problems, we propose a novel\napproach, Graph Factorization Machine (GraphFM), by naturally representing\nfeatures in the graph structure. In particular, we design a mechanism to select\nthe beneficial feature interactions and formulate them as edges between\nfeatures. Then the proposed model, which integrates the interaction function of\nFM into the feature aggregation strategy of Graph Neural Network (GNN), can\nmodel arbitrary-order feature interactions on the graph-structured features by\nstacking layers. Experimental results on several real-world datasets have\ndemonstrated the rationality and effectiveness of our proposed approach. The\ncode and data are available at\nhttps://github.com/CRIPAC-DIG/GraphCTR}{https://github.com/CRIPAC-DIG/GraphCTR\n","authors":["Shu Wu","Zekun Li","Yunyue Su","Zeyu Cui","Xiaoyu Zhang","Liang Wang"],"pdf_url":"https://arxiv.org/pdf/2105.11866v5.pdf","comment":"The code and data are available at\n  https://github.com/CRIPAC-DIG/GraphCTR"},{"id":"http://arxiv.org/abs/2502.15317v1","updated":"2025-02-21T09:11:07Z","published":"2025-02-21T09:11:07Z","title":"Utilizing Sequential Information of General Lab-test Results and\n  Diagnoses History for Differential Diagnosis of Dementia","summary":"  Early diagnosis of Alzheimer's Disease (AD) faces multiple data-related\nchallenges, including high variability in patient data, limited access to\nspecialized diagnostic tests, and overreliance on single-type indicators. These\nchallenges are exacerbated by the progressive nature of AD, where subtle\npathophysiological changes often precede clinical symptoms by decades. To\naddress these limitations, this study proposes a novel approach that takes\nadvantage of routinely collected general laboratory test histories for the\nearly detection and differential diagnosis of AD. By modeling lab test\nsequences as \"sentences\", we apply word embedding techniques to capture latent\nrelationships between tests and employ deep time series models, including\nlong-short-term memory (LSTM) and Transformer networks, to model temporal\npatterns in patient records. Experimental results demonstrate that our approach\nimproves diagnostic accuracy and enables scalable and costeffective AD\nscreening in diverse clinical settings.\n","authors":["Yizong Xing","Dhita Putri Pratama","Yuke Wang","Yufan Zhang","Brian E. Chapman"],"pdf_url":"https://arxiv.org/pdf/2502.15317v1.pdf","comment":"7 pages, 6 figures. This work has been submitted to the IEEE for\n  possible publication"},{"id":"http://arxiv.org/abs/2502.15315v1","updated":"2025-02-21T09:10:54Z","published":"2025-02-21T09:10:54Z","title":"Tight Clusters Make Specialized Experts","summary":"  Sparse Mixture-of-Experts (MoE) architectures have emerged as a promising\napproach to decoupling model capacity from computational cost. At the core of\nthe MoE model is the router, which learns the underlying clustering structure\nof the input distribution in order to send input tokens to appropriate experts.\nHowever, latent clusters may be unidentifiable in high dimension, which causes\nslow convergence, susceptibility to data contamination, and overall degraded\nrepresentations as the router is unable to perform appropriate token-expert\nmatching. We examine the router through the lens of clustering optimization and\nderive optimal feature weights that maximally identify the latent clusters. We\nuse these weights to compute the token-expert routing assignments in an\nadaptively transformed space that promotes well-separated clusters, which helps\nidentify the best-matched expert for each token. In particular, for each expert\ncluster, we compute a set of weights that scales features according to whether\nthat expert clusters tightly along that feature. We term this novel router the\nAdaptive Clustering (AC) router. Our AC router enables the MoE model to obtain\nthree connected benefits: 1) faster convergence, 2) better robustness to data\ncorruption, and 3) overall performance improvement, as experts are specialized\nin semantically distinct regions of the input space. We empirically demonstrate\nthe advantages of our AC router over baseline routing methods when applied on a\nvariety of MoE backbones for language modeling and image recognition tasks in\nboth clean and corrupted settings.\n","authors":["Stefan K. Nielsen","Rachel S. Y. Teo","Laziz U. Abdullaev","Tan M. Nguyen"],"pdf_url":"https://arxiv.org/pdf/2502.15315v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.14487v2","updated":"2025-02-21T09:05:35Z","published":"2025-02-20T12:09:30Z","title":"Temporal Misalignment in ANN-SNN Conversion and Its Mitigation via\n  Probabilistic Spiking Neurons","summary":"  Spiking Neural Networks (SNNs) offer a more energy-efficient alternative to\nArtificial Neural Networks (ANNs) by mimicking biological neural principles,\nestablishing them as a promising approach to mitigate the increasing energy\ndemands of large-scale neural models. However, fully harnessing the\ncapabilities of SNNs remains challenging due to their discrete signal\nprocessing and temporal dynamics. ANN-SNN conversion has emerged as a practical\napproach, enabling SNNs to achieve competitive performance on complex machine\nlearning tasks. In this work, we identify a phenomenon in the ANN-SNN\nconversion framework, termed temporal misalignment, in which random spike\nrearrangement across SNN layers leads to performance improvements. Based on\nthis observation, we introduce biologically plausible two-phase probabilistic\n(TPP) spiking neurons, further enhancing the conversion process. We demonstrate\nthe advantages of our proposed method both theoretically and empirically\nthrough comprehensive experiments on CIFAR-10/100, CIFAR10-DVS, and ImageNet\nacross a variety of architectures, achieving state-of-the-art results.\n","authors":["Velibor Bojković","Xiaofeng Wu","Bin Gu"],"pdf_url":"https://arxiv.org/pdf/2502.14487v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.15306v1","updated":"2025-02-21T09:02:22Z","published":"2025-02-21T09:02:22Z","title":"A Data-Driven Real-Time Optimal Power Flow Algorithm Using Local\n  Feedback","summary":"  The increasing penetration of distributed energy resources (DERs) adds\nvariability as well as fast control capabilities to power networks. Dispatching\nthe DERs based on local information to provide real-time optimal network\noperation is the desideratum. In this paper, we propose a data-driven real-time\nalgorithm that uses only the local measurements to solve time-varying AC\noptimal power flow (OPF). Specifically, we design a learnable function that\ntakes the local feedback as input in the algorithm. The learnable function,\nunder certain conditions, will result in a unique stationary point of the\nalgorithm, which in turn transfers the OPF problems to be optimized over the\nparameters of the function. We then develop a stochastic primal-dual update to\nsolve the variant of the OPF problems based on a deep neural network (DNN)\nparametrization of the learnable function, which is referred to as the training\nstage. We also design a gradient-free alternative to bypass the cumbersome\ngradient calculation of the nonlinear power flow model. The OPF\nsolution-tracking error bound is established in the sense of universal\napproximation of DNN. Numerical results on the IEEE 37-bus test feeder show\nthat the proposed method can track the time-varying OPF solutions with higher\naccuracy and faster computation compared to benchmark methods.\n","authors":["Heng Liang","Yujin Huang","Changhong Zhao"],"pdf_url":"https://arxiv.org/pdf/2502.15306v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.09800v3","updated":"2025-02-21T08:59:15Z","published":"2024-12-13T02:39:04Z","title":"Infinite-dimensional next-generation reservoir computing","summary":"  Next-generation reservoir computing (NG-RC) has attracted much attention due\nto its excellent performance in spatio-temporal forecasting of complex systems\nand its ease of implementation. This paper shows that NG-RC can be encoded as a\nkernel ridge regression that makes training efficient and feasible even when\nthe space of chosen polynomial features is very large. Additionally, an\nextension to an infinite number of covariates is possible, which makes the\nmethodology agnostic with respect to the lags into the past that are considered\nas explanatory factors, as well as with respect to the number of polynomial\ncovariates, an important hyperparameter in traditional NG-RC. We show that this\napproach has solid theoretical backing and good behavior based on kernel\nuniversality properties previously established in the literature. Various\nnumerical illustrations show that these generalizations of NG-RC outperform the\ntraditional approach in several forecasting applications.\n","authors":["Lyudmila Grigoryeva","Hannah Lim Jing Ting","Juan-Pablo Ortega"],"pdf_url":"https://arxiv.org/pdf/2412.09800v3.pdf","comment":"14 pages, 2 figures, 3 tables; corrected typos, added GitHub link,\n  added acknowledgments; editorial changes to improve clarity, corrected typos,\n  replaced GitHub link with repository citation"},{"id":"http://arxiv.org/abs/2502.15304v1","updated":"2025-02-21T08:55:21Z","published":"2025-02-21T08:55:21Z","title":"SVDq: 1.25-bit and 410x Key Cache Compression for LLM Attention","summary":"  For the efficient inference of Large Language Models (LLMs), the effective\ncompression of key-value (KV) cache is essential. Three main types of KV cache\ncompression techniques, namely sparsity, channel compression, and quantization,\nhave been identified. This study presents SVDq, a Singular Value Decomposition\n(SVD) - based mixed precision quantization method for K cache. Initially, K\ncache is transformed into latent channels using SVD basis representations.\nSince the values in latent channels decay rapidly and become negligible after\nonly a few latent channels, our method then incorporates importance-aware\nquantization and compression for latent channels. This enables the effective\nallocation of higher precision to more significant channels. Theoretically, we\nprove that SVDq results in quantization errors (x0.1 or even lower) that are\nmuch lower than those of per-channel key quantization in the original space.\nOur findings based on RULER and LongBench benchmarks demonstrate that SVDq can\nachieve an equivalent key cache precision as low as 1.25-bit. When combined\nwith key sparsity, it can reach a key compression ratio of up to 410x for\nattention computation, all while maintaining comparable model performance.\nNotably, our method is nearly lossless for LongBench datasets. This indicates\nthat SVDq enables high-precision low-bit quantization, providing a more\nefficient solution for KV cache compression in LLMs.\n","authors":["Hong Yankun","Li Xing","Zhen Hui-Ling","Yu Xianzhi","Liu Wulong","Yuan Mingxuan"],"pdf_url":"https://arxiv.org/pdf/2502.15304v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.05655v3","updated":"2025-02-21T08:46:59Z","published":"2024-09-09T14:22:19Z","title":"Interactive incremental learning of generalizable skills with local\n  trajectory modulation","summary":"  The problem of generalization in learning from demonstration (LfD) has\nreceived considerable attention over the years, particularly within the context\nof movement primitives, where a number of approaches have emerged. Recently,\ntwo important approaches have gained recognition. While one leverages\nvia-points to adapt skills locally by modulating demonstrated trajectories,\nanother relies on so-called task-parameterized models that encode movements\nwith respect to different coordinate systems, using a product of probabilities\nfor generalization. While the former are well-suited to precise, local\nmodulations, the latter aim at generalizing over large regions of the workspace\nand often involve multiple objects. Addressing the quality of generalization by\nleveraging both approaches simultaneously has received little attention. In\nthis work, we propose an interactive imitation learning framework that\nsimultaneously leverages local and global modulations of trajectory\ndistributions. Building on the kernelized movement primitives (KMP) framework,\nwe introduce novel mechanisms for skill modulation from direct human corrective\nfeedback. Our approach particularly exploits the concept of via-points to\nincrementally and interactively 1) improve the model accuracy locally, 2) add\nnew objects to the task during execution and 3) extend the skill into regions\nwhere demonstrations were not provided. We evaluate our method on a bearing\nring-loading task using a torque-controlled, 7-DoF, DLR SARA robot.\n","authors":["Markus Knauer","Alin Albu-Schäffer","Freek Stulp","João Silvério"],"pdf_url":"https://arxiv.org/pdf/2409.05655v3.pdf","comment":"Accepted at IEEE Robotics and Automation Letters (RA-L), 16 pages, 19\n  figures, 6 tables. See\n  https://github.com/DLR-RM/interactive-incremental-learning for further\n  information and video"},{"id":"http://arxiv.org/abs/2502.15296v1","updated":"2025-02-21T08:43:26Z","published":"2025-02-21T08:43:26Z","title":"Beyond Fixed Variables: Expanding-variate Time Series Forecasting via\n  Flat Scheme and Spatio-temporal Focal Learning","summary":"  Multivariate Time Series Forecasting (MTSF) has long been a key research\nfocus. Traditionally, these studies assume a fixed number of variables, but in\nreal-world applications, Cyber-Physical Systems often expand as new sensors are\ndeployed, increasing variables in MTSF. In light of this, we introduce a novel\ntask, Expanding-variate Time Series Forecasting (EVTSF). This task presents\nunique challenges, specifically (1) handling inconsistent data shapes caused by\nadding new variables, and (2) addressing imbalanced spatio-temporal learning,\nwhere expanding variables have limited observed data due to the necessity for\ntimely operation. To address these challenges, we propose STEV, a flexible\nspatio-temporal forecasting framework. STEV includes a new Flat Scheme to\ntackle the inconsistent data shape issue, which extends the graph-based\nspatio-temporal modeling architecture into 1D space by flattening the 2D\nsamples along the variable dimension, making the model variable-scale-agnostic\nwhile still preserving dynamic spatial correlations through a holistic graph.\nWe introduce a novel Spatio-temporal Focal Learning strategy that incorporates\na negative filter to resolve potential conflicts between contrastive learning\nand graph representation, and a focal contrastive loss as its core to guide the\nframework to focus on optimizing the expanding variables. We benchmark EVTSF\nperformance using three real-world datasets and compare it against three\npotential solutions employing SOTA MTSF models tailored for EVSTF. Experimental\nresults show that STEV significantly outperforms its competitors, particularly\non expanding variables. Notably, STEV, with only 5% of observations from the\nexpanding period, is on par with SOTA MTSF models trained with complete\nobservations. Further exploration of various expanding strategies underscores\nthe generalizability of STEV in real-world applications.\n","authors":["Minbo Ma","Kai Tang","Huan Li","Fei Teng","Dalin Zhang","Tianrui Li"],"pdf_url":"https://arxiv.org/pdf/2502.15296v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.02145v4","updated":"2025-02-21T08:32:57Z","published":"2024-10-03T02:11:35Z","title":"Active Learning of Deep Neural Networks via Gradient-Free Cutting Planes","summary":"  Active learning methods aim to improve sample complexity in machine learning.\nIn this work, we investigate an active learning scheme via a novel\ngradient-free cutting-plane training method for ReLU networks of arbitrary\ndepth and develop a convergence theory. We demonstrate, for the first time,\nthat cutting-plane algorithms, traditionally used in linear models, can be\nextended to deep neural networks despite their nonconvexity and nonlinear\ndecision boundaries. Moreover, this training method induces the first deep\nactive learning scheme known to achieve convergence guarantees, revealing a\ngeometric contraction rate of the feasible set. We exemplify the effectiveness\nof our proposed active learning method against popular deep active learning\nbaselines via both synthetic data experiments and sentimental classification\ntask on real datasets.\n","authors":["Erica Zhang","Fangzhao Zhang","Mert Pilanci"],"pdf_url":"https://arxiv.org/pdf/2410.02145v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2209.06418v2","updated":"2025-02-21T08:19:31Z","published":"2022-09-14T05:05:55Z","title":"Graph Perceiver IO: A General Architecture for Graph Structured Data","summary":"  Multimodal machine learning has been widely studied for the development of\ngeneral intelligence. Recently, the Perceiver and Perceiver IO, show\ncompetitive results for diverse dataset domains and tasks. However, recent\nworks, Perceiver and Perceiver IO, have focused on heterogeneous modalities,\nincluding image, text, and there are few research works for graph structured\ndatasets. A graph has an adjacency matrix different from other datasets such as\ntext and image, and it is not trivial to handle the topological information. In\nthis study, we provide a Graph Perceiver IO (GPIO), the Perceiver IO for the\ngraph structured dataset. We keep the main structure of the GPIO as the\nPerceiver IO because the Perceiver IO already handles the diverse dataset well,\nexcept for the graph structured dataset. The GPIO is a general method that\nhandles diverse datasets, such as graph-structured data, text, and images, by\nleveraging positional encoding and output query smoothing. Compared to graph\nneural networks (GNNs), GPIO requires lower complexity and can efficiently\nincorporate global and local information, which is also empirically validated\nthrough experiments. Furthermore, we propose GPIO+ for the multimodal few-shot\nclassification that incorporates both images and graphs simultaneously. GPIO\nachieves higher benchmark accuracy than GNNs across multiple tasks, including\ngraph classification, node classification, and multimodal text classification,\nwhile also attaining superior AP and AUC in link prediction. Additionally,\nGPIO+ outperforms GNNs in multimodal few-shot classification. Our GPIO(+) can\nserve as a general architecture for handling various modalities and tasks.\n","authors":["Seyun Bae","Hoyoon Byun","Changdae Oh","Yoon-Sik Cho","Kyungwoo Song"],"pdf_url":"https://arxiv.org/pdf/2209.06418v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.15280v1","updated":"2025-02-21T08:17:24Z","published":"2025-02-21T08:17:24Z","title":"Hyperspherical Normalization for Scalable Deep Reinforcement Learning","summary":"  Scaling up the model size and computation has brought consistent performance\nimprovements in supervised learning. However, this lesson often fails to apply\nto reinforcement learning (RL) because training the model on non-stationary\ndata easily leads to overfitting and unstable optimization. In response, we\nintroduce SimbaV2, a novel RL architecture designed to stabilize optimization\nby (i) constraining the growth of weight and feature norm by hyperspherical\nnormalization; and (ii) using a distributional value estimation with reward\nscaling to maintain stable gradients under varying reward magnitudes. Using the\nsoft actor-critic as a base algorithm, SimbaV2 scales up effectively with\nlarger models and greater compute, achieving state-of-the-art performance on 57\ncontinuous control tasks across 4 domains. The code is available at\nhttps://dojeon-ai.github.io/SimbaV2.\n","authors":["Hojoon Lee","Youngdo Lee","Takuma Seno","Donghu Kim","Peter Stone","Jaegul Choo"],"pdf_url":"https://arxiv.org/pdf/2502.15280v1.pdf","comment":"50 pages. Preprint"},{"id":"http://arxiv.org/abs/2411.00750v2","updated":"2025-02-21T08:00:10Z","published":"2024-11-01T17:18:45Z","title":"Mitigating Tail Narrowing in LLM Self-Improvement via Socratic-Guided\n  Sampling","summary":"  Self-improvement methods enable large language models (LLMs) to generate\nsolutions themselves and iteratively train on filtered, high-quality\nrationales. This process proves effective and reduces the reliance on human\nsupervision in LLMs' reasoning, but the performance soon plateaus. We delve\ninto the process and find that models tend to over-sample on easy queries and\nunder-sample on queries they have yet to master. As iterations proceed, this\nimbalance in sampling is exacerbated, leading to a long-tail distribution where\nsolutions to difficult queries almost diminish. This phenomenon limits the\nperformance gain of self-improving models. A straightforward solution is\nbrute-force sampling to balance the distribution, which significantly raises\ncomputational costs. In this paper, we introduce Guided Self-Improvement (GSI),\na strategy aimed at improving the efficiency of sampling challenging\nheavy-tailed data. It leverages Socratic-style guidance signals to help LLM\nreasoning with complex queries, reducing the exploration effort and minimizing\ncomputational overhead. Experiments on four models across diverse mathematical\ntasks show that GSI strikes a balance between performance and efficiency, while\nalso being effective on held-out tasks.\n","authors":["Yiwen Ding","Zhiheng Xi","Wei He","Zhuoyuan Li","Yitao Zhai","Xiaowei Shi","Xunliang Cai","Tao Gui","Qi Zhang","Xuanjing Huang"],"pdf_url":"https://arxiv.org/pdf/2411.00750v2.pdf","comment":"Accepted to NAACL 2025 Main Conference. Codes are publicly available\n  at https://github.com/Yiwen-Ding/Guided-Self-Improvement"},{"id":"http://arxiv.org/abs/2501.01045v3","updated":"2025-02-21T08:00:02Z","published":"2025-01-02T04:10:17Z","title":"ZeroFlow: Overcoming Catastrophic Forgetting is Easier than You Think","summary":"  Backpropagation provides a generalized configuration for overcoming\ncatastrophic forgetting. Like, SGD and Adam are commonly used for weight\nupdates in continual learning and continual pre-training. In practice,\npermission to access gradient information is not always granted (the gradient\nban), such as black-box APIs, hardware limitations, and non-differentiable\nsystems. To bridge this gap, we introduce the first benchmark ZeroFlow to\nevaluate gradient-free optimization algorithms for overcoming forgetting. This\nbenchmark examines a suite of forward pass methods across multiple methods,\nforgetting scenarios, and datasets. We find that forward passes alone are\nenough to overcome forgetting. Our findings reveal new optimization principles\nthat highlight the potential of forward-pass in mitigating forgetting, managing\ntask conflicts, and reducing memory demands, alongside novel enhancements that\nfurther mitigate forgetting with just one forward pass. This work provides\nessential insights and tools for advancing forward pass methods to overcome\nforgetting.\n","authors":["Tao Feng","Wei Li","Didi Zhu","Hangjie Yuan","Wendi Zheng","Dan Zhang","Jie Tang"],"pdf_url":"https://arxiv.org/pdf/2501.01045v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.18566v2","updated":"2025-02-21T07:45:50Z","published":"2024-09-27T09:10:44Z","title":"Optimizing DNN Inference on Multi-Accelerator SoCs at Training-time","summary":"  The demand for executing Deep Neural Networks (DNNs) with low latency and\nminimal power consumption at the edge has led to the development of advanced\nheterogeneous Systems-on-Chips (SoCs) that incorporate multiple specialized\ncomputing units (CUs), such as accelerators. Offloading DNN computations to a\nspecific CU from the available set often exposes accuracy vs efficiency\ntrade-offs, due to differences in their supported operations (e.g., standard\nvs. depthwise convolution) or data representations (e.g., more/less\naggressively quantized). A challenging yet unresolved issue is how to map a DNN\nonto these multi-CU systems to maximally exploit the parallelization\npossibilities while taking accuracy into account. To address this problem, we\npresent ODiMO, a hardware-aware tool that efficiently explores fine-grain\nmapping of DNNs among various on-chip CUs, during the training phase. ODiMO\nstrategically splits individual layers of the neural network and executes them\nin parallel on the multiple available CUs, aiming to balance the total\ninference energy consumption or latency with the resulting accuracy, impacted\nby the unique features of the different hardware units. We test our approach on\nCIFAR-10, CIFAR-100, and ImageNet, targeting two open-source heterogeneous\nSoCs, i.e., DIANA and Darkside. We obtain a rich collection of Pareto-optimal\nnetworks in the accuracy vs. energy or latency space. We show that ODiMO\nreduces the latency of a DNN executed on the Darkside SoC by up to 8x at\niso-accuracy, compared to manual heuristic mappings. When targeting energy, on\nthe same SoC, ODiMO produced up to 50.8x more efficient mappings, with minimal\naccuracy drop (< 0.3%).\n","authors":["Matteo Risso","Alessio Burrello","Daniele Jahier Pagliari"],"pdf_url":"https://arxiv.org/pdf/2409.18566v2.pdf","comment":"Accepted for publication in the IEEE Transactions on Computer-Aided\n  Design of Integrated Circuits and Systems"},{"id":"http://arxiv.org/abs/2502.15262v1","updated":"2025-02-21T07:43:24Z","published":"2025-02-21T07:43:24Z","title":"Towards a Reward-Free Reinforcement Learning Framework for Vehicle\n  Control","summary":"  Reinforcement learning plays a crucial role in vehicle control by guiding\nagents to learn optimal control strategies through designing or learning\nappropriate reward signals. However, in vehicle control applications, rewards\ntypically need to be manually designed while considering multiple implicit\nfactors, which easily introduces human biases. Although imitation learning\nmethods does not rely on explicit reward signals, they necessitate high-quality\nexpert actions, which are often challenging to acquire. To address these\nissues, we propose a reward-free reinforcement learning framework (RFRLF). This\nframework directly learns the target states to optimize agent behavior through\na target state prediction network (TSPN) and a reward-free state-guided policy\nnetwork (RFSGPN), avoiding the dependence on manually designed reward signals.\nSpecifically, the policy network is learned via minimizing the differences\nbetween the predicted state and the expert state. Experimental results\ndemonstrate the effectiveness of the proposed RFRLF in controlling vehicle\ndriving, showing its advantages in improving learning efficiency and adapting\nto reward-free environments.\n","authors":["Jielong Yang","Daoyuan Huang"],"pdf_url":"https://arxiv.org/pdf/2502.15262v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.14420v2","updated":"2025-02-21T07:28:36Z","published":"2025-02-20T10:16:18Z","title":"ChatVLA: Unified Multimodal Understanding and Robot Control with\n  Vision-Language-Action Model","summary":"  Humans possess a unified cognitive ability to perceive, comprehend, and\ninteract with the physical world. Why can't large language models replicate\nthis holistic understanding? Through a systematic analysis of existing training\nparadigms in vision-language-action models (VLA), we identify two key\nchallenges: spurious forgetting, where robot training overwrites crucial\nvisual-text alignments, and task interference, where competing control and\nunderstanding tasks degrade performance when trained jointly. To overcome these\nlimitations, we propose ChatVLA, a novel framework featuring Phased Alignment\nTraining, which incrementally integrates multimodal data after initial control\nmastery, and a Mixture-of-Experts architecture to minimize task interference.\nChatVLA demonstrates competitive performance on visual question-answering\ndatasets and significantly surpasses state-of-the-art vision-language-action\n(VLA) methods on multimodal understanding benchmarks. Notably, it achieves a\nsix times higher performance on MMMU and scores 47.2% on MMStar with a more\nparameter-efficient design than ECoT. Furthermore, ChatVLA demonstrates\nsuperior performance on 25 real-world robot manipulation tasks compared to\nexisting VLA methods like OpenVLA. Our findings highlight the potential of our\nunified framework for achieving both robust multimodal understanding and\neffective robot control.\n","authors":["Zhongyi Zhou","Yichen Zhu","Minjie Zhu","Junjie Wen","Ning Liu","Zhiyuan Xu","Weibin Meng","Ran Cheng","Yaxin Peng","Chaomin Shen","Feifei Feng"],"pdf_url":"https://arxiv.org/pdf/2502.14420v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.12128v2","updated":"2025-02-21T07:25:23Z","published":"2025-02-17T18:49:13Z","title":"LaM-SLidE: Latent Space Modeling of Spatial Dynamical Systems via Linked\n  Entities","summary":"  Generative models are spearheading recent progress in deep learning, showing\nstrong promise for trajectory sampling in dynamical systems as well. However,\nwhile latent space modeling paradigms have transformed image and video\ngeneration, similar approaches are more difficult for most dynamical systems.\nSuch systems -- from chemical molecule structures to collective human behavior\n-- are described by interactions of entities, making them inherently linked to\nconnectivity patterns and the traceability of entities over time. Our approach,\nLaM-SLidE (Latent Space Modeling of Spatial Dynamical Systems via Linked\nEntities), combines the advantages of graph neural networks, i.e., the\ntraceability of entities across time-steps, with the efficiency and scalability\nof recent advances in image and video generation, where pre-trained encoder and\ndecoder are frozen to enable generative modeling in the latent space. The core\nidea of LaM-SLidE is to introduce identifier representations (IDs) to allow for\nretrieval of entity properties, e.g., entity coordinates, from latent system\nrepresentations and thus enables traceability. Experimentally, across different\ndomains, we show that LaM-SLidE performs favorably in terms of speed, accuracy,\nand generalizability. Code is available at https://github.com/ml-jku/LaM-SLidE .\n","authors":["Florian Sestak","Artur Toshev","Andreas Fürst","Günter Klambauer","Andreas Mayr","Johannes Brandstetter"],"pdf_url":"https://arxiv.org/pdf/2502.12128v2.pdf","comment":"Project page: https://ml-jku.github.io/LaM-SLidE/"},{"id":"http://arxiv.org/abs/2502.15252v1","updated":"2025-02-21T07:04:34Z","published":"2025-02-21T07:04:34Z","title":"Real-Time Moving Flock Detection in Pedestrian Trajectories Using\n  Sequential Deep Learning Models","summary":"  Understanding collective pedestrian movement is crucial for applications in\ncrowd management, autonomous navigation, and human-robot interaction. This\npaper investigates the use of sequential deep learning models, including\nRecurrent Neural Networks (RNNs), Long Short-Term Memory (LSTM) networks, and\nTransformers, for real-time flock detection in multi-pedestrian trajectories.\nOur proposed approach consists of a two-stage process: first, a pre-trained\nbinary classification model is used for pairwise trajectory classification, and\nsecond, the learned representations are applied to identify multi-agent flocks\ndynamically.\n  We validate our method using real-world group movement datasets,\ndemonstrating its robustness across varying sequence lengths and diverse\nmovement patterns. Experimental results indicate that our model consistently\ndetects pedestrian flocks with high accuracy and stability, even in dynamic and\nnoisy environments. Furthermore, we extend our approach to identify other forms\nof collective motion, such as convoys and swarms, paving the way for more\ncomprehensive multi-agent behavior analysis.\n","authors":["Amartaivan Sanjjamts","Hiroshi Morita","Togootogtokh Enkhtogtokh"],"pdf_url":"https://arxiv.org/pdf/2502.15252v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.12631v2","updated":"2025-02-21T06:56:09Z","published":"2025-02-18T08:22:20Z","title":"Score-Based Diffusion Policy Compatible with Reinforcement Learning via\n  Optimal Transport","summary":"  Diffusion policies have shown promise in learning complex behaviors from\ndemonstrations, particularly for tasks requiring precise control and long-term\nplanning. However, they face challenges in robustness when encountering\ndistribution shifts. This paper explores improving diffusion-based imitation\nlearning models through online interactions with the environment. We propose\nOTPR (Optimal Transport-guided score-based diffusion Policy for Reinforcement\nlearning fine-tuning), a novel method that integrates diffusion policies with\nRL using optimal transport theory. OTPR leverages the Q-function as a transport\ncost and views the policy as an optimal transport map, enabling efficient and\nstable fine-tuning. Moreover, we introduce masked optimal transport to guide\nstate-action matching using expert keypoints and a compatibility-based\nresampling strategy to enhance training stability. Experiments on three\nsimulation tasks demonstrate OTPR's superior performance and robustness\ncompared to existing methods, especially in complex and sparse-reward\nenvironments. In sum, OTPR provides an effective framework for combining IL and\nRL, achieving versatile and reliable policy learning. The code will be released\nat https://github.com/Sunmmyy/OTPR.git.\n","authors":["Mingyang Sun","Pengxiang Ding","Weinan Zhang","Donglin Wang"],"pdf_url":"https://arxiv.org/pdf/2502.12631v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.15245v1","updated":"2025-02-21T06:38:03Z","published":"2025-02-21T06:38:03Z","title":"Steganographic Embeddings as an Effective Data Augmentation","summary":"  Image Steganography is a cryptographic technique that embeds secret\ninformation into an image, ensuring the hidden data remains undetectable to the\nhuman eye while preserving the image's original visual integrity. Least\nSignificant Bit (LSB) Steganography achieves this by replacing the k least\nsignificant bits of an image with the k most significant bits of a secret\nimage, maintaining the appearance of the original image while simultaneously\nencoding the essential elements of the hidden data. In this work, we shift away\nfrom conventional applications of steganography in deep learning and explore\nits potential from a new angle. We present experimental results on CIFAR-10\nshowing that LSB Steganography, when used as a data augmentation strategy for\ndownstream computer vision tasks such as image classification, can\nsignificantly improve the training efficiency of deep neural networks. It can\nalso act as an implicit, uniformly discretized piecewise linear approximation\nof color augmentations such as (brightness, contrast, hue, and saturation),\nwithout introducing additional training overhead through a new joint image\ntraining regime that disregards the need for tuning sensitive augmentation\nhyperparameters.\n","authors":["Nicholas DiSalvo"],"pdf_url":"https://arxiv.org/pdf/2502.15245v1.pdf","comment":"10 pages, 4 figures. For associated code and experiments, see this\n  http URL https://github.com/nickd16/steganographic-augmentations"},{"id":"http://arxiv.org/abs/2402.01830v3","updated":"2025-02-21T06:33:26Z","published":"2024-02-02T18:49:26Z","title":"PiCO: Peer Review in LLMs based on the Consistency Optimization","summary":"  Existing large language models (LLMs) evaluation methods typically focus on\ntesting the performance on some closed-environment and domain-specific\nbenchmarks with human annotations. In this paper, we explore a novel\nunsupervised evaluation direction, utilizing peer-review mechanisms to measure\nLLMs automatically. In this setting, both open-source and closed-source LLMs\nlie in the same environment, capable of answering unlabeled questions and\nevaluating each other, where each LLM's response score is jointly determined by\nother anonymous ones. To obtain the ability hierarchy among these models, we\nassign each LLM a learnable capability parameter to adjust the final ranking.\nWe formalize it as a constrained optimization problem, intending to maximize\nthe consistency of each LLM's capabilities and scores. The key assumption\nbehind is that high-level LLM can evaluate others' answers more accurately than\nlow-level ones, while higher-level LLM can also achieve higher response scores.\nMoreover, we propose three metrics called PEN, CIN, and LIS to evaluate the gap\nin aligning human rankings. We perform experiments on multiple datasets with\nthese metrics, validating the effectiveness of the proposed approach.\n","authors":["Kun-Peng Ning","Shuo Yang","Yu-Yang Liu","Jia-Yu Yao","Zhen-Hui Liu","Yong-Hong Tian","Yibing Song","Li Yuan"],"pdf_url":"https://arxiv.org/pdf/2402.01830v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.15240v1","updated":"2025-02-21T06:28:49Z","published":"2025-02-21T06:28:49Z","title":"Multi-agent Multi-armed Bandits with Minimum Reward Guarantee Fairness","summary":"  We investigate the problem of maximizing social welfare while ensuring\nfairness in a multi-agent multi-armed bandit (MA-MAB) setting. In this problem,\na centralized decision-maker takes actions over time, generating random rewards\nfor various agents. Our goal is to maximize the sum of expected cumulative\nrewards, a.k.a. social welfare, while ensuring that each agent receives an\nexpected reward that is at least a constant fraction of the maximum possible\nexpected reward.\n  Our proposed algorithm, RewardFairUCB, leverages the Upper Confidence Bound\n(UCB) technique to achieve sublinear regret bounds for both fairness and social\nwelfare. The fairness regret measures the positive difference between the\nminimum reward guarantee and the expected reward of a given policy, whereas the\nsocial welfare regret measures the difference between the social welfare of the\noptimal fair policy and that of the given policy.\n  We show that RewardFairUCB algorithm achieves instance-independent social\nwelfare regret guarantees of $\\tilde{O}(T^{1/2})$ and a fairness regret upper\nbound of $\\tilde{O}(T^{3/4})$. We also give the lower bound of\n$\\Omega(\\sqrt{T})$ for both social welfare and fairness regret. We evaluate\nRewardFairUCB's performance against various baseline and heuristic algorithms\nusing simulated data and real world data, highlighting trade-offs between\nfairness and social welfare regrets.\n","authors":["Piyushi Manupriya"," Himanshu","SakethaNath Jagarlapudi","Ganesh Ghalme"],"pdf_url":"https://arxiv.org/pdf/2502.15240v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.02968v4","updated":"2025-02-21T06:23:51Z","published":"2024-05-05T15:27:05Z","title":"CoverLib: Classifiers-equipped Experience Library by Iterative Problem\n  Distribution Coverage Maximization for Domain-tuned Motion Planning","summary":"  Library-based methods are known to be very effective for fast motion planning\nby adapting an experience retrieved from a precomputed library. This article\npresents CoverLib, a principled approach for constructing and utilizing such a\nlibrary. CoverLib iteratively adds an experience-classifier-pair to the\nlibrary, where each classifier corresponds to an adaptable region of the\nexperience within the problem space. This iterative process is an active\nprocedure, as it selects the next experience based on its ability to\neffectively cover the uncovered region. During the query phase, these\nclassifiers are utilized to select an experience that is expected to be\nadaptable for a given problem. Experimental results demonstrate that CoverLib\neffectively mitigates the trade-off between plannability and speed observed in\nglobal (e.g. sampling-based) and local (e.g. optimization-based) methods. As a\nresult, it achieves both fast planning and high success rates over the problem\ndomain. Moreover, due to its adaptation-algorithm-agnostic nature, CoverLib\nseamlessly integrates with various adaptation methods, including nonlinear\nprogramming-based and sampling-based algorithms.\n","authors":["Hirokazu Ishida","Naoki Hiraoka","Kei Okada","Masayuki Inaba"],"pdf_url":"https://arxiv.org/pdf/2405.02968v4.pdf","comment":"Accepted for publication in IEEE Transactions on Robotics"},{"id":"http://arxiv.org/abs/2502.11517v2","updated":"2025-02-21T06:14:19Z","published":"2025-02-17T07:39:16Z","title":"Learning to Keep a Promise: Scaling Language Model Decoding Parallelism\n  with Learned Asynchronous Decoding","summary":"  Decoding with autoregressive large language models (LLMs) traditionally\noccurs sequentially, generating one token after another. An emerging line of\nwork explored parallel decoding by identifying and simultaneously generating\nsemantically independent chunks of LLM responses. However, these techniques\nrely on hand-crafted heuristics tied to syntactic structures like lists and\nparagraphs, making them rigid and imprecise. We present PASTA, a learning-based\nsystem that teaches LLMs to identify semantic independence and express parallel\ndecoding opportunities in their own responses. At its core are PASTA-LANG and\nits interpreter: PASTA-LANG is an annotation language that enables LLMs to\nexpress semantic independence in their own responses; the language interpreter\nacts on these annotations to orchestrate parallel decoding on-the-fly at\ninference time. Through a two-stage finetuning process, we train LLMs to\ngenerate PASTA-LANG annotations that optimize both response quality and\ndecoding speed. Evaluation on AlpacaEval, an instruction following benchmark,\nshows that our approach Pareto-dominates existing methods in terms of decoding\nspeed and response quality; our results demonstrate geometric mean speedups\nranging from 1.21x to 1.93x with corresponding quality changes of +2.2% to\n-7.1%, measured by length-controlled win rates against sequential decoding\nbaseline.\n","authors":["Tian Jin","Ellie Y. Cheng","Zack Ankner","Nikunj Saunshi","Blake M. Elias","Amir Yazdanbakhsh","Jonathan Ragan-Kelley","Suvinay Subramanian","Michael Carbin"],"pdf_url":"https://arxiv.org/pdf/2502.11517v2.pdf","comment":"15 pages"},{"id":"http://arxiv.org/abs/2412.18735v2","updated":"2025-02-21T06:11:41Z","published":"2024-12-25T01:47:39Z","title":"Automatic Self-supervised Learning for Social Recommendations","summary":"  In recent years, researchers have attempted to exploit social relations to\nimprove the performance in recommendation systems. Generally, most existing\nsocial recommendation methods heavily depends on substantial domain knowledge\nand expertise in primary recommendation tasks for designing useful auxiliary\ntasks. Meanwhile, Self-Supervised Learning (SSL) recently has received\nconsiderable attention in the field of recommendation, since it can provide\nself-supervision signals in assisting the improvement of target recommendation\nsystems by constructing self-supervised auxiliary tasks from raw data without\nhuman-annotated labels. Despite the great success, these SSL-based social\nrecommendations are insufficient to adaptively balance various self-supervised\nauxiliary tasks, since assigning equal weights on various auxiliary tasks can\nresult in sub-optimal recommendation performance, where different\nself-supervised auxiliary tasks may contribute differently to improving the\nprimary social recommendation across different datasets. To address this issue,\nin this work, we propose Adaptive Self-supervised Learning for Social\nRecommendations (AdasRec) by taking advantage of various self-supervised\nauxiliary tasks. More specifically, an adaptive weighting mechanism is proposed\nto learn adaptive weights for various self-supervised auxiliary tasks, so as to\nbalance the contribution of such self-supervised auxiliary tasks for enhancing\nrepresentation learning in social recommendations. The adaptive weighting\nmechanism is used to assign different weights on auxiliary tasks to achieve an\noverall weighting of the entire auxiliary tasks and ultimately assist the\nprimary recommendation task, achieved by a meta learning optimization problem\nwith an adaptive weighting network. Comprehensive experiments on various\nreal-world datasets are constructed to verify the effectiveness of our proposed\nmethod.\n","authors":["Xin He","Wenqi Fan","Mingchen Sun","Ying Wang","Xin Wang"],"pdf_url":"https://arxiv.org/pdf/2412.18735v2.pdf","comment":"13 pages, 4 figures"},{"id":"http://arxiv.org/abs/2409.19976v4","updated":"2025-02-21T05:53:49Z","published":"2024-09-30T06:04:04Z","title":"Deep Parallel Spectral Neural Operators for Solving Partial Differential\n  Equations with Enhanced Low-Frequency Learning Capability","summary":"  Designing universal artificial intelligence (AI) solver for partial\ndifferential equations (PDEs) is an open-ended problem and a significant\nchallenge in science and engineering. Currently, data-driven solvers have\nachieved great success, such as neural operators. However, the ability of\nvarious neural operator solvers to learn low-frequency information still needs\nimprovement. In this study, we propose a Deep Parallel Spectral Neural Operator\n(DPNO) to enhance the ability to learn low-frequency information. Our method\nenhances the neural operator's ability to learn low-frequency information\nthrough parallel modules. In addition, due to the presence of truncation\ncoefficients, some high-frequency information is lost during the nonlinear\nlearning process. We smooth this information through convolutional mappings,\nthereby reducing high-frequency errors. We selected several challenging partial\ndifferential equation datasets for experimentation, and DPNO performed\nexceptionally well. As a neural operator, DPNO also possesses the capability of\nresolution invariance.\n","authors":["Qinglong Ma","Peizhi Zhao","Sen Wang","Tao Song"],"pdf_url":"https://arxiv.org/pdf/2409.19976v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.15224v1","updated":"2025-02-21T05:35:20Z","published":"2025-02-21T05:35:20Z","title":"Auto-Bench: An Automated Benchmark for Scientific Discovery in LLMs","summary":"  Given the remarkable performance of Large Language Models (LLMs), an\nimportant question arises: Can LLMs conduct human-like scientific research and\ndiscover new knowledge, and act as an AI scientist? Scientific discovery is an\niterative process that demands efficient knowledge updating and encoding. It\ninvolves understanding the environment, identifying new hypotheses, and\nreasoning about actions; however, no standardized benchmark specifically\ndesigned for scientific discovery exists for LLM agents. In response to these\nlimitations, we introduce a novel benchmark, \\textit{Auto-Bench}, that\nencompasses necessary aspects to evaluate LLMs for scientific discovery in both\nnatural and social sciences. Our benchmark is based on the principles of causal\ngraph discovery. It challenges models to uncover hidden structures and make\noptimal decisions, which includes generating valid justifications. By engaging\ninteractively with an oracle, the models iteratively refine their understanding\nof underlying interactions, the chemistry and social interactions, through\nstrategic interventions. We evaluate state-of-the-art LLMs, including GPT-4,\nGemini, Qwen, Claude, and Llama, and observe a significant performance drop as\nthe problem complexity increases, which suggests an important gap between\nmachine and human intelligence that future development of LLMs need to take\ninto consideration.\n","authors":["Tingting Chen","Srinivas Anumasa","Beibei Lin","Vedant Shah","Anirudh Goyal","Dianbo Liu"],"pdf_url":"https://arxiv.org/pdf/2502.15224v1.pdf","comment":"13 pages"},{"id":"http://arxiv.org/abs/2502.15217v1","updated":"2025-02-21T05:20:04Z","published":"2025-02-21T05:20:04Z","title":"FormalSpecCpp: A Dataset of C++ Formal Specifications created using LLMs","summary":"  FormalSpecCpp is a dataset designed to fill the gap in standardized\nbenchmarks for verifying formal specifications in C++ programs. To the best of\nour knowledge, this is the first comprehensive collection of C++ programs with\nwell-defined preconditions and postconditions. It provides a structured\nbenchmark for evaluating specification inference tools and testing theaccuracy\nof generated specifications. Researchers and developers can use this dataset to\nbenchmark specification inference tools,fine-tune Large Language Models (LLMs)\nfor automated specification generation, and analyze the role of formal\nspecifications in improving program verification and automated testing. By\nmaking this dataset publicly available, we aim to advance research in program\nverification, specification inference, and AI-assisted software development.\nThe dataset and the code are available at\nhttps://github.com/MadhuNimmo/FormalSpecCpp.\n","authors":["Madhurima Chakraborty","Peter Pirkelbauer","Qing Yi"],"pdf_url":"https://arxiv.org/pdf/2502.15217v1.pdf","comment":"Accepted at the 2025 IEEE/ACM 22nd International Conference on Mining\n  Software Repositories (MSR)"},{"id":"http://arxiv.org/abs/2502.15214v1","updated":"2025-02-21T05:01:30Z","published":"2025-02-21T05:01:30Z","title":"The Evolving Landscape of LLM- and VLM-Integrated Reinforcement Learning","summary":"  Reinforcement learning (RL) has shown impressive results in sequential\ndecision-making tasks. Meanwhile, Large Language Models (LLMs) and\nVision-Language Models (VLMs) have emerged, exhibiting impressive capabilities\nin multimodal understanding and reasoning. These advances have led to a surge\nof research integrating LLMs and VLMs into RL. In this survey, we review\nrepresentative works in which LLMs and VLMs are used to overcome key challenges\nin RL, such as lack of prior knowledge, long-horizon planning, and reward\ndesign. We present a taxonomy that categorizes these LLM/VLM-assisted RL\napproaches into three roles: agent, planner, and reward. We conclude by\nexploring open problems, including grounding, bias mitigation, improved\nrepresentations, and action advice. By consolidating existing research and\nidentifying future directions, this survey establishes a framework for\nintegrating LLMs and VLMs into RL, advancing approaches that unify natural\nlanguage and visual understanding with sequential decision-making.\n","authors":["Sheila Schoepp","Masoud Jafaripour","Yingyue Cao","Tianpei Yang","Fatemeh Abdollahi","Shadan Golestan","Zahin Sufiyan","Osmar R. Zaiane","Matthew E. Taylor"],"pdf_url":"https://arxiv.org/pdf/2502.15214v1.pdf","comment":"9 pages, 4 figures"},{"id":"http://arxiv.org/abs/2410.03043v2","updated":"2025-02-21T04:44:56Z","published":"2024-10-03T23:41:42Z","title":"Instance-Level Difficulty: A Missing Perspective in Machine Unlearning","summary":"  Current research on deep machine unlearning primarily focuses on improving or\nevaluating the overall effectiveness of unlearning methods while overlooking\nthe varying difficulty of unlearning individual training samples. As a result,\nthe broader feasibility of machine unlearning remains under-explored. This\npaper studies the cruxes that make machine unlearning difficult through a\nthorough instance-level unlearning performance analysis over various unlearning\nalgorithms and datasets. In particular, we summarize four factors that make\nunlearning a data point difficult, and we empirically show that these factors\nare independent of a specific unlearning algorithm but only relevant to the\ntarget model and its training data. Given these findings, we argue that machine\nunlearning research should pay attention to the instance-level difficulty of\nunlearning.\n","authors":["Hammad Rizwan","Mahtab Sarvmaili","Hassan Sajjad","Ga Wu"],"pdf_url":"https://arxiv.org/pdf/2410.03043v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.04556v4","updated":"2025-02-21T04:43:43Z","published":"2024-08-08T16:13:26Z","title":"BA-LoRA: Bias-Alleviating Low-Rank Adaptation to Mitigate Catastrophic\n  Inheritance in Large Language Models","summary":"  Large language models (LLMs) have demonstrated remarkable proficiency across\nvarious natural language processing (NLP) tasks. However, adapting LLMs to\ndownstream applications requires computationally intensive and memory-demanding\nfine-tuning procedures. To alleviate these burdens, parameter-efficient\nfine-tuning (PEFT) techniques have emerged as a promising approach to tailor\nLLMs with minimal computational overhead. While PEFT methods offer substantial\nadvantages, they do not fully address the pervasive issue of bias propagation\nfrom pre-training data. This work introduces Bias-Alleviating Low-Rank\nAdaptation (BA-LoRA), a novel PEFT method designed to counteract bias\ninheritance. BA-LoRA incorporates three distinct regularization terms: (1) a\nconsistency regularizer, (2) a diversity regularizer, and (3) a singular value\ndecomposition regularizer. These regularizers aim to enhance the models'\nconsistency, diversity, and generalization capabilities during fine-tuning. We\nconduct extensive experiments on natural language understanding (NLU) and\nnatural language generation (NLG) tasks using prominent LLMs such as LLaMA,\nMistral, and Gemma. The results demonstrate that BA-LoRA outperforms LoRA and\nits state-of-the-art variants. Moreover, our method effectively mitigates the\nadverse effects of pre-training bias, leading to more reliable and robust model\noutputs. The code is available at https://github.com/cyp-jlu-ai/BA-LoRA.\n","authors":["Yupeng Chang","Yi Chang","Yuan Wu"],"pdf_url":"https://arxiv.org/pdf/2408.04556v4.pdf","comment":"25 pages"},{"id":"http://arxiv.org/abs/2502.15198v1","updated":"2025-02-21T04:24:34Z","published":"2025-02-21T04:24:34Z","title":"Graph-Based Deep Learning on Stereo EEG for Predicting Seizure Freedom\n  in Epilepsy Patients","summary":"  Predicting seizure freedom is essential for tailoring epilepsy treatment. But\naccurate prediction remains challenging with traditional methods, especially\nwith diverse patient populations. This study developed a deep learning-based\ngraph neural network (GNN) model to predict seizure freedom from stereo\nelectroencephalography (sEEG) data in patients with refractory epilepsy. We\nutilized high-quality sEEG data from 15 pediatric patients to train a deep\nlearning model that can accurately predict seizure freedom outcomes and advance\nunderstanding of brain connectivity at the seizure onset zone. Our model\nintegrates local and global connectivity using graph convolutions with\nmulti-scale attention mechanisms to capture connections between\ndifficult-to-study regions such as the thalamus and motor regions. The model\nachieved an accuracy of 92.4% in binary class analysis, 86.6% in patient-wise\nanalysis, and 81.4% in multi-class analysis. Node and edge-level feature\nanalysis highlighted the anterior cingulate and frontal pole regions as key\ncontributors to seizure freedom outcomes. The nodes identified by our model\nwere also more likely to coincide with seizure onset zones. Our findings\nunderscore the potential of new connectivity-based deep learning models such as\nGNNs for enhancing the prediction of seizure freedom, predicting seizure onset\nzones, connectivity analysis of the brain during seizure, as well as informing\nAI-assisted personalized epilepsy treatment planning.\n","authors":["Artur Agaronyan","Syeda Abeera Amir","Nunthasiri Wittayanakorn","John Schreiber","Marius G. Linguraru","William Gaillard","Chima Oluigbo","Syed Muhammad Anwar"],"pdf_url":"https://arxiv.org/pdf/2502.15198v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.21621v2","updated":"2025-02-21T03:21:24Z","published":"2024-10-29T00:01:04Z","title":"Refined Risk Bounds for Unbounded Losses via Transductive Priors","summary":"  We revisit the sequential variants of linear regression with the squared\nloss, classification problems with hinge loss, and logistic regression, all\ncharacterized by unbounded losses in the setup where no assumptions are made on\nthe magnitude of design vectors and the norm of the optimal vector of\nparameters. The key distinction from existing results lies in our assumption\nthat the set of design vectors is known in advance (though their order is not),\na setup sometimes referred to as transductive online learning. While this\nassumption seems similar to fixed design regression or denoising, we\ndemonstrate that the sequential nature of our algorithms allows us to convert\nour bounds into statistical ones with random design without making any\nadditional assumptions about the distribution of the design vectors--an\nimpossibility for standard denoising results. Our key tools are based on the\nexponential weights algorithm with carefully chosen transductive\n(design-dependent) priors, which exploit the full horizon of the design\nvectors.\n  Our classification regret bounds have a feature that is only attributed to\nbounded losses in the literature: they depend solely on the dimension of the\nparameter space and on the number of rounds, independent of the design vectors\nor the norm of the optimal solution. For linear regression with squared loss,\nwe further extend our analysis to the sparse case, providing sparsity regret\nbounds that additionally depend on the magnitude of the response variables. We\nargue that these improved bounds are specific to the transductive setting and\nunattainable in the worst-case sequential setup. Our algorithms, in several\ncases, have polynomial time approximations and reduce to sampling with respect\nto log-concave measures instead of aggregating over hard-to-construct\n$\\varepsilon$-covers of classes.\n","authors":["Jian Qian","Alexander Rakhlin","Nikita Zhivotovskiy"],"pdf_url":"https://arxiv.org/pdf/2410.21621v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.15177v1","updated":"2025-02-21T03:16:19Z","published":"2025-02-21T03:16:19Z","title":"Optimizing Product Provenance Verification using Data Valuation Methods","summary":"  Determining and verifying product provenance remains a critical challenge in\nglobal supply chains, particularly as geopolitical conflicts and shifting\nborders create new incentives for misrepresentation of commodities, such as\nhiding the origin of illegally harvested timber or stolen agricultural\nproducts. Stable Isotope Ratio Analysis (SIRA), combined with Gaussian process\nregression-based isoscapes, has emerged as a powerful tool for geographic\norigin verification. However, the effectiveness of these models is often\nconstrained by data scarcity and suboptimal dataset selection. In this work, we\nintroduce a novel data valuation framework designed to enhance the selection\nand utilization of training data for machine learning models applied in SIRA.\nBy prioritizing high-informative samples, our approach improves model\nrobustness and predictive accuracy across diverse datasets and geographies. We\nvalidate our methodology with extensive experiments, demonstrating its\npotential to significantly enhance provenance verification, mitigate fraudulent\ntrade practices, and strengthen regulatory enforcement of global supply chains.\n","authors":["Raquib Bin Yousuf","Hoang Anh Just","Shengzhe Xu","Brian Mayer","Victor Deklerck","Jakub Truszkowski","John C. Simeone","Jade Saunders","Chang-Tien Lu","Ruoxi Jia","Naren Ramakrishnan"],"pdf_url":"https://arxiv.org/pdf/2502.15177v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.02357v2","updated":"2025-02-21T03:14:05Z","published":"2024-05-03T02:54:43Z","title":"Large Language Models for Mobility Analysis in Transportation Systems: A\n  Survey on Forecasting Tasks","summary":"  Mobility analysis is a crucial element in the research area of transportation\nsystems. Forecasting traffic information offers a viable solution to address\nthe conflict between increasing transportation demands and the limitations of\ntransportation infrastructure. Predicting human travel is significant in aiding\nvarious transportation and urban management tasks, such as taxi dispatch and\nurban planning. Machine learning and deep learning methods are favored for\ntheir flexibility and accuracy. Nowadays, with the advent of large language\nmodels (LLMs), many researchers have combined these models with previous\ntechniques or applied LLMs to directly predict future traffic information and\nhuman travel behaviors. However, there is a lack of comprehensive studies on\nhow LLMs can contribute to this field. This survey explores existing approaches\nusing LLMs for time series forecasting problems for mobility in transportation\nsystems. We provide a literature review concerning the forecasting applications\nwithin transportation systems, elucidating how researchers utilize LLMs,\nshowcasing recent state-of-the-art advancements, and identifying the challenges\nthat must be overcome to fully leverage LLMs in this domain.\n","authors":["Zijian Zhang","Yujie Sun","Zepu Wang","Yuqi Nie","Xiaobo Ma","Ruolin Li","Peng Sun","Xuegang Ban"],"pdf_url":"https://arxiv.org/pdf/2405.02357v2.pdf","comment":"27 pages, presented in 2025 TRB meeting"},{"id":"http://arxiv.org/abs/2408.09121v4","updated":"2025-02-21T03:02:41Z","published":"2024-08-17T07:11:02Z","title":"Selective Prompt Anchoring for Code Generation","summary":"  Recent advances in large language models (LLMs) have transformed software\ndevelopment by automatically generating code from natural language. Yet\nchallenges remain in generating fully correct code that aligns with user\nintent. Our study reveals that LLMs tend to pay less attention to user prompts\nas more code tokens are generated. We hypothesize that this attention dilution\nissue is an important reason for code generation errors. To mitigate this\nissue, we propose Selective Prompt Anchoring (SPA) to guide code LLMs to pay\nmore attention to user intent when generating code. We evaluate SPA using six\nbase LLMs across six benchmarks. Our results demonstrate that SPA enhances\nPass@1 by up to 12.9%, consistently outperforming SOTA code generation methods\nin all settings. Our code is available at\nhttps://github.com/magic-YuanTian/Selective-Prompt-Anchoring.\n","authors":["Yuan Tian","Tianyi Zhang"],"pdf_url":"https://arxiv.org/pdf/2408.09121v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.05711v4","updated":"2025-02-21T03:00:55Z","published":"2024-10-08T06:08:33Z","title":"TimeDART: A Diffusion Autoregressive Transformer for Self-Supervised\n  Time Series Representation","summary":"  Self-supervised learning has garnered increasing attention in time series\nanalysis for benefiting various downstream tasks and reducing reliance on\nlabeled data. Despite its effectiveness, existing methods often struggle to\ncomprehensively capture both long-term dynamic evolution and subtle local\npatterns in a unified manner. In this work, we propose TimeDART, a novel\nself-supervised time series pre-training framework that unifies two powerful\ngenerative paradigms to learn more transferable representations. Specifically,\nwe first employ a causal Transformer encoder, accompanied by a patch-based\nembedding strategy, to model the evolving trends from left to right. Building\non this global modeling, we further introduce a denoising diffusion process to\ncapture fine-grained local patterns through forward diffusion and reverse\ndenoising. Finally, we optimize the model in an autoregressive manner. As a\nresult, TimeDART effectively accounts for both global and local sequence\nfeatures in a coherent way. We conduct extensive experiments on public datasets\nfor time series forecasting and classification. The experimental results\ndemonstrate that TimeDART consistently outperforms previous compared methods,\nvalidating the effectiveness of our approach. Our code is available at\nhttps://github.com/Melmaphother/TimeDART.\n","authors":["Daoyu Wang","Mingyue Cheng","Zhiding Liu","Qi Liu","Enhong Chen"],"pdf_url":"https://arxiv.org/pdf/2410.05711v4.pdf","comment":"22 pages, 8 figures"},{"id":"http://arxiv.org/abs/2502.13417v2","updated":"2025-02-21T02:51:18Z","published":"2025-02-19T04:25:11Z","title":"RLTHF: Targeted Human Feedback for LLM Alignment","summary":"  Fine-tuning large language models (LLMs) to align with user preferences is\nchallenging due to the high cost of quality human annotations in Reinforcement\nLearning from Human Feedback (RLHF) and the generalizability limitations of AI\nFeedback. To address these challenges, we propose RLTHF, a human-AI hybrid\nframework that combines LLM-based initial alignment with selective human\nannotations to achieve full-human annotation alignment with minimal effort.\nRLTHF identifies hard-to-annotate samples mislabeled by LLMs using a reward\nmodel's reward distribution and iteratively enhances alignment by integrating\nstrategic human corrections while leveraging LLM's correctly labeled samples.\nEvaluations on HH-RLHF and TL;DR datasets show that RLTHF reaches full-human\nannotation-level alignment with only 6-7% of the human annotation effort.\nFurthermore, models trained on RLTHF's curated datasets for downstream tasks\noutperform those trained on fully human-annotated datasets, underscoring the\neffectiveness of RLTHF's strategic data curation.\n","authors":["Yifei Xu","Tusher Chakraborty","Emre Kıcıman","Bibek Aryal","Eduardo Rodrigues","Srinagesh Sharma","Roberto Estevao","Maria Angels de Luis Balaguer","Jessica Wolk","Rafael Padilha","Leonardo Nunes","Shobana Balakrishnan","Songwu Lu","Ranveer Chandra"],"pdf_url":"https://arxiv.org/pdf/2502.13417v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.14704v2","updated":"2025-02-21T02:25:30Z","published":"2025-02-20T16:29:37Z","title":"Not All Data are Good Labels: On the Self-supervised Labeling for Time\n  Series Forecasting","summary":"  Time Series Forecasting (TSF) is a crucial task in various domains, yet\nexisting TSF models rely heavily on high-quality data and insufficiently\nexploit all available data. This paper explores a novel self-supervised\napproach to re-label time series datasets by inherently constructing candidate\ndatasets. During the optimization of a simple reconstruction network,\nintermediates are used as pseudo labels in a self-supervised paradigm,\nimproving generalization for any predictor. We introduce the Self-Correction\nwith Adaptive Mask (SCAM), which discards overfitted components and selectively\nreplaces them with pseudo labels generated from reconstructions. Additionally,\nwe incorporate Spectral Norm Regularization (SNR) to further suppress\noverfitting from a loss landscape perspective. Our experiments on eleven\nreal-world datasets demonstrate that SCAM consistently improves the performance\nof various backbone models. This work offers a new perspective on constructing\ndatasets and enhancing the generalization of TSF models through self-supervised\nlearning.\n","authors":["Yuxuan Yang","Dalin Zhang","Yuxuan Liang","Hua Lu","Gang Chen","Huan Li"],"pdf_url":"https://arxiv.org/pdf/2502.14704v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.05634v2","updated":"2025-02-21T02:20:25Z","published":"2024-10-08T02:32:36Z","title":"Identification and estimation for matrix time series CP-factor models","summary":"  We propose a new method for identifying and estimating the CP-factor models\nfor matrix time series. Unlike the generalized eigenanalysis-based method of\nChang et al.(2023) for which the convergence rates may suffer from small\neigengaps as the asymptotic theory is based on some matrix perturbation\nanalysis, the proposed new method enjoys faster convergence rates which are\nfree from any eigengaps. It achieves this by turning the problem into a joint\ndiagonalization of several matrices whose elements are determined by a basis of\na linear system, and by choosing the basis carefully to avoid near co-linearity\n(see Proposition 5 and Section 4.3 below). Furthermore, unlike Chang et\nal.(2023) which requires the two factor loading matrices to be full-ranked, the\nnew method can handle rank-deficient factor loading matrices. Illustration with\nboth simulated and real matrix time series data shows the advantages of the\nproposed new method.\n","authors":["Jinyuan Chang","Yue Du","Guanglin Huang","Qiwei Yao"],"pdf_url":"https://arxiv.org/pdf/2410.05634v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.08877v5","updated":"2025-02-21T02:06:51Z","published":"2024-04-13T02:36:40Z","title":"Aligning the Objective of LLM-based Program Repair","summary":"  Large language models (LLMs) have achieved decent results on automated\nprogram repair (APR). However, the next token prediction training objective of\ndecoder-only LLMs (e.g., GPT-4) is misaligned with the masked span prediction\nobjective of current infilling-style methods, which impedes LLMs from fully\nleveraging pre-trained knowledge for program repair. In addition, while some\nLLMs can locate and repair bugs in certain functions using the related\nartifacts (e.g., test cases), existing methods still depend on statement-level\nfault localization methods to provide a list of buggy hunks for repair. This\nrestriction hinders LLMs from exploring potential patches beyond the given\nlocations.\n  In this paper, we investigate a new approach to adapt LLMs to program repair.\nOur core insight is that LLM's APR capability can be greatly improved by simply\naligning the output to their training objective and allowing them to refine the\nwhole program without first identifying faulty statements. Based on this\ninsight, we designed D4C, a straightforward prompting framework for APR. D4C\ncan repair 180 bugs correctly in Defects4J, with each patch being sampled only\n10 times. This surpasses the SOTA APR methods with perfect fault localization\nby 10% and reduces the patch sampling number by 90%. Our findings reveal that\n(1) objective alignment is crucial for fully exploiting LLM's pre-trained\ncapability, and (2) replacing the traditional localize-buggy-hunks-then-repair\nworkflow with direct debugging is more effective for LLM-based APR methods.\nThus, we believe this paper introduces a new mindset for harnessing LLMs in\nAPR.\n","authors":["Junjielong Xu","Ying Fu","Shin Hwei Tan","Pinjia He"],"pdf_url":"https://arxiv.org/pdf/2404.08877v5.pdf","comment":"Accepted by ICSE'25"},{"id":"http://arxiv.org/abs/2401.04979v5","updated":"2025-02-21T02:04:28Z","published":"2024-01-10T07:51:02Z","title":"DualDynamics: Synergizing Implicit and Explicit Methods for Robust\n  Irregular Time Series Analysis","summary":"  Real-world time series analysis faces significant challenges when dealing\nwith irregular and incomplete data. While Neural Differential Equation (NDE)\nbased methods have shown promise, they struggle with limited expressiveness,\nscalability issues, and stability concerns. Conversely, Neural Flows offer\nstability but falter with irregular data. We introduce 'DualDynamics', a novel\nframework that synergistically combines NDE-based method and Neural Flow-based\nmethod. This approach enhances expressive power while balancing computational\ndemands, addressing critical limitations of existing techniques. We demonstrate\nDualDynamics' effectiveness across diverse tasks: classification of robustness\nto dataset shift, irregularly-sampled series analysis, interpolation of missing\ndata, and forecasting with partial observations. Our results show consistent\noutperformance over state-of-the-art methods, indicating DualDynamics'\npotential to advance irregular time series analysis significantly.\n","authors":["YongKyung Oh","Dong-Young Lim","Sungil Kim"],"pdf_url":"https://arxiv.org/pdf/2401.04979v5.pdf","comment":"Published at the 39th Annual AAAI Conference on Artificial\n  Intelligence (AAAI 2025)"},{"id":"http://arxiv.org/abs/2502.15132v1","updated":"2025-02-21T01:24:54Z","published":"2025-02-21T01:24:54Z","title":"CoT-ICL Lab: A Petri Dish for Studying Chain-of-Thought Learning from\n  In-Context Demonstrations","summary":"  We introduce CoT-ICL Lab, a framework and methodology to generate synthetic\ntokenized datasets and systematically study chain-of-thought (CoT) in-context\nlearning (ICL) in language models. CoT-ICL Lab allows fine grained control over\nthe complexity of in-context examples by decoupling (1) the causal structure\ninvolved in chain token generation from (2) the underlying token processing\nfunctions. We train decoder-only transformers (up to 700M parameters) on these\ndatasets and show that CoT accelerates the accuracy transition to higher values\nacross model sizes. In particular, we find that model depth is crucial for\nleveraging CoT with limited in-context examples, while more examples help\nshallow models match deeper model performance. Additionally, limiting the\ndiversity of token processing functions throughout training improves causal\nstructure learning via ICL. We also interpret these transitions by analyzing\ntransformer embeddings and attention maps. Overall, CoT-ICL Lab serves as a\nsimple yet powerful testbed for theoretical and empirical insights into ICL and\nCoT in language models.\n","authors":["Vignesh Kothapalli","Hamed Firooz","Maziar Sanjabi"],"pdf_url":"https://arxiv.org/pdf/2502.15132v1.pdf","comment":"22 pages, 27 figures, 3 tables"},{"id":"http://arxiv.org/abs/2502.15131v1","updated":"2025-02-21T01:24:27Z","published":"2025-02-21T01:24:27Z","title":"Optimal and Provable Calibration in High-Dimensional Binary\n  Classification: Angular Calibration and Platt Scaling","summary":"  We study the fundamental problem of calibrating a linear binary classifier of\nthe form $\\sigma(\\hat{w}^\\top x)$, where the feature vector $x$ is Gaussian,\n$\\sigma$ is a link function, and $\\hat{w}$ is an estimator of the true linear\nweight $w^\\star$. By interpolating with a noninformative $\\textit{chance\nclassifier}$, we construct a well-calibrated predictor whose interpolation\nweight depends on the angle $\\angle(\\hat{w}, w_\\star)$ between the estimator\n$\\hat{w}$ and the true linear weight $w_\\star$. We establish that this angular\ncalibration approach is provably well-calibrated in a high-dimensional regime\nwhere the number of samples and features both diverge, at a comparable rate.\nThe angle $\\angle(\\hat{w}, w_\\star)$ can be consistently estimated.\nFurthermore, the resulting predictor is uniquely $\\textit{Bregman-optimal}$,\nminimizing the Bregman divergence to the true label distribution within a\nsuitable class of calibrated predictors. Our work is the first to provide a\ncalibration strategy that satisfies both calibration and optimality properties\nprovably in high dimensions. Additionally, we identify conditions under which a\nclassical Platt-scaling predictor converges to our Bregman-optimal calibrated\nsolution. Thus, Platt-scaling also inherits these desirable properties provably\nin high dimensions.\n","authors":["Yufan Li","Pragya Sur"],"pdf_url":"https://arxiv.org/pdf/2502.15131v1.pdf","comment":null}],"Multimedia":[{"id":"http://arxiv.org/abs/2502.15589v1","updated":"2025-02-21T16:57:22Z","published":"2025-02-21T16:57:22Z","title":"LightThinker: Thinking Step-by-Step Compression","summary":"  Large language models (LLMs) have shown remarkable performance in complex\nreasoning tasks, but their efficiency is hindered by the substantial memory and\ncomputational costs associated with generating lengthy tokens. In this paper,\nwe propose LightThinker, a novel method that enables LLMs to dynamically\ncompress intermediate thoughts during reasoning. Inspired by human cognitive\nprocesses, LightThinker compresses verbose thought steps into compact\nrepresentations and discards the original reasoning chains, thereby\nsignificantly reducing the number of tokens stored in the context window. This\nis achieved by training the model on when and how to perform compression\nthrough data construction, mapping hidden states to condensed gist tokens, and\ncreating specialized attention masks. Additionally, we introduce the Dependency\n(Dep) metric to quantify the degree of compression by measuring the reliance on\nhistorical tokens during generation. Extensive experiments on four datasets and\ntwo models show that LightThinker reduces peak memory usage and inference time,\nwhile maintaining competitive accuracy. Our work provides a new direction for\nimproving the efficiency of LLMs in complex reasoning tasks without sacrificing\nperformance. Code will be released at https://github.com/zjunlp/LightThinker.\n","authors":["Jintian Zhang","Yuqi Zhu","Mengshu Sun","Yujie Luo","Shuofei Qiao","Lun Du","Da Zheng","Huajun Chen","Ningyu Zhang"],"pdf_url":"https://arxiv.org/pdf/2502.15589v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.09638v2","updated":"2025-02-21T11:36:07Z","published":"2024-09-15T07:15:55Z","title":"Multi-view Hypergraph-based Contrastive Learning Model for Cold-Start\n  Micro-video Recommendation","summary":"  With the widespread use of mobile devices and the rapid growth of micro-video\nplatforms such as TikTok and Kwai, the demand for personalized micro-video\nrecommendation systems has significantly increased. Micro-videos typically\ncontain diverse information, such as textual metadata, visual cues (e.g., cover\nimages), and dynamic video content, significantly affecting user interaction\nand engagement patterns. However, most existing approaches often suffer from\nthe problem of over-smoothing, which limits their ability to capture\ncomprehensive interaction information effectively. Additionally, cold-start\nscenarios present ongoing challenges due to sparse interaction data and the\nunderutilization of available interaction signals. To address these issues, we\npropose a Multi-view Hypergraph-based Contrastive learning model for cold-start\nmicro-video Recommendation (MHCR). MHCR introduces a multi-view multimodal\nfeature extraction layer to capture interaction signals from various\nperspectives and incorporates multi-view self-supervised learning tasks to\nprovide additional supervisory signals. Through extensive experiments on two\nreal-world datasets, we show that MHCR significantly outperforms existing video\nrecommendation models and effectively mitigates cold-start challenges. Our code\nis available at https://github.com/sisuolv/MHCR.\n","authors":["Sisuo Lyu","Xiuze Zhou","Xuming Hu"],"pdf_url":"https://arxiv.org/pdf/2409.09638v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.11701v2","updated":"2025-02-21T09:48:58Z","published":"2024-10-15T15:39:37Z","title":"Magnifier Prompt: Tackling Multimodal Hallucination via Extremely Simple\n  Instructions","summary":"  Hallucinations in multimodal large language models (MLLMs) hinder their\npractical applications. To address this, we propose a Magnifier Prompt\n(MagPrompt), a simple yet effective method to tackle hallucinations in MLLMs\nvia extremely simple instructions. MagPrompt is based on the following two key\nprinciples, which guide the design of various effective prompts, demonstrating\nrobustness: (1) MLLMs should focus more on the image. (2) When there are\nconflicts between the image and the model's inner knowledge, MLLMs should\nprioritize the image. MagPrompt is training-free and can be applied to\nopen-source and closed-source models, such as GPT-4o and Gemini-pro. It\nperforms well across many datasets and its effectiveness is comparable or even\nbetter than more complex methods like VCD. Furthermore, our prompt design\nprinciples and experimental analyses provide valuable insights into multimodal\nhallucination.\n","authors":["Yuhan Fu","Ruobing Xie","Jiazhen Liu","Bangxiang Lan","Xingwu Sun","Zhanhui Kang","Xirong Li"],"pdf_url":"https://arxiv.org/pdf/2410.11701v2.pdf","comment":"The proposed method does not work for up-to-date MLLMs."},{"id":"http://arxiv.org/abs/2502.15290v1","updated":"2025-02-21T08:36:10Z","published":"2025-02-21T08:36:10Z","title":"Multimodal Graph-Based Variational Mixture of Experts Network for\n  Zero-Shot Multimodal Information Extraction","summary":"  Multimodal information extraction on social media is a series of fundamental\ntasks to construct the multimodal knowledge graph. The tasks aim to extract the\nstructural information in free texts with the incorporate images, including:\nmultimodal named entity typing and multimodal relation extraction. However, the\ngrowing number of multimodal data implies a growing category set and the newly\nemerged entity types or relations should be recognized without additional\ntraining. To address the aforementioned challenges, we focus on the zero-shot\nmultimodal information extraction tasks which require using textual and visual\nmodalities for recognizing unseen categories. Compared with text-based\nzero-shot information extraction models, the existing multimodal ones make the\ntextual and visual modalities aligned directly and exploit various fusion\nstrategies to improve their performances. But the existing methods ignore the\nfine-grained semantic correlation of text-image pairs and samples. Therefore,\nwe propose the multimodal graph-based variational mixture of experts network\n(MG-VMoE) which takes the MoE network as the backbone and exploits it for\naligning multimodal representations in a fine-grained way. Considering to learn\ninformative representations of multimodal data, we design each expert network\nas a variational information bottleneck to process two modalities in a\nuni-backbone. Moreover, we also propose the multimodal graph-based virtual\nadversarial training to learn the semantic correlation between the samples. The\nexperimental results on the two benchmark datasets demonstrate the superiority\nof MG-VMoE over the baselines.\n","authors":["Baohang Zhou","Ying Zhang","Yu Zhao","Xuhui Sui","Xiaojie Yuan"],"pdf_url":"https://arxiv.org/pdf/2502.15290v1.pdf","comment":"Accepted by WWW 2025"},{"id":"http://arxiv.org/abs/2407.21363v2","updated":"2025-02-21T06:56:59Z","published":"2024-07-31T06:20:21Z","title":"ESIQA: Perceptual Quality Assessment of Vision-Pro-based Egocentric\n  Spatial Images","summary":"  With the development of eXtended Reality (XR), photo capturing and display\ntechnology based on head-mounted displays (HMDs) have experienced significant\nadvancements and gained considerable attention. Egocentric spatial images and\nvideos are emerging as a compelling form of stereoscopic XR content. The\nassessment for the Quality of Experience (QoE) of XR content is important to\nensure a high-quality viewing experience. Different from traditional 2D images,\negocentric spatial images present challenges for perceptual quality assessment\ndue to their special shooting, processing methods, and stereoscopic\ncharacteristics. However, the corresponding image quality assessment (IQA)\nresearch for egocentric spatial images is still lacking. In this paper, we\nestablish the Egocentric Spatial Images Quality Assessment Database (ESIQAD),\nthe first IQA database dedicated for egocentric spatial images as far as we\nknow. Our ESIQAD includes 500 egocentric spatial images and the corresponding\nmean opinion scores (MOSs) under three display modes, including 2D display,\n3D-window display, and 3D-immersive display. Based on our ESIQAD, we propose a\nnovel mamba2-based multi-stage feature fusion model, termed ESIQAnet, which\npredicts the perceptual quality of egocentric spatial images under the three\ndisplay modes. Specifically, we first extract features from multiple visual\nstate space duality (VSSD) blocks, then apply cross attention to fuse binocular\nview information and use transposed attention to further refine the features.\nThe multi-stage features are finally concatenated and fed into a quality\nregression network to predict the quality score. Extensive experimental results\ndemonstrate that the ESIQAnet outperforms 22 state-of-the-art IQA models on the\nESIQAD under all three display modes. The database and code are available at\nhttps://github.com/IntMeGroup/ESIQA.\n","authors":["Xilei Zhu","Liu Yang","Huiyu Duan","Xiongkuo Min","Guangtao Zhai","Patrick Le Callet"],"pdf_url":"https://arxiv.org/pdf/2407.21363v2.pdf","comment":"9 pages, 12 figures"},{"id":"http://arxiv.org/abs/2502.15245v1","updated":"2025-02-21T06:38:03Z","published":"2025-02-21T06:38:03Z","title":"Steganographic Embeddings as an Effective Data Augmentation","summary":"  Image Steganography is a cryptographic technique that embeds secret\ninformation into an image, ensuring the hidden data remains undetectable to the\nhuman eye while preserving the image's original visual integrity. Least\nSignificant Bit (LSB) Steganography achieves this by replacing the k least\nsignificant bits of an image with the k most significant bits of a secret\nimage, maintaining the appearance of the original image while simultaneously\nencoding the essential elements of the hidden data. In this work, we shift away\nfrom conventional applications of steganography in deep learning and explore\nits potential from a new angle. We present experimental results on CIFAR-10\nshowing that LSB Steganography, when used as a data augmentation strategy for\ndownstream computer vision tasks such as image classification, can\nsignificantly improve the training efficiency of deep neural networks. It can\nalso act as an implicit, uniformly discretized piecewise linear approximation\nof color augmentations such as (brightness, contrast, hue, and saturation),\nwithout introducing additional training overhead through a new joint image\ntraining regime that disregards the need for tuning sensitive augmentation\nhyperparameters.\n","authors":["Nicholas DiSalvo"],"pdf_url":"https://arxiv.org/pdf/2502.15245v1.pdf","comment":"10 pages, 4 figures. For associated code and experiments, see this\n  http URL https://github.com/nickd16/steganographic-augmentations"}]},"2025-02-24T00:00:00Z":{"Computation and Language":[{"id":"http://arxiv.org/abs/2411.06438v5","updated":"2025-02-24T09:11:03Z","published":"2024-11-10T11:49:36Z","title":"Conditional [MASK] Discrete Diffusion Language Model","summary":"  Although auto-regressive models excel in natural language processing, they\noften struggle to generate diverse text and provide limited controllability.\nNon-auto-regressive methods could be an alternative but often produce\ndegenerate outputs and exhibit shortcomings in conditional generation. To\naddress these challenges, we propose Diffusion-EAGS, a novel framework that\nintegrates conditional masked language models into diffusion language models\nthrough the theoretical lens of a conditional Markov Random Field. In doing so,\nwe propose entropy-adaptive Gibbs sampling and entropy-based noise scheduling\nto counterbalance each model's shortcomings. Experimental results show that\nDiffusion-EAGS outperforms baselines and achieves the best quality-diversity\ntradeoff, demonstrating its effectiveness in non-autoregressive text\ngeneration.\n","authors":["Hyukhun Koh","Minha Jhang","Dohyung Kim","Sangmook Lee","Kyomin Jung"],"pdf_url":"https://arxiv.org/pdf/2411.06438v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.15485v2","updated":"2025-02-24T14:30:32Z","published":"2025-02-21T14:18:18Z","title":"Enhancing RWKV-based Language Models for Long-Sequence Text Generation","summary":"  This paper introduces an enhanced RWKV architecture with adaptive temporal\ngating mechanisms for improved long-context language modeling. We propose two\nprincipal innovations: (1) a position-aware convolutional shift operator that\ncaptures local syntactic patterns while preserving global coherence, and (2) a\nneurally-gated information routing mechanism that dynamically regulates\ninter-token information flow. Through comprehensive experiments on text\ngeneration tasks, our enhanced model demonstrates superior performance compared\nto the baseline RWKV, achieving 96.5 relative improvement in ROUGE-L scores\nwith only 2.95 increased inference latency. Ablation studies validate the\nindividual contributions of each component, while linguistic analysis reveals\nthe model's adaptive attention to syntactic boundaries and entity coherence.\nThe proposed modifications maintain RWKV's linear computational complexity\nwhile significantly enhancing its contextual modeling capabilities,\nestablishing new state-of-the-art performance for recurrent-style architectures\nin long-form text generation.\n","authors":["Xinghan Pan"],"pdf_url":"https://arxiv.org/pdf/2502.15485v2.pdf","comment":"8 pages, 2 tables, 3 figures"},{"id":"http://arxiv.org/abs/2502.15411v2","updated":"2025-02-24T14:45:27Z","published":"2025-02-21T12:19:08Z","title":"HiFi-KPI: A Dataset for Hierarchical KPI Extraction from Earnings\n  Filings","summary":"  The U.S. Securities and Exchange Commission (SEC) requires that public\ncompanies file financial reports tagging numbers with the machine readable\ninline eXtensible Business Reporting Language (iXBRL) standard. However, the\nhighly complex and highly granular taxonomy defined by iXBRL limits label\ntransferability across domains. In this paper, we introduce the Hierarchical\nFinancial Key Performance Indicator (HiFi-KPI) dataset, designed to facilitate\nnumerical KPI extraction at specified levels of granularity from unstructured\nfinancial text. Our approach organizes a 218,126-label hierarchy using a\ntaxonomy based grouping method, investigating which taxonomy layer provides the\nmost meaningful structure. HiFi-KPI comprises ~1.8M paragraphs and ~5M\nentities, each linked to a label in the iXBRL-specific calculation and\npresentation taxonomies. We provide baselines using encoder-based approaches\nand structured extraction using Large Language Models (LLMs). To simplify LLM\ninference and evaluation, we additionally release HiFi-KPI Lite, a manually\ncurated subset with four expert-mapped labels. We publicly release all\nartifacts.\n","authors":["Rasmus Aavang","Giovanni Rizzi","Rasmus Bøggild","Alexandre Iolov","Mike Zhang","Johannes Bjerva"],"pdf_url":"https://arxiv.org/pdf/2502.15411v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.15294v2","updated":"2025-02-24T13:35:18Z","published":"2025-02-21T08:40:07Z","title":"Round Attention: A Novel Round-Level Attention Mechanism to Accelerate\n  LLM Inference","summary":"  The increasing context window size in large language models (LLMs) has\nimproved their ability to handle complex, long-text tasks. However, as the\nconversation rounds continue, it is required to store a large amount of KV\ncache in GPU memory, which significantly affects the efficiency and even\navailability of the model serving systems. This paper analyzes dialogue data\nfrom real users and discovers that the LLM inference manifests a watershed\nlayer, after which the distribution of round-level attention shows notable\nsimilarity. We propose Round Attention, a novel round-level attention mechanism\nthat only recalls and computes the KV cache of the most relevant rounds. The\nexperiments show that our method saves 55\\% memory usage without compromising\nmodel performance.\n","authors":["Yaohua Tang","Zhicheng Hu","Kun Cheng","Fan Mo","Qiheng Lv","Hua Wang","Zhi Chen"],"pdf_url":"https://arxiv.org/pdf/2502.15294v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.15218v2","updated":"2025-02-24T15:31:58Z","published":"2025-02-21T05:21:58Z","title":"ESPnet-SpeechLM: An Open Speech Language Model Toolkit","summary":"  We present ESPnet-SpeechLM, an open toolkit designed to democratize the\ndevelopment of speech language models (SpeechLMs) and voice-driven agentic\napplications. The toolkit standardizes speech processing tasks by framing them\nas universal sequential modeling problems, encompassing a cohesive workflow of\ndata preprocessing, pre-training, inference, and task evaluation. With\nESPnet-SpeechLM, users can easily define task templates and configure key\nsettings, enabling seamless and streamlined SpeechLM development. The toolkit\nensures flexibility, efficiency, and scalability by offering highly\nconfigurable modules for every stage of the workflow. To illustrate its\ncapabilities, we provide multiple use cases demonstrating how competitive\nSpeechLMs can be constructed with ESPnet-SpeechLM, including a 1.7B-parameter\nmodel pre-trained on both text and speech tasks, across diverse benchmarks. The\ntoolkit and its recipes are fully transparent and reproducible at:\nhttps://github.com/espnet/espnet/tree/speechlm.\n","authors":["Jinchuan Tian","Jiatong Shi","William Chen","Siddhant Arora","Yoshiki Masuyama","Takashi Maekaku","Yihan Wu","Junyi Peng","Shikhar Bharadwaj","Yiwen Zhao","Samuele Cornell","Yifan Peng","Xiang Yue","Chao-Han Huck Yang","Graham Neubig","Shinji Watanabe"],"pdf_url":"https://arxiv.org/pdf/2502.15218v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.15210v2","updated":"2025-02-24T15:01:43Z","published":"2025-02-21T04:53:11Z","title":"PairBench: A Systematic Framework for Selecting Reliable Judge VLMs","summary":"  As large vision language models (VLMs) are increasingly used as automated\nevaluators, understanding their ability to effectively compare data pairs as\ninstructed in the prompt becomes essential. To address this, we present\nPairBench, a low-cost framework that systematically evaluates VLMs as\ncustomizable similarity tools across various modalities and scenarios. Through\nPairBench, we introduce four metrics that represent key desiderata of\nsimilarity scores: alignment with human annotations, consistency for data pairs\nirrespective of their order, smoothness of similarity distributions, and\ncontrollability through prompting. Our analysis demonstrates that no model,\nwhether closed- or open-source, is superior on all metrics; the optimal choice\ndepends on an auto evaluator's desired behavior (e.g., a smooth vs. a sharp\njudge), highlighting risks of widespread adoption of VLMs as evaluators without\nthorough assessment. For instance, the majority of VLMs struggle with\nmaintaining symmetric similarity scores regardless of order. Additionally, our\nresults show that the performance of VLMs on the metrics in PairBench closely\ncorrelates with popular benchmarks, showcasing its predictive power in ranking\nmodels.\n","authors":["Aarash Feizi","Sai Rajeswar","Adriana Romero-Soriano","Reihaneh Rabbany","Spandana Gella","Valentina Zantedeschi","João Monteiro"],"pdf_url":"https://arxiv.org/pdf/2502.15210v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.05315v3","updated":"2025-02-24T17:53:06Z","published":"2024-06-08T01:27:19Z","title":"Aligned at the Start: Conceptual Groupings in LLM Embeddings","summary":"  This paper shifts focus to the often-overlooked input embeddings - the\ninitial representations fed into transformer blocks. Using fuzzy graph,\nk-nearest neighbor (k-NN), and community detection, we analyze embeddings from\ndiverse LLMs, finding significant categorical community structure aligned with\npredefined concepts and categories aligned with humans. We observe these\ngroupings exhibit within-cluster organization (such as hierarchies, topological\nordering, etc.), hypothesizing a fundamental structure that precedes contextual\nprocessing. To further investigate the conceptual nature of these groupings, we\nexplore cross-model alignments across different LLM categories within their\ninput embeddings, observing a medium to high degree of alignment. Furthermore,\nprovide evidence that manipulating these groupings can play a functional role\nin mitigating ethnicity bias in LLM tasks.\n","authors":["Mehrdad Khatir","Sanchit Kabra","Chandan K. Reddy"],"pdf_url":"https://arxiv.org/pdf/2406.05315v3.pdf","comment":null}],"Computer Vision and Pattern Recognition":[{"id":"http://arxiv.org/abs/2502.15635v2","updated":"2025-02-24T03:48:35Z","published":"2025-02-21T18:03:56Z","title":"Para-Lane: Multi-Lane Dataset Registering Parallel Scans for\n  Benchmarking Novel View Synthesis","summary":"  To evaluate end-to-end autonomous driving systems, a simulation environment\nbased on Novel View Synthesis (NVS) techniques is essential, which synthesizes\nphoto-realistic images and point clouds from previously recorded sequences\nunder new vehicle poses, particularly in cross-lane scenarios. Therefore, the\ndevelopment of a multi-lane dataset and benchmark is necessary. While recent\nsynthetic scene-based NVS datasets have been prepared for cross-lane\nbenchmarking, they still lack the realism of captured images and point clouds.\nTo further assess the performance of existing methods based on NeRF and 3DGS,\nwe present the first multi-lane dataset registering parallel scans specifically\nfor novel driving view synthesis dataset derived from real-world scans,\ncomprising 25 groups of associated sequences, including 16,000 front-view\nimages, 64,000 surround-view images, and 16,000 LiDAR frames. All frames are\nlabeled to differentiate moving objects from static elements. Using this\ndataset, we evaluate the performance of existing approaches in various testing\nscenarios at different lanes and distances. Additionally, our method provides\nthe solution for solving and assessing the quality of multi-sensor poses for\nmulti-modal data alignment for curating such a dataset in real-world. We plan\nto continually add new sequences to test the generalization of existing methods\nacross different scenarios. The dataset is released publicly at the project\npage: https://nizqleo.github.io/paralane-dataset/.\n","authors":["Ziqian Ni","Sicong Du","Zhenghua Hou","Chenming Wu","Sheng Yang"],"pdf_url":"https://arxiv.org/pdf/2502.15635v2.pdf","comment":"Accepted by International Conference on 3D Vision (3DV) 2025"},{"id":"http://arxiv.org/abs/2502.15342v2","updated":"2025-02-24T06:22:17Z","published":"2025-02-21T09:57:53Z","title":"PFSD: A Multi-Modal Pedestrian-Focus Scene Dataset for Rich Tasks in\n  Semi-Structured Environments","summary":"  Recent advancements in autonomous driving perception have revealed\nexceptional capabilities within structured environments dominated by vehicular\ntraffic. However, current perception models exhibit significant limitations in\nsemi-structured environments, where dynamic pedestrians with more diverse\nirregular movement and occlusion prevail. We attribute this shortcoming to the\nscarcity of high-quality datasets in semi-structured scenes, particularly\nconcerning pedestrian perception and prediction. In this work, we present the\nmulti-modal Pedestrian-Focused Scene Dataset(PFSD), rigorously annotated in\nsemi-structured scenes with the format of nuScenes. PFSD provides comprehensive\nmulti-modal data annotations with point cloud segmentation, detection, and\nobject IDs for tracking. It encompasses over 130,000 pedestrian instances\ncaptured across various scenarios with varying densities, movement patterns,\nand occlusions. Furthermore, to demonstrate the importance of addressing the\nchallenges posed by more diverse and complex semi-structured environments, we\npropose a novel Hybrid Multi-Scale Fusion Network (HMFN). Specifically, to\ndetect pedestrians in densely populated and occluded scenarios, our method\neffectively captures and fuses multi-scale features using a meticulously\ndesigned hybrid framework that integrates sparse and vanilla convolutions.\nExtensive experiments on PFSD demonstrate that HMFN attains improvement in mean\nAverage Precision (mAP) over existing methods, thereby underscoring its\nefficacy in addressing the challenges of 3D pedestrian detection in complex\nsemi-structured environments. Coding and benchmark are available.\n","authors":["Yueting Liu","Hanshi Wang","Yunfei Lei","Zhengjun Zha","Weiming Hu","Jin Gao"],"pdf_url":"https://arxiv.org/pdf/2502.15342v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.15250v2","updated":"2025-02-24T13:22:17Z","published":"2025-02-21T07:00:09Z","title":"An ocean front detection and tracking algorithm","summary":"  Existing ocean front detection methods--including histogram-based variance\nanalysis, Lyapunov exponent, gradient thresholding, and machine\nlearning--suffer from critical limitations: discontinuous outputs,\nover-detection, reliance on single-threshold decisions, and lack of open-source\nimplementations. To address these challenges, this paper proposes the Bayesian\nFront Detection and Tracking framework with Metric Space Analysis (BFDT-MSA).\nThe framework introduces three innovations: (1) a Bayesian decision mechanism\nthat integrates gradient priors and field operators to eliminate manual\nthreshold sensitivity; (2) morphological refinement algorithms for merging\nfragmented fronts, deleting spurious rings, and thinning frontal zones to\npixel-level accuracy; and (3) a novel metric space definition for temporal\nfront tracking, enabling systematic analysis of front evolution. Validated on\nglobal SST data (2022--2024), BFDT-MSA reduces over-detection by $73\\%$\ncompared to histogram-based methods while achieving superior intensity\n($0.16^\\circ$C/km), continuity, and spatiotemporal coherence. The open-source\nrelease bridges a critical gap in reproducible oceanographic research.\n","authors":["Yishuo Wang","Feng Zhou"],"pdf_url":"https://arxiv.org/pdf/2502.15250v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.15193v2","updated":"2025-02-24T05:36:07Z","published":"2025-02-21T04:07:48Z","title":"Image Translation-Based Unsupervised Cross-Modality Domain Adaptation\n  for Medical Image Segmentation","summary":"  Supervised deep learning usually faces more challenges in medical images than\nin natural images. Since annotations in medical images require the expertise of\ndoctors and are more time-consuming and expensive. Thus, some researchers turn\nto unsupervised learning methods, which usually face inevitable performance\ndrops. In addition, medical images may have been acquired at different medical\ncenters with different scanners and under different image acquisition\nprotocols, so the modalities of the medical images are often inconsistent. This\nmodality difference (domain shift) also reduces the applicability of deep\nlearning methods. In this regard, we propose an unsupervised crossmodality\ndomain adaptation method based on image translation by transforming the source\nmodality image with annotation into the unannotated target modality and using\nits annotation to achieve supervised learning of the target modality. In\naddition, the subtle differences between translated pseudo images and real\nimages are overcome by self-training methods to further improve the task\nperformance of deep learning. The proposed method showed mean Dice Similarity\nCoefficient (DSC) and Average Symmetric Surface Distance (ASSD) of $0.8351 \\pm\n0.1152$ and $1.6712 \\pm 2.1948$ for vestibular schwannoma (VS), $0.8098 \\pm\n0.0233$ and $0.2317 \\pm 0.1577$ for cochlea on the VS and cochlea segmentation\ntask of the Cross-Modality Domain Adaptation (crossMoDA 2022) challenge\nvalidation phase leaderboard.\n","authors":["Tao Yang","Lisheng Wang"],"pdf_url":"https://arxiv.org/pdf/2502.15193v2.pdf","comment":"5 pages, 1 figure"}],"Information Retrieval":[{"id":"http://arxiv.org/abs/2409.05806v3","updated":"2025-02-24T11:02:47Z","published":"2024-09-09T17:11:51Z","title":"CKnowEdit: A New Chinese Knowledge Editing Dataset for Linguistics,\n  Facts, and Logic Error Correction in LLMs","summary":"  Chinese, as a linguistic system rich in depth and complexity, is\ncharacterized by distinctive elements such as ancient poetry, proverbs, idioms,\nand other cultural constructs. However, current Large Language Models (LLMs)\nface limitations in these specialized domains, highlighting the need for the\ndevelopment of comprehensive datasets that can assess, continuously update, and\nprogressively improve these culturally-grounded linguistic competencies through\ntargeted training optimizations. To address this gap, we introduce CKnowEdit,\nthe first-ever Chinese knowledge editing dataset designed to correct\nlinguistic, factual, and logical errors in LLMs. We collect seven types of\nknowledge from a wide range of sources, including classical texts, idioms, and\ncontent from Baidu Tieba Ruozhiba, taking into account the unique polyphony,\nantithesis, and logical structures inherent in the Chinese language. By\nanalyzing this dataset, we highlight the challenges current LLMs face in\nmastering Chinese. Furthermore, our evaluation of state-of-the-art knowledge\nediting techniques reveals opportunities to advance the correction of Chinese\nknowledge. Code and dataset are available at\nhttps://github.com/zjunlp/EasyEdit.\n","authors":["Jizhan Fang","Tianhe Lu","Yunzhi Yao","Ziyan Jiang","Xin Xu","Ningyu Zhang","Huajun Chen"],"pdf_url":"https://arxiv.org/pdf/2409.05806v3.pdf","comment":"Ongoing work; project website is available at\n  https://zjunlp.github.io/project/CKnowEdit code and dataset are available at\n  https://github.com/zjunlp/EasyEdit"},{"id":"http://arxiv.org/abs/2502.14918v2","updated":"2025-02-24T08:29:03Z","published":"2025-02-19T13:59:06Z","title":"RAPTOR: Refined Approach for Product Table Object Recognition","summary":"  Extracting tables from documents is a critical task across various\nindustries, especially on business documents like invoices and reports.\nExisting systems based on DEtection TRansformer (DETR) such as TAble\nTRansformer (TATR), offer solutions for Table Detection (TD) and Table\nStructure Recognition (TSR) but face challenges with diverse table formats and\ncommon errors like incorrect area detection and overlapping columns. This\nresearch introduces RAPTOR, a modular post-processing system designed to\nenhance state-of-the-art models for improved table extraction, particularly for\nproduct tables. RAPTOR addresses recurrent TD and TSR issues, improving both\nprecision and structural predictions. For TD, we use DETR (trained on ICDAR\n2019) and TATR (trained on PubTables-1M and FinTabNet), while TSR only relies\non TATR. A Genetic Algorithm is incorporated to optimize RAPTOR's module\nparameters, using a private dataset of product tables to align with industrial\nneeds. We evaluate our method on two private datasets of product tables, the\npublic DOCILE dataset (which contains tables similar to our target product\ntables), and the ICDAR 2013 and ICDAR 2019 datasets. The results demonstrate\nthat while our approach excels at product tables, it also maintains reasonable\nperformance across diverse table formats. An ablation study further validates\nthe contribution of each module in our system.\n","authors":["Eliott Thomas","Mickael Coustaty","Aurelie Joseph","Gaspar Deloin","Elodie Carel","Vincent Poulain D'Andecy","Jean-Marc Ogier"],"pdf_url":"https://arxiv.org/pdf/2502.14918v2.pdf","comment":"Accepted for WACVW 2025 (VisionDocs)"}],"Machine Learning":[{"id":"http://arxiv.org/abs/2502.15657v2","updated":"2025-02-24T18:14:15Z","published":"2025-02-21T18:28:36Z","title":"Superintelligent Agents Pose Catastrophic Risks: Can Scientist AI Offer\n  a Safer Path?","summary":"  The leading AI companies are increasingly focused on building generalist AI\nagents -- systems that can autonomously plan, act, and pursue goals across\nalmost all tasks that humans can perform. Despite how useful these systems\nmight be, unchecked AI agency poses significant risks to public safety and\nsecurity, ranging from misuse by malicious actors to a potentially irreversible\nloss of human control. We discuss how these risks arise from current AI\ntraining methods. Indeed, various scenarios and experiments have demonstrated\nthe possibility of AI agents engaging in deception or pursuing goals that were\nnot specified by human operators and that conflict with human interests, such\nas self-preservation. Following the precautionary principle, we see a strong\nneed for safer, yet still useful, alternatives to the current agency-driven\ntrajectory. Accordingly, we propose as a core building block for further\nadvances the development of a non-agentic AI system that is trustworthy and\nsafe by design, which we call Scientist AI. This system is designed to explain\nthe world from observations, as opposed to taking actions in it to imitate or\nplease humans. It comprises a world model that generates theories to explain\ndata and a question-answering inference machine. Both components operate with\nan explicit notion of uncertainty to mitigate the risks of overconfident\npredictions. In light of these considerations, a Scientist AI could be used to\nassist human researchers in accelerating scientific progress, including in AI\nsafety. In particular, our system can be employed as a guardrail against AI\nagents that might be created despite the risks involved. Ultimately, focusing\non non-agentic AI may enable the benefits of AI innovation while avoiding the\nrisks associated with the current trajectory. We hope these arguments will\nmotivate researchers, developers, and policymakers to favor this safer path.\n","authors":["Yoshua Bengio","Michael Cohen","Damiano Fornasiere","Joumana Ghosn","Pietro Greiner","Matt MacDermott","Sören Mindermann","Adam Oberman","Jesse Richardson","Oliver Richardson","Marc-Antoine Rondeau","Pierre-Luc St-Charles","David Williams-King"],"pdf_url":"https://arxiv.org/pdf/2502.15657v2.pdf","comment":"v2 with fixed formatting for URLs and hyperlinks"},{"id":"http://arxiv.org/abs/2502.11071v3","updated":"2025-02-24T16:32:42Z","published":"2025-02-16T10:40:19Z","title":"Generalization of the Gibbs algorithm with high probability at low\n  temperatures","summary":"  The paper gives a bound on the generalization error of the Gibbs algorithm,\nwhich recovers known data-independent bounds for the high temperature range and\nextends to the low-temperature range, where generalization depends critically\non the data-dependent loss-landscape. It is shown, that with high probability\nthe generalization error of a single hypothesis drawn from the Gibbs posterior\ndecreases with the total prior volume of all hypotheses with similar or smaller\nempirical error. This gives theoretical support to the belief in the benefit of\nflat minima. The zero temperature limit is discussed and the bound is extended\nto a class of similar stochastic algorithms.\n","authors":["Andreas Maurer"],"pdf_url":"https://arxiv.org/pdf/2502.11071v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.15425v2","updated":"2025-02-24T15:22:12Z","published":"2025-02-21T12:52:16Z","title":"TAG: A Decentralized Framework for Multi-Agent Hierarchical\n  Reinforcement Learning","summary":"  Hierarchical organization is fundamental to biological systems and human\nsocieties, yet artificial intelligence systems often rely on monolithic\narchitectures that limit adaptability and scalability. Current hierarchical\nreinforcement learning (HRL) approaches typically restrict hierarchies to two\nlevels or require centralized training, which limits their practical\napplicability. We introduce TAME Agent Framework (TAG), a framework for\nconstructing fully decentralized hierarchical multi-agent systems.TAG enables\nhierarchies of arbitrary depth through a novel LevelEnv concept, which\nabstracts each hierarchy level as the environment for the agents above it. This\napproach standardizes information flow between levels while preserving loose\ncoupling, allowing for seamless integration of diverse agent types. We\ndemonstrate the effectiveness of TAG by implementing hierarchical architectures\nthat combine different RL agents across multiple levels, achieving improved\nperformance over classical multi-agent RL baselines on standard benchmarks. Our\nresults show that decentralized hierarchical organization enhances both\nlearning speed and final performance, positioning TAG as a promising direction\nfor scalable multi-agent systems.\n","authors":["Giuseppe Paolo","Abdelhakim Benechehab","Hamza Cherkaoui","Albert Thomas","Balázs Kégl"],"pdf_url":"https://arxiv.org/pdf/2502.15425v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.15297v2","updated":"2025-02-24T07:30:05Z","published":"2025-02-21T08:43:50Z","title":"Comparative Analysis of Black Hole Mass Estimation in Type-2 AGNs:\n  Classical vs. Quantum Machine Learning and Deep Learning Approaches","summary":"  In the case of Type-2 AGNs, estimating the mass of the black hole is\nchallenging. Understanding how galaxies form and evolve requires considerable\ninsight into the mass of black holes. This work compared different classical\nand quantum machine learning (QML) algorithms for black hole mass estimation,\nwherein the classical algorithms are Linear Regression, XGBoost Regression,\nRandom Forest Regressor, Support Vector Regressor (SVR), Lasso Regression,\nRidge Regression, Elastic Net Regression, Bayesian Regression, Decision Tree\nRegressor, Gradient Booster Regressor, Classical Neural Networks, Gated\nRecurrent Unit (GRU), LSTM, Deep Residual Networks (ResNets) and\nTransformer-Based Regression. On the other hand, quantum algorithms including\nHybrid Quantum Neural Networks (QNN), Quantum Long Short-Term Memory (Q-LSTM),\nSampler-QNN, Estimator-QNN, Variational Quantum Regressor (VQR), Quantum Linear\nRegression(Q-LR), QML with JAX optimization were also tested. The results\nrevealed that classical algorithms gave better R^2, MAE, MSE, and RMSE results\nthan the quantum models. Among the classical models, LSTM has the best result\nwith an accuracy of 99.77%. Estimator-QNN has the highest accuracy for quantum\nalgorithms with an MSE of 0.0124 and an accuracy of 99.75%. This study\nascertains both the strengths and weaknesses of the classical and the quantum\napproaches. As far as our knowledge goes, this work could pave the way for the\nfuture application of quantum algorithms in astrophysical data analysis.\n","authors":["Sathwik Narkedimilli","Venkata Sriram Amballa","N V Saran Kumar","R Arun Kumar","R Praneeth Reddy","Satvik Raghav","Manish M","Aswath Babu H"],"pdf_url":"https://arxiv.org/pdf/2502.15297v2.pdf","comment":"29 pages, 12 Figures, 6 Tables"},{"id":"http://arxiv.org/abs/2502.15215v2","updated":"2025-02-24T02:53:24Z","published":"2025-02-21T05:15:38Z","title":"Tensor Product Neural Networks for Functional ANOVA Model","summary":"  Interpretability for machine learning models is becoming more and more\nimportant as machine learning models become more complex. The functional ANOVA\nmodel, which decomposes a high-dimensional function into a sum of lower\ndimensional functions so called components, is one of the most popular tools\nfor interpretable AI, and recently, various neural network models have been\ndeveloped for estimating each component in the functional ANOVA model. However,\nsuch neural networks are highly unstable when estimating components since the\ncomponents themselves are not uniquely defined. That is, there are multiple\nfunctional ANOVA decompositions for a given function. In this paper, we propose\na novel interpretable model which guarantees a unique functional ANOVA\ndecomposition and thus is able to estimate each component stably. We call our\nproposed model ANOVA-NODE since it is a modification of Neural Oblivious\nDecision Ensembles (NODE) for the functional ANOVA model. Theoretically, we\nprove that ANOVA-NODE can approximate a smooth function well. Additionally, we\nexperimentally show that ANOVA-NODE provides much more stable estimation of\neach component and thus much more stable interpretation when training data and\ninitial values of the model parameters vary than existing neural network models\ndo.\n","authors":["Seokhun Park","Insung Kong","Yongchan Choi","Chanmoo Park","Yongdai Kim"],"pdf_url":"https://arxiv.org/pdf/2502.15215v2.pdf","comment":"45 pages"},{"id":"http://arxiv.org/abs/2502.15210v2","updated":"2025-02-24T15:01:43Z","published":"2025-02-21T04:53:11Z","title":"PairBench: A Systematic Framework for Selecting Reliable Judge VLMs","summary":"  As large vision language models (VLMs) are increasingly used as automated\nevaluators, understanding their ability to effectively compare data pairs as\ninstructed in the prompt becomes essential. To address this, we present\nPairBench, a low-cost framework that systematically evaluates VLMs as\ncustomizable similarity tools across various modalities and scenarios. Through\nPairBench, we introduce four metrics that represent key desiderata of\nsimilarity scores: alignment with human annotations, consistency for data pairs\nirrespective of their order, smoothness of similarity distributions, and\ncontrollability through prompting. Our analysis demonstrates that no model,\nwhether closed- or open-source, is superior on all metrics; the optimal choice\ndepends on an auto evaluator's desired behavior (e.g., a smooth vs. a sharp\njudge), highlighting risks of widespread adoption of VLMs as evaluators without\nthorough assessment. For instance, the majority of VLMs struggle with\nmaintaining symmetric similarity scores regardless of order. Additionally, our\nresults show that the performance of VLMs on the metrics in PairBench closely\ncorrelates with popular benchmarks, showcasing its predictive power in ranking\nmodels.\n","authors":["Aarash Feizi","Sai Rajeswar","Adriana Romero-Soriano","Reihaneh Rabbany","Spandana Gella","Valentina Zantedeschi","João Monteiro"],"pdf_url":"https://arxiv.org/pdf/2502.15210v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.15145v2","updated":"2025-02-24T06:06:04Z","published":"2025-02-21T01:56:52Z","title":"Projection Optimization: A General Framework for Multi-Objective and\n  Multi-Group RLHF","summary":"  Reinforcement Learning with Human Feedback (RLHF) is a widely used\nfine-tuning approach that aligns machine learning model, particularly Language\nModel (LM) with human preferences. There are typically multiple objectives\ndriving the preference, hence humans find it easier to express per-objective\ncomparisons rather than a global preference between two choices.\nMulti-Objective RLHF (MORLHF) aims to use per-objective preference feedback and\nachieve Pareto optimality among these objectives by aggregating them into a\nsingle unified objective for optimization. However, nearly all prior works rely\non linear aggregation, which rules out policies that favor specific objectives\nsuch as the worst one. The only existing approach using non-linear aggregation\nis computationally expensive due to its reward-based nature and the need for\nretraining whenever the aggregation parameters change. In this work, we address\nthis limitation by transforming the non-linear aggregation maximization problem\ninto a series of sub-problems. Each sub-problem involves only linear\naggregation, making it computationally efficient to solve. We further extend\nour framework to handle multi-group scenarios, where each group has distinct\nweights for the objectives. Our method enables achieving consensus or\nmaximizing the aggregated objective across all groups. Theoretically, we\ndemonstrate that our algorithmic framework achieves sublinear regret and can be\neasily adapted to a reward-free algorithm. Empirically, leveraging our\ntheoretical insights, we propose a nearly training-free algorithm once the\noptimal policies for individual objectives are obtained.\n","authors":["Nuoya Xiong","Aarti Singh"],"pdf_url":"https://arxiv.org/pdf/2502.15145v2.pdf","comment":null}]},"2025-02-20T00:00:00Z":{"Computation and Language":[{"id":"http://arxiv.org/abs/2502.15097v1","updated":"2025-02-20T23:30:45Z","published":"2025-02-20T23:30:45Z","title":"LUME: LLM Unlearning with Multitask Evaluations","summary":"  Unlearning aims to remove copyrighted, sensitive, or private content from\nlarge language models (LLMs) without a full retraining. In this work, we\ndevelop a multi-task unlearning benchmark (LUME) which features three tasks:\n(1) unlearn synthetically generated creative short novels, (2) unlearn\nsynthetic biographies with sensitive information, and (3) unlearn a collection\nof public biographies. We further release two fine-tuned LLMs of 1B and 7B\nparameter sizes as the target models. We conduct detailed evaluations of\nseveral recently proposed unlearning algorithms and present results on\ncarefully crafted metrics to understand their behavior and limitations.\n","authors":["Anil Ramakrishna","Yixin Wan","Xiaomeng Jin","Kai-Wei Chang","Zhiqi Bu","Bhanukiran Vinzamuri","Volkan Cevher","Mingyi Hong","Rahul Gupta"],"pdf_url":"https://arxiv.org/pdf/2502.15097v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.04177v2","updated":"2025-02-20T23:26:44Z","published":"2024-02-06T17:31:20Z","title":"Scaling Laws for Downstream Task Performance in Machine Translation","summary":"  Scaling laws provide important insights that can guide the design of large\nlanguage models (LLMs). Existing work has primarily focused on studying scaling\nlaws for pretraining (upstream) loss. However, in transfer learning settings,\nin which LLMs are pretrained on an unsupervised dataset and then finetuned on a\ndownstream task, we often also care about the downstream performance. In this\nwork, we study the scaling behavior in a transfer learning setting, where LLMs\nare finetuned for machine translation tasks. Specifically, we investigate how\nthe choice of the pretraining data and its size affect downstream performance\n(translation quality) as judged by: downstream cross-entropy and translation\nquality metrics such as BLEU and COMET scores. Our experiments indicate that\nthe size of the finetuning dataset and the distribution alignment between the\npretraining and downstream data significantly influence the scaling behavior.\nWith sufficient alignment, both downstream cross-entropy and translation\nquality scores improve monotonically with more pretraining data. In such cases,\nwe show that it is possible to predict the downstream translation quality\nmetrics with good accuracy using a log-law. However, there are cases where\nmoderate misalignment causes the downstream translation scores to fluctuate or\nget worse with more pretraining, whereas downstream cross-entropy monotonically\nimproves. By analyzing these, we provide new practical insights for choosing\nappropriate pretraining data.\n","authors":["Berivan Isik","Natalia Ponomareva","Hussein Hazimeh","Dimitris Paparas","Sergei Vassilvitskii","Sanmi Koyejo"],"pdf_url":"https://arxiv.org/pdf/2402.04177v2.pdf","comment":"Published at the International Conference on Learning Representations\n  (ICLR) 2025. Previous title: \"Scaling Laws for Downstream Task Performance of\n  Large Language Models\""},{"id":"http://arxiv.org/abs/2502.15094v1","updated":"2025-02-20T23:23:45Z","published":"2025-02-20T23:23:45Z","title":"Judging It, Washing It: Scoring and Greenwashing Corporate Climate\n  Disclosures using Large Language Models","summary":"  We study the use of large language models (LLMs) to both evaluate and\ngreenwash corporate climate disclosures. First, we investigate the use of the\nLLM-as-a-Judge (LLMJ) methodology for scoring company-submitted reports on\nemissions reduction targets and progress. Second, we probe the behavior of an\nLLM when it is prompted to greenwash a response subject to accuracy and length\nconstraints. Finally, we test the robustness of the LLMJ methodology against\nresponses that may be greenwashed using an LLM. We find that two LLMJ scoring\nsystems, numerical rating and pairwise comparison, are effective in\ndistinguishing high-performing companies from others, with the pairwise\ncomparison system showing greater robustness against LLM-greenwashed responses.\n","authors":["Marianne Chuang","Gabriel Chuang","Cheryl Chuang","John Chuang"],"pdf_url":"https://arxiv.org/pdf/2502.15094v1.pdf","comment":"16 pages, 12 figures"},{"id":"http://arxiv.org/abs/2502.15092v1","updated":"2025-02-20T23:18:39Z","published":"2025-02-20T23:18:39Z","title":"Optimizing Singular Spectrum for Large Language Model Compression","summary":"  Large language models (LLMs) have demonstrated remarkable capabilities, yet\nprohibitive parameter complexity often hinders their deployment. Existing\nsingular value decomposition (SVD) based compression methods simply deem\nsingular values as importance scores of decomposed components. However, this\nimportance ordered by singular values does not necessarily correlate with the\nperformance of a downstream task. In this work, we introduce SoCo (Singular\nspectrum optimization for large language model Compression), a novel\ncompression framework that learns to rescale the decomposed components of SVD\nin a data-driven manner. Concretely, we employ a learnable diagonal matrix to\nassign importance scores for singular spectrum and develop a three-stage\ntraining process that progressively refines these scores from initial coarse\ncompression to fine-grained sparsification-thereby striking an effective\nbalance between aggressive model compression and performance preservation.\nThanks to the learnable singular spectrum, SoCo adaptively prunes components\naccording to the sparsified importance scores, rather than relying on the fixed\norder of singular values. More importantly, the remaining components with\namplified importance scores can compensate for the loss of the pruned ones.\nExperimental evaluations across multiple LLMs and benchmarks demonstrate that\nSoCo surpasses the state-of-the-art methods in model compression.\n","authors":["Dengjie Li","Tiancheng Shen","Yao Zhou","Baisong Yang","Zhongying Liu","Masheng Yang","Bernard Ghanem","Yibo Yang","Yujie Zhong","Ming-Hsuan Yang"],"pdf_url":"https://arxiv.org/pdf/2502.15092v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.15090v1","updated":"2025-02-20T23:08:03Z","published":"2025-02-20T23:08:03Z","title":"Analyze the Neurons, not the Embeddings: Understanding When and Where\n  LLM Representations Align with Humans","summary":"  Modern large language models (LLMs) achieve impressive performance on some\ntasks, while exhibiting distinctly non-human-like behaviors on others. This\nraises the question of how well the LLM's learned representations align with\nhuman representations. In this work, we introduce a novel approach to the study\nof representation alignment: we adopt a method from research on activation\nsteering to identify neurons responsible for specific concepts (e.g., 'cat')\nand then analyze the corresponding activation patterns. Our findings reveal\nthat LLM representations closely align with human representations inferred from\nbehavioral data. Notably, this alignment surpasses that of word embeddings,\nwhich have been center stage in prior work on human and model alignment.\nAdditionally, our approach enables a more granular view of how LLMs represent\nconcepts. Specifically, we show that LLMs organize concepts in a way that\nreflects hierarchical relationships interpretable to humans (e.g.,\n'animal'-'dog').\n","authors":["Masha Fedzechkina","Eleonora Gualdoni","Sinead Williamson","Katherine Metcalf","Skyler Seto","Barry-John Theobald"],"pdf_url":"https://arxiv.org/pdf/2502.15090v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.15086v1","updated":"2025-02-20T22:58:44Z","published":"2025-02-20T22:58:44Z","title":"Is Safety Standard Same for Everyone? User-Specific Safety Evaluation of\n  Large Language Models","summary":"  As the use of large language model (LLM) agents continues to grow, their\nsafety vulnerabilities have become increasingly evident. Extensive benchmarks\nevaluate various aspects of LLM safety by defining the safety relying heavily\non general standards, overlooking user-specific standards. However, safety\nstandards for LLM may vary based on a user-specific profiles rather than being\nuniversally consistent across all users. This raises a critical research\nquestion: Do LLM agents act safely when considering user-specific safety\nstandards? Despite its importance for safe LLM use, no benchmark datasets\ncurrently exist to evaluate the user-specific safety of LLMs. To address this\ngap, we introduce U-SAFEBENCH, the first benchmark designed to assess\nuser-specific aspect of LLM safety. Our evaluation of 18 widely used LLMs\nreveals current LLMs fail to act safely when considering user-specific safety\nstandards, marking a new discovery in this field. To address this\nvulnerability, we propose a simple remedy based on chain-of-thought,\ndemonstrating its effectiveness in improving user-specific safety. Our\nbenchmark and code are available at https://github.com/yeonjun-in/U-SafeBench.\n","authors":["Yeonjun In","Wonjoong Kim","Kanghoon Yoon","Sungchul Kim","Mehrab Tanjim","Kibum Kim","Chanyoung Park"],"pdf_url":"https://arxiv.org/pdf/2502.15086v1.pdf","comment":"Under review"},{"id":"http://arxiv.org/abs/2502.15082v1","updated":"2025-02-20T22:51:10Z","published":"2025-02-20T22:51:10Z","title":"UPCORE: Utility-Preserving Coreset Selection for Balanced Unlearning","summary":"  User specifications or legal frameworks often require information to be\nremoved from pretrained models, including large language models (LLMs). This\nrequires deleting or \"forgetting\" a set of data points from an already-trained\nmodel, which typically degrades its performance on other data points. Thus, a\nbalance must be struck between removing information and keeping the model's\nother abilities intact, with a failure to balance this trade-off leading to\npoor deletion or an unusable model. To this end, we propose UPCORE\n(Utility-Preserving Coreset Selection), a method-agnostic data selection\nframework for mitigating collateral damage during unlearning. Finding that the\nmodel damage is correlated with the variance of the model's representations on\nthe forget set, we selectively prune the forget set to remove outliers, thereby\nminimizing model degradation after unlearning. We evaluate UPCORE across three\nstandard unlearning methods consistently achieving a superior balance between\nthe competing objectives of deletion efficacy and model preservation. To better\nevaluate this trade-off, we introduce a new metric, measuring the\narea-under-the-curve (AUC) across standard metrics. We find that UPCORE\nimproves both standard metrics and AUC, benefitting from positive transfer\nbetween the coreset and pruned points while reducing negative transfer from the\nforget set to points outside of it.\n","authors":["Vaidehi Patil","Elias Stengel-Eskin","Mohit Bansal"],"pdf_url":"https://arxiv.org/pdf/2502.15082v1.pdf","comment":"Code: https://github.com/Vaidehi99/UPCORE"},{"id":"http://arxiv.org/abs/2502.15079v1","updated":"2025-02-20T22:43:22Z","published":"2025-02-20T22:43:22Z","title":"Can Hallucination Correction Improve Video-Language Alignment?","summary":"  Large Vision-Language Models often generate hallucinated content that is not\ngrounded in its visual inputs. While prior work focuses on mitigating\nhallucinations, we instead explore leveraging hallucination correction as a\ntraining objective to improve video-language alignment. We introduce HACA, a\nself-training framework learning to correct hallucinations in descriptions that\ndo not align with the video content. By identifying and correcting\ninconsistencies, HACA enhances the model's ability to align video and textual\nrepresentations for spatio-temporal reasoning. Our experimental results show\nconsistent gains in video-caption binding and text-to-video retrieval tasks,\ndemonstrating that hallucination correction-inspired tasks serve as an\neffective strategy for improving vision and language alignment.\n","authors":["Lingjun Zhao","Mingyang Xie","Paola Cascante-Bonilla","Hal Daumé III","Kwonjoon Lee"],"pdf_url":"https://arxiv.org/pdf/2502.15079v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.15069v1","updated":"2025-02-20T22:02:52Z","published":"2025-02-20T22:02:52Z","title":"Rare Disease Differential Diagnosis with Large Language Models at Scale:\n  From Abdominal Actinomycosis to Wilson's Disease","summary":"  Large language models (LLMs) have demonstrated impressive capabilities in\ndisease diagnosis. However, their effectiveness in identifying rarer diseases,\nwhich are inherently more challenging to diagnose, remains an open question.\nRare disease performance is critical with the increasing use of LLMs in\nhealthcare settings. This is especially true if a primary care physician needs\nto make a rarer prognosis from only a patient conversation so that they can\ntake the appropriate next step. To that end, several clinical decision support\nsystems are designed to support providers in rare disease identification. Yet\ntheir utility is limited due to their lack of knowledge of common disorders and\ndifficulty of use.\n  In this paper, we propose RareScale to combine the knowledge LLMs with expert\nsystems. We use jointly use an expert system and LLM to simulate rare disease\nchats. This data is used to train a rare disease candidate predictor model.\nCandidates from this smaller model are then used as additional inputs to\nblack-box LLM to make the final differential diagnosis. Thus, RareScale allows\nfor a balance between rare and common diagnoses. We present results on over 575\nrare diseases, beginning with Abdominal Actinomycosis and ending with Wilson's\nDisease. Our approach significantly improves the baseline performance of\nblack-box LLMs by over 17% in Top-5 accuracy. We also find that our candidate\ngeneration performance is high (e.g. 88.8% on gpt-4o generated chats).\n","authors":["Elliot Schumacher","Dhruv Naik","Anitha Kannan"],"pdf_url":"https://arxiv.org/pdf/2502.15069v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.05893v5","updated":"2025-02-20T21:57:27Z","published":"2024-04-08T22:29:53Z","title":"Use of a Structured Knowledge Base Enhances Metadata Curation by Large\n  Language Models","summary":"  Metadata play a crucial role in ensuring the findability, accessibility,\ninteroperability, and reusability of datasets. This paper investigates the\npotential of large language models (LLMs), specifically GPT-4, to improve\nadherence to metadata standards. We conducted experiments on 200 random data\nrecords describing human samples relating to lung cancer from the NCBI\nBioSample repository, evaluating GPT-4's ability to suggest edits for adherence\nto metadata standards. We computed the adherence accuracy of field name-field\nvalue pairs through a peer review process, and we observed a marginal average\nimprovement in adherence to the standard data dictionary from 79% to 80%\n(p<0.5). We then prompted GPT-4 with domain information in the form of the\ntextual descriptions of CEDAR templates and recorded a significant improvement\nto 97% from 79% (p<0.01). These results indicate that, while LLMs may not be\nable to correct legacy metadata to ensure satisfactory adherence to standards\nwhen unaided, they do show promise for use in automated metadata curation when\nintegrated with a structured knowledge base\n","authors":["Sowmya S. Sundaram","Benjamin Solomon","Avani Khatri","Anisha Laumas","Purvesh Khatri","Mark A. Musen"],"pdf_url":"https://arxiv.org/pdf/2404.05893v5.pdf","comment":null}],"Computer Vision and Pattern Recognition":[{"id":"http://arxiv.org/abs/2402.10093v4","updated":"2025-02-20T23:59:44Z","published":"2024-02-15T16:46:16Z","title":"MIM-Refiner: A Contrastive Learning Boost from Intermediate Pre-Trained\n  Representations","summary":"  We introduce MIM (Masked Image Modeling)-Refiner, a contrastive learning\nboost for pre-trained MIM models. MIM-Refiner is motivated by the insight that\nstrong representations within MIM models generally reside in intermediate\nlayers. Accordingly, MIM-Refiner leverages multiple contrastive heads that are\nconnected to different intermediate layers. In each head, a modified nearest\nneighbor objective constructs semantic clusters that capture semantic\ninformation which improves performance on downstream tasks, including\noff-the-shelf and fine-tuning settings.\n  The refinement process is short and simple - yet highly effective. Within a\nfew epochs, we refine the features of MIM models from subpar to\nstate-of-the-art, off-the-shelf features. Refining a ViT-H, pre-trained with\ndata2vec 2.0 on ImageNet-1K, sets a new state-of-the-art in linear probing\n(84.7%) and low-shot classification among models that are pre-trained on\nImageNet-1K. MIM-Refiner efficiently combines the advantages of MIM and ID\nobjectives and compares favorably against previous state-of-the-art SSL models\non a variety of benchmarks such as low-shot classification, long-tailed\nclassification, clustering and semantic segmentation.\n","authors":["Benedikt Alkin","Lukas Miklautz","Sepp Hochreiter","Johannes Brandstetter"],"pdf_url":"https://arxiv.org/pdf/2402.10093v4.pdf","comment":"Published as a conference paper at ICLR 2025. Github:\n  https://github.com/ml-jku/MIM-Refiner"},{"id":"http://arxiv.org/abs/2406.04303v3","updated":"2025-02-20T23:20:40Z","published":"2024-06-06T17:49:21Z","title":"Vision-LSTM: xLSTM as Generic Vision Backbone","summary":"  Transformers are widely used as generic backbones in computer vision, despite\ninitially introduced for natural language processing. Recently, the Long\nShort-Term Memory (LSTM) has been extended to a scalable and performant\narchitecture - the xLSTM - which overcomes long-standing LSTM limitations via\nexponential gating and parallelizable matrix memory structure. In this report,\nwe introduce Vision-LSTM (ViL), an adaption of the xLSTM building blocks to\ncomputer vision. ViL comprises a stack of xLSTM blocks where odd blocks process\nthe sequence of patch tokens from top to bottom while even blocks go from\nbottom to top. Experiments show that ViL holds promise to be further deployed\nas new generic backbone for computer vision architectures.\n","authors":["Benedikt Alkin","Maximilian Beck","Korbinian Pöppel","Sepp Hochreiter","Johannes Brandstetter"],"pdf_url":"https://arxiv.org/pdf/2406.04303v3.pdf","comment":"Published as a conference paper at ICLR 2025, Github:\n  https://github.com/NX-AI/vision-lstm"},{"id":"http://arxiv.org/abs/2502.15079v1","updated":"2025-02-20T22:43:22Z","published":"2025-02-20T22:43:22Z","title":"Can Hallucination Correction Improve Video-Language Alignment?","summary":"  Large Vision-Language Models often generate hallucinated content that is not\ngrounded in its visual inputs. While prior work focuses on mitigating\nhallucinations, we instead explore leveraging hallucination correction as a\ntraining objective to improve video-language alignment. We introduce HACA, a\nself-training framework learning to correct hallucinations in descriptions that\ndo not align with the video content. By identifying and correcting\ninconsistencies, HACA enhances the model's ability to align video and textual\nrepresentations for spatio-temporal reasoning. Our experimental results show\nconsistent gains in video-caption binding and text-to-video retrieval tasks,\ndemonstrating that hallucination correction-inspired tasks serve as an\neffective strategy for improving vision and language alignment.\n","authors":["Lingjun Zhao","Mingyang Xie","Paola Cascante-Bonilla","Hal Daumé III","Kwonjoon Lee"],"pdf_url":"https://arxiv.org/pdf/2502.15079v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.00259v2","updated":"2025-02-20T22:42:18Z","published":"2024-06-01T01:49:27Z","title":"PuzzleFusion++: Auto-agglomerative 3D Fracture Assembly by Denoise and\n  Verify","summary":"  This paper proposes a novel \"auto-agglomerative\" 3D fracture assembly method,\nPuzzleFusion++, resembling how humans solve challenging spatial puzzles.\nStarting from individual fragments, the approach 1) aligns and merges fragments\ninto larger groups akin to agglomerative clustering and 2) repeats the process\niteratively in completing the assembly akin to auto-regressive methods.\nConcretely, a diffusion model denoises the 6-DoF alignment parameters of the\nfragments simultaneously, and a transformer model verifies and merges pairwise\nalignments into larger ones, whose process repeats iteratively. Extensive\nexperiments on the Breaking Bad dataset show that PuzzleFusion++ outperforms\nall other state-of-the-art techniques by significant margins across all\nmetrics, in particular by over 10% in part accuracy and 50% in Chamfer\ndistance. The code will be available on our project page:\nhttps://puzzlefusion-plusplus.github.io.\n","authors":["Zhengqing Wang","Jiacheng Chen","Yasutaka Furukawa"],"pdf_url":"https://arxiv.org/pdf/2406.00259v2.pdf","comment":"Project page: https://puzzlefusion-plusplus.github.io"},{"id":"http://arxiv.org/abs/2502.15077v1","updated":"2025-02-20T22:29:24Z","published":"2025-02-20T22:29:24Z","title":"Hardware-Friendly Static Quantization Method for Video Diffusion\n  Transformers","summary":"  Diffusion Transformers for video generation have gained significant research\ninterest since the impressive performance of SORA. Efficient deployment of such\ngenerative-AI models on GPUs has been demonstrated with dynamic quantization.\nHowever, resource-constrained devices cannot support dynamic quantization, and\nneed static quantization of the models for their efficient deployment on AI\nprocessors. In this paper, we propose a novel method for the post-training\nquantization of OpenSora\\cite{opensora}, a Video Diffusion Transformer, without\nrelying on dynamic quantization techniques. Our approach employs static\nquantization, achieving video quality comparable to FP16 and dynamically\nquantized ViDiT-Q methods, as measured by CLIP, and VQA metrics. In particular,\nwe utilize per-step calibration data to adequately provide a post-training\nstatically quantized model for each time step, incorporating channel-wise\nquantization for weights and tensor-wise quantization for activations. By\nfurther applying the smooth-quantization technique, we can obtain high-quality\nvideo outputs with the statically quantized models. Extensive experimental\nresults demonstrate that static quantization can be a viable alternative to\ndynamic quantization for video diffusion transformers, offering a more\nefficient approach without sacrificing performance.\n","authors":["Sanghyun Yi","Qingfeng Liu","Mostafa El-Khamy"],"pdf_url":"https://arxiv.org/pdf/2502.15077v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.15076v1","updated":"2025-02-20T22:27:42Z","published":"2025-02-20T22:27:42Z","title":"Synth It Like KITTI: Synthetic Data Generation for Object Detection in\n  Driving Scenarios","summary":"  An important factor in advancing autonomous driving systems is simulation.\nYet, there is rather small progress for transferability between the virtual and\nreal world. We revisit this problem for 3D object detection on LiDAR point\nclouds and propose a dataset generation pipeline based on the CARLA simulator.\nUtilizing domain randomization strategies and careful modeling, we are able to\ntrain an object detector on the synthetic data and demonstrate strong\ngeneralization capabilities to the KITTI dataset. Furthermore, we compare\ndifferent virtual sensor variants to gather insights, which sensor attributes\ncan be responsible for the prevalent domain gap. Finally, fine-tuning with a\nsmall portion of real data almost matches the baseline and with the full\ntraining set slightly surpasses it.\n","authors":["Richard Marcus","Christian Vogel","Inga Jatzkowski","Niklas Knoop","Marc Stamminger"],"pdf_url":"https://arxiv.org/pdf/2502.15076v1.pdf","comment":"Preprint, to appear in ROBOVIS 2025"},{"id":"http://arxiv.org/abs/2409.19178v2","updated":"2025-02-20T21:07:20Z","published":"2024-09-27T23:18:27Z","title":"FLINT: Learning-based Flow Estimation and Temporal Interpolation for\n  Scientific Ensemble Visualization","summary":"  We present FLINT (learning-based FLow estimation and temporal INTerpolation),\na novel deep learning-based approach to estimate flow fields for 2D+time and\n3D+time scientific ensemble data. FLINT can flexibly handle different types of\nscenarios with (1) a flow field being partially available for some members\n(e.g., omitted due to space constraints) or (2) no flow field being available\nat all (e.g., because it could not be acquired during an experiment). The\ndesign of our architecture allows to flexibly cater to both cases simply by\nadapting our modular loss functions, effectively treating the different\nscenarios as flow-supervised and flow-unsupervised problems, respectively (with\nrespect to the presence or absence of ground-truth flow). To the best of our\nknowledge, FLINT is the first approach to perform flow estimation from\nscientific ensembles, generating a corresponding flow field for each discrete\ntimestep, even in the absence of original flow information. Additionally, FLINT\nproduces high-quality temporal interpolants between scalar fields. FLINT\nemploys several neural blocks, each featuring several convolutional and\ndeconvolutional layers. We demonstrate performance and accuracy for different\nusage scenarios with scientific ensembles from both simulations and\nexperiments.\n","authors":["Hamid Gadirov","Jos B. T. M. Roerdink","Steffen Frey"],"pdf_url":"https://arxiv.org/pdf/2409.19178v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.18783v2","updated":"2025-02-20T21:00:50Z","published":"2025-01-30T22:19:15Z","title":"RUN: Reversible Unfolding Network for Concealed Object Segmentation","summary":"  Existing concealed object segmentation (COS) methods frequently utilize\nreversible strategies to address uncertain regions. However, these approaches\nare typically restricted to the mask domain, leaving the potential of the RGB\ndomain underexplored. To address this, we propose the Reversible Unfolding\nNetwork (RUN), which applies reversible strategies across both mask and RGB\ndomains through a theoretically grounded framework, enabling accurate\nsegmentation. RUN first formulates a novel COS model by incorporating an extra\nresidual sparsity constraint to minimize segmentation uncertainties. The\niterative optimization steps of the proposed model are then unfolded into a\nmultistage network, with each step corresponding to a stage. Each stage of RUN\nconsists of two reversible modules: the Segmentation-Oriented Foreground\nSeparation (SOFS) module and the Reconstruction-Oriented Background Extraction\n(ROBE) module. SOFS applies the reversible strategy at the mask level and\nintroduces Reversible State Space to capture non-local information. ROBE\nextends this to the RGB domain, employing a reconstruction network to address\nconflicting foreground and background regions identified as distortion-prone\nareas, which arise from their separate estimation by independent modules. As\nthe stages progress, RUN gradually facilitates reversible modeling of\nforeground and background in both the mask and RGB domains, directing the\nnetwork's attention to uncertain regions and mitigating false-positive and\nfalse-negative results. Extensive experiments demonstrate the superior\nperformance of RUN and highlight the potential of unfolding-based frameworks\nfor COS and other high-level vision tasks. We will release the code and models.\n","authors":["Chunming He","Rihan Zhang","Fengyang Xiao","Chengyu Fang","Longxiang Tang","Yulun Zhang","Linghe Kong","Deng-Ping Fan","Kai Li","Sina Farsiu"],"pdf_url":"https://arxiv.org/pdf/2501.18783v2.pdf","comment":"13 tables, 8 figures"},{"id":"http://arxiv.org/abs/2502.15039v1","updated":"2025-02-20T20:51:31Z","published":"2025-02-20T20:51:31Z","title":"Fostering Inclusion: A Virtual Reality Experience to Raise Awareness of\n  Dyslexia-Related Barriers in University Settings","summary":"  This work introduces the design, implementation, and validation of a virtual\nreality (VR) experience aimed at promoting the inclusion of individuals with\ndyslexia in university settings. Unlike traditional awareness methods, this\nimmersive approach offers a novel way to foster empathy by allowing\nparticipants to experience firsthand the challenges faced by students with\ndyslexia. Specifically, the experience raises awareness by exposing\nnon-dyslexic individuals to the difficulties commonly encountered by dyslexic\nstudents. In the virtual environment, participants explore a virtual campus\nwith multiple buildings, navigating between them while completing tasks and\nsimultaneously encountering barriers that simulate some of the challenges faced\nby individuals with dyslexia. These barriers include reading signs with\nshifting letters, following directional arrows that may point incorrectly, and\ndealing with a lack of assistance. The campus is a comprehensive model\nfeaturing both indoor and outdoor spaces and supporting various modes of\nlocomotion. To validate the experience, more than 30 non-dyslexic participants\nfrom the university environment, mainly professors and students, evaluated it\nthrough ad hoc satisfaction surveys. The results indicated heightened awareness\nof the barriers encountered by students with dyslexia, with participants\ndeeming the experience a valuable tool for increasing visibility and fostering\nunderstanding of dyslexic students.\n","authors":["José Manuel Alcalde-Llergo","Pilar Aparicio-Martínez","Andrea Zingoni","Sara Pinzi","Enrique Yeguas-Bolívar"],"pdf_url":"https://arxiv.org/pdf/2502.15039v1.pdf","comment":"20 pages, 9 figures"},{"id":"http://arxiv.org/abs/2411.17251v6","updated":"2025-02-20T20:34:39Z","published":"2024-11-26T09:29:27Z","title":"DGNN-YOLO: Interpretable Dynamic Graph Neural Networks with YOLO11 for\n  Small Occluded Object Detection and Tracking","summary":"  The detection and tracking of small, occluded objects such as pedestrians,\ncyclists, and motorbikes pose significant challenges for traffic surveillance\nsystems because of their erratic movement, frequent occlusion, and poor\nvisibility in dynamic urban environments. Traditional methods like YOLO11,\nwhile proficient in spatial feature extraction for precise detection, often\nstruggle with these small and dynamically moving objects, particularly in\nhandling real-time data updates and resource efficiency. This paper introduces\nDGNN-YOLO, a novel framework that integrates dynamic graph neural networks\n(DGNNs) with YOLO11 to address these limitations. Unlike standard GNNs, DGNNs\nare chosen for their superior ability to dynamically update graph structures in\nreal-time, which enables adaptive and robust tracking of objects in highly\nvariable urban traffic scenarios. This framework constructs and regularly\nupdates its graph representations, capturing objects as nodes and their\ninteractions as edges, thus effectively responding to rapidly changing\nconditions. Additionally, DGNN-YOLO incorporates Grad-CAM, Grad-CAM++, and\nEigen-CAM visualization techniques to enhance interpretability and foster\ntrust, offering insights into the model's decision-making process. Extensive\nexperiments validate the framework's performance, achieving a precision of\n0.8382, recall of 0.6875, and mAP@0.5:0.95 of 0.6476, significantly\noutperforming existing methods. This study offers a scalable and interpretable\nsolution for real-time traffic surveillance and significantly advances\nintelligent transportation systems' capabilities by addressing the critical\nchallenge of detecting and tracking small, occluded objects.\n","authors":["Shahriar Soudeep","M. F. Mridha","Md Abrar Jahin","Nilanjan Dey"],"pdf_url":"https://arxiv.org/pdf/2411.17251v6.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.15027v1","updated":"2025-02-20T20:27:06Z","published":"2025-02-20T20:27:06Z","title":"InterFeedback: Unveiling Interactive Intelligence of Large Multimodal\n  Models via Human Feedback","summary":"  Existing benchmarks do not test Large Multimodal Models (LMMs) on their\ninteractive intelligence with human users which is vital for developing\ngeneral-purpose AI assistants. We design InterFeedback, an interactive\nframework, which can be applied to any LMM and dataset to assess this ability\nautonomously. On top of this, we introduce InterFeedback-Bench which evaluates\ninteractive intelligence using two representative datasets, MMMU-Pro and\nMathVerse, to test 10 different open-source LMMs. Additionally, we present\nInterFeedback-Human, a newly collected dataset of 120 cases designed for\nmanually testing interactive performance in leading models such as OpenAI-o1\nand Claude-3.5-Sonnet. Our evaluation results show that even state-of-the-art\nLMM (like OpenAI-o1) can correct their results through human feedback less than\n50%. Our findings point to the need for methods that can enhance the LMMs'\ncapability to interpret and benefit from feedback.\n","authors":["Henry Hengyuan Zhao","Wenqi Pei","Yifei Tao","Haiyang Mei","Mike Zheng Shou"],"pdf_url":"https://arxiv.org/pdf/2502.15027v1.pdf","comment":"18 pages, 10 figures"},{"id":"http://arxiv.org/abs/2502.15021v1","updated":"2025-02-20T20:16:10Z","published":"2025-02-20T20:16:10Z","title":"Simpler Fast Vision Transformers with a Jumbo CLS Token","summary":"  We introduce a simple enhancement to the global processing of vision\ntransformers (ViTs) to improve accuracy while maintaining throughput. Our\napproach, Jumbo, creates a wider CLS token, which is split to match the patch\ntoken width before attention, processed with self-attention, and reassembled.\nAfter attention, Jumbo applies a dedicated, wider FFN to this token. Jumbo\nsignificantly improves over ViT+Registers on ImageNet-1K at high speeds (by\n3.2% for ViT-tiny and 13.5% for ViT-nano); these Jumbo models even outperform\nspecialized compute-efficient models while preserving the architectural\nadvantages of plain ViTs. Although Jumbo sees no gains for ViT-small on\nImageNet-1K, it gains 3.4% on ImageNet-21K over ViT+Registers. Both findings\nindicate that Jumbo is most helpful when the ViT is otherwise too narrow for\nthe task. Finally, we show that Jumbo can be easily adapted to excel on data\nbeyond images, e.g., time series.\n","authors":["Anthony Fuller","Yousef Yassin","Daniel G. Kyrollos","Evan Shelhamer","James R. Green"],"pdf_url":"https://arxiv.org/pdf/2502.15021v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.15011v1","updated":"2025-02-20T20:05:30Z","published":"2025-02-20T20:05:30Z","title":"CrossOver: 3D Scene Cross-Modal Alignment","summary":"  Multi-modal 3D object understanding has gained significant attention, yet\ncurrent approaches often assume complete data availability and rigid alignment\nacross all modalities. We present CrossOver, a novel framework for cross-modal\n3D scene understanding via flexible, scene-level modality alignment. Unlike\ntraditional methods that require aligned modality data for every object\ninstance, CrossOver learns a unified, modality-agnostic embedding space for\nscenes by aligning modalities - RGB images, point clouds, CAD models,\nfloorplans, and text descriptions - with relaxed constraints and without\nexplicit object semantics. Leveraging dimensionality-specific encoders, a\nmulti-stage training pipeline, and emergent cross-modal behaviors, CrossOver\nsupports robust scene retrieval and object localization, even with missing\nmodalities. Evaluations on ScanNet and 3RScan datasets show its superior\nperformance across diverse metrics, highlighting adaptability for real-world\napplications in 3D scene understanding.\n","authors":["Sayan Deb Sarkar","Ondrej Miksik","Marc Pollefeys","Daniel Barath","Iro Armeni"],"pdf_url":"https://arxiv.org/pdf/2502.15011v1.pdf","comment":"Project Page: sayands.github.io/crossover/"},{"id":"http://arxiv.org/abs/2502.15004v1","updated":"2025-02-20T19:58:11Z","published":"2025-02-20T19:58:11Z","title":"Digital implementations of deep feature extractors are intrinsically\n  informative","summary":"  Rapid information (energy) propagation in deep feature extractors is crucial\nto balance computational complexity versus expressiveness as a representation\nof the input. We prove an upper bound for the speed of energy propagation in a\nunified framework that covers different neural network models, both over\nEuclidean and non-Euclidean domains. Additional structural information about\nthe signal domain can be used to explicitly determine or improve the rate of\ndecay. To illustrate this, we show global exponential energy decay for a range\nof 1) feature extractors with discrete-domain input signals, and 2)\nconvolutional neural networks (CNNs) via scattering over locally compact\nabelian (LCA) groups.\n","authors":["Max Getter"],"pdf_url":"https://arxiv.org/pdf/2502.15004v1.pdf","comment":"6 pages"},{"id":"http://arxiv.org/abs/2502.14996v1","updated":"2025-02-20T19:38:52Z","published":"2025-02-20T19:38:52Z","title":"A Rapid Test for Accuracy and Bias of Face Recognition Technology","summary":"  Measuring the accuracy of face recognition (FR) systems is essential for\nimproving performance and ensuring responsible use. Accuracy is typically\nestimated using large annotated datasets, which are costly and difficult to\nobtain. We propose a novel method for 1:1 face verification that benchmarks FR\nsystems quickly and without manual annotation, starting from approximate labels\n(e.g., from web search results). Unlike previous methods for training set label\ncleaning, ours leverages the embedding representation of the models being\nevaluated, achieving high accuracy in smaller-sized test datasets. Our approach\nreliably estimates FR accuracy and ranking, significantly reducing the time and\ncost of manual labeling. We also introduce the first public benchmark of five\nFR cloud services, revealing demographic biases, particularly lower accuracy\nfor Asian women. Our rapid test method can democratize FR testing, promoting\nscrutiny and responsible use of the technology. Our method is provided as a\npublicly accessible tool at https://github.com/caltechvisionlab/frt-rapid-test\n","authors":["Manuel Knott","Ignacio Serna","Ethan Mann","Pietro Perona"],"pdf_url":"https://arxiv.org/pdf/2502.14996v1.pdf","comment":"Accepted as a conference paper for WACV 2025. Manuel Knott, Ignacio\n  Serna, and Ethan Mann contributed equally"},{"id":"http://arxiv.org/abs/2502.14994v1","updated":"2025-02-20T19:34:58Z","published":"2025-02-20T19:34:58Z","title":"LAVID: An Agentic LVLM Framework for Diffusion-Generated Video Detection","summary":"  The impressive achievements of generative models in creating high-quality\nvideos have raised concerns about digital integrity and privacy\nvulnerabilities. Recent works of AI-generated content detection have been\nwidely studied in the image field (e.g., deepfake), yet the video field has\nbeen unexplored. Large Vision Language Model (LVLM) has become an emerging tool\nfor AI-generated content detection for its strong reasoning and multimodal\ncapabilities. It breaks the limitations of traditional deep learning based\nmethods faced with like lack of transparency and inability to recognize new\nartifacts. Motivated by this, we propose LAVID, a novel LVLMs-based\nai-generated video detection with explicit knowledge enhancement. Our insight\nlist as follows: (1) The leading LVLMs can call external tools to extract\nuseful information to facilitate its own video detection task; (2) Structuring\nthe prompt can affect LVLM's reasoning ability to interpret information in\nvideo content. Our proposed pipeline automatically selects a set of explicit\nknowledge tools for detection, and then adaptively adjusts the structure prompt\nby self-rewriting. Different from prior SOTA that trains additional detectors,\nour method is fully training-free and only requires inference of the LVLM for\ndetection. To facilitate our research, we also create a new benchmark \\vidfor\nwith high-quality videos generated from multiple sources of video generation\ntools. Evaluation results show that LAVID improves F1 scores by 6.2 to 30.2%\nover the top baselines on our datasets across four SOTA LVLMs.\n","authors":["Qingyuan Liu","Yun-Yun Tsai","Ruijian Zha","Victoria Li","Pengyuan Shi","Chengzhi Mao","Junfeng Yang"],"pdf_url":"https://arxiv.org/pdf/2502.14994v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.14992v1","updated":"2025-02-20T19:29:08Z","published":"2025-02-20T19:29:08Z","title":"Ultra-High-Frequency Harmony: mmWave Radar and Event Camera Orchestrate\n  Accurate Drone Landing","summary":"  For precise, efficient, and safe drone landings, ground platforms should\nreal-time, accurately locate descending drones and guide them to designated\nspots. While mmWave sensing combined with cameras improves localization\naccuracy, the lower sampling frequency of traditional frame cameras compared to\nmmWave radar creates bottlenecks in system throughput. In this work, we replace\nthe traditional frame camera with event camera, a novel sensor that harmonizes\nin sampling frequency with mmWave radar within the ground platform setup, and\nintroduce mmE-Loc, a high-precision, low-latency ground localization system\ndesigned for drone landings. To fully leverage the \\textit{temporal\nconsistency} and \\textit{spatial complementarity} between these modalities, we\npropose two innovative modules, \\textit{consistency-instructed collaborative\ntracking} and \\textit{graph-informed adaptive joint optimization}, for accurate\ndrone measurement extraction and efficient sensor fusion. Extensive real-world\nexperiments in landing scenarios from a leading drone delivery company\ndemonstrate that mmE-Loc outperforms state-of-the-art methods in both\nlocalization accuracy and latency.\n","authors":["Haoyang Wang","Jingao Xu","Xinyu Luo","Xuecheng Chen","Ting Zhang","Ruiyang Duan","Yunhao Liu","Xinlei Chen"],"pdf_url":"https://arxiv.org/pdf/2502.14992v1.pdf","comment":"This paper is accepted by ACM SenSys 2025"},{"id":"http://arxiv.org/abs/2408.10575v2","updated":"2025-02-20T19:28:55Z","published":"2024-08-20T06:30:37Z","title":"MUSE: Mamba is Efficient Multi-scale Learner for Text-video Retrieval","summary":"  Text-Video Retrieval (TVR) aims to align and associate relevant video content\nwith corresponding natural language queries. Most existing TVR methods are\nbased on large-scale pre-trained vision-language models (e.g., CLIP). However,\ndue to the inherent plain structure of CLIP, few TVR methods explore the\nmulti-scale representations which offer richer contextual information for a\nmore thorough understanding. To this end, we propose MUSE, a multi-scale mamba\nwith linear computational complexity for efficient cross-resolution modeling.\nSpecifically, the multi-scale representations are generated by applying a\nfeature pyramid on the last single-scale feature map. Then, we employ the Mamba\nstructure as an efficient multi-scale learner to jointly learn scale-wise\nrepresentations. Furthermore, we conduct comprehensive studies to investigate\ndifferent model structures and designs. Extensive results on three popular\nbenchmarks have validated the superiority of MUSE.\n","authors":["Haoran Tang","Meng Cao","Jinfa Huang","Ruyang Liu","Peng Jin","Ge Li","Xiaodan Liang"],"pdf_url":"https://arxiv.org/pdf/2408.10575v2.pdf","comment":"Accepted by AAAI 2025"},{"id":"http://arxiv.org/abs/2502.14977v1","updated":"2025-02-20T19:13:29Z","published":"2025-02-20T19:13:29Z","title":"Few-shot Species Range Estimation","summary":"  Knowing where a particular species can or cannot be found on Earth is crucial\nfor ecological research and conservation efforts. By mapping the spatial ranges\nof all species, we would obtain deeper insights into how global biodiversity is\naffected by climate change and habitat loss. However, accurate range estimates\nare only available for a relatively small proportion of all known species. For\nthe majority of the remaining species, we often only have a small number of\nrecords denoting the spatial locations where they have previously been\nobserved. We outline a new approach for few-shot species range estimation to\naddress the challenge of accurately estimating the range of a species from\nlimited data. During inference, our model takes a set of spatial locations as\ninput, along with optional metadata such as text or an image, and outputs a\nspecies encoding that can be used to predict the range of a previously unseen\nspecies in feed-forward manner. We validate our method on two challenging\nbenchmarks, where we obtain state-of-the-art range estimation performance, in a\nfraction of the compute time, compared to recent alternative approaches.\n","authors":["Christian Lange","Max Hamilton","Elijah Cole","Alexander Shepard","Samuel Heinrich","Angela Zhu","Subhransu Maji","Grant Van Horn","Oisin Mac Aodha"],"pdf_url":"https://arxiv.org/pdf/2502.14977v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.14976v1","updated":"2025-02-20T19:10:51Z","published":"2025-02-20T19:10:51Z","title":"EigenShield: Causal Subspace Filtering via Random Matrix Theory for\n  Adversarially Robust Vision-Language Models","summary":"  Vision-Language Models (VLMs) inherit adversarial vulnerabilities of Large\nLanguage Models (LLMs), which are further exacerbated by their multimodal\nnature. Existing defenses, including adversarial training, input\ntransformations, and heuristic detection, are computationally expensive,\narchitecture-dependent, and fragile against adaptive attacks. We introduce\nEigenShield, an inference-time defense leveraging Random Matrix Theory to\nquantify adversarial disruptions in high-dimensional VLM representations.\nUnlike prior methods that rely on empirical heuristics, EigenShield employs the\nspiked covariance model to detect structured spectral deviations. Using a\nRobustness-based Nonconformity Score (RbNS) and quantile-based thresholding, it\nseparates causal eigenvectors, which encode semantic information, from\ncorrelational eigenvectors that are susceptible to adversarial artifacts. By\nprojecting embeddings onto the causal subspace, EigenShield filters adversarial\nnoise without modifying model parameters or requiring adversarial training.\nThis architecture-independent, attack-agnostic approach significantly reduces\nthe attack success rate, establishing spectral analysis as a principled\nalternative to conventional defenses. Our results demonstrate that EigenShield\nconsistently outperforms all existing defenses, including adversarial training,\nUNIGUARD, and CIDER.\n","authors":["Nastaran Darabi","Devashri Naik","Sina Tayebati","Dinithi Jayasuriya","Ranganath Krishnan","Amit Ranjan Trivedi"],"pdf_url":"https://arxiv.org/pdf/2502.14976v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.14865v1","updated":"2025-02-20T18:59:51Z","published":"2025-02-20T18:59:51Z","title":"Time Travel: A Comprehensive Benchmark to Evaluate LMMs on Historical\n  and Cultural Artifacts","summary":"  Understanding historical and cultural artifacts demands human expertise and\nadvanced computational techniques, yet the process remains complex and\ntime-intensive. While large multimodal models offer promising support, their\nevaluation and improvement require a standardized benchmark. To address this,\nwe introduce TimeTravel, a benchmark of 10,250 expert-verified samples spanning\n266 distinct cultures across 10 major historical regions. Designed for\nAI-driven analysis of manuscripts, artworks, inscriptions, and archaeological\ndiscoveries, TimeTravel provides a structured dataset and robust evaluation\nframework to assess AI models' capabilities in classification, interpretation,\nand historical comprehension. By integrating AI with historical research,\nTimeTravel fosters AI-powered tools for historians, archaeologists,\nresearchers, and cultural tourists to extract valuable insights while ensuring\ntechnology contributes meaningfully to historical discovery and cultural\nheritage preservation. We evaluate contemporary AI models on TimeTravel,\nhighlighting their strengths and identifying areas for improvement. Our goal is\nto establish AI as a reliable partner in preserving cultural heritage, ensuring\nthat technological advancements contribute meaningfully to historical\ndiscovery. Our code is available at:\n\\url{https://github.com/mbzuai-oryx/TimeTravel}.\n","authors":["Sara Ghaboura","Ketan More","Ritesh Thawkar","Wafa Alghallabi","Omkar Thawakar","Fahad Shahbaz Khan","Hisham Cholakkal","Salman Khan","Rao Muhammad Anwer"],"pdf_url":"https://arxiv.org/pdf/2502.14865v1.pdf","comment":"4 pages, 6 figures"},{"id":"http://arxiv.org/abs/2502.14864v1","updated":"2025-02-20T18:59:42Z","published":"2025-02-20T18:59:42Z","title":"Benchmarking Multimodal RAG through a Chart-based Document\n  Question-Answering Generation Framework","summary":"  Multimodal Retrieval-Augmented Generation (MRAG) enhances reasoning\ncapabilities by integrating external knowledge. However, existing benchmarks\nprimarily focus on simple image-text interactions, overlooking complex visual\nformats like charts that are prevalent in real-world applications. In this\nwork, we introduce a novel task, Chart-based MRAG, to address this limitation.\nTo semi-automatically generate high-quality evaluation samples, we propose\nCHARt-based document question-answering GEneration (CHARGE), a framework that\nproduces evaluation data through structured keypoint extraction, crossmodal\nverification, and keypoint-based generation. By combining CHARGE with expert\nvalidation, we construct Chart-MRAG Bench, a comprehensive benchmark for\nchart-based MRAG evaluation, featuring 4,738 question-answering pairs across 8\ndomains from real-world documents. Our evaluation reveals three critical\nlimitations in current approaches: (1) unified multimodal embedding retrieval\nmethods struggles in chart-based scenarios, (2) even with ground-truth\nretrieval, state-of-the-art MLLMs achieve only 58.19% Correctness and 73.87%\nCoverage scores, and (3) MLLMs demonstrate consistent text-over-visual modality\nbias during Chart-based MRAG reasoning. The CHARGE and Chart-MRAG Bench are\nreleased at https://github.com/Nomothings/CHARGE.git.\n","authors":["Yuming Yang","Jiang Zhong","Li Jin","Jingwang Huang","Jingpeng Gao","Qing Liu","Yang Bai","Jingyuan Zhang","Rui Jiang","Kaiwen Wei"],"pdf_url":"https://arxiv.org/pdf/2502.14864v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.06020v2","updated":"2025-02-20T18:56:25Z","published":"2023-04-12T17:57:15Z","title":"VidStyleODE: Disentangled Video Editing via StyleGAN and NeuralODEs","summary":"  We propose $\\textbf{VidStyleODE}$, a spatiotemporally continuous disentangled\n$\\textbf{Vid}$eo representation based upon $\\textbf{Style}$GAN and\nNeural-$\\textbf{ODE}$s. Effective traversal of the latent space learned by\nGenerative Adversarial Networks (GANs) has been the basis for recent\nbreakthroughs in image editing. However, the applicability of such advancements\nto the video domain has been hindered by the difficulty of representing and\ncontrolling videos in the latent space of GANs. In particular, videos are\ncomposed of content (i.e., appearance) and complex motion components that\nrequire a special mechanism to disentangle and control. To achieve this,\nVidStyleODE encodes the video content in a pre-trained StyleGAN $\\mathcal{W}_+$\nspace and benefits from a latent ODE component to summarize the spatiotemporal\ndynamics of the input video. Our novel continuous video generation process then\ncombines the two to generate high-quality and temporally consistent videos with\nvarying frame rates. We show that our proposed method enables a variety of\napplications on real videos: text-guided appearance manipulation, motion\nmanipulation, image animation, and video interpolation and extrapolation.\nProject website: https://cyberiada.github.io/VidStyleODE\n","authors":["Moayed Haji Ali","Andrew Bond","Tolga Birdal","Duygu Ceylan","Levent Karacan","Erkut Erdem","Aykut Erdem"],"pdf_url":"https://arxiv.org/pdf/2304.06020v2.pdf","comment":"Project website: https://cyberiada.github.io/VidStyleODE"},{"id":"http://arxiv.org/abs/2502.14846v1","updated":"2025-02-20T18:55:30Z","published":"2025-02-20T18:55:30Z","title":"Scaling Text-Rich Image Understanding via Code-Guided Synthetic\n  Multimodal Data Generation","summary":"  Reasoning about images with rich text, such as charts and documents, is a\ncritical application of vision-language models (VLMs). However, VLMs often\nstruggle in these domains due to the scarcity of diverse text-rich\nvision-language data. To address this challenge, we present CoSyn, a framework\nthat leverages the coding capabilities of text-only large language models\n(LLMs) to automatically create synthetic text-rich multimodal data. Given input\ntext describing a target domain (e.g., \"nutrition fact labels\"), CoSyn prompts\nan LLM to generate code (Python, HTML, LaTeX, etc.) for rendering synthetic\nimages. With the underlying code as textual representations of the synthetic\nimages, CoSyn can generate high-quality instruction-tuning data, again relying\non a text-only LLM. Using CoSyn, we constructed a dataset comprising 400K\nimages and 2.7M rows of vision-language instruction-tuning data. Comprehensive\nexperiments on seven benchmarks demonstrate that models trained on our\nsynthetic data achieve state-of-the-art performance among competitive\nopen-source models, including Llama 3.2, and surpass proprietary models such as\nGPT-4V and Gemini 1.5 Flash. Furthermore, CoSyn can produce synthetic pointing\ndata, enabling VLMs to ground information within input images, showcasing its\npotential for developing multimodal agents capable of acting in real-world\nenvironments.\n","authors":["Yue Yang","Ajay Patel","Matt Deitke","Tanmay Gupta","Luca Weihs","Andrew Head","Mark Yatskar","Chris Callison-Burch","Ranjay Krishna","Aniruddha Kembhavi","Christopher Clark"],"pdf_url":"https://arxiv.org/pdf/2502.14846v1.pdf","comment":"20 pages, 19 figures, 9 tables, website:\n  https://yueyang1996.github.io/cosyn/"},{"id":"http://arxiv.org/abs/2502.14844v1","updated":"2025-02-20T18:53:39Z","published":"2025-02-20T18:53:39Z","title":"Dynamic Concepts Personalization from Single Videos","summary":"  Personalizing generative text-to-image models has seen remarkable progress,\nbut extending this personalization to text-to-video models presents unique\nchallenges. Unlike static concepts, personalizing text-to-video models has the\npotential to capture dynamic concepts, i.e., entities defined not only by their\nappearance but also by their motion. In this paper, we introduce\nSet-and-Sequence, a novel framework for personalizing Diffusion Transformers\n(DiTs)-based generative video models with dynamic concepts. Our approach\nimposes a spatio-temporal weight space within an architecture that does not\nexplicitly separate spatial and temporal features. This is achieved in two key\nstages. First, we fine-tune Low-Rank Adaptation (LoRA) layers using an\nunordered set of frames from the video to learn an identity LoRA basis that\nrepresents the appearance, free from temporal interference. In the second\nstage, with the identity LoRAs frozen, we augment their coefficients with\nMotion Residuals and fine-tune them on the full video sequence, capturing\nmotion dynamics. Our Set-and-Sequence framework results in a spatio-temporal\nweight space that effectively embeds dynamic concepts into the video model's\noutput domain, enabling unprecedented editability and compositionality while\nsetting a new benchmark for personalizing dynamic concepts.\n","authors":["Rameen Abdal","Or Patashnik","Ivan Skorokhodov","Willi Menapace","Aliaksandr Siarohin","Sergey Tulyakov","Daniel Cohen-Or","Kfir Aberman"],"pdf_url":"https://arxiv.org/pdf/2502.14844v1.pdf","comment":"Webpage: https://snap-research.github.io/dynamic_concepts/"},{"id":"http://arxiv.org/abs/2502.14834v1","updated":"2025-02-20T18:47:36Z","published":"2025-02-20T18:47:36Z","title":"LongWriter-V: Enabling Ultra-Long and High-Fidelity Generation in\n  Vision-Language Models","summary":"  Existing Large Vision-Language Models (LVLMs) can process inputs with context\nlengths up to 128k visual and text tokens, yet they struggle to generate\ncoherent outputs beyond 1,000 words. We find that the primary limitation is the\nabsence of long output examples during supervised fine-tuning (SFT). To tackle\nthis issue, we introduce LongWriter-V-22k, a SFT dataset comprising 22,158\nexamples, each with multiple input images, an instruction, and corresponding\noutputs ranging from 0 to 10,000 words. Moreover, to achieve long outputs that\nmaintain high-fidelity to the input images, we employ Direct Preference\nOptimization (DPO) to the SFT model. Given the high cost of collecting human\nfeedback for lengthy outputs (e.g., 3,000 words), we propose IterDPO, which\nbreaks long outputs into segments and uses iterative corrections to form\npreference pairs with the original outputs. Additionally, we develop\nMMLongBench-Write, a benchmark featuring six tasks to evaluate the\nlong-generation capabilities of VLMs. Our 7B parameter model, trained with\nLongWriter-V-22k and IterDPO, achieves impressive performance on this\nbenchmark, outperforming larger proprietary models like GPT-4o. Code and data:\nhttps://github.com/THU-KEG/LongWriter-V\n","authors":["Shangqing Tu","Yucheng Wang","Daniel Zhang-Li","Yushi Bai","Jifan Yu","Yuhao Wu","Lei Hou","Huiqin Liu","Zhiyuan Liu","Bin Xu","Juanzi Li"],"pdf_url":"https://arxiv.org/pdf/2502.14834v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.14831v1","updated":"2025-02-20T18:45:44Z","published":"2025-02-20T18:45:44Z","title":"Improving the Diffusability of Autoencoders","summary":"  Latent diffusion models have emerged as the leading approach for generating\nhigh-quality images and videos, utilizing compressed latent representations to\nreduce the computational burden of the diffusion process. While recent\nadvancements have primarily focused on scaling diffusion backbones and\nimproving autoencoder reconstruction quality, the interaction between these\ncomponents has received comparatively less attention. In this work, we perform\na spectral analysis of modern autoencoders and identify inordinate\nhigh-frequency components in their latent spaces, which are especially\npronounced in the autoencoders with a large bottleneck channel size. We\nhypothesize that this high-frequency component interferes with the\ncoarse-to-fine nature of the diffusion synthesis process and hinders the\ngeneration quality. To mitigate the issue, we propose scale equivariance: a\nsimple regularization strategy that aligns latent and RGB spaces across\nfrequencies by enforcing scale equivariance in the decoder. It requires minimal\ncode changes and only up to 20K autoencoder fine-tuning steps, yet\nsignificantly improves generation quality, reducing FID by 19% for image\ngeneration on ImageNet-1K 256x256 and FVD by at least 44% for video generation\non Kinetics-700 17x256x256.\n","authors":["Ivan Skorokhodov","Sharath Girish","Benran Hu","Willi Menapace","Yanyu Li","Rameen Abdal","Sergey Tulyakov","Aliaksandr Siarohin"],"pdf_url":"https://arxiv.org/pdf/2502.14831v1.pdf","comment":"26 pages, 22 figures, 9 tables"},{"id":"http://arxiv.org/abs/2502.14827v1","updated":"2025-02-20T18:45:00Z","published":"2025-02-20T18:45:00Z","title":"Exploring Advanced Techniques for Visual Question Answering: A\n  Comprehensive Comparison","summary":"  Visual Question Answering (VQA) has emerged as a pivotal task in the\nintersection of computer vision and natural language processing, requiring\nmodels to understand and reason about visual content in response to natural\nlanguage questions. Analyzing VQA datasets is essential for developing robust\nmodels that can handle the complexities of multimodal reasoning. Several\napproaches have been developed to examine these datasets, each offering\ndistinct perspectives on question diversity, answer distribution, and\nvisual-textual correlations. Despite significant progress, existing VQA models\nface challenges related to dataset bias, limited model complexity, commonsense\nreasoning gaps, rigid evaluation methods, and generalization to real world\nscenarios. This paper presents a comprehensive comparative study of five\nadvanced VQA models: ABC-CNN, KICNLE, Masked Vision and Language Modeling,\nBLIP-2, and OFA, each employing distinct methodologies to address these\nchallenges.\n","authors":["Aiswarya Baby","Tintu Thankom Koshy"],"pdf_url":"https://arxiv.org/pdf/2502.14827v1.pdf","comment":"8 pages, No figures"},{"id":"http://arxiv.org/abs/2502.14949v1","updated":"2025-02-20T18:41:23Z","published":"2025-02-20T18:41:23Z","title":"KITAB-Bench: A Comprehensive Multi-Domain Benchmark for Arabic OCR and\n  Document Understanding","summary":"  With the growing adoption of Retrieval-Augmented Generation (RAG) in document\nprocessing, robust text recognition has become increasingly critical for\nknowledge extraction. While OCR (Optical Character Recognition) for English and\nother languages benefits from large datasets and well-established benchmarks,\nArabic OCR faces unique challenges due to its cursive script, right-to-left\ntext flow, and complex typographic and calligraphic features. We present\nKITAB-Bench, a comprehensive Arabic OCR benchmark that fills the gaps in\ncurrent evaluation systems. Our benchmark comprises 8,809 samples across 9\nmajor domains and 36 sub-domains, encompassing diverse document types including\nhandwritten text, structured tables, and specialized coverage of 21 chart types\nfor business intelligence. Our findings show that modern vision-language models\n(such as GPT-4, Gemini, and Qwen) outperform traditional OCR approaches (like\nEasyOCR, PaddleOCR, and Surya) by an average of 60% in Character Error Rate\n(CER). Furthermore, we highlight significant limitations of current Arabic OCR\nmodels, particularly in PDF-to-Markdown conversion, where the best model\nGemini-2.0-Flash achieves only 65% accuracy. This underscores the challenges in\naccurately recognizing Arabic text, including issues with complex fonts,\nnumeral recognition errors, word elongation, and table structure detection.\nThis work establishes a rigorous evaluation framework that can drive\nimprovements in Arabic document analysis methods and bridge the performance gap\nwith English OCR technologies.\n","authors":["Ahmed Heakl","Abdullah Sohail","Mukul Ranjan","Rania Hossam","Ghazi Ahmed","Mohamed El-Geish","Omar Maher","Zhiqiang Shen","Fahad Khan","Salman Khan"],"pdf_url":"https://arxiv.org/pdf/2502.14949v1.pdf","comment":"17 pages, 5 figures, ACL 2025"},{"id":"http://arxiv.org/abs/2502.14807v1","updated":"2025-02-20T18:30:34Z","published":"2025-02-20T18:30:34Z","title":"FetalCLIP: A Visual-Language Foundation Model for Fetal Ultrasound Image\n  Analysis","summary":"  Foundation models are becoming increasingly effective in the medical domain,\noffering pre-trained models on large datasets that can be readily adapted for\ndownstream tasks. Despite progress, fetal ultrasound images remain a\nchallenging domain for foundation models due to their inherent complexity,\noften requiring substantial additional training and facing limitations due to\nthe scarcity of paired multimodal data. To overcome these challenges, here we\nintroduce FetalCLIP, a vision-language foundation model capable of generating\nuniversal representation of fetal ultrasound images. FetalCLIP was pre-trained\nusing a multimodal learning approach on a diverse dataset of 210,035 fetal\nultrasound images paired with text. This represents the largest paired dataset\nof its kind used for foundation model development to date. This unique training\napproach allows FetalCLIP to effectively learn the intricate anatomical\nfeatures present in fetal ultrasound images, resulting in robust\nrepresentations that can be used for a variety of downstream applications. In\nextensive benchmarking across a range of key fetal ultrasound applications,\nincluding classification, gestational age estimation, congenital heart defect\n(CHD) detection, and fetal structure segmentation, FetalCLIP outperformed all\nbaselines while demonstrating remarkable generalizability and strong\nperformance even with limited labeled data. We plan to release the FetalCLIP\nmodel publicly for the benefit of the broader scientific community.\n","authors":["Fadillah Maani","Numan Saeed","Tausifa Saleem","Zaid Farooq","Hussain Alasmawi","Werner Diehl","Ameera Mohammad","Gareth Waring","Saudabi Valappi","Leanne Bricker","Mohammad Yaqub"],"pdf_url":"https://arxiv.org/pdf/2502.14807v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.14799v1","updated":"2025-02-20T18:19:57Z","published":"2025-02-20T18:19:57Z","title":"A Survey on Text-Driven 360-Degree Panorama Generation","summary":"  The advent of text-driven 360-degree panorama generation, enabling the\nsynthesis of 360-degree panoramic images directly from textual descriptions,\nmarks a transformative advancement in immersive visual content creation. This\ninnovation significantly simplifies the traditionally complex process of\nproducing such content. Recent progress in text-to-image diffusion models has\naccelerated the rapid development in this emerging field. This survey presents\na comprehensive review of text-driven 360-degree panorama generation, offering\nan in-depth analysis of state-of-the-art algorithms and their expanding\napplications in 360-degree 3D scene generation. Furthermore, we critically\nexamine current limitations and propose promising directions for future\nresearch. A curated project page with relevant resources and research papers is\navailable at https://littlewhitesea.github.io/Text-Driven-Pano-Gen/.\n","authors":["Hai Wang","Xiaoyu Xiang","Weihao Xia","Jing-Hao Xue"],"pdf_url":"https://arxiv.org/pdf/2502.14799v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.14792v1","updated":"2025-02-20T18:11:44Z","published":"2025-02-20T18:11:44Z","title":"RendBEV: Semantic Novel View Synthesis for Self-Supervised Bird's Eye\n  View Segmentation","summary":"  Bird's Eye View (BEV) semantic maps have recently garnered a lot of attention\nas a useful representation of the environment to tackle assisted and autonomous\ndriving tasks. However, most of the existing work focuses on the fully\nsupervised setting, training networks on large annotated datasets. In this\nwork, we present RendBEV, a new method for the self-supervised training of BEV\nsemantic segmentation networks, leveraging differentiable volumetric rendering\nto receive supervision from semantic perspective views computed by a 2D\nsemantic segmentation model. Our method enables zero-shot BEV semantic\nsegmentation, and already delivers competitive results in this challenging\nsetting. When used as pretraining to then fine-tune on labeled BEV\nground-truth, our method significantly boosts performance in low-annotation\nregimes, and sets a new state of the art when fine-tuning on all available\nlabels.\n","authors":["Henrique Piñeiro Monteagudo","Leonardo Taccari","Aurel Pjetri","Francesco Sambo","Samuele Salti"],"pdf_url":"https://arxiv.org/pdf/2502.14792v1.pdf","comment":"Accepted at WACV 2025"},{"id":"http://arxiv.org/abs/2502.14789v1","updated":"2025-02-20T18:09:27Z","published":"2025-02-20T18:09:27Z","title":"Structurally Disentangled Feature Fields Distillation for 3D\n  Understanding and Editing","summary":"  Recent work has demonstrated the ability to leverage or distill pre-trained\n2D features obtained using large pre-trained 2D models into 3D features,\nenabling impressive 3D editing and understanding capabilities using only 2D\nsupervision. Although impressive, models assume that 3D features are captured\nusing a single feature field and often make a simplifying assumption that\nfeatures are view-independent. In this work, we propose instead to capture 3D\nfeatures using multiple disentangled feature fields that capture different\nstructural components of 3D features involving view-dependent and\nview-independent components, which can be learned from 2D feature supervision\nonly. Subsequently, each element can be controlled in isolation, enabling\nsemantic and structural understanding and editing capabilities. For instance,\nusing a user click, one can segment 3D features corresponding to a given object\nand then segment, edit, or remove their view-dependent (reflective) properties.\nWe evaluate our approach on the task of 3D segmentation and demonstrate a set\nof novel understanding and editing tasks.\n","authors":["Yoel Levy","David Shavin","Itai Lang","Sagie Benaim"],"pdf_url":"https://arxiv.org/pdf/2502.14789v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.14786v1","updated":"2025-02-20T18:08:29Z","published":"2025-02-20T18:08:29Z","title":"SigLIP 2: Multilingual Vision-Language Encoders with Improved Semantic\n  Understanding, Localization, and Dense Features","summary":"  We introduce SigLIP 2, a family of new multilingual vision-language encoders\nthat build on the success of the original SigLIP. In this second iteration, we\nextend the original image-text training objective with several prior,\nindependently developed techniques into a unified recipe -- this includes\ncaptioning-based pretraining, self-supervised losses (self-distillation, masked\nprediction) and online data curation. With these changes, SigLIP 2 models\noutperform their SigLIP counterparts at all model scales in core capabilities,\nincluding zero-shot classification, image-text retrieval, and transfer\nperformance when extracting visual representations for Vision-Language Models\n(VLMs). Furthermore, the new training recipe leads to significant improvements\non localization and dense prediction tasks. We also train variants which\nsupport multiple resolutions and preserve the input's native aspect ratio.\nFinally, we train on a more diverse data-mixture that includes de-biasing\ntechniques, leading to much better multilingual understanding and improved\nfairness. To allow users to trade off inference cost with performance, we\nrelease model checkpoints at four sizes: ViT-B (86M), L (303M), So400m (400M),\nand g (1B).\n","authors":["Michael Tschannen","Alexey Gritsenko","Xiao Wang","Muhammad Ferjad Naeem","Ibrahim Alabdulmohsin","Nikhil Parthasarathy","Talfan Evans","Lucas Beyer","Ye Xia","Basil Mustafa","Olivier Hénaff","Jeremiah Harmsen","Andreas Steiner","Xiaohua Zhai"],"pdf_url":"https://arxiv.org/pdf/2502.14786v1.pdf","comment":"Model checkpoints are available at\n  https://github.com/google-research/big_vision/tree/main/big_vision/configs/proj/image_text/README_siglip2.md"},{"id":"http://arxiv.org/abs/2410.00255v2","updated":"2025-02-20T18:06:19Z","published":"2024-09-30T21:55:38Z","title":"Robin3D: Improving 3D Large Language Model via Robust Instruction Tuning","summary":"  Recent advancements in 3D Large Language Models (3DLLMs) have highlighted\ntheir potential in building general-purpose agents in the 3D real world, yet\nchallenges remain due to the lack of high-quality robust instruction-following\ndata, leading to limited discriminative power and generalization of 3DLLMs. In\nthis paper, we introduce Robin3D, a powerful 3DLLM trained on large-scale\ninstruction-following data generated by our novel data engine, Robust\nInstruction Generation (RIG) engine. RIG generates two key instruction data: 1)\nthe Adversarial Instruction-following data, which features mixed negative and\npositive samples to enhance the model's discriminative understanding. 2) the\nDiverse Instruction-following data, which contains various instruction styles\nto enhance model's generalization. As a result, we construct 1 million\ninstruction-following data, consisting of 344K Adversarial samples, 508K\nDiverse samples, and 165K benchmark training set samples. To better handle\nthese complex instructions, Robin3D first incorporates Relation-Augmented\nProjector to enhance spatial understanding, and then strengthens the object\nreferring and grounding ability through ID-Feature Bonding. Robin3D\nconsistently outperforms previous methods across five widely-used 3D multimodal\nlearning benchmarks, without the need for task-specific fine-tuning. Notably,\nwe achieve a 7.8\\% improvement in the grounding task (Multi3DRefer) and a 6.9\\%\nimprovement in the captioning task (Scan2Cap).\n","authors":["Weitai Kang","Haifeng Huang","Yuzhang Shang","Mubarak Shah","Yan Yan"],"pdf_url":"https://arxiv.org/pdf/2410.00255v2.pdf","comment":"8 pages"},{"id":"http://arxiv.org/abs/2502.14780v1","updated":"2025-02-20T18:01:41Z","published":"2025-02-20T18:01:41Z","title":"ReVision: A Dataset and Baseline VLM for Privacy-Preserving\n  Task-Oriented Visual Instruction Rewriting","summary":"  Efficient and privacy-preserving multimodal interaction is essential as AR,\nVR, and modern smartphones with powerful cameras become primary interfaces for\nhuman-computer communication. Existing powerful large vision-language models\n(VLMs) enabling multimodal interaction often rely on cloud-based processing,\nraising significant concerns about (1) visual privacy by transmitting sensitive\nvision data to servers, and (2) their limited real-time, on-device usability.\nThis paper explores Visual Instruction Rewriting, a novel approach that\ntransforms multimodal instructions into text-only commands, allowing seamless\nintegration of lightweight on-device instruction rewriter VLMs (250M\nparameters) with existing conversational AI systems, enhancing vision data\nprivacy. To achieve this, we present a dataset of over 39,000 examples across\n14 domains and develop a compact VLM, pretrained on image captioning datasets\nand fine-tuned for instruction rewriting. Experimental results, evaluated\nthrough NLG metrics such as BLEU, METEOR, and ROUGE, along with semantic\nparsing analysis, demonstrate that even a quantized version of the model\n(<500MB storage footprint) can achieve effective instruction rewriting, thus\nenabling privacy-focused, multimodal AI applications.\n","authors":["Abhijit Mishra","Richard Noh","Hsiang Fu","Mingda Li","Minji Kim"],"pdf_url":"https://arxiv.org/pdf/2502.14780v1.pdf","comment":"12 pages, 7 figures, 3 tables"},{"id":"http://arxiv.org/abs/2502.14779v1","updated":"2025-02-20T18:01:02Z","published":"2025-02-20T18:01:02Z","title":"DC-ControlNet: Decoupling Inter- and Intra-Element Conditions in Image\n  Generation with Diffusion Models","summary":"  In this paper, we introduce DC (Decouple)-ControlNet, a highly flexible and\nprecisely controllable framework for multi-condition image generation. The core\nidea behind DC-ControlNet is to decouple control conditions, transforming\nglobal control into a hierarchical system that integrates distinct elements,\ncontents, and layouts. This enables users to mix these individual conditions\nwith greater flexibility, leading to more efficient and accurate image\ngeneration control. Previous ControlNet-based models rely solely on global\nconditions, which affect the entire image and lack the ability of element- or\nregion-specific control. This limitation reduces flexibility and can cause\ncondition misunderstandings in multi-conditional image generation. To address\nthese challenges, we propose both intra-element and Inter-element Controllers\nin DC-ControlNet. The Intra-Element Controller handles different types of\ncontrol signals within individual elements, accurately describing the content\nand layout characteristics of the object. For interactions between elements, we\nintroduce the Inter-Element Controller, which accurately handles multi-element\ninteractions and occlusion based on user-defined relationships. Extensive\nevaluations show that DC-ControlNet significantly outperforms existing\nControlNet models and Layout-to-Image generative models in terms of control\nflexibility and precision in multi-condition control.\n","authors":["Hongji Yang","Wencheng Han","Yucheng Zhou","Jianbing Shen"],"pdf_url":"https://arxiv.org/pdf/2502.14779v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.14778v1","updated":"2025-02-20T17:59:59Z","published":"2025-02-20T17:59:59Z","title":"Harnessing PDF Data for Improving Japanese Large Multimodal Models","summary":"  Large Multimodal Models (LMMs) have demonstrated strong performance in\nEnglish, but their effectiveness in Japanese remains limited due to the lack of\nhigh-quality training data. Current Japanese LMMs often rely on translated\nEnglish datasets, restricting their ability to capture Japan-specific cultural\nknowledge. To address this, we explore the potential of Japanese PDF data as a\ntraining resource, an area that remains largely underutilized. We introduce a\nfully automated pipeline that leverages pretrained models to extract image-text\npairs from PDFs through layout analysis, OCR, and vision-language pairing,\nremoving the need for manual annotation. Additionally, we construct instruction\ndata from extracted image-text pairs to enrich the training data. To evaluate\nthe effectiveness of PDF-derived data, we train Japanese LMMs and assess their\nperformance on the Japanese LMM Benchmark. Our results demonstrate substantial\nimprovements, with performance gains ranging from 3.9% to 13.8% on Heron-Bench.\nFurther analysis highlights the impact of PDF-derived data on various factors,\nsuch as model size and language models, reinforcing its value as a multimodal\nresource for Japanese LMMs. We plan to make the source code and data publicly\navailable upon acceptance.\n","authors":["Jeonghun Baek","Akiko Aizawa","Kiyoharu Aizawa"],"pdf_url":"https://arxiv.org/pdf/2502.14778v1.pdf","comment":"15 pages, 8 figures"},{"id":"http://arxiv.org/abs/2502.14942v1","updated":"2025-02-20T17:37:55Z","published":"2025-02-20T17:37:55Z","title":"Design of a Visual Pose Estimation Algorithm for Moon Landing","summary":"  In order to make a pinpoint landing on the Moon, the spacecraft's navigation\nsystem must be accurate. To achieve the desired accuracy, navigational drift\ncaused by the inertial sensors must be corrected. One way to correct this drift\nis to use absolute navigation solutions. In this study, a terrain absolute\nnavigation method to estimate the spacecraft's position and attitude is\nproposed. This algorithm uses the position of the craters below the spacecraft\nfor estimation. Craters seen by the camera onboard the spacecraft are detected\nand identified using a crater database known beforehand. In order to focus on\nestimation algorithms, image processing and crater matching steps are skipped.\nThe accuracy of the algorithm and the effect of the crater number used for\nestimation are inspected by performing simulations.\n","authors":["Atakan Süslü","Betül Rana Kuran","Halil Ersin Söken"],"pdf_url":"https://arxiv.org/pdf/2502.14942v1.pdf","comment":"6 pages, 8 figures, Presented in 11th Nano-Satellite Symposium"},{"id":"http://arxiv.org/abs/2502.14762v1","updated":"2025-02-20T17:37:08Z","published":"2025-02-20T17:37:08Z","title":"Sculpting [CLS] Features for Pre-Trained Model-Based Class-Incremental\n  Learning","summary":"  Class-incremental learning requires models to continually acquire knowledge\nof new classes without forgetting old ones. Although pre-trained models have\ndemonstrated strong performance in class-incremental learning, they remain\nsusceptible to catastrophic forgetting when learning new concepts. Excessive\nplasticity in the models breaks generalizability and causes forgetting, while\nstrong stability results in insufficient adaptation to new classes. This\nnecessitates effective adaptation with minimal modifications to preserve the\ngeneral knowledge of pre-trained models. To address this challenge, we first\nintroduce a new parameter-efficient fine-tuning module 'Learn and Calibrate',\nor LuCA, designed to acquire knowledge through an adapter-calibrator couple,\nenabling effective adaptation with well-refined feature representations.\nSecond, for each learning session, we deploy a sparse LuCA module on top of the\nlast token just before the classifier, which we refer to as 'Token-level Sparse\nCalibration and Adaptation', or TOSCA. This strategic design improves the\northogonality between the modules and significantly reduces both training and\ninference complexity. By leaving the generalization capabilities of the\npre-trained models intact and adapting exclusively via the last token, our\napproach achieves a harmonious balance between stability and plasticity.\nExtensive experiments demonstrate TOSCA's state-of-the-art performance while\nintroducing ~8 times fewer parameters compared to prior methods.\n","authors":["Murat Onur Yildirim","Elif Ceren Gok Yildirim","Joaquin Vanschoren"],"pdf_url":"https://arxiv.org/pdf/2502.14762v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.14940v1","updated":"2025-02-20T17:32:41Z","published":"2025-02-20T17:32:41Z","title":"FacaDiffy: Inpainting Unseen Facade Parts Using Diffusion Models","summary":"  High-detail semantic 3D building models are frequently utilized in robotics,\ngeoinformatics, and computer vision. One key aspect of creating such models is\nemploying 2D conflict maps that detect openings' locations in building facades.\nYet, in reality, these maps are often incomplete due to obstacles encountered\nduring laser scanning. To address this challenge, we introduce FacaDiffy, a\nnovel method for inpainting unseen facade parts by completing conflict maps\nwith a personalized Stable Diffusion model. Specifically, we first propose a\ndeterministic ray analysis approach to derive 2D conflict maps from existing 3D\nbuilding models and corresponding laser scanning point clouds. Furthermore, we\nfacilitate the inpainting of unseen facade objects into these 2D conflict maps\nby leveraging the potential of personalizing a Stable Diffusion model. To\ncomplement the scarcity of real-world training data, we also develop a scalable\npipeline to produce synthetic conflict maps using random city model generators\nand annotated facade images. Extensive experiments demonstrate that FacaDiffy\nachieves state-of-the-art performance in conflict map completion compared to\nvarious inpainting baselines and increases the detection rate by $22\\%$ when\napplying the completed conflict maps for high-definition 3D semantic building\nreconstruction. The code is be publicly available in the corresponding GitHub\nrepository: https://github.com/ThomasFroech/InpaintingofUnseenFacadeObjects\n","authors":["Thomas Froech","Olaf Wysocki","Yan Xia","Junyu Xie","Benedikt Schwab","Daniel Cremers","Thomas H. Kolbe"],"pdf_url":"https://arxiv.org/pdf/2502.14940v1.pdf","comment":"Accepted for GeoSpatial Week 2025, ISPRS Annals"},{"id":"http://arxiv.org/abs/2502.14939v1","updated":"2025-02-20T17:27:55Z","published":"2025-02-20T17:27:55Z","title":"Online hand gesture recognition using Continual Graph Transformers","summary":"  Online continuous action recognition has emerged as a critical research area\ndue to its practical implications in real-world applications, such as\nhuman-computer interaction, healthcare, and robotics. Among various modalities,\nskeleton-based approaches have gained significant popularity, demonstrating\ntheir effectiveness in capturing 3D temporal data while ensuring robustness to\nenvironmental variations. However, most existing works focus on segment-based\nrecognition, making them unsuitable for real-time, continuous recognition\nscenarios. In this paper, we propose a novel online recognition system designed\nfor real-time skeleton sequence streaming. Our approach leverages a hybrid\narchitecture combining Spatial Graph Convolutional Networks (S-GCN) for spatial\nfeature extraction and a Transformer-based Graph Encoder (TGE) for capturing\ntemporal dependencies across frames. Additionally, we introduce a continual\nlearning mechanism to enhance model adaptability to evolving data\ndistributions, ensuring robust recognition in dynamic environments. We evaluate\nour method on the SHREC'21 benchmark dataset, demonstrating its superior\nperformance in online hand gesture recognition. Our approach not only achieves\nstate-of-the-art accuracy but also significantly reduces false positive rates,\nmaking it a compelling solution for real-time applications. The proposed system\ncan be seamlessly integrated into various domains, including human-robot\ncollaboration and assistive technologies, where natural and intuitive\ninteraction is crucial.\n","authors":["Rim Slama","Wael Rabah","Hazem Wannous"],"pdf_url":"https://arxiv.org/pdf/2502.14939v1.pdf","comment":null}],"Information Retrieval":[{"id":"http://arxiv.org/abs/2404.05893v5","updated":"2025-02-20T21:57:27Z","published":"2024-04-08T22:29:53Z","title":"Use of a Structured Knowledge Base Enhances Metadata Curation by Large\n  Language Models","summary":"  Metadata play a crucial role in ensuring the findability, accessibility,\ninteroperability, and reusability of datasets. This paper investigates the\npotential of large language models (LLMs), specifically GPT-4, to improve\nadherence to metadata standards. We conducted experiments on 200 random data\nrecords describing human samples relating to lung cancer from the NCBI\nBioSample repository, evaluating GPT-4's ability to suggest edits for adherence\nto metadata standards. We computed the adherence accuracy of field name-field\nvalue pairs through a peer review process, and we observed a marginal average\nimprovement in adherence to the standard data dictionary from 79% to 80%\n(p<0.5). We then prompted GPT-4 with domain information in the form of the\ntextual descriptions of CEDAR templates and recorded a significant improvement\nto 97% from 79% (p<0.01). These results indicate that, while LLMs may not be\nable to correct legacy metadata to ensure satisfactory adherence to standards\nwhen unaided, they do show promise for use in automated metadata curation when\nintegrated with a structured knowledge base\n","authors":["Sowmya S. Sundaram","Benjamin Solomon","Avani Khatri","Anisha Laumas","Purvesh Khatri","Mark A. Musen"],"pdf_url":"https://arxiv.org/pdf/2404.05893v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.15025v1","updated":"2025-02-20T20:21:46Z","published":"2025-02-20T20:21:46Z","title":"Is Relevance Propagated from Retriever to Generator in RAG?","summary":"  Retrieval Augmented Generation (RAG) is a framework for incorporating\nexternal knowledge, usually in the form of a set of documents retrieved from a\ncollection, as a part of a prompt to a large language model (LLM) to\npotentially improve the performance of a downstream task, such as question\nanswering. Different from a standard retrieval task's objective of maximising\nthe relevance of a set of top-ranked documents, a RAG system's objective is\nrather to maximise their total utility, where the utility of a document\nindicates whether including it as a part of the additional contextual\ninformation in an LLM prompt improves a downstream task. Existing studies\ninvestigate the role of the relevance of a RAG context for knowledge-intensive\nlanguage tasks (KILT), where relevance essentially takes the form of answer\ncontainment. In contrast, in our work, relevance corresponds to that of topical\noverlap between a query and a document for an information seeking task.\nSpecifically, we make use of an IR test collection to empirically investigate\nwhether a RAG context comprised of topically relevant documents leads to\nimproved downstream performance. Our experiments lead to the following\nfindings: (a) there is a small positive correlation between relevance and\nutility; (b) this correlation decreases with increasing context sizes (higher\nvalues of k in k-shot); and (c) a more effective retrieval model generally\nleads to better downstream RAG performance.\n","authors":["Fangzheng Tian","Debasis Ganguly","Craig Macdonald"],"pdf_url":"https://arxiv.org/pdf/2502.15025v1.pdf","comment":"18 pages (including reference), 5 figures, 1 table, 48 references;\n  this paper has been accepted by ECIR'25 as a full paper"},{"id":"http://arxiv.org/abs/2403.05668v3","updated":"2025-02-20T19:30:53Z","published":"2024-03-08T20:44:59Z","title":"CFaiRLLM: Consumer Fairness Evaluation in Large-Language Model\n  Recommender System","summary":"  This work takes a critical stance on previous studies concerning fairness\nevaluation in Large Language Model (LLM)-based recommender systems, which have\nprimarily assessed consumer fairness by comparing recommendation lists\ngenerated with and without sensitive user attributes. Such approaches\nimplicitly treat discrepancies in recommended items as biases, overlooking\nwhether these changes might stem from genuine personalization aligned with the\ntrue preferences of users. Moreover, these earlier studies typically address\nsingle sensitive attributes in isolation, neglecting the complex interplay of\nintersectional identities. In response to these shortcomings, we introduce\nCFaiRLLM, an enhanced evaluation framework that not only incorporates true\npreference alignment but also rigorously examines intersectional fairness by\nconsidering overlapping sensitive attributes. Additionally, CFaiRLLM introduces\ndiverse user profile sampling strategies-random, top-rated, and\nrecency-focused-to better understand the impact of profile generation fed to\nLLMs in light of inherent token limitations in these systems. Given that\nfairness depends on accurately understanding users' tastes and preferences,\nthese strategies provide a more realistic assessment of fairness within\nRecLLMs.\n  To validate the efficacy of CFaiRLLM, we conducted extensive experiments\nusing MovieLens and LastFM datasets, applying various sampling strategies and\nsensitive attribute configurations. The evaluation metrics include both item\nsimilarity measures and true preference alignment considering both hit and\nranking (Jaccard Similarity and PRAG), thereby conducting a multifaceted\nanalysis of recommendation fairness.\n","authors":["Yashar Deldjoo","Tommaso di Noia"],"pdf_url":"https://arxiv.org/pdf/2403.05668v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.14862v1","updated":"2025-02-20T18:59:34Z","published":"2025-02-20T18:59:34Z","title":"Interpretable Text Embeddings and Text Similarity Explanation: A Primer","summary":"  Text embeddings and text embedding models are a backbone of many AI and NLP\nsystems, particularly those involving search. However, interpretability\nchallenges persist, especially in explaining obtained similarity scores, which\nis crucial for applications requiring transparency. In this paper, we give a\nstructured overview of interpretability methods specializing in explaining\nthose similarity scores, an emerging research area. We study the methods'\nindividual ideas and techniques, evaluating their potential for improving\ninterpretability of text embeddings and explaining predicted similarities.\n","authors":["Juri Opitz","Lucas Möller","Andrianos Michail","Simon Clematide"],"pdf_url":"https://arxiv.org/pdf/2502.14862v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.14822v1","updated":"2025-02-20T18:42:58Z","published":"2025-02-20T18:42:58Z","title":"A Survey of Model Architectures in Information Retrieval","summary":"  This survey examines the evolution of model architectures in information\nretrieval (IR), focusing on two key aspects: backbone models for feature\nextraction and end-to-end system architectures for relevance estimation. The\nreview intentionally separates architectural considerations from training\nmethodologies to provide a focused analysis of structural innovations in IR\nsystems.We trace the development from traditional term-based methods to modern\nneural approaches, particularly highlighting the impact of transformer-based\nmodels and subsequent large language models (LLMs). We conclude by discussing\nemerging challenges and future directions, including architectural\noptimizations for performance and scalability, handling of multimodal,\nmultilingual data, and adaptation to novel application domains beyond\ntraditional search paradigms.\n","authors":["Zhichao Xu","Fengran Mo","Zhiqi Huang","Crystina Zhang","Puxuan Yu","Bei Wang","Jimmy Lin","Vivek Srikumar"],"pdf_url":"https://arxiv.org/pdf/2502.14822v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.14796v1","updated":"2025-02-20T18:17:26Z","published":"2025-02-20T18:17:26Z","title":"A Multi-Agent Perspective on Modern Information Retrieval","summary":"  The rise of large language models (LLMs) has introduced a new era in\ninformation retrieval (IR), where queries and documents that were once assumed\nto be generated exclusively by humans can now also be created by automated\nagents. These agents can formulate queries, generate documents, and perform\nranking. This shift challenges some long-standing IR paradigms and calls for a\nreassessment of both theoretical frameworks and practical methodologies. We\nadvocate for a multi-agent perspective to better capture the complex\ninteractions between query agents, document agents, and ranker agents. Through\nempirical exploration of various multi-agent retrieval settings, we reveal the\nsignificant impact of these interactions on system performance. Our findings\nunderscore the need to revisit classical IR paradigms and develop new\nframeworks for more effective modeling and evaluation of modern retrieval\nsystems.\n","authors":["Haya Nachimovsky","Moshe Tennenholtz","Oren Kurland"],"pdf_url":"https://arxiv.org/pdf/2502.14796v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.14735v1","updated":"2025-02-20T17:01:57Z","published":"2025-02-20T17:01:57Z","title":"EAGER-LLM: Enhancing Large Language Models as Recommenders through\n  Exogenous Behavior-Semantic Integration","summary":"  Large language models (LLMs) are increasingly leveraged as foundational\nbackbones in the development of advanced recommender systems, offering enhanced\ncapabilities through their extensive knowledge and reasoning. Existing\nllm-based recommender systems (RSs) often face challenges due to the\nsignificant differences between the linguistic semantics of pre-trained LLMs\nand the collaborative semantics essential for RSs. These systems use\npre-trained linguistic semantics but learn collaborative semantics from scratch\nvia the llm-Backbone. However, LLMs are not designed for recommendations,\nleading to inefficient collaborative learning, weak result correlations, and\npoor integration of traditional RS features. To address these challenges, we\npropose EAGER-LLM, a decoder-only llm-based generative recommendation framework\nthat integrates endogenous and exogenous behavioral and semantic information in\na non-intrusive manner. Specifically, we propose 1)dual-source knowledge-rich\nitem indices that integrates indexing sequences for exogenous signals, enabling\nefficient link-wide processing; 2)non-invasive multiscale alignment\nreconstruction tasks guide the model toward a deeper understanding of both\ncollaborative and semantic signals; 3)an annealing adapter designed to finely\nbalance the model's recommendation performance with its comprehension\ncapabilities. We demonstrate EAGER-LLM's effectiveness through rigorous testing\non three public benchmarks.\n","authors":["Minjie Hong","Yan Xia","Zehan Wang","Jieming Zhu","Ye Wang","Sihang Cai","Xiaoda Yang","Quanyu Dai","Zhenhua Dong","Zhimeng Zhang","Zhou Zhao"],"pdf_url":"https://arxiv.org/pdf/2502.14735v1.pdf","comment":"9 pages, 6 figures, accpeted by WWW 2025"},{"id":"http://arxiv.org/abs/2502.14714v1","updated":"2025-02-20T16:39:57Z","published":"2025-02-20T16:39:57Z","title":"From Knowledge Generation to Knowledge Verification: Examining the\n  BioMedical Generative Capabilities of ChatGPT","summary":"  The generative capabilities of LLM models present opportunities in\naccelerating tasks and concerns with the authenticity of the knowledge it\nproduces. To address the concerns, we present a computational approach that\nsystematically evaluates the factual accuracy of biomedical knowledge that an\nLLM model has been prompted to generate. Our approach encompasses two\nprocesses: the generation of disease-centric associations and the verification\nof them using the semantic knowledge of the biomedical ontologies. Using\nChatGPT as the select LLM model, we designed a set of prompt-engineering\nprocesses to generate linkages between diseases, drugs, symptoms, and genes to\nestablish grounds for assessments. Experimental results demonstrate high\naccuracy in identifying disease terms (88%-97%), drug names (90%-91%), and\ngenetic information (88%-98%). The symptom term identification accuracy was\nnotably lower (49%-61%), as verified against the DOID, ChEBI, SYMPTOM, and GO\nontologies accordingly. The verification of associations reveals literature\ncoverage rates of (89%-91%) among disease-drug and disease-gene associations.\nThe low identification accuracy for symptom terms also contributed to the\nverification of symptom-related associations (49%-62%).\n","authors":["Ahmed Abdeen Hamed","Byung Suk Lee"],"pdf_url":"https://arxiv.org/pdf/2502.14714v1.pdf","comment":"26 pages, 6 figures, In Review with a Cell Press Journal"},{"id":"http://arxiv.org/abs/2502.14662v1","updated":"2025-02-20T15:58:25Z","published":"2025-02-20T15:58:25Z","title":"InstructAgent: Building User Controllable Recommender via LLM Agent","summary":"  Traditional recommender systems usually take the user-platform paradigm,\nwhere users are directly exposed under the control of the platform's\nrecommendation algorithms. However, the defect of recommendation algorithms may\nput users in very vulnerable positions under this paradigm. First, many\nsophisticated models are often designed with commercial objectives in mind,\nfocusing on the platform's benefits, which may hinder their ability to protect\nand capture users' true interests. Second, these models are typically optimized\nusing data from all users, which may overlook individual user's preferences.\nDue to these shortcomings, users may experience several disadvantages under the\ntraditional user-platform direct exposure paradigm, such as lack of control\nover the recommender system, potential manipulation by the platform, echo\nchamber effects, or lack of personalization for less active users due to the\ndominance of active users during collaborative learning. Therefore, there is an\nurgent need to develop a new paradigm to protect user interests and alleviate\nthese issues. Recently, some researchers have introduced LLM agents to simulate\nuser behaviors, these approaches primarily aim to optimize platform-side\nperformance, leaving core issues in recommender systems unresolved. To address\nthese limitations, we propose a new user-agent-platform paradigm, where agent\nserves as the protective shield between user and recommender system that\nenables indirect exposure. To this end, we first construct four recommendation\ndatasets, denoted as $\\dataset$, along with user instructions for each record.\n","authors":["Wujiang Xu","Yunxiao Shi","Zujie Liang","Xuying Ning","Kai Mei","Kun Wang","Xi Zhu","Min Xu","Yongfeng Zhang"],"pdf_url":"https://arxiv.org/pdf/2502.14662v1.pdf","comment":"WWW2025@HCRS"},{"id":"http://arxiv.org/abs/2501.09751v2","updated":"2025-02-20T15:05:18Z","published":"2025-01-16T18:58:06Z","title":"OmniThink: Expanding Knowledge Boundaries in Machine Writing through\n  Thinking","summary":"  Machine writing with large language models often relies on\nretrieval-augmented generation. However, these approaches remain confined\nwithin the boundaries of the model's predefined scope, limiting the generation\nof content with rich information. Specifically, vanilla-retrieved information\ntends to lack depth, novelty, and suffers from redundancy, which negatively\nimpacts the quality of generated articles, leading to shallow, unoriginal, and\nrepetitive outputs. To address these issues, we propose OmniThink, a\nslow-thinking machine writing framework that emulates the human-like process of\niterative expansion and reflection. The core idea behind OmniThink is to\nsimulate the cognitive behavior of learners as they slowly deepen their\nknowledge of the topics. Experimental results demonstrate that OmniThink\nimproves the knowledge density of generated articles without compromising\nmetrics such as coherence and depth. Human evaluations and expert feedback\nfurther highlight the potential of OmniThink to address real-world challenges\nin the generation of long-form articles.\n","authors":["Zekun Xi","Wenbiao Yin","Jizhan Fang","Jialong Wu","Runnan Fang","Ningyu Zhang","Jiang Yong","Pengjun Xie","Fei Huang","Huajun Chen"],"pdf_url":"https://arxiv.org/pdf/2501.09751v2.pdf","comment":"Code is available at https://github.com/zjunlp/OmniThink"},{"id":"http://arxiv.org/abs/2502.14625v1","updated":"2025-02-20T15:05:00Z","published":"2025-02-20T15:05:00Z","title":"Multi-Record Web Page Information Extraction From News Websites","summary":"  In this paper, we focused on the problem of extracting information from web\npages containing many records, a task of growing importance in the era of\nmassive web data. Recently, the development of neural network methods has\nimproved the quality of information extraction from web pages. Nevertheless,\nmost of the research and datasets are aimed at studying detailed pages. This\nhas left multi-record \"list pages\" relatively understudied, despite their\nwidespread presence and practical significance.\n  To address this gap, we created a large-scale, open-access dataset\nspecifically designed for list pages. This is the first dataset for this task\nin the Russian language. Our dataset contains 13,120 web pages with news lists,\nsignificantly exceeding existing datasets in both scale and complexity. Our\ndataset contains attributes of various types, including optional and\nmulti-valued, providing a realistic representation of real-world list pages.\nThese features make our dataset a valuable resource for studying information\nextraction from pages containing many records.\n  Furthermore, we proposed our own multi-stage information extraction methods.\nIn this work, we explore and demonstrate several strategies for applying\nMarkupLM to the specific challenges of multi-record web pages. Our experiments\nvalidate the advantages of our methods.\n  By releasing our dataset to the public, we aim to advance the field of\ninformation extraction from multi-record pages.\n","authors":["Alexander Kustenkov","Maksim Varlamov","Alexander Yatskov"],"pdf_url":"https://arxiv.org/pdf/2502.14625v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.08073v2","updated":"2025-02-20T14:33:47Z","published":"2024-08-15T10:54:55Z","title":"Extracting Sentence Embeddings from Pretrained Transformer Models","summary":"  Pre-trained transformer models shine in many natural language processing\ntasks and therefore are expected to bear the representation of the input\nsentence or text meaning. These sentence-level embeddings are also important in\nretrieval-augmented generation. But do commonly used plain averaging or prompt\ntemplates sufficiently capture and represent the underlying meaning? After\nproviding a comprehensive review of existing sentence embedding extraction and\nrefinement methods, we thoroughly test different combinations and our original\nextensions of the most promising ones on pretrained models. Namely, given 110 M\nparameters, BERT's hidden representations from multiple layers, and many\ntokens, we try diverse ways to extract optimal sentence embeddings. We test\nvarious token aggregation and representation post-processing techniques. We\nalso test multiple ways of using a general Wikitext dataset to complement\nBERT's sentence embeddings. All methods are tested on eight Semantic Textual\nSimilarity (STS), six short text clustering, and twelve classification tasks.\nWe also evaluate our representation-shaping techniques on other static models,\nincluding random token representations. Proposed representation extraction\nmethods improve the performance on STS and clustering tasks for all models\nconsidered. Very high improvements for static token-based models, especially\nrandom embeddings for STS tasks, almost reach the performance of BERT-derived\nrepresentations. Our work shows that the representation-shaping techniques\nsignificantly improve sentence embeddings extracted from BERT-based and simple\nbaseline models.\n","authors":["Lukas Stankevičius","Mantas Lukoševičius"],"pdf_url":"https://arxiv.org/pdf/2408.08073v2.pdf","comment":"Postprint update"},{"id":"http://arxiv.org/abs/2502.14409v1","updated":"2025-02-20T09:57:42Z","published":"2025-02-20T09:57:42Z","title":"Unstructured Evidence Attribution for Long Context Query Focused\n  Summarization","summary":"  Large language models (LLMs) are capable of generating coherent summaries\nfrom very long contexts given a user query. Extracting and properly citing\nevidence spans could help improve the transparency and reliability of these\nsummaries. At the same time, LLMs suffer from positional biases in terms of\nwhich information they understand and attend to, which could affect evidence\ncitation. Whereas previous work has focused on evidence citation with\npredefined levels of granularity (e.g. sentence, paragraph, document, etc.), we\npropose the task of long-context query focused summarization with unstructured\nevidence citation. We show how existing systems struggle to generate and\nproperly cite unstructured evidence from their context, and that evidence tends\nto be \"lost-in-the-middle\". To help mitigate this, we create the Summaries with\nUnstructured Evidence Text dataset (SUnsET), a synthetic dataset generated\nusing a novel domain-agnostic pipeline which can be used as supervision to\nadapt LLMs to this task. We demonstrate across 5 LLMs of different sizes and 4\ndatasets with varying document types and lengths that LLMs adapted with SUnsET\ndata generate more relevant and factually consistent evidence than their base\nmodels, extract evidence from more diverse locations in their context, and can\ngenerate more relevant and consistent summaries.\n","authors":["Dustin Wright","Zain Muhammad Mujahid","Lu Wang","Isabelle Augenstein","David Jurgens"],"pdf_url":"https://arxiv.org/pdf/2502.14409v1.pdf","comment":"24 pages; 21 figures; 5 tables"},{"id":"http://arxiv.org/abs/2502.14361v1","updated":"2025-02-20T08:40:09Z","published":"2025-02-20T08:40:09Z","title":"Retrieval-Augmented Process Reward Model for Generalizable Mathematical\n  Reasoning","summary":"  While large language models (LLMs) have significantly advanced mathematical\nreasoning, Process Reward Models (PRMs) have been developed to evaluate the\nlogical validity of reasoning steps. However, PRMs still struggle with\nout-of-distribution (OOD) challenges. This paper identifies key OOD issues,\nincluding step OOD, caused by differences in reasoning patterns across model\ntypes and sizes, and question OOD, which arises from dataset shifts between\ntraining data and real-world problems. To address these issues, we introduce\nRetrieval-Augmented Process Reward Model (RetrievalPRM), a novel framework\ndesigned to tackle these OOD issues. By utilizing a two-stage\nretrieval-enhanced mechanism, RetrievalPRM retrieves semantically similar\nquestions and steps as a warmup, enhancing PRM's ability to evaluate target\nsteps and improving generalization and reasoning consistency across different\nmodels and problem types. Our extensive experiments demonstrate that\nRetrievalPRM outperforms existing baselines across multiple real-world\ndatasets. Our open-source contributions include a retrieval-enhanced dataset, a\ntuning framework for PRM training, and the RetrievalPRM model, establishing a\nnew standard for PRM performance.\n","authors":["Jiachen Zhu","Congmin Zheng","Jianghao Lin","Kounianhua Du","Ying Wen","Yong Yu","Jun Wang","Weinan Zhang"],"pdf_url":"https://arxiv.org/pdf/2502.14361v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.14332v1","updated":"2025-02-20T07:30:43Z","published":"2025-02-20T07:30:43Z","title":"A Collaborative Jade Recognition System for Mobile Devices Based on\n  Lightweight and Large Models","summary":"  With the widespread adoption and development of mobile devices, vision-based\nrecognition applications have become a hot topic in research. Jade, as an\nimportant cultural heritage and artistic item, has significant applications in\nfields such as jewelry identification and cultural relic preservation. However,\nexisting jade recognition systems still face challenges in mobile\nimplementation, such as limited computing resources, real-time requirements,\nand accuracy issues. To address these challenges, this paper proposes a jade\nrecognition system based on size model collaboration, aiming to achieve\nefficient and accurate jade identification using mobile devices such as\nsmartphones.First, we design a size model based on multi-scale image\nprocessing, extracting key visual information by analyzing jade's dimensions,\nshapes, and surface textures. Then, a collaborative multi-model classification\nframework is built by combining deep learning and traditional computer vision\nalgorithms. This framework can effectively select and adjust models based on\ndifferent jade characteristics, providing high accuracy results across various\nenvironments and devices.Experimental results show that the proposed system can\nprovide high recognition accuracy and fast processing time on mobile devices,\nwhile consuming relatively low computational resources. The system not only\nholds great application potential but also provides new ideas and technical\nsupport for the intelligent development of jade identification.\n","authors":["Zhenyu Wang","Wenjia Li","Pengyu Zhu"],"pdf_url":"https://arxiv.org/pdf/2502.14332v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.14305v1","updated":"2025-02-20T06:40:12Z","published":"2025-02-20T06:40:12Z","title":"Efficient AI in Practice: Training and Deployment of Efficient LLMs for\n  Industry Applications","summary":"  Large language models (LLMs) have demonstrated remarkable performance across\na wide range of industrial applications, from search and recommendations to\ngenerative tasks. Although scaling laws indicate that larger models generally\nyield better generalization and performance, their substantial computational\nrequirements often render them impractical for many real-world scenarios at\nscale. In this paper, we present methods and insights for training small\nlanguage models (SLMs) that deliver high performance and efficiency in\ndeployment. We focus on two key techniques: (1) knowledge distillation and (2)\nmodel compression via quantization and pruning. These approaches enable SLMs to\nretain much of the quality of their larger counterparts while significantly\nreducing training, serving costs, and latency. We detail the impact of these\ntechniques on a variety of use cases at a large professional social network\nplatform and share deployment lessons - including hardware optimization\nstrategies that enhance speed and throughput for both predictive and\nreasoning-based applications.\n","authors":["Kayhan Behdin","Yun Dai","Ata Fatahibaarzi","Aman Gupta","Qingquan Song","Shao Tang","Hejian Sang","Gregory Dexter","Sirou Zhu","Siyu Zhu","Tejas Dharamsi","Maziar Sanjabi","Vignesh Kothapalli","Hamed Firooz","Zhoutong Fu","Yihan Cao","Pin-Lun Hsu","Fedor Borisyuk","Zhipeng Wang","Rahul Mazumder","Natesh Pillai","Luke Simon"],"pdf_url":"https://arxiv.org/pdf/2502.14305v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.08976v3","updated":"2025-02-20T04:50:17Z","published":"2024-02-14T06:43:02Z","title":"Confidence-aware Fine-tuning of Sequential Recommendation Systems via\n  Conformal Prediction","summary":"  In Sequential Recommendation Systems (SRecsys), traditional training\napproaches that rely on Cross-Entropy (CE) loss often prioritize accuracy but\nfail to align well with user satisfaction metrics. CE loss focuses on\nmaximizing the confidence of the ground truth item, which is challenging to\nachieve universally across all users and sessions. It also overlooks the\npractical acceptability of ranking the ground truth item within the top-$K$\npositions, a common metric in SRecsys. To address this limitation, we propose\n\\textbf{CPFT}, a novel fine-tuning framework that integrates Conformal\nPrediction (CP)-based losses with CE loss to optimize accuracy alongside\nconfidence that better aligns with widely used top-$K$ metrics. CPFT embeds CP\nprinciples into the training loop using differentiable proxy losses and\ncomputationally efficient calibration strategies, enabling the generation of\nhigh-confidence prediction sets. These sets focus on items with high relevance\nwhile maintaining robust coverage guarantees. Extensive experiments on five\nreal-world datasets and four distinct sequential models demonstrate that CPFT\nimproves precision metrics and confidence calibration. Our results highlight\nthe importance of confidence-aware fine-tuning in delivering accurate,\ntrustworthy recommendations that enhance user satisfaction.\n","authors":["Chen Wang","Fangxin Wang","Ruocheng Guo","Yueqing Liang","Philip S. Yu"],"pdf_url":"https://arxiv.org/pdf/2402.08976v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.11589v4","updated":"2025-02-20T03:41:23Z","published":"2024-06-17T14:34:14Z","title":"CoSQA+: Pioneering the Multi-Choice Code Search Benchmark with\n  Test-Driven Agents","summary":"  Semantic code search, retrieving code that matches a given natural language\nquery, is an important task to improve productivity in software engineering.\nExisting code search datasets face limitations: they rely on human annotators\nwho assess code primarily through semantic understanding rather than functional\nverification, leading to potential inaccuracies and scalability issues.\nAdditionally, current evaluation metrics often overlook the multi-choice nature\nof code search. This paper introduces CoSQA+, pairing high-quality queries from\nCoSQA with multiple suitable codes. We develop an automated pipeline featuring\nmultiple model-based candidate selections and the novel test-driven agent\nannotation system. Among a single Large Language Model (LLM) annotator and\nPython expert annotators (without test-based verification), agents leverage\ntest-based verification and achieve the highest accuracy of 96.4%. Through\nextensive experiments, CoSQA+ has demonstrated superior quality over CoSQA.\nModels trained on CoSQA+ exhibit improved performance. We provide the code and\ndata at https://github.com/DeepSoftwareAnalytics/CoSQA_Plus.\n","authors":["Jing Gong","Yanghui Wu","Linxi Liang","Yanlin Wang","Jiachi Chen","Mingwei Liu","Zibin Zheng"],"pdf_url":"https://arxiv.org/pdf/2406.11589v4.pdf","comment":"15 pages, 4 figures, conference"},{"id":"http://arxiv.org/abs/2502.14212v1","updated":"2025-02-20T02:47:09Z","published":"2025-02-20T02:47:09Z","title":"Less is More: On the Importance of Data Quality for Unit Test Generation","summary":"  Unit testing is crucial for software development and maintenance. Effective\nunit testing ensures and improves software quality, but writing unit tests is\ntime-consuming and labor-intensive. Recent studies have proposed deep learning\n(DL) techniques or large language models (LLMs) to automate unit test\ngeneration. These models are usually trained or fine-tuned on large-scale\ndatasets. Despite growing awareness of the importance of data quality, there\nhas been limited research on the quality of datasets used for test generation.\nTo bridge this gap, we systematically examine the impact of noise on the\nperformance of learning-based test generation models. We first apply the open\ncard sorting method to analyze the most popular and largest test generation\ndataset, Methods2Test, to categorize eight distinct types of noise. Further, we\nconduct detailed interviews with 17 domain experts to validate and assess the\nimportance, reasonableness, and correctness of the noise taxonomy. Then, we\npropose CleanTest, an automated noise-cleaning framework designed to improve\nthe quality of test generation datasets. CleanTest comprises three filters: a\nrule-based syntax filter, a rule-based relevance filter, and a model-based\ncoverage filter. To evaluate its effectiveness, we apply CleanTest on two\nwidely-used test generation datasets, i.e., Methods2Test and Atlas. Our\nfindings indicate that 43.52% and 29.65% of datasets contain noise,\nhighlighting its prevalence. Finally, we conduct comparative experiments using\nfour LLMs (i.e., CodeBERT, AthenaTest, StarCoder, and CodeLlama7B) to assess\nthe impact of noise on test generation performance. The results show that\nfiltering noise positively influences the test generation ability of the\nmodels.\n","authors":["Junwei Zhang","Xing Hu","Shan Gao","Xin Xia","David Lo","Shanping Li"],"pdf_url":"https://arxiv.org/pdf/2502.14212v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.13713v2","updated":"2025-02-20T02:43:15Z","published":"2025-02-19T13:28:20Z","title":"TALKPLAY: Multimodal Music Recommendation with Large Language Models","summary":"  We present TalkPlay, a multimodal music recommendation system that\nreformulates the recommendation task as large language model token generation.\nTalkPlay represents music through an expanded token vocabulary that encodes\nmultiple modalities - audio, lyrics, metadata, semantic tags, and playlist\nco-occurrence. Using these rich representations, the model learns to generate\nrecommendations through next-token prediction on music recommendation\nconversations, that requires learning the associations natural language query\nand response, as well as music items. In other words, the formulation\ntransforms music recommendation into a natural language understanding task,\nwhere the model's ability to predict conversation tokens directly optimizes\nquery-item relevance. Our approach eliminates traditional\nrecommendation-dialogue pipeline complexity, enabling end-to-end learning of\nquery-aware music recommendations. In the experiment, TalkPlay is successfully\ntrained and outperforms baseline methods in various aspects, demonstrating\nstrong context understanding as a conversational music recommender.\n","authors":["Seungheon Doh","Keunwoo Choi","Juhan Nam"],"pdf_url":"https://arxiv.org/pdf/2502.13713v2.pdf","comment":null}],"Multimedia":[{"id":"http://arxiv.org/abs/2402.05887v2","updated":"2025-02-20T22:48:30Z","published":"2024-02-08T18:23:50Z","title":"Sandwiched Compression: Repurposing Standard Codecs with Neural Network\n  Wrappers","summary":"  We propose sandwiching standard image and video codecs between pre- and\npost-processing neural networks. The networks are jointly trained through a\ndifferentiable codec proxy to minimize a given rate-distortion loss. This\nsandwich architecture not only improves the standard codec's performance on its\nintended content, but more importantly, adapts the codec to other types of\nimage/video content and to other distortion measures. The sandwich learns to\ntransmit ``neural code images'' that optimize and improve overall\nrate-distortion performance, with the improvements becoming significant\nespecially when the overall problem is well outside of the scope of the codec's\ndesign. We apply the sandwich architecture to standard codecs with mismatched\nsources transporting different numbers of channels, higher resolution, higher\ndynamic range, computer graphics, and with perceptual distortion measures. The\nresults demonstrate substantial improvements (up to 9 dB gains or up to 30\\%\nbitrate reductions) compared to alternative adaptations. We establish\noptimality properties for sandwiched compression and design differentiable\ncodec proxies approximating current standard codecs. We further analyze model\ncomplexity, visual quality under perceptual metrics, as well as sandwich\nconfigurations that offer interesting potentials in video compression and\nstreaming.\n","authors":["Onur G. Guleryuz","Philip A. Chou","Berivan Isik","Hugues Hoppe","Danhang Tang","Ruofei Du","Jonathan Taylor","Philip Davidson","Sean Fanello"],"pdf_url":"https://arxiv.org/pdf/2402.05887v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.00358v2","updated":"2025-02-20T22:37:12Z","published":"2025-02-01T07:40:29Z","title":"Do Audio-Visual Segmentation Models Truly Segment Sounding Objects?","summary":"  Unlike traditional visual segmentation, audio-visual segmentation (AVS)\nrequires the model not only to identify and segment objects but also to\ndetermine whether they are sound sources. Recent AVS approaches, leveraging\ntransformer architectures and powerful foundation models like SAM, have\nachieved impressive performance on standard benchmarks. Yet, an important\nquestion remains: Do these models genuinely integrate audio-visual cues to\nsegment sounding objects? In this paper, we systematically investigate this\nissue in the context of robust AVS. Our study reveals a fundamental bias in\ncurrent methods: they tend to generate segmentation masks based predominantly\non visual salience, irrespective of the audio context. This bias results in\nunreliable predictions when sounds are absent or irrelevant. To address this\nchallenge, we introduce AVSBench-Robust, a comprehensive benchmark\nincorporating diverse negative audio scenarios including silence, ambient\nnoise, and off-screen sounds. We also propose a simple yet effective approach\ncombining balanced training with negative samples and classifier-guided\nsimilarity learning. Our extensive experiments show that state-of-theart AVS\nmethods consistently fail under negative audio conditions, demonstrating the\nprevalence of visual bias. In contrast, our approach achieves remarkable\nimprovements in both standard metrics and robustness measures, maintaining\nnear-perfect false positive rates while preserving highquality segmentation\nperformance.\n","authors":["Jia Li","Wenjie Zhao","Ziru Huang","Yunhui Guo","Yapeng Tian"],"pdf_url":"https://arxiv.org/pdf/2502.00358v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.06616v3","updated":"2025-02-20T14:27:15Z","published":"2025-02-10T16:12:47Z","title":"From Code to Canvas","summary":"  The web-based dynamic geometry software CindyJS is a versatile tool to create\ninteractive applications for mathematics and other topics. In this workshop, we\nwill look at a code package that makes the creation of animations in CindyJS\neasier and more streamlined. Animations, which can then be embedded into\npresentations or be used in (lecture) videos. The focus lies on the creation of\nthe animations themselves and some of the technical and artistic fundamentals\nto do so.\n","authors":["Bernhard O. Werner"],"pdf_url":"https://arxiv.org/pdf/2502.06616v3.pdf","comment":"A workshop paper for the Bridges 2025 conference"},{"id":"http://arxiv.org/abs/2502.14439v1","updated":"2025-02-20T10:42:29Z","published":"2025-02-20T10:42:29Z","title":"Visual and Auditory Aesthetic Preferences Across Cultures","summary":"  Research on how humans perceive aesthetics in shapes, colours, and music has\npredominantly focused on Western populations, limiting our understanding of how\ncultural environments shape aesthetic preferences. We present a large-scale\ncross-cultural study examining aesthetic preferences across five distinct\nmodalities extensively explored in the literature: shape, curvature, colour,\nmusical harmony and melody. Our investigation gathers 401,403 preference\njudgements from 4,835 participants across 10 countries, systematically sampling\ntwo-dimensional parameter spaces for each modality. The findings reveal both\nuniversal patterns and cultural variations. Preferences for shape and curvature\ncross-culturally demonstrate a consistent preference for symmetrical forms.\nWhile colour preferences are categorically consistent, relational preferences\nvary across cultures. Musical harmony shows strong agreement in interval\nrelationships despite differing regions of preference within the broad\nfrequency spectrum, while melody shows the highest cross-cultural variation.\nThese results suggest that aesthetic preferences emerge from an interplay\nbetween shared perceptual mechanisms and cultural learning.\n","authors":["Harin Lee","Eline Van Geert","Elif Celen","Raja Marjieh","Pol van Rijn","Minsu Park","Nori Jacoby"],"pdf_url":"https://arxiv.org/pdf/2502.14439v1.pdf","comment":"Submission to CogSci 2025"},{"id":"http://arxiv.org/abs/2502.14273v1","updated":"2025-02-20T05:18:36Z","published":"2025-02-20T05:18:36Z","title":"LLM-EvRep: Learning an LLM-Compatible Event Representation Using a\n  Self-Supervised Framework","summary":"  Recent advancements in event-based recognition have demonstrated significant\npromise, yet most existing approaches rely on extensive training, limiting\ntheir adaptability for efficient processing of event-driven visual content.\nMeanwhile, large language models (LLMs) have exhibited remarkable zero-shot\ncapabilities across diverse domains, but their application to event-based\nvisual recognition remains largely unexplored. To bridge this gap, we propose\n\\textbf{LLM-EvGen}, an event representation generator that produces\nLLM-compatible event representations \\textbf{LLM-EvRep}, thereby enhancing the\nperformance of LLMs on event recognition tasks. The generator is trained using\na self-supervised framework, aligning the generated representations with\nsemantic consistency and structural fidelity. Comprehensive experiments were\nconducted on three datasets: N-ImageNet, N-Caltech101, and N-MNIST. The results\ndemonstrate that our method, \\textbf{LLM-EvRep}, outperforms the event-to-video\nmethod, E2VID, by 15.93\\%, 0.82\\%, and 50.21\\%, respectively, in recognition\ntasks when evaluated using GPT-4o.\n","authors":["Zongyou Yu","Qiang Qu","Qian Zhang","Nan Zhang","Xiaoming Chen"],"pdf_url":"https://arxiv.org/pdf/2502.14273v1.pdf","comment":"6 pages, 2 figures,Companion Proceedings of the ACM Web Conference\n  2025 (WWW Companion '25)"},{"id":"http://arxiv.org/abs/2502.14178v1","updated":"2025-02-20T01:16:11Z","published":"2025-02-20T01:16:11Z","title":"NeRF-3DTalker: Neural Radiance Field with 3D Prior Aided Audio\n  Disentanglement for Talking Head Synthesis","summary":"  Talking head synthesis is to synthesize a lip-synchronized talking head video\nusing audio. Recently, the capability of NeRF to enhance the realism and\ntexture details of synthesized talking heads has attracted the attention of\nresearchers. However, most current NeRF methods based on audio are exclusively\nconcerned with the rendering of frontal faces. These methods are unable to\ngenerate clear talking heads in novel views. Another prevalent challenge in\ncurrent 3D talking head synthesis is the difficulty in aligning acoustic and\nvisual spaces, which often results in suboptimal lip-syncing of the generated\ntalking heads. To address these issues, we propose Neural Radiance Field with\n3D Prior Aided Audio Disentanglement for Talking Head Synthesis\n(NeRF-3DTalker). Specifically, the proposed method employs 3D prior information\nto synthesize clear talking heads with free views. Additionally, we propose a\n3D Prior Aided Audio Disentanglement module, which is designed to disentangle\nthe audio into two distinct categories: features related to 3D awarded speech\nmovements and features related to speaking style. Moreover, to reposition the\ngenerated frames that are distant from the speaker's motion space in the real\nspace, we have devised a local-global Standardized Space. This method\nnormalizes the irregular positions in the generated frames from both global and\nlocal semantic perspectives. Through comprehensive qualitative and quantitative\nexperiments, it has been demonstrated that our NeRF-3DTalker outperforms\nstate-of-the-art in synthesizing realistic talking head videos, exhibiting\nsuperior image quality and lip synchronization. Project page:\nhttps://nerf-3dtalker.github.io/NeRF-3Dtalker.\n","authors":["Xiaoxing Liu","Zhilei Liu","Chongke Bi"],"pdf_url":"https://arxiv.org/pdf/2502.14178v1.pdf","comment":"Accepted by ICASSP 2025"}]},"2025-02-22T00:00:00Z":{"Information Retrieval":[{"id":"http://arxiv.org/abs/2502.14297v2","updated":"2025-02-22T11:35:41Z","published":"2025-02-20T06:22:03Z","title":"Evaluating Sakana's AI Scientist for Autonomous Research: Wishful\n  Thinking or an Emerging Reality Towards 'Artificial Research Intelligence'\n  (ARI)?","summary":"  A major step toward Artificial General Intelligence (AGI) and Super\nIntelligence is AI's ability to autonomously conduct research - what we term\nArtificial Research Intelligence (ARI). If machines could generate hypotheses,\nconduct experiments, and write research papers without human intervention, it\nwould transform science. Sakana recently introduced the 'AI Scientist',\nclaiming to conduct research autonomously, i.e. they imply to have achieved\nwhat we term Artificial Research Intelligence (ARI). The AI Scientist gained\nmuch attention, but a thorough independent evaluation has yet to be conducted.\n  Our evaluation of the AI Scientist reveals critical shortcomings. The\nsystem's literature reviews produced poor novelty assessments, often\nmisclassifying established concepts (e.g., micro-batching for stochastic\ngradient descent) as novel. It also struggles with experiment execution: 42% of\nexperiments failed due to coding errors, while others produced flawed or\nmisleading results. Code modifications were minimal, averaging 8% more\ncharacters per iteration, suggesting limited adaptability. Generated\nmanuscripts were poorly substantiated, with a median of five citations, most\noutdated (only five of 34 from 2020 or later). Structural errors were frequent,\nincluding missing figures, repeated sections, and placeholder text like\n'Conclusions Here'. Some papers contained hallucinated numerical results.\n  Despite these flaws, the AI Scientist represents a leap forward in research\nautomation. It generates full research manuscripts with minimal human input,\nchallenging expectations of AI-driven science. Many reviewers might struggle to\ndistinguish its work from human researchers. While its quality resembles a\nrushed undergraduate paper, its speed and cost efficiency are unprecedented,\nproducing a full paper for USD 6 to 15 with 3.5 hours of human involvement, far\noutpacing traditional researchers.\n","authors":["Joeran Beel","Min-Yen Kan","Moritz Baumgart"],"pdf_url":"https://arxiv.org/pdf/2502.14297v2.pdf","comment":"16 pages"},{"id":"http://arxiv.org/abs/2502.14100v2","updated":"2025-02-22T23:03:21Z","published":"2025-02-19T20:59:35Z","title":"Towards Context-Robust LLMs: A Gated Representation Fine-tuning Approach","summary":"  Large Language Models (LLMs) enhanced with external contexts, such as through\nretrieval-augmented generation (RAG), often face challenges in handling\nimperfect evidence. They tend to over-rely on external knowledge, making them\nvulnerable to misleading and unhelpful contexts. To address this, we propose\nthe concept of context-robust LLMs, which can effectively balance internal\nknowledge with external context, similar to human cognitive processes.\nSpecifically, context-robust LLMs should rely on external context only when\nlacking internal knowledge, identify contradictions between internal and\nexternal knowledge, and disregard unhelpful contexts. To achieve this goal, we\nintroduce Grft, a lightweight and plug-and-play gated representation\nfine-tuning approach. Grft consists of two key components: a gating mechanism\nto detect and filter problematic inputs, and low-rank representation adapters\nto adjust hidden representations. By training a lightweight intervention\nfunction with only 0.0004\\% of model size on fewer than 200 examples, Grft can\neffectively adapt LLMs towards context-robust behaviors.\n","authors":["Shenglai Zeng","Pengfei He","Kai Guo","Tianqi Zheng","Hanqing Lu","Yue Xing","Hui Liu"],"pdf_url":"https://arxiv.org/pdf/2502.14100v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.11916v4","updated":"2025-02-22T06:40:08Z","published":"2025-01-21T06:43:16Z","title":"Generating with Fairness: A Modality-Diffused Counterfactual Framework\n  for Incomplete Multimodal Recommendations","summary":"  Incomplete scenario is a prevalent, practical, yet challenging setting in\nMultimodal Recommendations (MMRec), where some item modalities are missing due\nto various factors. Recently, a few efforts have sought to improve the\nrecommendation accuracy by exploring generic structures from incomplete data.\nHowever, two significant gaps persist: 1) the difficulty in accurately\ngenerating missing data due to the limited ability to capture modality\ndistributions; and 2) the critical but overlooked visibility bias, where items\nwith missing modalities are more likely to be disregarded due to the\nprioritization of items' multimodal data over user preference alignment. This\nbias raises serious concerns about the fair treatment of items. To bridge these\ntwo gaps, we propose a novel Modality-Diffused Counterfactual (MoDiCF)\nframework for incomplete multimodal recommendations. MoDiCF features two key\nmodules: a novel modality-diffused data completion module and a new\ncounterfactual multimodal recommendation module. The former, equipped with a\nparticularly designed multimodal generative framework, accurately generates and\niteratively refines missing data from learned modality-specific distribution\nspaces. The latter, grounded in the causal perspective, effectively mitigates\nthe negative causal effects of visibility bias and thus assures fairness in\nrecommendations. Both modules work collaboratively to address the two\naforementioned significant gaps for generating more accurate and fair results.\nExtensive experiments on three real-world datasets demonstrate the superior\nperformance of MoDiCF in terms of both recommendation accuracy and fairness.\nThe code and processed datasets are released at\nhttps://github.com/JinLi-i/MoDiCF.\n","authors":["Jin Li","Shoujin Wang","Qi Zhang","Shui Yu","Fang Chen"],"pdf_url":"https://arxiv.org/pdf/2501.11916v4.pdf","comment":"Accepted by WWW 2025"},{"id":"http://arxiv.org/abs/2502.11374v2","updated":"2025-02-22T15:23:07Z","published":"2025-02-17T02:41:11Z","title":"Leave No One Behind: Enhancing Diversity While Maintaining Accuracy in\n  Social Recommendation","summary":"  Social recommendation, which incorporates social connections into recommender\nsystems, has proven effective in improving recommendation accuracy. However,\nbeyond accuracy, diversity is also crucial for enhancing user engagement.\nDespite its importance, the impact of social recommendation models on diversity\nremains largely unexplored. In this study, we systematically examine the dual\nperformance of existing social recommendation algorithms in terms of both\naccuracy and diversity. Our empirical analysis reveals a concerning trend:\nwhile social recommendation models enhance accuracy, they often reduce\ndiversity. To address this issue, we propose Diversified Social Recommendation\n(DivSR), a novel approach that employs relational knowledge distillation to\ntransfer high-diversity structured knowledge from non-social recommendation\nmodels to social recommendation models. DivSR is a lightweight, model-agnostic\nframework that seamlessly integrates with existing social recommendation\narchitectures. Experiments on three benchmark datasets demonstrate that DivSR\nsignificantly enhances diversity while maintaining competitive accuracy,\nachieving a superior accuracy-diversity trade-off. Our code and data are\npublicly available at: https://github.com/ll0ruc/DivSR.\n","authors":["Lei Li","Xiao Zhou"],"pdf_url":"https://arxiv.org/pdf/2502.11374v2.pdf","comment":"Accepted by DASFAA2025"}],"Multimedia":[{"id":"http://arxiv.org/abs/2502.04128v2","updated":"2025-02-22T11:32:13Z","published":"2025-02-06T15:04:00Z","title":"Llasa: Scaling Train-Time and Inference-Time Compute for Llama-based\n  Speech Synthesis","summary":"  Recent advances in text-based large language models (LLMs), particularly in\nthe GPT series and the o1 model, have demonstrated the effectiveness of scaling\nboth training-time and inference-time compute. However, current\nstate-of-the-art TTS systems leveraging LLMs are often multi-stage, requiring\nseparate models (e.g., diffusion models after LLM), complicating the decision\nof whether to scale a particular model during training or testing. This work\nmakes the following contributions: First, we explore the scaling of train-time\nand inference-time compute for speech synthesis. Second, we propose a simple\nframework Llasa for speech synthesis that employs a single-layer vector\nquantizer (VQ) codec and a single Transformer architecture to fully align with\nstandard LLMs such as Llama. Our experiments reveal that scaling train-time\ncompute for Llasa consistently improves the naturalness of synthesized speech\nand enables the generation of more complex and accurate prosody patterns.\nFurthermore, from the perspective of scaling inference-time compute, we employ\nspeech understanding models as verifiers during the search, finding that\nscaling inference-time compute shifts the sampling modes toward the preferences\nof specific verifiers, thereby improving emotional expressiveness, timbre\nconsistency, and content accuracy. In addition, we released the checkpoint and\ntraining code for our TTS model (1B, 3B, 8B) and codec model publicly\navailable.\n","authors":["Zhen Ye","Xinfa Zhu","Chi-Min Chan","Xinsheng Wang","Xu Tan","Jiahe Lei","Yi Peng","Haohe Liu","Yizhu Jin","Zheqi Dai","Hongzhan Lin","Jianyi Chen","Xingjian Du","Liumeng Xue","Yunlin Chen","Zhifei Li","Lei Xie","Qiuqiang Kong","Yike Guo","Wei Xue"],"pdf_url":"https://arxiv.org/pdf/2502.04128v2.pdf","comment":null}]},"2025-02-19T00:00:00Z":{"Information Retrieval":[{"id":"http://arxiv.org/abs/2410.16458v2","updated":"2025-02-19T23:34:29Z","published":"2024-10-21T19:34:40Z","title":"STAR: A Simple Training-free Approach for Recommendations using Large\n  Language Models","summary":"  Recent progress in large language models (LLMs) offers promising new\napproaches for recommendation system tasks. While the current state-of-the-art\nmethods rely on fine-tuning LLMs to achieve optimal results, this process is\ncostly and introduces significant engineering complexities. Conversely, methods\nthat directly use LLMs without additional fine-tuning result in a large drop in\nrecommendation quality, often due to the inability to capture collaborative\ninformation. In this paper, we propose a Simple Training-free Approach for\nRecommendation (STAR), a framework that utilizes LLMs and can be applied to\nvarious recommendation tasks without the need for fine-tuning, while\nmaintaining high quality recommendation performance. Our approach involves a\nretrieval stage that uses semantic embeddings from LLMs combined with\ncollaborative user information to retrieve candidate items. We then apply an\nLLM for pairwise ranking to enhance next-item prediction. Experimental results\non the Amazon Review dataset show competitive performance for next item\nprediction, even with our retrieval stage alone. Our full method achieves\nHits@10 performance of +23.8% on Beauty, +37.5% on Toys & Games, and -1.8% on\nSports & Outdoors relative to the best supervised models. This framework offers\nan effective alternative to traditional supervised models, highlighting the\npotential of LLMs in recommendation systems without extensive training or\ncustom architectures.\n","authors":["Dong-Ho Lee","Adam Kraft","Long Jin","Nikhil Mehta","Taibai Xu","Lichan Hong","Ed H. Chi","Xinyang Yi"],"pdf_url":"https://arxiv.org/pdf/2410.16458v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.14137v1","updated":"2025-02-19T22:47:40Z","published":"2025-02-19T22:47:40Z","title":"Collaborative Retrieval for Large Language Model-based Conversational\n  Recommender Systems","summary":"  Conversational recommender systems (CRS) aim to provide personalized\nrecommendations via interactive dialogues with users. While large language\nmodels (LLMs) enhance CRS with their superior understanding of context-aware\nuser preferences, they typically struggle to leverage behavioral data, which\nhave proven to be important for classical collaborative filtering (CF)-based\napproaches. For this reason, we propose CRAG, Collaborative Retrieval Augmented\nGeneration for LLM-based CRS. To the best of our knowledge, CRAG is the first\napproach that combines state-of-the-art LLMs with CF for conversational\nrecommendations. Our experiments on two publicly available movie conversational\nrecommendation datasets, i.e., a refined Reddit dataset (which we name\nReddit-v2) as well as the Redial dataset, demonstrate the superior item\ncoverage and recommendation performance of CRAG, compared to several CRS\nbaselines. Moreover, we observe that the improvements are mainly due to better\nrecommendation accuracy on recently released movies. The code and data are\navailable at https://github.com/yaochenzhu/CRAG.\n","authors":["Yaochen Zhu","Chao Wan","Harald Steck","Dawen Liang","Yesu Feng","Nathan Kallus","Jundong Li"],"pdf_url":"https://arxiv.org/pdf/2502.14137v1.pdf","comment":"Accepted by WWW'2025"},{"id":"http://arxiv.org/abs/2502.02464v3","updated":"2025-02-19T22:46:25Z","published":"2025-02-04T16:33:25Z","title":"Rankify: A Comprehensive Python Toolkit for Retrieval, Re-Ranking, and\n  Retrieval-Augmented Generation","summary":"  Retrieval, re-ranking, and retrieval-augmented generation (RAG) are critical\ncomponents of modern applications in information retrieval, question answering,\nor knowledge-based text generation. However, existing solutions are often\nfragmented, lacking a unified framework that easily integrates these essential\nprocesses. The absence of a standardized implementation, coupled with the\ncomplexity of retrieval and re-ranking workflows, makes it challenging for\nresearchers to compare and evaluate different approaches in a consistent\nenvironment. While existing toolkits such as Rerankers and RankLLM provide\ngeneral-purpose reranking pipelines, they often lack the flexibility required\nfor fine-grained experimentation and benchmarking. In response to these\nchallenges, we introduce Rankify, a powerful and modular open-source toolkit\ndesigned to unify retrieval, re-ranking, and RAG within a cohesive framework.\nRankify supports a wide range of retrieval techniques, including dense and\nsparse retrievers, while incorporating state-of-the-art re-ranking models to\nenhance retrieval quality. Additionally, Rankify includes a collection of\npre-retrieved datasets to facilitate benchmarking, available at Huggingface\n(https://huggingface.co/datasets/abdoelsayed/reranking-datasets-light). To\nencourage adoption and ease of integration, we provide comprehensive\ndocumentation (http://rankify.readthedocs.io/), an open-source implementation\non GitHub (https://github.com/DataScienceUIBK/rankify), and a PyPI package for\neasy installation (https://pypi.org/project/rankify/). As a unified and\nlightweight framework, Rankify allows researchers and practitioners to advance\nretrieval and re-ranking methodologies while ensuring consistency, scalability,\nand ease of use.\n","authors":["Abdelrahman Abdallah","Bhawna Piryani","Jamshid Mozafari","Mohammed Ali","Adam Jatowt"],"pdf_url":"https://arxiv.org/pdf/2502.02464v3.pdf","comment":"Work in Progress"},{"id":"http://arxiv.org/abs/2406.14043v3","updated":"2025-02-19T22:05:46Z","published":"2024-06-20T07:06:58Z","title":"Taxonomy-Guided Zero-Shot Recommendations with LLMs","summary":"  With the emergence of large language models (LLMs) and their ability to\nperform a variety of tasks, their application in recommender systems (RecSys)\nhas shown promise. However, we are facing significant challenges when deploying\nLLMs into RecSys, such as limited prompt length, unstructured item information,\nand un-constrained generation of recommendations, leading to sub-optimal\nperformance. To address these issues, we propose a novel method using a\ntaxonomy dictionary. This method provides a systematic framework for\ncategorizing and organizing items, improving the clarity and structure of item\ninformation. By incorporating the taxonomy dictionary into LLM prompts, we\nachieve efficient token utilization and controlled feature generation, leading\nto more accurate and contextually relevant recommendations. Our Taxonomy-guided\nRecommendation (TaxRec) approach features a two-step process: one-time taxonomy\ncategorization and LLM-based recommendation, enabling zero-shot recommendations\nwithout the need for domain-specific fine-tuning. Experimental results\ndemonstrate TaxRec significantly enhances recommendation quality compared to\ntraditional zero-shot approaches, showcasing its efficacy as personal\nrecommender with LLMs. Code is available at\nhttps://github.com/yueqingliang1/TaxRec.\n","authors":["Yueqing Liang","Liangwei Yang","Chen Wang","Xiongxiao Xu","Philip S. Yu","Kai Shu"],"pdf_url":"https://arxiv.org/pdf/2406.14043v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.10916v2","updated":"2025-02-19T19:36:25Z","published":"2025-02-15T22:08:53Z","title":"An Open-Source Web-Based Tool for Evaluating Open-Source Large Language\n  Models Leveraging Information Retrieval from Custom Documents","summary":"  In our work, we present the first-of-its-kind open-source web-based tool\nwhich is able to demonstrate the impacts of a user's speech act during\ndiscourse with conversational agents, which leverages open-source large\nlanguage models. With this software resource, it is possible for researchers\nand experts to evaluate the performance of various dialogues, visualize the\nuser's communicative intents, and utilise uploaded specific documents for the\nchat agent to use for its information retrieval to respond to the user query.\nThe context gathered by these models is obtained from a set of linguistic\nfeatures extracted, which forms the context embeddings of the models.\nRegardless of these models showing good context understanding based on these\nfeatures, there still remains a gap in including deeper pragmatic features to\nimprove the model's comprehension of the query, hence the efforts to develop\nthis web resource, which is able to extract and then inject this overlooked\nfeature in the encoder-decoder pipeline of the conversational agent. To\ndemonstrate the effect and impact of the resource, we carried out an experiment\nwhich evaluated the system using 2 knowledge files for information retrieval,\nwith two user queries each, across 5 open-source large language models using 10\nstandard metrics. Our results showed that larger open-source models,\ndemonstrated an improved alignment when the user speech act was included with\ntheir query. The smaller models in contrast showed an increased perplexity and\nmixed performance, which explicitly indicated struggles in processing queries\nthat explicitly included speech acts. The results from the analysis using the\ndeveloped web resource highlight the potential of speech acts towards enhancing\nconversational depths while underscoring the need for model-specific\noptimizations to address increased computational costs and response times.\n","authors":["Godfrey I"],"pdf_url":"https://arxiv.org/pdf/2502.10916v2.pdf","comment":"19 pages, 1 figure, 6 tables"},{"id":"http://arxiv.org/abs/2502.13912v1","updated":"2025-02-19T17:44:13Z","published":"2025-02-19T17:44:13Z","title":"Optimizing Research Portfolio For Semantic Impact","summary":"  Citation metrics are widely used to assess academic impact but suffer from\nsocial biases, including institutional prestige and journal visibility. Here we\nintroduce rXiv Semantic Impact (XSI), a novel framework that predicts research\nimpact by analyzing how scientific semantic graphs evolve in underlying fabric\nof science. Rather than counting citations, XSI tracks the evolution of\nresearch concepts in the academic knowledge graph (KG). Starting with a\nconstruction of a comprehensive KG from 324K biomedical publications\n(2003-2025), we demonstrate that XSI can predict a paper's future semantic\nimpact (SI) with remarkable accuracy ($R^2$ = 0.69) three years in advance. We\nleverage these predictions to develop an optimization framework for research\nportfolio selection that systematically outperforms random allocation. We\npropose SI as a complementary metric to citations and present XSI as a tool to\nguide funding and publishing decisions, enhancing research impact while\nmitigating risk.\n","authors":["Alexander V. Belikov"],"pdf_url":"https://arxiv.org/pdf/2502.13912v1.pdf","comment":"24 pages; 13 figures"},{"id":"http://arxiv.org/abs/2502.13908v1","updated":"2025-02-19T17:40:32Z","published":"2025-02-19T17:40:32Z","title":"Judging the Judges: A Collection of LLM-Generated Relevance Judgements","summary":"  Using Large Language Models (LLMs) for relevance assessments offers promising\nopportunities to improve Information Retrieval (IR), Natural Language\nProcessing (NLP), and related fields. Indeed, LLMs hold the promise of allowing\nIR experimenters to build evaluation collections with a fraction of the manual\nhuman labor currently required. This could help with fresh topics on which\nthere is still limited knowledge and could mitigate the challenges of\nevaluating ranking systems in low-resource scenarios, where it is challenging\nto find human annotators. Given the fast-paced recent developments in the\ndomain, many questions concerning LLMs as assessors are yet to be answered.\nAmong the aspects that require further investigation, we can list the impact of\nvarious components in a relevance judgment generation pipeline, such as the\nprompt used or the LLM chosen.\n  This paper benchmarks and reports on the results of a large-scale automatic\nrelevance judgment evaluation, the LLMJudge challenge at SIGIR 2024, where\ndifferent relevance assessment approaches were proposed. In detail, we release\nand benchmark 42 LLM-generated labels of the TREC 2023 Deep Learning track\nrelevance judgments produced by eight international teams who participated in\nthe challenge. Given their diverse nature, these automatically generated\nrelevance judgments can help the community not only investigate systematic\nbiases caused by LLMs but also explore the effectiveness of ensemble models,\nanalyze the trade-offs between different models and human assessors, and\nadvance methodologies for improving automated evaluation techniques. The\nreleased resource is available at the following link:\nhttps://llm4eval.github.io/LLMJudge-benchmark/\n","authors":["Hossein A. Rahmani","Clemencia Siro","Mohammad Aliannejadi","Nick Craswell","Charles L. A. Clarke","Guglielmo Faggioli","Bhaskar Mitra","Paul Thomas","Emine Yilmaz"],"pdf_url":"https://arxiv.org/pdf/2502.13908v1.pdf","comment":"11 pages"},{"id":"http://arxiv.org/abs/2408.10604v2","updated":"2025-02-19T17:25:39Z","published":"2024-08-20T07:37:06Z","title":"Multilingual Non-Factoid Question Answering with Answer Paragraph\n  Selection","summary":"  Most existing Question Answering Datasets (QuADs) primarily focus on\nfactoid-based short-context Question Answering (QA) in high-resource languages.\nHowever, the scope of such datasets for low-resource languages remains limited,\nwith only a few works centered on factoid-based QuADs and none on non-factoid\nQuADs. Therefore, this work presents MuNfQuAD, a multilingual QuAD with\nnon-factoid questions. It utilizes interrogative sub-headings from BBC news\narticles as questions and the corresponding paragraphs as silver answers. The\ndataset comprises over 578K QA pairs across 38 languages, encompassing several\nlow-resource languages, and stands as the largest multilingual QA dataset to\ndate. Based on the manual annotations of 790 QA-pairs from MuNfQuAD (golden\nset), we observe that 98\\% of questions can be answered using their\ncorresponding silver answer. Our fine-tuned Answer Paragraph Selection (APS)\nmodel outperforms the baselines. The APS model attained an accuracy of 80\\% and\n72\\%, as well as a macro F1 of 72\\% and 66\\%, on the MuNfQuAD testset and the\ngolden set, respectively. Furthermore, the APS model effectively generalizes a\ncertain language within the golden set, even after being fine-tuned on silver\nlabels. We also observe that the fine-tuned APS model is beneficial for\nreducing the context of a question. These findings suggest that this resource\nwould be a valuable contribution to the QA research community.\n","authors":["Ritwik Mishra","Sreeram Vennam","Rajiv Ratn Shah","Ponnurangam Kumaraguru"],"pdf_url":"https://arxiv.org/pdf/2408.10604v2.pdf","comment":"Shorter version accepted into DSFA, a special session in PAKDD 2025,\n  Sydney"},{"id":"http://arxiv.org/abs/2502.13881v1","updated":"2025-02-19T17:05:42Z","published":"2025-02-19T17:05:42Z","title":"PSCon: Toward Conversational Product Search","summary":"  Conversational Product Search (CPS) is confined to simulated conversations\ndue to the lack of real-world CPS datasets that reflect human-like language.\nAdditionally, current conversational datasets are limited to support\ncross-market and multi-lingual usage. In this paper, we introduce a new CPS\ndata collection protocol and present PSCon, a novel CPS dataset designed to\nassist product search via human-like conversations. The dataset is constructed\nusing a coached human-to-human data collection protocol and supports two\nlanguages and dual markets. Also, the dataset enables thorough exploration of\nsix subtasks of CPS: user intent detection, keyword extraction, system action\nprediction, question selection, item ranking, and response generation.\nFurthermore, we also offer an analysis of the dataset and propose a benchmark\nmodel on the proposed CPS dataset.\n","authors":["Jie Zou","Mohammad Aliannejadi","Evangelos Kanoulas","Shuxi Han","Heli Ma","Zheng Wang","Yang Yang","Heng Tao Shen"],"pdf_url":"https://arxiv.org/pdf/2502.13881v1.pdf","comment":"11 pages"},{"id":"http://arxiv.org/abs/2501.19241v4","updated":"2025-02-19T16:41:06Z","published":"2025-01-31T15:55:14Z","title":"Emancipatory Information Retrieval","summary":"  Our world today is facing a confluence of several mutually reinforcing crises\neach of which intersects with concerns of social justice and emancipation. This\npaper is a provocation for the role of computer-mediated information access in\nour emancipatory struggles. We define emancipatory information retrieval as the\nstudy and development of information access methods that challenge various\nforms of human oppression, and situates its activities within broader\ncollective emancipatory praxis. The term \"emancipatory\" here signifies the\nmoral concerns of universal humanization of all peoples and the elimination of\noppression to create the conditions under which we can collectively flourish.\nTo develop an emancipatory research agenda for information retrieval (IR), in\nthis paper we speculate about the practices that the community can adopt,\nenumerate some of the projects that the field should undertake, and discuss\nprovocations to spark new ideas and directions for research. We challenge the\nfield of IR research to embrace humanistic values and commit to universal\nemancipation and social justice. We also invite scholars from fields such as\nhuman-computer interaction, information sciences, media studies, design, social\nsciences, humanities, democratic theory, and critical theory, as well as legal\nand policy experts, civil rights and social justice activists, and artists to\njoin us in realizing this transformation. In this process, we must both imagine\npost-oppressive worlds, and reimagine the role of IR in that world and in the\njourney that leads us there.\n","authors":["Bhaskar Mitra"],"pdf_url":"https://arxiv.org/pdf/2501.19241v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.13845v1","updated":"2025-02-19T16:08:17Z","published":"2025-02-19T16:08:17Z","title":"Enhancing LLM-Based Recommendations Through Personalized Reasoning","summary":"  Current recommendation systems powered by large language models (LLMs) often\nunderutilize their reasoning capabilities due to a lack of explicit logical\nstructuring. To address this limitation, we introduce CoT-Rec, a framework that\nintegrates Chain-of-Thought (CoT) reasoning into LLM-driven recommendations by\nincorporating two crucial processes: user preference analysis and item\nperception evaluation. CoT-Rec operates in two key phases: (1) personalized\ndata extraction, where user preferences and item perceptions are identified,\nand (2) personalized data application, where this information is leveraged to\nrefine recommendations. Our experimental analysis demonstrates that CoT-Rec\nimproves recommendation accuracy by making better use of LLMs' reasoning\npotential. The implementation is publicly available at\nhttps://anonymous.4open.science/r/CoT-Rec.\n","authors":["Jiahao Liu","Xueshuo Yan","Dongsheng Li","Guangping Zhang","Hansu Gu","Peng Zhang","Tun Lu","Li Shang","Ning Gu"],"pdf_url":"https://arxiv.org/pdf/2502.13845v1.pdf","comment":"7 pages, under review"},{"id":"http://arxiv.org/abs/2502.13843v1","updated":"2025-02-19T16:02:59Z","published":"2025-02-19T16:02:59Z","title":"Enhancing Cross-Domain Recommendations with Memory-Optimized LLM-Based\n  User Agents","summary":"  Large Language Model (LLM)-based user agents have emerged as a powerful tool\nfor improving recommender systems by simulating user interactions. However,\nexisting methods struggle with cross-domain scenarios due to inefficient memory\nstructures, leading to irrelevant information retention and failure to account\nfor social influence factors such as popularity. To address these limitations,\nwe introduce AgentCF++, a novel framework featuring a dual-layer memory\narchitecture and a two-step fusion mechanism to filter domain-specific\npreferences effectively. Additionally, we propose interest groups with shared\nmemory, allowing the model to capture the impact of popularity trends on users\nwith similar interests. Through extensive experiments on multiple cross-domain\ndatasets, AgentCF++ demonstrates superior performance over baseline models,\nhighlighting its effectiveness in refining user behavior simulation for\nrecommender systems. Our code is available at\nhttps://anonymous.4open.science/r/AgentCF-plus.\n","authors":["Jiahao Liu","Shengkang Gu","Dongsheng Li","Guangping Zhang","Mingzhe Han","Hansu Gu","Peng Zhang","Tun Lu","Li Shang","Ning Gu"],"pdf_url":"https://arxiv.org/pdf/2502.13843v1.pdf","comment":"6 pages, under review"},{"id":"http://arxiv.org/abs/2502.13840v1","updated":"2025-02-19T15:59:49Z","published":"2025-02-19T15:59:49Z","title":"Mitigating Popularity Bias in Collaborative Filtering through Fair\n  Sampling","summary":"  Recommender systems often suffer from popularity bias, where frequently\ninteracted items are overrepresented in recommendations. This bias stems from\npropensity factors influencing training data, leading to imbalanced exposure.\nIn this paper, we introduce a Fair Sampling (FS) approach to address this issue\nby ensuring that both users and items are selected with equal probability as\npositive and negative instances. Unlike traditional inverse propensity score\n(IPS) methods, FS does not require propensity estimation, eliminating errors\nassociated with inaccurate calculations. Our theoretical analysis demonstrates\nthat FS effectively neutralizes the influence of propensity factors, achieving\nunbiased learning. Experimental results validate that FS outperforms\nstate-of-the-art methods in both point-wise and pair-wise recommendation tasks,\nenhancing recommendation fairness without sacrificing accuracy. The\nimplementation is available at https://anonymous.4open.science/r/Fair-Sampling.\n","authors":["Jiahao Liu","Dongsheng Li","Hansu Gu","Peng Zhang","Tun Lu","Li Shang","Ning Gu"],"pdf_url":"https://arxiv.org/pdf/2502.13840v1.pdf","comment":"6 pages, under review"},{"id":"http://arxiv.org/abs/2502.13826v1","updated":"2025-02-19T15:41:08Z","published":"2025-02-19T15:41:08Z","title":"In-Place Updates of a Graph Index for Streaming Approximate Nearest\n  Neighbor Search","summary":"  Indices for approximate nearest neighbor search (ANNS) are a basic component\nfor information retrieval and widely used in database, search, recommendation\nand RAG systems. In these scenarios, documents or other objects are inserted\ninto and deleted from the working set at a high rate, requiring a stream of\nupdates to the vector index. Algorithms based on proximity graph indices are\nthe most efficient indices for ANNS, winning many benchmark competitions.\nHowever, it is challenging to update such graph index at a high rate, while\nsupporting stable recall after many updates. Since the graph is singly-linked,\ndeletions are hard because there is no fast way to find in-neighbors of a\ndeleted vertex. Therefore, to update the graph, state-of-the-art algorithms\nsuch as FreshDiskANN accumulate deletions in a batch and periodically\nconsolidate, removing edges to deleted vertices and modifying the graph to\nensure recall stability. In this paper, we present IP-DiskANN\n(InPlaceUpdate-DiskANN), the first algorithm to avoid batch consolidation by\nefficiently processing each insertion and deletion in-place. Our experiments\nusing standard benchmarks show that IP-DiskANN has stable recall over various\nlengthy update patterns in both high-recall and low-recall regimes. Further,\nits query throughput and update speed are better than using the batch\nconsolidation algorithm and HNSW.\n","authors":["Haike Xu","Magdalen Dobson Manohar","Philip A. Bernstein","Badrish Chandramouli","Richard Wen","Harsha Vardhan Simhadri"],"pdf_url":"https://arxiv.org/pdf/2502.13826v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.13783v1","updated":"2025-02-19T14:48:25Z","published":"2025-02-19T14:48:25Z","title":"Generative Large Recommendation Models: Emerging Trends in LLMs for\n  Recommendation","summary":"  In the era of information overload, recommendation systems play a pivotal\nrole in filtering data and delivering personalized content. Recent advancements\nin feature interaction and user behavior modeling have significantly enhanced\nthe recall and ranking processes of these systems. With the rise of large\nlanguage models (LLMs), new opportunities have emerged to further improve\nrecommendation systems. This tutorial explores two primary approaches for\nintegrating LLMs: LLMs-enhanced recommendations, which leverage the reasoning\ncapabilities of general LLMs, and generative large recommendation models, which\nfocus on scaling and sophistication. While the former has been extensively\ncovered in existing literature, the latter remains underexplored. This tutorial\naims to fill this gap by providing a comprehensive overview of generative large\nrecommendation models, including their recent advancements, challenges, and\npotential research directions. Key topics include data quality, scaling laws,\nuser behavior mining, and efficiency in training and inference. By engaging\nwith this tutorial, participants will gain insights into the latest\ndevelopments and future opportunities in the field, aiding both academic\nresearch and practical applications. The timely nature of this exploration\nsupports the rapid evolution of recommendation systems, offering valuable\nguidance for researchers and practitioners alike.\n","authors":["Hao Wang","Wei Guo","Luankang Zhang","Jin Yao Chin","Yufei Ye","Huifeng Guo","Yong Liu","Defu Lian","Ruiming Tang","Enhong Chen"],"pdf_url":"https://arxiv.org/pdf/2502.13783v1.pdf","comment":"This paper has been accepted for the tutorial track at WWW 2025"},{"id":"http://arxiv.org/abs/2502.13763v1","updated":"2025-02-19T14:23:18Z","published":"2025-02-19T14:23:18Z","title":"Unsupervised Graph Embeddings for Session-based Recommendation with Item\n  Features","summary":"  In session-based recommender systems, predictions are based on the user's\npreceding behavior in the session. State-of-the-art sequential recommendation\nalgorithms either use graph neural networks to model sessions in a graph or\nleverage the similarity of sessions by exploiting item features. In this paper,\nwe combine these two approaches and propose a novel method, Graph Convolutional\nNetwork Extension (GCNext), which incorporates item features directly into the\ngraph representation via graph convolutional networks. GCNext creates a\nfeature-rich item co-occurrence graph and learns the corresponding item\nembeddings in an unsupervised manner. We show on three datasets that\nintegrating GCNext into sequential recommendation algorithms significantly\nboosts the performance of nearest-neighbor methods as well as neural network\nmodels. Our flexible extension is easy to incorporate in state-of-the-art\nmethods and increases the MRR@20 by up to 12.79%.\n","authors":["Andreas Peintner","Marta Moscati","Emilia Parada-Cabaleiro","Markus Schedl","Eva Zangerle"],"pdf_url":"https://arxiv.org/pdf/2502.13763v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.03365v3","updated":"2025-02-19T14:15:17Z","published":"2024-01-31T11:03:58Z","title":"Heterophily-Aware Fair Recommendation using Graph Convolutional Networks","summary":"  In recent years, graph neural networks (GNNs) have become a popular tool to\nimprove the accuracy and performance of recommender systems. Modern recommender\nsystems are not only designed to serve end users, but also to benefit other\nparticipants, such as items and item providers. These participants may have\ndifferent or conflicting goals and interests, which raises the need for\nfairness and popularity bias considerations. GNN-based recommendation methods\nalso face the challenges of unfairness and popularity bias, and their\nnormalization and aggregation processes suffer from these challenges. In this\npaper, we propose a fair GNN-based recommender system, called HetroFair, to\nimprove item-side fairness. HetroFair uses two separate components to generate\nfairness-aware embeddings: i) Fairness-aware attention, which incorporates the\ndot product in the normalization process of GNNs to decrease the effect of\nnodes' degrees. ii) Heterophily feature weighting, to assign distinct weights\nto different features during the aggregation process. To evaluate the\neffectiveness of HetroFair, we conduct extensive experiments over six\nreal-world datasets. Our experimental results reveal that HetroFair not only\nalleviates unfairness and popularity bias on the item side but also achieves\nsuperior accuracy on the user side. Our implementation is publicly available at\nhttps://github.com/NematGH/HetroFair.\n","authors":["Nemat Gholinejad","Mostafa Haghir Chehreghani"],"pdf_url":"https://arxiv.org/pdf/2402.03365v3.pdf","comment":"24 pages"},{"id":"http://arxiv.org/abs/2502.13719v1","updated":"2025-02-19T13:45:27Z","published":"2025-02-19T13:45:27Z","title":"TrustRAG: An Information Assistant with Retrieval Augmented Generation","summary":"  \\Ac{RAG} has emerged as a crucial technique for enhancing large models with\nreal-time and domain-specific knowledge. While numerous improvements and\nopen-source tools have been proposed to refine the \\ac{RAG} framework for\naccuracy, relatively little attention has been given to improving the\ntrustworthiness of generated results. To address this gap, we introduce\nTrustRAG, a novel framework that enhances \\ac{RAG} from three perspectives:\nindexing, retrieval, and generation. Specifically, in the indexing stage, we\npropose a semantic-enhanced chunking strategy that incorporates hierarchical\nindexing to supplement each chunk with contextual information, ensuring\nsemantic completeness. In the retrieval stage, we introduce a utility-based\nfiltering mechanism to identify high-quality information, supporting answer\ngeneration while reducing input length. In the generation stage, we propose\nfine-grained citation enhancement, which detects opinion-bearing sentences in\nresponses and infers citation relationships at the sentence-level, thereby\nimproving citation accuracy. We open-source the TrustRAG framework and provide\na demonstration studio designed for excerpt-based question answering tasks\n\\footnote{https://huggingface.co/spaces/golaxy/TrustRAG}. Based on these, we\naim to help researchers: 1) systematically enhancing the trustworthiness of\n\\ac{RAG} systems and (2) developing their own \\ac{RAG} systems with more\nreliable outputs.\n","authors":["Yixing Fan","Qiang Yan","Wenshan Wang","Jiafeng Guo","Ruqing Zhang","Xueqi Cheng"],"pdf_url":"https://arxiv.org/pdf/2502.13719v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.13668v1","updated":"2025-02-19T12:24:46Z","published":"2025-02-19T12:24:46Z","title":"PeerQA: A Scientific Question Answering Dataset from Peer Reviews","summary":"  We present PeerQA, a real-world, scientific, document-level Question\nAnswering (QA) dataset. PeerQA questions have been sourced from peer reviews,\nwhich contain questions that reviewers raised while thoroughly examining the\nscientific article. Answers have been annotated by the original authors of each\npaper. The dataset contains 579 QA pairs from 208 academic articles, with a\nmajority from ML and NLP, as well as a subset of other scientific communities\nlike Geoscience and Public Health. PeerQA supports three critical tasks for\ndeveloping practical QA systems: Evidence retrieval, unanswerable question\nclassification, and answer generation. We provide a detailed analysis of the\ncollected dataset and conduct experiments establishing baseline systems for all\nthree tasks. Our experiments and analyses reveal the need for\ndecontextualization in document-level retrieval, where we find that even simple\ndecontextualization approaches consistently improve retrieval performance\nacross architectures. On answer generation, PeerQA serves as a challenging\nbenchmark for long-context modeling, as the papers have an average size of 12k\ntokens. Our code and data is available at https://github.com/UKPLab/peerqa.\n","authors":["Tim Baumgärtner","Ted Briscoe","Iryna Gurevych"],"pdf_url":"https://arxiv.org/pdf/2502.13668v1.pdf","comment":"Accepted at NAACL 2025"},{"id":"http://arxiv.org/abs/2404.16130v2","updated":"2025-02-19T10:49:41Z","published":"2024-04-24T18:38:11Z","title":"From Local to Global: A Graph RAG Approach to Query-Focused\n  Summarization","summary":"  The use of retrieval-augmented generation (RAG) to retrieve relevant\ninformation from an external knowledge source enables large language models\n(LLMs) to answer questions over private and/or previously unseen document\ncollections. However, RAG fails on global questions directed at an entire text\ncorpus, such as \"What are the main themes in the dataset?\", since this is\ninherently a query-focused summarization (QFS) task, rather than an explicit\nretrieval task. Prior QFS methods, meanwhile, do not scale to the quantities of\ntext indexed by typical RAG systems. To combine the strengths of these\ncontrasting methods, we propose GraphRAG, a graph-based approach to question\nanswering over private text corpora that scales with both the generality of\nuser questions and the quantity of source text. Our approach uses an LLM to\nbuild a graph index in two stages: first, to derive an entity knowledge graph\nfrom the source documents, then to pregenerate community summaries for all\ngroups of closely related entities. Given a question, each community summary is\nused to generate a partial response, before all partial responses are again\nsummarized in a final response to the user. For a class of global sensemaking\nquestions over datasets in the 1 million token range, we show that GraphRAG\nleads to substantial improvements over a conventional RAG baseline for both the\ncomprehensiveness and diversity of generated answers.\n","authors":["Darren Edge","Ha Trinh","Newman Cheng","Joshua Bradley","Alex Chao","Apurva Mody","Steven Truitt","Dasha Metropolitansky","Robert Osazuwa Ness","Jonathan Larson"],"pdf_url":"https://arxiv.org/pdf/2404.16130v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.13595v1","updated":"2025-02-19T10:13:43Z","published":"2025-02-19T10:13:43Z","title":"MMTEB: Massive Multilingual Text Embedding Benchmark","summary":"  Text embeddings are typically evaluated on a limited set of tasks, which are\nconstrained by language, domain, and task diversity. To address these\nlimitations and provide a more comprehensive evaluation, we introduce the\nMassive Multilingual Text Embedding Benchmark (MMTEB) - a large-scale,\ncommunity-driven expansion of MTEB, covering over 500 quality-controlled\nevaluation tasks across 250+ languages. MMTEB includes a diverse set of\nchallenging, novel tasks such as instruction following, long-document\nretrieval, and code retrieval, representing the largest multilingual collection\nof evaluation tasks for embedding models to date. Using this collection, we\ndevelop several highly multilingual benchmarks, which we use to evaluate a\nrepresentative set of models. We find that while large language models (LLMs)\nwith billions of parameters can achieve state-of-the-art performance on certain\nlanguage subsets and task categories, the best-performing publicly available\nmodel is multilingual-e5-large-instruct with only 560 million parameters. To\nfacilitate accessibility and reduce computational cost, we introduce a novel\ndownsampling method based on inter-task correlation, ensuring a diverse\nselection while preserving relative model rankings. Furthermore, we optimize\ntasks such as retrieval by sampling hard negatives, creating smaller but\neffective splits. These optimizations allow us to introduce benchmarks that\ndrastically reduce computational demands. For instance, our newly introduced\nzero-shot English benchmark maintains a ranking order similar to the full-scale\nversion but at a fraction of the computational cost.\n","authors":["Kenneth Enevoldsen","Isaac Chung","Imene Kerboua","Márton Kardos","Ashwin Mathur","David Stap","Jay Gala","Wissam Siblini","Dominik Krzemiński","Genta Indra Winata","Saba Sturua","Saiteja Utpala","Mathieu Ciancone","Marion Schaeffer","Gabriel Sequeira","Diganta Misra","Shreeya Dhakal","Jonathan Rystrøm","Roman Solomatin","Ömer Çağatan","Akash Kundu","Martin Bernstorff","Shitao Xiao","Akshita Sukhlecha","Bhavish Pahwa","Rafał Poświata","Kranthi Kiran GV","Shawon Ashraf","Daniel Auras","Björn Plüster","Jan Philipp Harries","Loïc Magne","Isabelle Mohr","Mariya Hendriksen","Dawei Zhu","Hippolyte Gisserot-Boukhlef","Tom Aarsen","Jan Kostkan","Konrad Wojtasik","Taemin Lee","Marek Šuppa","Crystina Zhang","Roberta Rocca","Mohammed Hamdy","Andrianos Michail","John Yang","Manuel Faysse","Aleksei Vatolin","Nandan Thakur","Manan Dey","Dipam Vasani","Pranjal Chitale","Simone Tedeschi","Nguyen Tai","Artem Snegirev","Michael Günther","Mengzhou Xia","Weijia Shi","Xing Han Lù","Jordan Clive","Gayatri Krishnakumar","Anna Maksimova","Silvan Wehrli","Maria Tikhonova","Henil Panchal","Aleksandr Abramov","Malte Ostendorff","Zheng Liu","Simon Clematide","Lester James Miranda","Alena Fenogenova","Guangyu Song","Ruqiya Bin Safi","Wen-Ding Li","Alessia Borghini","Federico Cassano","Hongjin Su","Jimmy Lin","Howard Yen","Lasse Hansen","Sara Hooker","Chenghao Xiao","Vaibhav Adlakha","Orion Weller","Siva Reddy","Niklas Muennighoff"],"pdf_url":"https://arxiv.org/pdf/2502.13595v1.pdf","comment":"Accepted for ICLR: https://openreview.net/forum?id=zl3pfz4VCV"},{"id":"http://arxiv.org/abs/2502.13581v1","updated":"2025-02-19T09:45:29Z","published":"2025-02-19T09:45:29Z","title":"ActionPiece: Contextually Tokenizing Action Sequences for Generative\n  Recommendation","summary":"  Generative recommendation (GR) is an emerging paradigm where user actions are\ntokenized into discrete token patterns and autoregressively generated as\npredictions. However, existing GR models tokenize each action independently,\nassigning the same fixed tokens to identical actions across all sequences\nwithout considering contextual relationships. This lack of context-awareness\ncan lead to suboptimal performance, as the same action may hold different\nmeanings depending on its surrounding context. To address this issue, we\npropose ActionPiece to explicitly incorporate context when tokenizing action\nsequences. In ActionPiece, each action is represented as a set of item\nfeatures, which serve as the initial tokens. Given the action sequence corpora,\nwe construct the vocabulary by merging feature patterns as new tokens, based on\ntheir co-occurrence frequency both within individual sets and across adjacent\nsets. Considering the unordered nature of feature sets, we further introduce\nset permutation regularization, which produces multiple segmentations of action\nsequences with the same semantics. Experiments on public datasets demonstrate\nthat ActionPiece consistently outperforms existing action tokenization methods,\nimproving NDCG@$10$ by $6.00\\%$ to $12.82\\%$.\n","authors":["Yupeng Hou","Jianmo Ni","Zhankui He","Noveen Sachdeva","Wang-Cheng Kang","Ed H. Chi","Julian McAuley","Derek Zhiyuan Cheng"],"pdf_url":"https://arxiv.org/pdf/2502.13581v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.00430v6","updated":"2025-02-19T09:23:03Z","published":"2024-11-30T10:56:30Z","title":"Optimizing Sequential Recommendation Models with Scaling Laws and\n  Approximate Entropy","summary":"  Scaling Laws have emerged as a powerful framework for understanding how model\nperformance evolves as they increase in size, providing valuable insights for\noptimizing computational resources. In the realm of Sequential Recommendation\n(SR), which is pivotal for predicting users' sequential preferences, these laws\noffer a lens through which to address the challenges posed by the scalability\nof SR models. However, the presence of structural and collaborative issues in\nrecommender systems prevents the direct application of the Scaling Law (SL) in\nthese systems. In response, we introduce the Performance Law for SR models,\nwhich aims to theoretically investigate and model the relationship between\nmodel performance and data quality. Specifically, we first fit the HR and NDCG\nmetrics to transformer-based SR models. Subsequently, we propose Approximate\nEntropy (ApEn) to assess data quality, presenting a more nuanced approach\ncompared to traditional data quantity metrics. Our method enables accurate\npredictions across various dataset scales and model sizes, demonstrating a\nstrong correlation in large SR models and offering insights into achieving\noptimal performance for any given model configuration.\n","authors":["Tingjia Shen","Hao Wang","Chuhan Wu","Jin Yao Chin","Wei Guo","Yong Liu","Huifeng Guo","Defu Lian","Ruiming Tang","Enhong Chen"],"pdf_url":"https://arxiv.org/pdf/2412.00430v6.pdf","comment":"12 pages, 5 figures"},{"id":"http://arxiv.org/abs/2502.13539v1","updated":"2025-02-19T08:47:42Z","published":"2025-02-19T08:47:42Z","title":"Bursting Filter Bubble: Enhancing Serendipity Recommendations with\n  Aligned Large Language Models","summary":"  Recommender systems (RSs) often suffer from the feedback loop phenomenon,\ne.g., RSs are trained on data biased by their recommendations. This leads to\nthe filter bubble effect that reinforces homogeneous content and reduces user\nsatisfaction. To this end, serendipity recommendations, which offer unexpected\nyet relevant items, are proposed. Recently, large language models (LLMs) have\nshown potential in serendipity prediction due to their extensive world\nknowledge and reasoning capabilities. However, they still face challenges in\naligning serendipity judgments with human assessments, handling long user\nbehavior sequences, and meeting the latency requirements of industrial RSs. To\naddress these issues, we propose SERAL (Serendipity Recommendations with\nAligned Large Language Models), a framework comprising three stages: (1)\nCognition Profile Generation to compress user behavior into multi-level\nprofiles; (2) SerenGPT Alignment to align serendipity judgments with human\npreferences using enriched training data; and (3) Nearline Adaptation to\nintegrate SerenGPT into industrial RSs pipelines efficiently. Online\nexperiments demonstrate that SERAL improves exposure ratio (PVR), clicks, and\ntransactions of serendipitous items by 5.7%, 29.56%, and 27.6%, enhancing user\nexperience without much impact on overall revenue. Now, it has been fully\ndeployed in the \"Guess What You Like\" of the Taobao App homepage.\n","authors":["Yunjia Xi","Muyan Weng","Wen Chen","Chao Yi","Dian Chen","Gaoyang Guo","Mao Zhang","Jian Wu","Yuning Jiang","Qingwen Liu","Yong Yu","Weinan Zhang"],"pdf_url":"https://arxiv.org/pdf/2502.13539v1.pdf","comment":"15 pages"},{"id":"http://arxiv.org/abs/2502.13530v1","updated":"2025-02-19T08:35:28Z","published":"2025-02-19T08:35:28Z","title":"Breaking the Clusters: Uniformity-Optimization for Text-Based Sequential\n  Recommendation","summary":"  Traditional sequential recommendation (SR) methods heavily rely on explicit\nitem IDs to capture user preferences over time. This reliance introduces\ncritical limitations in cold-start scenarios and domain transfer tasks, where\nunseen items and new contexts often lack established ID mappings. To overcome\nthese limitations, recent studies have shifted towards leveraging text-only\ninformation for recommendation, thereby improving model generalization and\nadaptability across domains. Although promising, text-based SR faces unique\ndifficulties: items' text descriptions often share semantic similarities that\nlead to clustered item representations, compromising their uniformity, a\nproperty essential for promoting diversity and enhancing generalization in\nrecommendation systems. In this paper, we explore a novel framework to improve\nthe uniformity of item representations in text-based SR. Our analysis reveals\nthat items within a sequence exhibit marked semantic similarity, meaning they\nare closer in representation than items overall, and that this effect is more\npronounced for less popular items, which form tighter clusters compared to\ntheir more popular counterparts. Based on these findings, we propose UniT, a\nframework that employs three pairwise item sampling strategies: Unified General\nSampling Strategy, Sequence-Driven Sampling Strategy, and Popularity-Driven\nSampling Strategy. Each strategy applies varying degrees of repulsion to\nselectively adjust the distances between item pairs, thereby refining\nrepresentation uniformity while considering both sequence context and item\npopularity. Extensive experiments on multiple real-world datasets demonstrate\nthat our proposed approach outperforms state-of-the-art models, validating the\neffectiveness of UniT in enhancing both representation uniformity and\nrecommendation accuracy.The source code is available at\nhttps://github.com/ccwwhhh/Model-Rec.\n","authors":["Wuhan Chen","Zongwei Wang","Min Gao","Xin Xia","Feng Jiang","Junhao Wen"],"pdf_url":"https://arxiv.org/pdf/2502.13530v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.06610v2","updated":"2025-02-19T08:20:01Z","published":"2023-05-11T07:13:24Z","title":"Planted vertex cover problem on regular random graphs and nonmonotonic\n  temperature-dependence in the supercooled region","summary":"  We introduce a planted vertex cover problem on regular random graphs and\nstudy it by the cavity method of statistical mechanics. Different from\nconventional Ising models, the equilibrium ferromagnetic phase transition of\nthis binary-spin two-body interaction system is discontinuous, as the\nparamagnetic phase is separated from the ferromagnetic phase by an extensive\nfree energy barrier. The free energy landscape can be distinguished into three\ndifferent types depending on the two degree parameters of the planted graph.\nThe critical inverse temperatures at which the paramagnetic phase becomes\nlocally unstable towards the ferromagnetic phase ($\\beta_{\\textrm{pf}}$) and\ntowards spin glass phases ($\\beta_{\\textrm{pg}}$) satisfy $\\beta_{\\textrm{pf}}\n> \\beta_{\\textrm{pg}}$, $\\beta_{\\textrm{pf}} < \\beta_{\\textrm{pg}}$ and\n$\\beta_{\\textrm{pf}} = \\beta_{\\textrm{pg}}$, respectively, in these three\nlandscapes. A locally stable anti-ferromagnetic phase emerges in the free\nenergy landscape if $\\beta_{\\textrm{pf}} < \\beta_{\\textrm{pg}}$. When exploring\nthe free energy landscape by stochastic local search dynamics, we find that in\nagreement with our theoretical prediction, the first-passage time from the\nparamagnetic phase to the ferromagnetic phase is nonmonotonic with the inverse\ntemperature. The potential relevance of the planted vertex cover model to\nsupercooled glass-forming liquids is briefly discussed.\n","authors":["Xin-Yi Fan","Hai-Jun Zhou"],"pdf_url":"https://arxiv.org/pdf/2305.06610v2.pdf","comment":"Extensively revised and expanded. Changed title. A mistake in\n  numerical simulation corrected. Accepted for publication in PRE as a regular\n  article"},{"id":"http://arxiv.org/abs/2502.14913v1","updated":"2025-02-19T07:51:50Z","published":"2025-02-19T07:51:50Z","title":"OpenSearch-SQL: Enhancing Text-to-SQL with Dynamic Few-shot and\n  Consistency Alignment","summary":"  Although multi-agent collaborative Large Language Models (LLMs) have achieved\nsignificant breakthroughs in the Text-to-SQL task, their performance is still\nconstrained by various factors. These factors include the incompleteness of the\nframework, failure to follow instructions, and model hallucination problems. To\naddress these problems, we propose OpenSearch-SQL, which divides the\nText-to-SQL task into four main modules: Preprocessing, Extraction, Generation,\nand Refinement, along with an Alignment module based on a consistency alignment\nmechanism. This architecture aligns the inputs and outputs of agents through\nthe Alignment module, reducing failures in instruction following and\nhallucination. Additionally, we designed an intermediate language called\nSQL-Like and optimized the structured CoT based on SQL-Like. Meanwhile, we\ndeveloped a dynamic few-shot strategy in the form of self-taught Query-CoT-SQL.\nThese methods have significantly improved the performance of LLMs in the\nText-to-SQL task.\n  In terms of model selection, we directly applied the base LLMs without any\npost-training, thereby simplifying the task chain and enhancing the framework's\nportability. Experimental results show that OpenSearch-SQL achieves an\nexecution accuracy(EX) of 69.3% on the BIRD development set, 72.28% on the test\nset, and a reward-based validity efficiency score (R-VES) of 69.36%, with all\nthree metrics ranking first at the time of submission. These results\ndemonstrate the comprehensive advantages of the proposed method in both\neffectiveness and efficiency.\n","authors":["Xiangjin Xie","Guangwei Xu","Lingyan Zhao","Ruijie Guo"],"pdf_url":"https://arxiv.org/pdf/2502.14913v1.pdf","comment":"15 pages"},{"id":"http://arxiv.org/abs/2502.13481v1","updated":"2025-02-19T07:10:23Z","published":"2025-02-19T07:10:23Z","title":"LLM4Tag: Automatic Tagging System for Information Retrieval via Large\n  Language Models","summary":"  Tagging systems play an essential role in various information retrieval\napplications such as search engines and recommender systems. Recently, Large\nLanguage Models (LLMs) have been applied in tagging systems due to their\nextensive world knowledge, semantic understanding, and reasoning capabilities.\nDespite achieving remarkable performance, existing methods still have\nlimitations, including difficulties in retrieving relevant candidate tags\ncomprehensively, challenges in adapting to emerging domain-specific knowledge,\nand the lack of reliable tag confidence quantification. To address these three\nlimitations above, we propose an automatic tagging system LLM4Tag. First, a\ngraph-based tag recall module is designed to effectively and comprehensively\nconstruct a small-scale highly relevant candidate tag set. Subsequently, a\nknowledge-enhanced tag generation module is employed to generate accurate tags\nwith long-term and short-term knowledge injection. Finally, a tag confidence\ncalibration module is introduced to generate reliable tag confidence scores.\nExtensive experiments over three large-scale industrial datasets show that\nLLM4Tag significantly outperforms the state-of-the-art baselines and LLM4Tag\nhas been deployed online for content tagging to serve hundreds of millions of\nusers.\n","authors":["Ruiming Tang","Chenxu Zhu","Bo Chen","Weipeng Zhang","Menghui Zhu","Xinyi Dai","Huifeng Guo"],"pdf_url":"https://arxiv.org/pdf/2502.13481v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.13465v1","updated":"2025-02-19T06:33:39Z","published":"2025-02-19T06:33:39Z","title":"HawkBench: Investigating Resilience of RAG Methods on Stratified\n  Information-Seeking Tasks","summary":"  In real-world information-seeking scenarios, users have dynamic and diverse\nneeds, requiring RAG systems to demonstrate adaptable resilience. To\ncomprehensively evaluate the resilience of current RAG methods, we introduce\nHawkBench, a human-labeled, multi-domain benchmark designed to rigorously\nassess RAG performance across categorized task types. By stratifying tasks\nbased on information-seeking behaviors, HawkBench provides a systematic\nevaluation of how well RAG systems adapt to diverse user needs.\n  Unlike existing benchmarks, which focus primarily on specific task types\n(mostly factoid queries) and rely on varying knowledge bases, HawkBench offers:\n(1) systematic task stratification to cover a broad range of query types,\nincluding both factoid and rationale queries, (2) integration of multi-domain\ncorpora across all task types to mitigate corpus bias, and (3) rigorous\nannotation for high-quality evaluation.\n  HawkBench includes 1,600 high-quality test samples, evenly distributed across\ndomains and task types. Using this benchmark, we evaluate representative RAG\nmethods, analyzing their performance in terms of answer quality and response\nlatency. Our findings highlight the need for dynamic task strategies that\nintegrate decision-making, query interpretation, and global knowledge\nunderstanding to improve RAG generalizability. We believe HawkBench serves as a\npivotal benchmark for advancing the resilience of RAG methods and their ability\nto achieve general-purpose information seeking.\n","authors":["Hongjin Qian","Zheng Liu","Chao Gao","Yankai Wang","Defu Lian","Zhicheng Dou"],"pdf_url":"https://arxiv.org/pdf/2502.13465v1.pdf","comment":"13 pages"},{"id":"http://arxiv.org/abs/2501.10548v2","updated":"2025-02-19T05:22:32Z","published":"2025-01-17T20:43:47Z","title":"Diffusion Models in Recommendation Systems: A Survey","summary":"  Recommender systems remain an essential topic due to its wide application in\nvarious domains and the business potential behind them. With the rise of deep\nlearning, common solutions have leveraged neural networks to facilitate\ncollaborative filtering, and some have turned to generative adversarial\nnetworks to augment the dataset and tackle the data sparsity issue. However,\nthey are limited in learning the complex user and item distribution and still\nsuffer from model collapse. Given the great generation capability exhibited by\ndiffusion models in computer vision recently, many recommender systems have\nadopted diffusion models and found improvements in performance for various\ntasks. Diffusion models in recommender systems excel in managing complex user\nand item distributions and do not suffer from mode collapse. With these\nadvantages, the amount of research in this domain have been growing rapidly and\ncalling for a systematic survey. In this survey paper, we present and propose a\ntaxonomy on past research papers in recommender systems that utilize diffusion\nmodels. Distinct from a prior survey paper that categorizes based on the role\nof the diffusion model, we categorize based on the recommendation task at hand.\nThe decision originates from the rationale that after all, the adoption of\ndiffusion models is to enhance the recommendation performance, not vice versa:\nadapting the recommendation task to enable diffusion models. Nonetheless, we\noffer a unique perspective for diffusion models in recommender systems\ncomplementary to existing surveys. We present the foundation algorithms in\ndiffusion models and their applications in recommender systems to summarize the\nrapid development in this field. Finally, we discuss open research directions\nto prepare and encourage further efforts to advance the field. We compile the\nrelevant papers in a public GitHub repository.\n","authors":["Ting-Ruen Wei","Yi Fang"],"pdf_url":"https://arxiv.org/pdf/2501.10548v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.14304v2","updated":"2025-02-19T02:43:51Z","published":"2023-10-22T13:56:14Z","title":"One Model for All: Large Language Models are Domain-Agnostic\n  Recommendation Systems","summary":"  Sequential recommendation systems aim to predict users' next likely\ninteraction based on their history. However, these systems face data sparsity\nand cold-start problems. Utilizing data from other domains, known as\nmulti-domain methods, is useful for alleviating these problems. However,\ntraditional multi-domain methods rely on meaningless ID-based item\nrepresentation, which makes it difficult to align items with similar meanings\nfrom different domains, yielding sup-optimal knowledge transfer. This paper\nintroduces LLM-Rec, a framework that utilizes pre-trained large language models\n(LLMs) for domain-agnostic recommendation. Specifically, we mix user's\nbehaviors from multiple domains and concatenate item titles into a sentence,\nthen use LLMs for generating user and item representations. By mixing behaviors\nacross different domains, we can exploit the knowledge encoded in LLMs to\nbridge the semantic across over multi-domain behaviors, thus obtaining\nsemantically rich representations and improving performance in all domains.\nFurthermore, we explore the underlying reasons why LLMs are effective and\ninvestigate whether LLMs can understand the semantic correlations as the\nrecommendation model, and if advanced techniques like scaling laws in NLP also\nwork in recommendations. We conduct extensive experiments with LLMs ranging\nfrom 40M to 6.7B to answer the above questions and to verify the effectiveness\nof LLM-Rec in multi-domain recommendation.\n","authors":["Zuoli Tang","Zhaoxin Huan","Zihao Li","Xiaolu Zhang","Jun Hu","Chilin Fu","Jun Zhou","Lixin Zou","Chenliang Li"],"pdf_url":"https://arxiv.org/pdf/2310.14304v2.pdf","comment":"27 pages, 15 figures, 7 tables, Accepted by TOIS"}],"Multimedia":[{"id":"http://arxiv.org/abs/2407.18552v3","updated":"2025-02-19T16:29:24Z","published":"2024-07-26T07:05:04Z","title":"Multimodal Emotion Recognition using Audio-Video Transformer Fusion with\n  Cross Attention","summary":"  Understanding emotions is a fundamental aspect of human communication.\nIntegrating audio and video signals offers a more comprehensive understanding\nof emotional states compared to traditional methods that rely on a single data\nsource, such as speech or facial expressions. Despite its potential, multimodal\nemotion recognition faces significant challenges, particularly in\nsynchronization, feature extraction, and fusion of diverse data sources. To\naddress these issues, this paper introduces a novel transformer-based model\nnamed Audio-Video Transformer Fusion with Cross Attention (AVT-CA). The AVT-CA\nmodel employs a transformer fusion approach to effectively capture and\nsynchronize interlinked features from both audio and video inputs, thereby\nresolving synchronization problems. Additionally, the Cross Attention mechanism\nwithin AVT-CA selectively extracts and emphasizes critical features while\ndiscarding irrelevant ones from both modalities, addressing feature extraction\nand fusion challenges. Extensive experimental analysis conducted on the\nCMU-MOSEI, RAVDESS and CREMA-D datasets demonstrates the efficacy of the\nproposed model. The results underscore the importance of AVT-CA in developing\nprecise and reliable multimodal emotion recognition systems for practical\napplications.\n","authors":["Joe Dhanith P R","Shravan Venkatraman","Vigya Sharma","Santhosh Malarvannan","Modigari Narendra"],"pdf_url":"https://arxiv.org/pdf/2407.18552v3.pdf","comment":"38 Pages, 9 Tables, 12 Figures"},{"id":"http://arxiv.org/abs/2501.08514v2","updated":"2025-02-19T14:02:50Z","published":"2025-01-15T01:52:54Z","title":"Multimodal Fake News Video Explanation Generation: Dataset, Model, and\n  Evaluation","summary":"  Although existing methods have addressed fake news video detection as a\nclassification problem, it is not clear why certain news content is identified\nas fake. Without proper explanation, end users may not be able to understand\nthe potential meaning of fake news. Therefore, we propose a novel task, Fake\nNews Video Explanation (FNVE), to generate natural language explanations that\nreveal the falseness of news videos. To this end, we first developed ONVE and\nVTSE, two new datasets to explain fake news video posts. Then, we propose a\nMultimodal Relation Graph Transformer (MRGT) model to benchmark ONVE and VTSE.\nMRGT introduces a multimodal relation graph to comprehensively represent\nmultimodal relations and then introduces a BART-based decoder to explain\ngenerations. The experimental results show that the proposed MRGT outperforms\nthe strong baselines. In addition, the human evaluation on the annotated ONVE\nand VTSE also achieves high scores in terms of adequacy rating.\n","authors":["Lizhi Chen","Zhong Qian","Peifeng Li","Qiaoming Zhu"],"pdf_url":"https://arxiv.org/pdf/2501.08514v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.14007v1","updated":"2025-02-19T11:54:45Z","published":"2025-02-19T11:54:45Z","title":"d-Sketch: Improving Visual Fidelity of Sketch-to-Image Translation with\n  Pretrained Latent Diffusion Models without Retraining","summary":"  Structural guidance in an image-to-image translation allows intricate control\nover the shapes of synthesized images. Generating high-quality realistic images\nfrom user-specified rough hand-drawn sketches is one such task that aims to\nimpose a structural constraint on the conditional generation process. While the\npremise is intriguing for numerous use cases of content creation and academic\nresearch, the problem becomes fundamentally challenging due to substantial\nambiguities in freehand sketches. Furthermore, balancing the trade-off between\nshape consistency and realistic generation contributes to additional complexity\nin the process. Existing approaches based on Generative Adversarial Networks\n(GANs) generally utilize conditional GANs or GAN inversions, often requiring\napplication-specific data and optimization objectives. The recent introduction\nof Denoising Diffusion Probabilistic Models (DDPMs) achieves a generational\nleap for low-level visual attributes in general image synthesis. However,\ndirectly retraining a large-scale diffusion model on a domain-specific subtask\nis often extremely difficult due to demanding computation costs and\ninsufficient data. In this paper, we introduce a technique for sketch-to-image\ntranslation by exploiting the feature generalization capabilities of a\nlarge-scale diffusion model without retraining. In particular, we use a\nlearnable lightweight mapping network to achieve latent feature translation\nfrom source to target domain. Experimental results demonstrate that the\nproposed method outperforms the existing techniques in qualitative and\nquantitative benchmarks, allowing high-resolution realistic image synthesis\nfrom rough hand-drawn sketches.\n","authors":["Prasun Roy","Saumik Bhattacharya","Subhankar Ghosh","Umapada Pal","Michael Blumenstein"],"pdf_url":"https://arxiv.org/pdf/2502.14007v1.pdf","comment":"Accepted in The International Conference on Pattern Recognition\n  (ICPR) 2024"},{"id":"http://arxiv.org/abs/2502.13637v1","updated":"2025-02-19T11:24:45Z","published":"2025-02-19T11:24:45Z","title":"Exploring Mutual Cross-Modal Attention for Context-Aware Human\n  Affordance Generation","summary":"  Human affordance learning investigates contextually relevant novel pose\nprediction such that the estimated pose represents a valid human action within\nthe scene. While the task is fundamental to machine perception and automated\ninteractive navigation agents, the exponentially large number of probable pose\nand action variations make the problem challenging and non-trivial. However,\nthe existing datasets and methods for human affordance prediction in 2D scenes\nare significantly limited in the literature. In this paper, we propose a novel\ncross-attention mechanism to encode the scene context for affordance prediction\nby mutually attending spatial feature maps from two different modalities. The\nproposed method is disentangled among individual subtasks to efficiently reduce\nthe problem complexity. First, we sample a probable location for a person\nwithin the scene using a variational autoencoder (VAE) conditioned on the\nglobal scene context encoding. Next, we predict a potential pose template from\na set of existing human pose candidates using a classifier on the local context\nencoding around the predicted location. In the subsequent steps, we use two\nVAEs to sample the scale and deformation parameters for the predicted pose\ntemplate by conditioning on the local context and template class. Our\nexperiments show significant improvements over the previous baseline of human\naffordance injection into complex 2D scenes.\n","authors":["Prasun Roy","Saumik Bhattacharya","Subhankar Ghosh","Umapada Pal","Michael Blumenstein"],"pdf_url":"https://arxiv.org/pdf/2502.13637v1.pdf","comment":"11 pages"},{"id":"http://arxiv.org/abs/2502.07160v2","updated":"2025-02-19T03:43:57Z","published":"2025-02-11T00:56:44Z","title":"HDCompression: Hybrid-Diffusion Image Compression for Ultra-Low Bitrates","summary":"  Image compression under ultra-low bitrates remains challenging for both\nconventional learned image compression (LIC) and generative vector-quantized\n(VQ) modeling. Conventional LIC suffers from severe artifacts due to heavy\nquantization, while generative VQ modeling gives poor fidelity due to the\nmismatch between learned generative priors and specific inputs. In this work,\nwe propose Hybrid-Diffusion Image Compression (HDCompression), a dual-stream\nframework that utilizes both generative VQ-modeling and diffusion models, as\nwell as conventional LIC, to achieve both high fidelity and high perceptual\nquality. Different from previous hybrid methods that directly use pre-trained\nLIC models to generate low-quality fidelity-preserving information from heavily\nquantized latent, we use diffusion models to extract high-quality complimentary\nfidelity information from the ground-truth input, which can enhance the system\nperformance in several aspects: improving indices map prediction, enhancing the\nfidelity-preserving output of the LIC stream, and refining conditioned image\nreconstruction with VQ-latent correction. In addition, our diffusion model is\nbased on a dense representative vector (DRV), which is lightweight with very\nsimple sampling schedulers. Extensive experiments demonstrate that our\nHDCompression outperforms the previous conventional LIC, generative\nVQ-modeling, and hybrid frameworks in both quantitative metrics and qualitative\nvisualization, providing balanced robust compression performance at ultra-low\nbitrates.\n","authors":["Lei Lu","Yize Li","Yanzhi Wang","Wei Wang","Wei Jiang"],"pdf_url":"https://arxiv.org/pdf/2502.07160v2.pdf","comment":"Under Review"},{"id":"http://arxiv.org/abs/2412.17847v2","updated":"2025-02-19T03:05:56Z","published":"2024-12-19T01:30:19Z","title":"Bridging the Data Provenance Gap Across Text, Speech and Video","summary":"  Progress in AI is driven largely by the scale and quality of training data.\nDespite this, there is a deficit of empirical analysis examining the attributes\nof well-established datasets beyond text. In this work we conduct the largest\nand first-of-its-kind longitudinal audit across modalities--popular text,\nspeech, and video datasets--from their detailed sourcing trends and use\nrestrictions to their geographical and linguistic representation. Our manual\nanalysis covers nearly 4000 public datasets between 1990-2024, spanning 608\nlanguages, 798 sources, 659 organizations, and 67 countries. We find that\nmultimodal machine learning applications have overwhelmingly turned to\nweb-crawled, synthetic, and social media platforms, such as YouTube, for their\ntraining sets, eclipsing all other sources since 2019. Secondly, tracing the\nchain of dataset derivations we find that while less than 33% of datasets are\nrestrictively licensed, over 80% of the source content in widely-used text,\nspeech, and video datasets, carry non-commercial restrictions. Finally, counter\nto the rising number of languages and geographies represented in public AI\ntraining datasets, our audit demonstrates measures of relative geographical and\nmultilingual representation have failed to significantly improve their coverage\nsince 2013. We believe the breadth of our audit enables us to empirically\nexamine trends in data sourcing, restrictions, and Western-centricity at an\necosystem-level, and that visibility into these questions are essential to\nprogress in responsible AI. As a contribution to ongoing improvements in\ndataset transparency and responsible use, we release our entire multimodal\naudit, allowing practitioners to trace data provenance across text, speech, and\nvideo.\n","authors":["Shayne Longpre","Nikhil Singh","Manuel Cherep","Kushagra Tiwary","Joanna Materzynska","William Brannon","Robert Mahari","Naana Obeng-Marnu","Manan Dey","Mohammed Hamdy","Nayan Saxena","Ahmad Mustafa Anis","Emad A. Alghamdi","Vu Minh Chien","Da Yin","Kun Qian","Yizhi Li","Minnie Liang","An Dinh","Shrestha Mohanty","Deividas Mataciunas","Tobin South","Jianguo Zhang","Ariel N. Lee","Campbell S. Lund","Christopher Klamm","Damien Sileo","Diganta Misra","Enrico Shippole","Kevin Klyman","Lester JV Miranda","Niklas Muennighoff","Seonghyeon Ye","Seungone Kim","Vipul Gupta","Vivek Sharma","Xuhui Zhou","Caiming Xiong","Luis Villa","Stella Biderman","Alex Pentland","Sara Hooker","Jad Kabbara"],"pdf_url":"https://arxiv.org/pdf/2412.17847v2.pdf","comment":"ICLR 2025. 10 pages, 5 figures (main paper)"},{"id":"http://arxiv.org/abs/2502.13352v1","updated":"2025-02-19T01:19:52Z","published":"2025-02-19T01:19:52Z","title":"Integrated Sensing and Communication for 6G Holographic Digital Twins","summary":"  With the advent of 6G networks, offering ultra-high bandwidth and ultra-low\nlatency, coupled with the enhancement of terminal device resolutions,\nholographic communication is gradually becoming a reality. Holographic digital\ntwin (HDT) is considered one of key applications of holographic communication,\ncapable of creating virtual replicas for real-time mapping and prediction of\nphysical entity states, and performing three-dimensional reproduction of\nspatial information. In this context, integrated sensing and communication\n(ISAC) is expected to be a crucial pathway for providing data sources to HDT.\nThis paper proposes a four-layer architecture assisted by ISAC for HDT,\nintegrating emerging paradigms and key technologies to achieve low-cost,\nhigh-precision environmental data collection for constructing HDT.\nSpecifically, to enhance sensing resolution, we explore super-resolution\ntechniques from the perspectives of parameter estimation and point cloud\nconstruction. Additionally, we focus on multi-point collaborative sensing for\nconstructing HDT, and provide a comprehensive review of four key techniques:\nnode selection, multi-band collaboration, cooperative beamforming, and data\nfusion. Finally, we highlight several interesting research directions to guide\nand inspire future work.\n","authors":["Haijun Zhang","Ziyang Zhang","Xiangnan Liu","Wei Li","Haojin Li","Chen Sun"],"pdf_url":"https://arxiv.org/pdf/2502.13352v1.pdf","comment":null}]},"2025-02-23T00:00:00Z":{"Information Retrieval":[{"id":"http://arxiv.org/abs/2410.09713v4","updated":"2025-02-23T03:23:46Z","published":"2024-10-13T03:45:24Z","title":"Agentic Information Retrieval","summary":"  Since the 1970s, information retrieval (IR) has long been defined as the\nprocess of acquiring relevant information items from a pre-defined corpus to\nsatisfy user information needs. Traditional IR systems, while effective in\ndomains like web search, are constrained by their reliance on static,\npre-defined information items. To this end, this paper introduces agentic\ninformation retrieval (Agentic IR), a transformative next-generation paradigm\nfor IR driven by large language models (LLMs) and AI agents. The central shift\nin agentic IR is the evolving definition of ``information'' from static,\npre-defined information items to dynamic, context-dependent information states.\nInformation state refers to a particular information context that the user is\nright in within a dynamic environment, encompassing not only the acquired\ninformation items but also real-time user preferences, contextual factors, and\ndecision-making processes. In such a way, traditional information retrieval,\nfocused on acquiring relevant information items based on user queries, can be\nnaturally extended to achieving the target information state given the user\ninstruction, which thereby defines the agentic information retrieval. We\nsystematically discuss agentic IR from various aspects, i.e., task formulation,\narchitecture, evaluation, case studies, as well as challenges and future\nprospects. We believe that the concept of agentic IR introduced in this paper\nnot only broadens the scope of information retrieval research but also lays the\nfoundation for a more adaptive, interactive, and intelligent next-generation IR\nparadigm.\n","authors":["Weinan Zhang","Junwei Liao","Ning Li","Kounianhua Du","Jianghao Lin"],"pdf_url":"https://arxiv.org/pdf/2410.09713v4.pdf","comment":"11 pages, perspective paper"}]},"2025-02-18T00:00:00Z":{"Information Retrieval":[{"id":"http://arxiv.org/abs/2502.13233v1","updated":"2025-02-18T19:12:15Z","published":"2025-02-18T19:12:15Z","title":"SearchRAG: Can Search Engines Be Helpful for LLM-based Medical Question\n  Answering?","summary":"  Large Language Models (LLMs) have shown remarkable capabilities in general\ndomains but often struggle with tasks requiring specialized knowledge.\nConventional Retrieval-Augmented Generation (RAG) techniques typically retrieve\nexternal information from static knowledge bases, which can be outdated or\nincomplete, missing fine-grained clinical details essential for accurate\nmedical question answering. In this work, we propose SearchRAG, a novel\nframework that overcomes these limitations by leveraging real-time search\nengines. Our method employs synthetic query generation to convert complex\nmedical questions into search-engine-friendly queries and utilizes\nuncertainty-based knowledge selection to filter and incorporate the most\nrelevant and informative medical knowledge into the LLM's input. Experimental\nresults demonstrate that our method significantly improves response accuracy in\nmedical question answering tasks, particularly for complex questions requiring\ndetailed and up-to-date knowledge.\n","authors":["Yucheng Shi","Tianze Yang","Canyu Chen","Quanzheng Li","Tianming Liu","Xiang Li","Ninghao Liu"],"pdf_url":"https://arxiv.org/pdf/2502.13233v1.pdf","comment":"8 pages, three figures"},{"id":"http://arxiv.org/abs/2502.11747v2","updated":"2025-02-18T16:24:11Z","published":"2025-02-17T12:40:35Z","title":"Open-Ended and Knowledge-Intensive Video Question Answering","summary":"  Video question answering that requires external knowledge beyond the visual\ncontent remains a significant challenge in AI systems. While models can\neffectively answer questions based on direct visual observations, they often\nfalter when faced with questions requiring broader contextual knowledge. To\naddress this limitation, we investigate knowledge-intensive video question\nanswering (KI-VideoQA) through the lens of multi-modal retrieval-augmented\ngeneration, with a particular focus on handling open-ended questions rather\nthan just multiple-choice formats. Our comprehensive analysis examines various\nretrieval augmentation approaches using cutting-edge retrieval and vision\nlanguage models, testing both zero-shot and fine-tuned configurations. We\ninvestigate several critical dimensions: the interplay between different\ninformation sources and modalities, strategies for integrating diverse\nmulti-modal contexts, and the dynamics between query formulation and retrieval\nresult utilization. Our findings reveal that while retrieval augmentation shows\npromise in improving model performance, its success is heavily dependent on the\nchosen modality and retrieval methodology. The study also highlights the\ncritical role of query construction and retrieval depth optimization in\neffective knowledge integration. Through our proposed approach, we achieve a\nsubstantial 17.5% improvement in accuracy on multiple choice questions in the\nKnowIT VQA dataset, establishing new state-of-the-art performance levels.\n","authors":["Md Zarif Ul Alam","Hamed Zamani"],"pdf_url":"https://arxiv.org/pdf/2502.11747v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.12974v1","updated":"2025-02-18T15:56:34Z","published":"2025-02-18T15:56:34Z","title":"Learning More Effective Representations for Dense Retrieval through\n  Deliberate Thinking Before Search","summary":"  Recent dense retrievers usually thrive on the emergency capabilities of Large\nLanguage Models (LLMs), using them to encode queries and documents into an\nembedding space for retrieval. These LLM-based dense retrievers have shown\npromising performance across various retrieval scenarios. However, relying on a\nsingle embedding to represent documents proves less effective in capturing\ndifferent perspectives of documents for matching. In this paper, we propose\nDeliberate Thinking based Dense Retriever (DEBATER), which enhances these\nLLM-based retrievers by enabling them to learn more effective document\nrepresentations through a step-by-step thinking process. DEBATER introduces the\nChain-of-Deliberation mechanism to iteratively optimize document\nrepresentations using a continuous chain of thought. To consolidate information\nfrom various thinking steps, DEBATER also incorporates the Self Distillation\nmechanism, which identifies the most informative thinking steps and integrates\nthem into a unified text embedding. Experimental results show that DEBATER\nsignificantly outperforms existing methods across several retrieval benchmarks,\ndemonstrating superior accuracy and robustness. All codes are available at\nhttps://github.com/OpenBMB/DEBATER.\n","authors":["Yifan Ji","Zhipeng Xu","Zhenghao Liu","Yukun Yan","Shi Yu","Yishan Li","Zhiyuan Liu","Yu Gu","Ge Yu","Maosong Sun"],"pdf_url":"https://arxiv.org/pdf/2502.12974v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.12799v1","updated":"2025-02-18T12:00:47Z","published":"2025-02-18T12:00:47Z","title":"Towards Text-Image Interleaved Retrieval","summary":"  Current multimodal information retrieval studies mainly focus on single-image\ninputs, which limits real-world applications involving multiple images and\ntext-image interleaved content. In this work, we introduce the text-image\ninterleaved retrieval (TIIR) task, where the query and document are interleaved\ntext-image sequences, and the model is required to understand the semantics\nfrom the interleaved context for effective retrieval. We construct a TIIR\nbenchmark based on naturally interleaved wikiHow tutorials, where a specific\npipeline is designed to generate interleaved queries. To explore the task, we\nadapt several off-the-shelf retrievers and build a dense baseline by\ninterleaved multimodal large language model (MLLM). We then propose a novel\nMatryoshka Multimodal Embedder (MME), which compresses the number of visual\ntokens at different granularity, to address the challenge of excessive visual\ntokens in MLLM-based TIIR models. Experiments demonstrate that simple adaption\nof existing models does not consistently yield effective results. Our MME\nachieves significant improvements over the baseline by substantially fewer\nvisual tokens. We provide extensive analysis and will release the dataset and\ncode to facilitate future research.\n","authors":["Xin Zhang","Ziqi Dai","Yongqi Li","Yanzhao Zhang","Dingkun Long","Pengjun Xie","Meishan Zhang","Jun Yu","Wenjie Li","Min Zhang"],"pdf_url":"https://arxiv.org/pdf/2502.12799v1.pdf","comment":"16 pages, 14 figures"},{"id":"http://arxiv.org/abs/2502.14902v1","updated":"2025-02-18T11:18:55Z","published":"2025-02-18T11:18:55Z","title":"PathRAG: Pruning Graph-based Retrieval Augmented Generation with\n  Relational Paths","summary":"  Retrieval-augmented generation (RAG) improves the response quality of large\nlanguage models (LLMs) by retrieving knowledge from external databases. Typical\nRAG approaches split the text database into chunks, organizing them in a flat\nstructure for efficient searches. To better capture the inherent dependencies\nand structured relationships across the text database, researchers propose to\norganize textual information into an indexing graph, known asgraph-based RAG.\nHowever, we argue that the limitation of current graph-based RAG methods lies\nin the redundancy of the retrieved information, rather than its insufficiency.\nMoreover, previous methods use a flat structure to organize retrieved\ninformation within the prompts, leading to suboptimal performance. To overcome\nthese limitations, we propose PathRAG, which retrieves key relational paths\nfrom the indexing graph, and converts these paths into textual form for\nprompting LLMs. Specifically, PathRAG effectively reduces redundant information\nwith flow-based pruning, while guiding LLMs to generate more logical and\ncoherent responses with path-based prompting. Experimental results show that\nPathRAG consistently outperforms state-of-the-art baselines across six datasets\nand five evaluation dimensions. The code is available at the following link:\nhttps://github.com/BUPT-GAMMA/PathRAG\n","authors":["Boyu Chen","Zirui Guo","Zidan Yang","Yuluo Chen","Junze Chen","Zhenghao Liu","Chuan Shi","Cheng Yang"],"pdf_url":"https://arxiv.org/pdf/2502.14902v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.02231v4","updated":"2025-02-18T11:00:19Z","published":"2024-03-04T17:21:19Z","title":"CODE-ACCORD: A Corpus of building regulatory data for rule generation\n  towards automatic compliance checking","summary":"  Automatic Compliance Checking (ACC) within the Architecture, Engineering, and\nConstruction (AEC) sector necessitates automating the interpretation of\nbuilding regulations to achieve its full potential. Converting textual rules\ninto machine-readable formats is challenging due to the complexities of natural\nlanguage and the scarcity of resources for advanced Machine Learning (ML).\nAddressing these challenges, we introduce CODE-ACCORD, a dataset of 862\nsentences from the building regulations of England and Finland. Only the\nself-contained sentences, which express complete rules without needing\nadditional context, were considered as they are essential for ACC. Each\nsentence was manually annotated with entities and relations by a team of 12\nannotators to facilitate machine-readable rule generation, followed by careful\ncuration to ensure accuracy. The final dataset comprises 4,297 entities and\n4,329 relations across various categories, serving as a robust ground truth.\nCODE-ACCORD supports a range of ML and Natural Language Processing (NLP) tasks,\nincluding text classification, entity recognition, and relation extraction. It\nenables applying recent trends, such as deep neural networks and large language\nmodels, to ACC.\n","authors":["Hansi Hettiarachchi","Amna Dridi","Mohamed Medhat Gaber","Pouyan Parsafard","Nicoleta Bocaneala","Katja Breitenfelder","Gonçal Costa","Maria Hedblom","Mihaela Juganaru-Mathieu","Thamer Mecharnia","Sumee Park","He Tan","Abdel-Rahman H. Tawil","Edlira Vakaj"],"pdf_url":"https://arxiv.org/pdf/2403.02231v4.pdf","comment":"This is a preprint of an article published in the Scientific Data\n  Journal"},{"id":"http://arxiv.org/abs/2501.09493v2","updated":"2025-02-18T10:46:28Z","published":"2025-01-16T12:06:56Z","title":"Large Language Models as Evaluators for Conversational Recommender\n  Systems: Benchmarking System Performance from a User-Centric Perspective","summary":"  Conversational recommender systems (CRS) involve both recommendation and\ndialogue tasks, which makes their evaluation a unique challenge. Although past\nresearch has analyzed various factors that may affect user satisfaction with\nCRS interactions from the perspective of user studies, few evaluation metrics\nfor CRS have been proposed. Recent studies have shown that LLMs can align with\nhuman preferences, and several LLM-based text quality evaluation measures have\nbeen introduced. However, the application of LLMs in CRS evaluation remains\nrelatively limited. To address this research gap and advance the development of\nuser-centric conversational recommender systems, this study proposes an\nautomated LLM-based CRS evaluation framework, building upon existing research\nin human-computer interaction and psychology. The framework evaluates CRS from\nfour dimensions: dialogue behavior, language expression, recommendation items,\nand response content. We use this framework to evaluate four different\nconversational recommender systems.\n","authors":["Nuo Chen","Quanyu Dai","Xiaoyu Dong","Xiao-Ming Wu","Zhenhua Dong"],"pdf_url":"https://arxiv.org/pdf/2501.09493v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.02850v2","updated":"2025-02-18T10:43:41Z","published":"2024-11-05T06:44:15Z","title":"WASHtsApp -- A RAG-powered WhatsApp Chatbot for supporting rural African\n  clean water access, sanitation and hygiene","summary":"  This paper introduces WASHtsApp, a WhatsApp-based chatbot designed to educate\nrural African communities on clean water access, sanitation, and hygiene (WASH)\nprinciples. WASHtsApp leverages a Retrieval-Augmented Generation (RAG) approach\nto address the limitations of previous approaches with limited reach or missing\ncontextualization. The paper details the development process, employing Design\nScience Research Methodology. The evaluation consisted of two phases: content\nvalidation by four WASH experts and community validation by potential users.\nContent validation confirmed WASHtsApp's ability to provide accurate and\nrelevant WASH-related information. Community validation indicated high user\nacceptance and perceived usefulness of the chatbot. The paper concludes by\ndiscussing the potential for further development, including incorporating local\nlanguages and user data analysis for targeted interventions. It also proposes\nfuture research cycles focused on wider deployment and leveraging user data for\neducational purposes.\n","authors":["Simon Kloker","Alex Cedric Luyima","Matthew Bazanya"],"pdf_url":"https://arxiv.org/pdf/2411.02850v2.pdf","comment":"Working Paper. Accepted at IST-Africa Conference 2025, Nairobi"},{"id":"http://arxiv.org/abs/2408.02854v4","updated":"2025-02-18T10:22:15Z","published":"2024-08-05T22:34:28Z","title":"Creating a Taxonomy for Retrieval Augmented Generation Applications","summary":"  In this research, we develop a taxonomy to conceptualize a comprehensive\noverview of the constituting characteristics that define retrieval augmented\ngeneration (RAG) applications, facilitating the adoption of this technology for\ndifferent application domains. To the best of our knowledge, no holistic RAG\napplication taxonomies have been developed so far. We employ the method foreign\nto ACL and thus contribute to the set of methods in the taxonomy creation. It\ncomprises four iterative phases designed to refine and enhance our\nunderstanding and presentation of RAG's core dimensions. We have developed a\ntotal of five meta-dimensions and sixteen dimensions to comprehensively capture\nthe concept of RAG applications. Thus, the taxonomy can be used to better\nunderstand RAG applications and to derive design knowledge for future solutions\nin specific application domains.\n","authors":["Irina Nikishina","Özge Sevgili","Mahei Manhai Li","Chris Biemann","Martin Semmann"],"pdf_url":"https://arxiv.org/pdf/2408.02854v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.08672v2","updated":"2025-02-18T10:06:06Z","published":"2024-04-05T05:14:46Z","title":"Taxonomy and Analysis of Sensitive User Queries in Generative AI Search","summary":"  Although there has been a growing interest among industries in integrating\ngenerative LLMs into their services, limited experience and scarcity of\nresources act as a barrier in launching and servicing large-scale LLM-based\nservices. In this paper, we share our experiences in developing and operating\ngenerative AI models within a national-scale search engine, with a specific\nfocus on the sensitiveness of user queries. We propose a taxonomy for sensitive\nsearch queries, outline our approaches, and present a comprehensive analysis\nreport on sensitive queries from actual users. We believe that our experiences\nin launching generative AI search systems can contribute to reducing the\nbarrier in building generative LLM-based services.\n","authors":["Hwiyeol Jo","Taiwoo Park","Hyunwoo Lee","Nayoung Choi","Changbong Kim","Ohjoon Kwon","Donghyeon Jeon","Eui-Hyeon Lee","Kyoungho Shin","Sun Suk Lim","Kyungmi Kim","Jihye Lee","Sun Kim"],"pdf_url":"https://arxiv.org/pdf/2404.08672v2.pdf","comment":"NAACL2025(Findings)"},{"id":"http://arxiv.org/abs/2402.08241v2","updated":"2025-02-18T09:36:34Z","published":"2024-02-13T06:12:17Z","title":"Causal Learning for Trustworthy Recommender Systems: A Survey","summary":"  Recommender Systems (RS) have significantly advanced online content filtering\nand personalized decision-making. However, emerging vulnerabilities in RS have\ncatalyzed a paradigm shift towards Trustworthy RS (TRS). Despite substantial\nprogress on TRS, most efforts focus on data correlations while overlooking the\nfundamental causal nature of recommendations. This drawback hinders TRS from\nidentifying the root cause of trustworthiness issues, leading to limited\nfairness, robustness, and explainability. To bridge this gap, causal learning\nemerges as a class of promising methods to augment TRS. These methods, grounded\nin reliable causality, excel in mitigating various biases and noise while\noffering insightful explanations for TRS. However, there is a lack of timely\nand dedicated surveys in this vibrant area. This paper creates an overview of\nTRS from the perspective of causal learning. We begin by presenting the\nadvantages and common procedures of Causality-oriented TRS (CTRS). Then, we\nidentify potential trustworthiness challenges at each stage and link them to\nviable causal solutions, followed by a classification of CTRS methods. Finally,\nwe discuss several future directions for advancing this field.\n","authors":["Jin Li","Shoujin Wang","Qi Zhang","Longbing Cao","Fang Chen","Xiuzhen Zhang","Dietmar Jannach","Charu C. Aggarwal"],"pdf_url":"https://arxiv.org/pdf/2402.08241v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.01269v5","updated":"2025-02-18T09:05:29Z","published":"2024-12-02T08:35:54Z","title":"CPRM: A LLM-based Continual Pre-training Framework for Relevance\n  Modeling in Commercial Search","summary":"  Relevance modeling between queries and items stands as a pivotal component in\ncommercial search engines, directly affecting the user experience. Given the\nremarkable achievements of large language models (LLMs) in various natural\nlanguage processing (NLP) tasks, LLM-based relevance modeling is gradually\nbeing adopted within industrial search systems. Nevertheless, foundational LLMs\nlack domain-specific knowledge and do not fully exploit the potential of\nin-context learning. Furthermore, structured item text remains underutilized,\nand there is a shortage in the supply of corresponding queries and background\nknowledge. We thereby propose CPRM (Continual Pre-training for Relevance\nModeling), a framework designed for the continual pre-training of LLMs to\naddress these issues. Our CPRM framework includes three modules: 1) employing\nboth queries and multi-field item to jointly pre-train for enhancing domain\nknowledge, 2) applying in-context pre-training, a novel approach where LLMs are\npre-trained on a sequence of related queries or items, and 3) conducting\nreading comprehension on items to produce associated domain knowledge and\nbackground information (e.g., generating summaries and corresponding queries)\nto further strengthen LLMs. Results on offline experiments and online A/B\ntesting demonstrate that our model achieves convincing performance compared to\nstrong baselines.\n","authors":["Kaixin Wu","Yixin Ji","Zeyuan Chen","Qiang Wang","Cunxiang Wang","Hong Liu","Baijun Ji","Jia Xu","Zhongyi Liu","Jinjie Gu","Yuan Zhou","Linjian Mo"],"pdf_url":"https://arxiv.org/pdf/2412.01269v5.pdf","comment":"NAACL 2025"},{"id":"http://arxiv.org/abs/2502.12634v1","updated":"2025-02-18T08:24:53Z","published":"2025-02-18T08:24:53Z","title":"Introducing Context Information in Lifelong Sequential Modeling using\n  Temporal Convolutional Networks","summary":"  The importance of lifelong sequential modeling (LSM) is growing in the realm\nof social media recommendation systems. A key component in this process is the\nattention module, which derives interest representations with respect to\ncandidate items from the sequence. Typically, attention modules function in a\npoint-wise fashion, concentrating only on the relevance of individual items in\nthe sequence to the candidate item. However, the context information in the\nneighboring items that is useful for more accurately evaluating the\nsignificance of each item has not been taken into account. In this study, we\nintroduce a novel network which employs the Temporal Convolutional Network\n(TCN) to generate context-aware representations for each item throughout the\nlifelong sequence. These improved representations are then utilized in the\nattention module to produce context-aware interest representations. Expanding\non this TCN framework, we present a enhancement module which includes multiple\nTCN layers and their respective attention modules to capture interest\nrepresentations across different context scopes. Additionally, we also\nincorporate a lightweight sub-network to create convolution filters based on\nusers' basic profile features. These personalized filters are then applied in\nthe TCN layers instead of the original global filters to produce more\nuser-specific representations. We performed experiments on both a public\ndataset and a proprietary dataset. The findings indicate that the proposed\nnetwork surpasses existing methods in terms of prediction accuracy and online\nperformance metrics.\n","authors":["Ting Guo","Zhaoyang Yang","Qinsong Zeng","Ming Chen"],"pdf_url":"https://arxiv.org/pdf/2502.12634v1.pdf","comment":"10 pages, including 1 page of reference, 7 figures"},{"id":"http://arxiv.org/abs/2502.11414v2","updated":"2025-02-18T07:04:29Z","published":"2025-02-17T03:55:51Z","title":"Unbiased Learning to Rank with Query-Level Click Propensity Estimation:\n  Beyond Pointwise Observation and Relevance","summary":"  Most existing unbiased learning-to-rank (ULTR) approaches are based on the\nuser examination hypothesis, which assumes that users will click a result only\nif it is both relevant and observed (typically modeled by position). However,\nin real-world scenarios, users often click only one or two results after\nexamining multiple relevant options, due to limited patience or because their\ninformation needs have already been satisfied. Motivated by this, we propose a\nquery-level click propensity model to capture the probability that users will\nclick on different result lists, allowing for non-zero probabilities that users\nmay not click on an observed relevant result. We hypothesize that this\npropensity increases when more potentially relevant results are present, and\nrefer to this user behavior as relevance saturation bias. Our method introduces\na Dual Inverse Propensity Weighting (DualIPW) mechanism -- combining\nquery-level and position-level IPW -- to address both relevance saturation and\nposition bias. Through theoretical derivation, we prove that DualIPW can learn\nan unbiased ranking model. Experiments on the real-world Baidu-ULTR dataset\ndemonstrate that our approach significantly outperforms state-of-the-art ULTR\nbaselines. The code and dataset information can be found at\nhttps://github.com/Trustworthy-Information-Access/DualIPW.\n","authors":["Lulu Yu","Keping Bi","Jiafeng Guo","Shihao Liu","Dawei Yin","Xueqi Cheng"],"pdf_url":"https://arxiv.org/pdf/2502.11414v2.pdf","comment":"5 pages, 3 figures, accepted by The ACM Web Conference (WWW) 2025\n  Short Paper Track"},{"id":"http://arxiv.org/abs/2404.09077v3","updated":"2025-02-18T06:52:49Z","published":"2024-04-13T20:43:46Z","title":"CuriousLLM: Elevating Multi-Document Question Answering with\n  LLM-Enhanced Knowledge Graph Reasoning","summary":"  Large Language Models (LLMs) have achieved significant success in open-domain\nquestion answering. However, they continue to face challenges such as\nhallucinations and knowledge cutoffs. These issues can be mitigated through\nin-context learning by providing LLMs with relevant context before generating\nanswers. Recent literature proposes Knowledge Graph Prompting (KGP) which\nintegrates knowledge graphs with an LLM-based traversal agent to substantially\nenhance document retrieval quality. However, KGP requires costly fine-tuning\nwith large datasets and remains prone to hallucination. In this paper, we\npropose CuriousLLM, an enhancement that integrates a curiosity-driven reasoning\nmechanism into an LLM agent. This mechanism enables the agent to generate\nrelevant follow-up questions, thereby guiding the information retrieval process\nmore efficiently. Central to our approach is the development of the new\nFollow-upQA dataset, which includes questions and supporting evidence as input,\nwith follow-up questions serving as ground truths. These follow-up questions\neither inquire about what is still missing to fully answer the user's query or\nuse special tokens to signify that the retrieved evidence is sufficient. Our\nexperiments show that CuriousLLM significantly boosts LLM performance in\nmulti-document question answering (MD-QA), circumventing the substantial\ncomputational costs and latency from the original KGP framework.\n","authors":["Zukang Yang","Zixuan Zhu","Xuan Zhu"],"pdf_url":"https://arxiv.org/pdf/2404.09077v3.pdf","comment":"Accepted for publication in NAACL 2025. The official version will be\n  available in the ACL Anthology"},{"id":"http://arxiv.org/abs/2502.12586v1","updated":"2025-02-18T06:42:38Z","published":"2025-02-18T06:42:38Z","title":"G-Refer: Graph Retrieval-Augmented Large Language Model for Explainable\n  Recommendation","summary":"  Explainable recommendation has demonstrated significant advantages in\ninforming users about the logic behind recommendations, thereby increasing\nsystem transparency, effectiveness, and trustworthiness. To provide\npersonalized and interpretable explanations, existing works often combine the\ngeneration capabilities of large language models (LLMs) with collaborative\nfiltering (CF) information. CF information extracted from the user-item\ninteraction graph captures the user behaviors and preferences, which is crucial\nfor providing informative explanations. However, due to the complexity of graph\nstructure, effectively extracting the CF information from graphs still remains\na challenge. Moreover, existing methods often struggle with the integration of\nextracted CF information with LLMs due to its implicit representation and the\nmodality gap between graph structures and natural language explanations. To\naddress these challenges, we propose G-Refer, a framework using graph\nretrieval-augmented large language models (LLMs) for explainable\nrecommendation. Specifically, we first employ a hybrid graph retrieval\nmechanism to retrieve explicit CF signals from both structural and semantic\nperspectives. The retrieved CF information is explicitly formulated as\nhuman-understandable text by the proposed graph translation and accounts for\nthe explanations generated by LLMs. To bridge the modality gap, we introduce\nknowledge pruning and retrieval-augmented fine-tuning to enhance the ability of\nLLMs to process and utilize the retrieved CF information to generate\nexplanations. Extensive experiments show that G-Refer achieves superior\nperformance compared with existing methods in both explainability and\nstability. Codes and data are available at https://github.com/Yuhan1i/G-Refer.\n","authors":["Yuhan Li","Xinni Zhang","Linhao Luo","Heng Chang","Yuxiang Ren","Irwin King","Jia Li"],"pdf_url":"https://arxiv.org/pdf/2502.12586v1.pdf","comment":"Accepted by WWW 2025, research track"},{"id":"http://arxiv.org/abs/2501.17191v2","updated":"2025-02-18T06:30:09Z","published":"2025-01-27T09:29:55Z","title":"Aspect-Aware Decomposition for Opinion Summarization","summary":"  Opinion summarization plays a key role in deriving meaningful insights from\nlarge-scale online reviews. To make this process more explainable and grounded,\nwe propose a modular approach guided by review aspects which separates the\ntasks of aspect identification, opinion consolidation, and meta-review\nsynthesis, enabling greater transparency and ease of inspection. We conduct\nextensive experiments across datasets representing scientific research,\nbusiness, and product domains. Results show that our method generates more\ngrounded summaries compared to strong baseline models, as verified through\nautomated and human evaluations. Additionally, our modular approach, which\nincorporates reasoning based on review aspects, produces more informative\nintermediate outputs than knowledge-agnostic decomposed prompting. These\nintermediate outputs can also effectively support humans in summarizing\nopinions from large volumes of reviews.\n","authors":["Miao Li","Jey Han Lau","Eduard Hovy","Mirella Lapata"],"pdf_url":"https://arxiv.org/pdf/2501.17191v2.pdf","comment":"35 pages"},{"id":"http://arxiv.org/abs/2312.16262v2","updated":"2025-02-18T06:06:11Z","published":"2023-12-26T08:24:24Z","title":"Adaptive In-Context Learning with Large Language Models for Bundle\n  Generation","summary":"  Most existing bundle generation approaches fall short in generating\nfixed-size bundles. Furthermore, they often neglect the underlying user intents\nreflected by the bundles in the generation process, resulting in less\nintelligible bundles. This paper addresses these limitations through the\nexploration of two interrelated tasks, i.e., personalized bundle generation and\nthe underlying intent inference, based on different user sessions. Inspired by\nthe reasoning capabilities of large language models (LLMs), we propose an\nadaptive in-context learning paradigm, which allows LLMs to draw tailored\nlessons from related sessions as demonstrations, enhancing the performance on\ntarget sessions. Specifically, we first employ retrieval augmented generation\nto identify nearest neighbor sessions, and then carefully design prompts to\nguide LLMs in executing both tasks on these neighbor sessions. To tackle\nreliability and hallucination challenges, we further introduce (1) a\nself-correction strategy promoting mutual improvements of the two tasks without\nsupervision signals and (2) an auto-feedback mechanism for adaptive supervision\nbased on the distinct mistakes made by LLMs on different neighbor sessions.\nThereby, the target session can gain customized lessons for improved\nperformance by observing the demonstrations of its neighbor sessions.\nExperiments on three real-world datasets demonstrate the effectiveness of our\nproposed method.\n","authors":["Zhu Sun","Kaidong Feng","Jie Yang","Xinghua Qu","Hui Fang","Yew-Soon Ong","Wenyuan Liu"],"pdf_url":"https://arxiv.org/pdf/2312.16262v2.pdf","comment":"Accepted by SIGIR 2024"},{"id":"http://arxiv.org/abs/2407.00077v5","updated":"2025-02-18T05:19:16Z","published":"2024-06-22T15:32:53Z","title":"Differentially Private Graph Diffusion with Applications in Personalized\n  PageRanks","summary":"  Graph diffusion, which iteratively propagates real-valued substances among\nthe graph, is used in numerous graph/network-involved applications. However,\nreleasing diffusion vectors may reveal sensitive linking information in the\ndata such as transaction information in financial network data. However,\nprotecting the privacy of graph data is challenging due to its interconnected\nnature. This work proposes a novel graph diffusion framework with edge-level\ndifferential privacy guarantees by using noisy diffusion iterates. The\nalgorithm injects Laplace noise per diffusion iteration and adopts a\ndegree-based thresholding function to mitigate the high sensitivity induced by\nlow-degree nodes. Our privacy loss analysis is based on Privacy Amplification\nby Iteration (PABI), which to our best knowledge, is the first effort that\nanalyzes PABI with Laplace noise and provides relevant applications. We also\nintroduce a novel Infinity-Wasserstein distance tracking method, which tightens\nthe analysis of privacy leakage and makes PABI more applicable in practice. We\nevaluate this framework by applying it to Personalized Pagerank computation for\nranking tasks. Experiments on real-world network data demonstrate the\nsuperiority of our method under stringent privacy conditions.\n","authors":["Rongzhe Wei","Eli Chien","Pan Li"],"pdf_url":"https://arxiv.org/pdf/2407.00077v5.pdf","comment":"Github Code Available"},{"id":"http://arxiv.org/abs/2410.07610v3","updated":"2025-02-18T03:55:28Z","published":"2024-10-10T04:54:37Z","title":"CSA: Data-efficient Mapping of Unimodal Features to Multimodal Features","summary":"  Multimodal encoders like CLIP excel in tasks such as zero-shot image\nclassification and cross-modal retrieval. However, they require excessive\ntraining data. We propose canonical similarity analysis (CSA), which uses two\nunimodal encoders to replicate multimodal encoders using limited data. CSA maps\nunimodal features into a multimodal space, using a new similarity score to\nretain only the multimodal information. CSA only involves the inference of\nunimodal encoders and a cubic-complexity matrix decomposition, eliminating the\nneed for extensive GPU-based model training. Experiments show that CSA\noutperforms CLIP while requiring $50,000\\times$ fewer multimodal data pairs to\nbridge the modalities given pre-trained unimodal encoders on ImageNet\nclassification and misinformative news caption detection. CSA surpasses the\nstate-of-the-art method to map unimodal features to multimodal features. We\nalso demonstrate the ability of CSA with modalities beyond image and text,\npaving the way for future modality pairs with limited paired multimodal data\nbut abundant unpaired unimodal data, such as lidar and text.\n","authors":["Po-han Li","Sandeep P. Chinchali","Ufuk Topcu"],"pdf_url":"https://arxiv.org/pdf/2410.07610v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.09628v2","updated":"2025-02-18T03:53:05Z","published":"2024-11-27T00:40:51Z","title":"Bridging AI and Science: Implications from a Large-Scale Literature\n  Analysis of AI4Science","summary":"  Artificial Intelligence has proven to be a transformative tool for advancing\nscientific research across a wide range of disciplines. However, a significant\ngap still exists between AI and scientific communities, limiting the full\npotential of AI methods in driving broad scientific discovery. Existing efforts\nin identifying and bridging this gap have often relied on qualitative\nexamination of small samples of literature, offering a limited perspective on\nthe broader AI4Science landscape. In this work, we present a large-scale\nanalysis of the AI4Science literature, starting by using large language models\nto identify scientific problems and AI methods in publications from top science\nand AI venues. Leveraging this new dataset, we quantitatively highlight key\ndisparities between AI methods and scientific problems, revealing substantial\nopportunities for deeper AI integration across scientific disciplines.\nFurthermore, we explore the potential and challenges of facilitating\ncollaboration between AI and scientific communities through the lens of link\nprediction. Our findings and tools aim to promote more impactful\ninterdisciplinary collaborations and accelerate scientific discovery through\ndeeper and broader AI integration. Our code and dataset are available at:\nhttps://github.com/charles-pyj/Bridging-AI-and-Science.\n","authors":["Yutong Xie","Yijun Pan","Hua Xu","Qiaozhu Mei"],"pdf_url":"https://arxiv.org/pdf/2412.09628v2.pdf","comment":"22 pages"},{"id":"http://arxiv.org/abs/2502.10157v2","updated":"2025-02-18T02:41:53Z","published":"2025-02-14T13:36:20Z","title":"SessionRec: Next Session Prediction Paradigm For Generative Sequential\n  Recommendation","summary":"  We introduce SessionRec, a novel next-session prediction paradigm (NSPP) for\ngenerative sequential recommendation, addressing the fundamental misalignment\nbetween conventional next-item prediction paradigm (NIPP) and real-world\nrecommendation scenarios. Unlike NIPP's item-level autoregressive generation\nthat contradicts actual session-based user interactions, our framework\nintroduces a session-aware representation learning through hierarchical\nsequence aggregation (intra/inter-session), reducing attention computation\ncomplexity while enabling implicit modeling of massive negative interactions,\nand a session-based prediction objective that better captures users' diverse\ninterests through multi-item recommendation in next sessions. Moreover, we\nfound that incorporating a rank loss for items within the session under the\nnext session prediction paradigm can significantly improve the ranking\neffectiveness of generative sequence recommendation models. We also verified\nthat SessionRec exhibits clear power-law scaling laws similar to those observed\nin LLMs. Extensive experiments conducted on public datasets and online A/B test\nin Meituan App demonstrate the effectiveness of SessionRec. The proposed\nparadigm establishes new foundations for developing industrial-scale generative\nrecommendation systems through its model-agnostic architecture and\ncomputational efficiency.\n","authors":["Lei Huang","Hao Guo","Linzhi Peng","Long Zhang","Xiaoteng Wang","Daoyuan Wang","Shichao Wang","Jinpeng Wang","Lei Wang","Sheng Chen"],"pdf_url":"https://arxiv.org/pdf/2502.10157v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.18511v4","updated":"2025-02-18T02:33:04Z","published":"2024-09-27T07:46:06Z","title":"Do We Need Domain-Specific Embedding Models? An Empirical Investigation","summary":"  Embedding models play a crucial role in representing and retrieving\ninformation across various NLP applications. Recent advancements in Large\nLanguage Models (LLMs) have further enhanced the performance of embedding\nmodels, which are trained on massive amounts of text covering almost every\ndomain. These models are often benchmarked on general-purpose datasets like\nMassive Text Embedding Benchmark (MTEB), where they demonstrate superior\nperformance. However, a critical question arises: Is the development of\ndomain-specific embedding models necessary when general-purpose models are\ntrained on vast corpora that already include specialized domain texts? In this\npaper, we empirically investigate this question, choosing the finance domain as\nan example. We introduce the Finance Massive Text Embedding Benchmark\n(FinMTEB), a counterpart to MTEB that consists of financial domain-specific\ntext datasets. We evaluate the performance of seven state-of-the-art embedding\nmodels on FinMTEB and observe a significant performance drop compared to their\nperformance on MTEB. To account for the possibility that this drop is driven by\nFinMTEB's higher complexity, we propose four measures to quantify dataset\ncomplexity and control for this factor in our analysis. Our analysis provides\ncompelling evidence that state-of-the-art embedding models struggle to capture\ndomain-specific linguistic and semantic patterns. Moreover, we find that the\nperformance of general-purpose embedding models on MTEB is not correlated with\ntheir performance on FinMTEB, indicating the need for domain-specific embedding\nbenchmarks for domain-specific embedding models. This study sheds light on\ndeveloping domain-specific embedding models in the LLM era. FinMTEB comes with\nopen-source code at https://github.com/yixuantt/FinMTEB\n","authors":["Yixuan Tang","Yi Yang"],"pdf_url":"https://arxiv.org/pdf/2409.18511v4.pdf","comment":"https://github.com/yixuantt/FinMTEB, The newer version:\n  arXiv:2502.10990"},{"id":"http://arxiv.org/abs/2502.12448v1","updated":"2025-02-18T02:29:51Z","published":"2025-02-18T02:29:51Z","title":"From Principles to Applications: A Comprehensive Survey of Discrete\n  Tokenizers in Generation, Comprehension, Recommendation, and Information\n  Retrieval","summary":"  Discrete tokenizers have emerged as indispensable components in modern\nmachine learning systems, particularly within the context of autoregressive\nmodeling and large language models (LLMs). These tokenizers serve as the\ncritical interface that transforms raw, unstructured data from diverse\nmodalities into discrete tokens, enabling LLMs to operate effectively across a\nwide range of tasks. Despite their central role in generation, comprehension,\nand recommendation systems, a comprehensive survey dedicated to discrete\ntokenizers remains conspicuously absent in the literature. This paper addresses\nthis gap by providing a systematic review of the design principles,\napplications, and challenges of discrete tokenizers. We begin by dissecting the\nsub-modules of tokenizers and systematically demonstrate their internal\nmechanisms to provide a comprehensive understanding of their functionality and\ndesign. Building on this foundation, we synthesize state-of-the-art methods,\ncategorizing them into multimodal generation and comprehension tasks, and\nsemantic tokens for personalized recommendations. Furthermore, we critically\nanalyze the limitations of existing tokenizers and outline promising directions\nfor future research. By presenting a unified framework for understanding\ndiscrete tokenizers, this survey aims to guide researchers and practitioners in\naddressing open challenges and advancing the field, ultimately contributing to\nthe development of more robust and versatile AI systems.\n","authors":["Jian Jia","Jingtong Gao","Ben Xue","Junhao Wang","Qingpeng Cai","Quan Chen","Xiangyu Zhao","Peng Jiang","Kun Gai"],"pdf_url":"https://arxiv.org/pdf/2502.12448v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.12442v1","updated":"2025-02-18T02:24:42Z","published":"2025-02-18T02:24:42Z","title":"HopRAG: Multi-Hop Reasoning for Logic-Aware Retrieval-Augmented\n  Generation","summary":"  Retrieval-Augmented Generation (RAG) systems often struggle with imperfect\nretrieval, as traditional retrievers focus on lexical or semantic similarity\nrather than logical relevance. To address this, we propose HopRAG, a novel RAG\nframework that augments retrieval with logical reasoning through\ngraph-structured knowledge exploration. During indexing, HopRAG constructs a\npassage graph, with text chunks as vertices and logical connections established\nvia LLM-generated pseudo-queries as edges. During retrieval, it employs a\nretrieve-reason-prune mechanism: starting with lexically or semantically\nsimilar passages, the system explores multi-hop neighbors guided by\npseudo-queries and LLM reasoning to identify truly relevant ones. Extensive\nexperiments demonstrate HopRAG's superiority, achieving 76.78\\% higher answer\naccuracy and 65.07\\% improved retrieval F1 score compared to conventional\nmethods. The repository is available at https://github.com/LIU-Hao-2002/HopRAG.\n","authors":["Hao Liu","Zhengren Wang","Xi Chen","Zhiyu Li","Feiyu Xiong","Qinhan Yu","Wentao Zhang"],"pdf_url":"https://arxiv.org/pdf/2502.12442v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.14898v1","updated":"2025-02-18T01:57:02Z","published":"2025-02-18T01:57:02Z","title":"Retrieval-augmented systems can be dangerous medical communicators","summary":"  Patients have long sought health information online, and increasingly, they\nare turning to generative AI to answer their health-related queries. Given the\nhigh stakes of the medical domain, techniques like retrieval-augmented\ngeneration and citation grounding have been widely promoted as methods to\nreduce hallucinations and improve the accuracy of AI-generated responses and\nhave been widely adopted into search engines. This paper argues that even when\nthese methods produce literally accurate content drawn from source documents\nsans hallucinations, they can still be highly misleading. Patients may derive\nsignificantly different interpretations from AI-generated outputs than they\nwould from reading the original source material, let alone consulting a\nknowledgeable clinician. Through a large-scale query analysis on topics\nincluding disputed diagnoses and procedure safety, we support our argument with\nquantitative and qualitative evidence of the suboptimal answers resulting from\ncurrent systems. In particular, we highlight how these models tend to\ndecontextualize facts, omit critical relevant sources, and reinforce patient\nmisconceptions or biases. We propose a series of recommendations -- such as the\nincorporation of communication pragmatics and enhanced comprehension of source\ndocuments -- that could help mitigate these issues and extend beyond the\nmedical domain.\n","authors":["Lionel Wong","Ayman Ali","Raymond Xiong","Shannon Zeijang Shen","Yoon Kim","Monica Agrawal"],"pdf_url":"https://arxiv.org/pdf/2502.14898v1.pdf","comment":"Preprint"},{"id":"http://arxiv.org/abs/2205.01845v2","updated":"2025-02-18T01:29:55Z","published":"2022-05-04T01:49:36Z","title":"Seed-Guided Topic Discovery with Out-of-Vocabulary Seeds","summary":"  Discovering latent topics from text corpora has been studied for decades.\nMany existing topic models adopt a fully unsupervised setting, and their\ndiscovered topics may not cater to users' particular interests due to their\ninability of leveraging user guidance. Although there exist seed-guided topic\ndiscovery approaches that leverage user-provided seeds to discover\ntopic-representative terms, they are less concerned with two factors: (1) the\nexistence of out-of-vocabulary seeds and (2) the power of pre-trained language\nmodels (PLMs). In this paper, we generalize the task of seed-guided topic\ndiscovery to allow out-of-vocabulary seeds. We propose a novel framework, named\nSeeTopic, wherein the general knowledge of PLMs and the local semantics learned\nfrom the input corpus can mutually benefit each other. Experiments on three\nreal datasets from different domains demonstrate the effectiveness of SeeTopic\nin terms of topic coherence, accuracy, and diversity.\n","authors":["Yu Zhang","Yu Meng","Xuan Wang","Sheng Wang","Jiawei Han"],"pdf_url":"https://arxiv.org/pdf/2205.01845v2.pdf","comment":"12 pages; Accepted to NAACL 2022"},{"id":"http://arxiv.org/abs/2502.12398v1","updated":"2025-02-18T00:12:52Z","published":"2025-02-18T00:12:52Z","title":"Solving the Cold Start Problem on One's Own as an End User via\n  Preference Transfer","summary":"  We propose a new approach that enables end users to directly solve the cold\nstart problem by themselves. The cold start problem is a common issue in\nrecommender systems, and many methods have been proposed to address the problem\non the service provider's side. However, when the service provider does not\ntake action, users are left with poor recommendations and no means to improve\ntheir experience. We propose an algorithm, Pretender, that allows end users to\nproactively solve the cold start problem on their own. Pretender does not\nrequire any special support from the service provider and can be deployed\nindependently by users. We formulate the problem as minimizing the distance\nbetween the source and target distributions and optimize item selection from\nthe target service accordingly. Furthermore, we establish theoretical\nguarantees for Pretender based on a discrete quadrature problem. We conduct\nexperiments on real-world datasets to demonstrate the effectiveness of\nPretender.\n","authors":["Ryoma Sato"],"pdf_url":"https://arxiv.org/pdf/2502.12398v1.pdf","comment":"25 pages"}],"Multimedia":[{"id":"http://arxiv.org/abs/2404.18398v2","updated":"2025-02-18T21:39:25Z","published":"2024-04-29T03:19:39Z","title":"UMETTS: A Unified Framework for Emotional Text-to-Speech Synthesis with\n  Multimodal Prompts","summary":"  Emotional Text-to-Speech (E-TTS) synthesis has garnered significant attention\nin recent years due to its potential to revolutionize human-computer\ninteraction. However, current E-TTS approaches often struggle to capture the\nintricacies of human emotions, primarily relying on oversimplified emotional\nlabels or single-modality input. In this paper, we introduce the Unified\nMultimodal Prompt-Induced Emotional Text-to-Speech System (UMETTS), a novel\nframework that leverages emotional cues from multiple modalities to generate\nhighly expressive and emotionally resonant speech. The core of UMETTS consists\nof two key components: the Emotion Prompt Alignment Module (EP-Align) and the\nEmotion Embedding-Induced TTS Module (EMI-TTS). (1) EP-Align employs\ncontrastive learning to align emotional features across text, audio, and visual\nmodalities, ensuring a coherent fusion of multimodal information. (2)\nSubsequently, EMI-TTS integrates the aligned emotional embeddings with\nstate-of-the-art TTS models to synthesize speech that accurately reflects the\nintended emotions. Extensive evaluations show that UMETTS achieves significant\nimprovements in emotion accuracy and speech naturalness, outperforming\ntraditional E-TTS methods on both objective and subjective metrics.\n","authors":["Zhi-Qi Cheng","Xiang Li","Jun-Yan He","Junyao Chen","Xiaomao Fan","Xiaojiang Peng","Alexander G. Hauptmann"],"pdf_url":"https://arxiv.org/pdf/2404.18398v2.pdf","comment":"Accepted to ICASSP 2025, Code available at\n  https://github.com/KTTRCDL/UMETTS"},{"id":"http://arxiv.org/abs/2406.19859v3","updated":"2025-02-18T20:28:02Z","published":"2024-06-28T11:58:26Z","title":"MetaDesigner: Advancing Artistic Typography Through AI-Driven,\n  User-Centric, and Multilingual WordArt Synthesis","summary":"  MetaDesigner introduces a transformative framework for artistic typography\nsynthesis, powered by Large Language Models (LLMs) and grounded in a\nuser-centric design paradigm. Its foundation is a multi-agent system comprising\nthe Pipeline, Glyph, and Texture agents, which collectively orchestrate the\ncreation of customizable WordArt, ranging from semantic enhancements to\nintricate textural elements. A central feedback mechanism leverages insights\nfrom both multimodal models and user evaluations, enabling iterative refinement\nof design parameters. Through this iterative process, MetaDesigner dynamically\nadjusts hyperparameters to align with user-defined stylistic and thematic\npreferences, consistently delivering WordArt that excels in visual quality and\ncontextual resonance. Empirical evaluations underscore the system's versatility\nand effectiveness across diverse WordArt applications, yielding outputs that\nare both aesthetically compelling and context-sensitive.\n","authors":["Jun-Yan He","Zhi-Qi Cheng","Chenyang Li","Jingdong Sun","Qi He","Wangmeng Xiang","Hanyuan Chen","Jin-Peng Lan","Xianhui Lin","Kang Zhu","Bin Luo","Yifeng Geng","Xuansong Xie","Alexander G. Hauptmann"],"pdf_url":"https://arxiv.org/pdf/2406.19859v3.pdf","comment":"Accepted by ICLR 2025, Project:\n  https://modelscope.cn/studios/WordArt/WordArt"},{"id":"http://arxiv.org/abs/2302.14728v2","updated":"2025-02-18T17:48:45Z","published":"2023-02-28T16:34:55Z","title":"Semantically Consistent Person Image Generation","summary":"  We propose a data-driven approach for context-aware person image generation.\nSpecifically, we attempt to generate a person image such that the synthesized\ninstance can blend into a complex scene. In our method, the position, scale,\nand appearance of the generated person are semantically conditioned on the\nexisting persons in the scene. The proposed technique is divided into three\nsequential steps. At first, we employ a Pix2PixHD model to infer a coarse\nsemantic mask that represents the new person's spatial location, scale, and\npotential pose. Next, we use a data-centric approach to select the closest\nrepresentation from a precomputed cluster of fine semantic masks. Finally, we\nadopt a multi-scale, attention-guided architecture to transfer the appearance\nattributes from an exemplar image. The proposed strategy enables us to\nsynthesize semantically coherent realistic persons that can blend into an\nexisting scene without altering the global context. We conclude our findings\nwith relevant qualitative and quantitative evaluations.\n","authors":["Prasun Roy","Saumik Bhattacharya","Subhankar Ghosh","Umapada Pal","Michael Blumenstein"],"pdf_url":"https://arxiv.org/pdf/2302.14728v2.pdf","comment":"Accepted in The International Conference on Pattern Recognition\n  (ICPR) 2024"},{"id":"http://arxiv.org/abs/2502.13196v1","updated":"2025-02-18T17:46:57Z","published":"2025-02-18T17:46:57Z","title":"GS-QA: Comprehensive Quality Assessment Benchmark for Gaussian Splatting\n  View Synthesis","summary":"  Gaussian Splatting (GS) offers a promising alternative to Neural Radiance\nFields (NeRF) for real-time 3D scene rendering. Using a set of 3D Gaussians to\nrepresent complex geometry and appearance, GS achieves faster rendering times\nand reduced memory consumption compared to the neural network approach used in\nNeRF. However, quality assessment of GS-generated static content is not yet\nexplored in-depth. This paper describes a subjective quality assessment study\nthat aims to evaluate synthesized videos obtained with several static GS\nstate-of-the-art methods. The methods were applied to diverse visual scenes,\ncovering both 360-degree and forward-facing (FF) camera trajectories. Moreover,\nthe performance of 18 objective quality metrics was analyzed using the scores\nresulting from the subjective study, providing insights into their strengths,\nlimitations, and alignment with human perception. All videos and scores are\nmade available providing a comprehensive database that can be used as benchmark\non GS view synthesis and objective quality metrics.\n","authors":["Pedro Martin","António Rodrigues","João Ascenso","Maria Paula Queluz"],"pdf_url":"https://arxiv.org/pdf/2502.13196v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2206.02717v2","updated":"2025-02-18T17:40:45Z","published":"2022-06-06T16:18:15Z","title":"Scene Aware Person Image Generation through Global Contextual\n  Conditioning","summary":"  Person image generation is an intriguing yet challenging problem. However,\nthis task becomes even more difficult under constrained situations. In this\nwork, we propose a novel pipeline to generate and insert contextually relevant\nperson images into an existing scene while preserving the global semantics.\nMore specifically, we aim to insert a person such that the location, pose, and\nscale of the person being inserted blends in with the existing persons in the\nscene. Our method uses three individual networks in a sequential pipeline. At\nfirst, we predict the potential location and the skeletal structure of the new\nperson by conditioning a Wasserstein Generative Adversarial Network (WGAN) on\nthe existing human skeletons present in the scene. Next, the predicted skeleton\nis refined through a shallow linear network to achieve higher structural\naccuracy in the generated image. Finally, the target image is generated from\nthe refined skeleton using another generative network conditioned on a given\nimage of the target person. In our experiments, we achieve high-resolution\nphoto-realistic generation results while preserving the general context of the\nscene. We conclude our paper with multiple qualitative and quantitative\nbenchmarks on the results.\n","authors":["Prasun Roy","Subhankar Ghosh","Saumik Bhattacharya","Umapada Pal","Michael Blumenstein"],"pdf_url":"https://arxiv.org/pdf/2206.02717v2.pdf","comment":"Accepted in The International Conference on Pattern Recognition\n  (ICPR) 2022"},{"id":"http://arxiv.org/abs/2207.11718v2","updated":"2025-02-18T17:28:45Z","published":"2022-07-24T11:14:46Z","title":"TIPS: Text-Induced Pose Synthesis","summary":"  In computer vision, human pose synthesis and transfer deal with probabilistic\nimage generation of a person in a previously unseen pose from an already\navailable observation of that person. Though researchers have recently proposed\nseveral methods to achieve this task, most of these techniques derive the\ntarget pose directly from the desired target image on a specific dataset,\nmaking the underlying process challenging to apply in real-world scenarios as\nthe generation of the target image is the actual aim. In this paper, we first\npresent the shortcomings of current pose transfer algorithms and then propose a\nnovel text-based pose transfer technique to address those issues. We divide the\nproblem into three independent stages: (a) text to pose representation, (b)\npose refinement, and (c) pose rendering. To the best of our knowledge, this is\none of the first attempts to develop a text-based pose transfer framework where\nwe also introduce a new dataset DF-PASS, by adding descriptive pose annotations\nfor the images of the DeepFashion dataset. The proposed method generates\npromising results with significant qualitative and quantitative scores in our\nexperiments.\n","authors":["Prasun Roy","Subhankar Ghosh","Saumik Bhattacharya","Umapada Pal","Michael Blumenstein"],"pdf_url":"https://arxiv.org/pdf/2207.11718v2.pdf","comment":"Accepted in The European Conference on Computer Vision (ECCV) 2022"},{"id":"http://arxiv.org/abs/2202.06777v2","updated":"2025-02-18T17:18:45Z","published":"2022-02-14T14:58:05Z","title":"Multi-scale Attention Guided Pose Transfer","summary":"  Pose transfer refers to the probabilistic image generation of a person with a\npreviously unseen novel pose from another image of that person having a\ndifferent pose. Due to potential academic and commercial applications, this\nproblem is extensively studied in recent years. Among the various approaches to\nthe problem, attention guided progressive generation is shown to produce\nstate-of-the-art results in most cases. In this paper, we present an improved\nnetwork architecture for pose transfer by introducing attention links at every\nresolution level of the encoder and decoder. By utilizing such dense\nmulti-scale attention guided approach, we are able to achieve significant\nimprovement over the existing methods both visually and analytically. We\nconclude our findings with extensive qualitative and quantitative comparisons\nagainst several existing methods on the DeepFashion dataset.\n","authors":["Prasun Roy","Saumik Bhattacharya","Subhankar Ghosh","Umapada Pal"],"pdf_url":"https://arxiv.org/pdf/2202.06777v2.pdf","comment":"Accepted in Pattern Recognition (PR) 2023"},{"id":"http://arxiv.org/abs/1903.01192v4","updated":"2025-02-18T16:58:45Z","published":"2019-03-04T11:56:53Z","title":"STEFANN: Scene Text Editor using Font Adaptive Neural Network","summary":"  Textual information in a captured scene plays an important role in scene\ninterpretation and decision making. Though there exist methods that can\nsuccessfully detect and interpret complex text regions present in a scene, to\nthe best of our knowledge, there is no significant prior work that aims to\nmodify the textual information in an image. The ability to edit text directly\non images has several advantages including error correction, text restoration\nand image reusability. In this paper, we propose a method to modify text in an\nimage at character-level. We approach the problem in two stages. At first, the\nunobserved character (target) is generated from an observed character (source)\nbeing modified. We propose two different neural network architectures - (a)\nFANnet to achieve structural consistency with source font and (b) Colornet to\npreserve source color. Next, we replace the source character with the generated\ncharacter maintaining both geometric and visual consistency with neighboring\ncharacters. Our method works as a unified platform for modifying text in\nimages. We present the effectiveness of our method on COCO-Text and ICDAR\ndatasets both qualitatively and quantitatively.\n","authors":["Prasun Roy","Saumik Bhattacharya","Subhankar Ghosh","Umapada Pal"],"pdf_url":"https://arxiv.org/pdf/1903.01192v4.pdf","comment":"Accepted in The IEEE/CVF Conference on Computer Vision and Pattern\n  Recognition (CVPR) 2020"},{"id":"http://arxiv.org/abs/2502.12623v1","updated":"2025-02-18T08:09:42Z","published":"2025-02-18T08:09:42Z","title":"DeepResonance: Enhancing Multimodal Music Understanding via\n  Music-centric Multi-way Instruction Tuning","summary":"  Recent advancements in music large language models (LLMs) have significantly\nimproved music understanding tasks, which involve the model's ability to\nanalyze and interpret various musical elements. These improvements primarily\nfocused on integrating both music and text inputs. However, the potential of\nincorporating additional modalities such as images, videos and textual music\nfeatures to enhance music understanding remains unexplored. To bridge this gap,\nwe propose DeepResonance, a multimodal music understanding LLM fine-tuned via\nmulti-way instruction tuning with multi-way aligned music, text, image, and\nvideo data. To this end, we construct Music4way-MI2T, Music4way-MV2T, and\nMusic4way-Any2T, three 4-way training and evaluation datasets designed to\nenable DeepResonance to integrate both visual and textual music feature\ncontent. We also introduce multi-sampled ImageBind embeddings and a\npre-alignment Transformer to enhance modality fusion prior to input into text\nLLMs, tailoring DeepResonance for multi-way instruction tuning. Our model\nachieves state-of-the-art performances across six music understanding tasks,\nhighlighting the benefits of the auxiliary modalities and the structural\nsuperiority of DeepResonance. We plan to open-source the models and the newly\nconstructed datasets.\n","authors":["Zhuoyuan Mao","Mengjie Zhao","Qiyu Wu","Hiromi Wakaki","Yuki Mitsufuji"],"pdf_url":"https://arxiv.org/pdf/2502.12623v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.12562v1","updated":"2025-02-18T05:57:35Z","published":"2025-02-18T05:57:35Z","title":"SEA: Low-Resource Safety Alignment for Multimodal Large Language Models\n  via Synthetic Embeddings","summary":"  Multimodal Large Language Models (MLLMs) have serious security\nvulnerabilities.While safety alignment using multimodal datasets consisting of\ntext and data of additional modalities can effectively enhance MLLM's security,\nit is costly to construct these datasets. Existing low-resource security\nalignment methods, including textual alignment, have been found to struggle\nwith the security risks posed by additional modalities. To address this, we\npropose Synthetic Embedding augmented safety Alignment (SEA), which optimizes\nembeddings of additional modality through gradient updates to expand textual\ndatasets. This enables multimodal safety alignment training even when only\ntextual data is available. Extensive experiments on image, video, and\naudio-based MLLMs demonstrate that SEA can synthesize a high-quality embedding\non a single RTX3090 GPU within 24 seconds. SEA significantly improves the\nsecurity of MLLMs when faced with threats from additional modalities. To assess\nthe security risks introduced by video and audio, we also introduced a new\nbenchmark called VA-SafetyBench. High attack success rates across multiple\nMLLMs validate its challenge. Our code and data will be available at\nhttps://github.com/ZeroNLP/SEA.\n","authors":["Weikai Lu","Hao Peng","Huiping Zhuang","Cen Chen","Ziqian Zeng"],"pdf_url":"https://arxiv.org/pdf/2502.12562v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.12489v1","updated":"2025-02-18T03:18:54Z","published":"2025-02-18T03:18:54Z","title":"A Comprehensive Survey on Generative AI for Video-to-Music Generation","summary":"  The burgeoning growth of video-to-music generation can be attributed to the\nascendancy of multimodal generative models. However, there is a lack of\nliterature that comprehensively combs through the work in this field. To fill\nthis gap, this paper presents a comprehensive review of video-to-music\ngeneration using deep generative AI techniques, focusing on three key\ncomponents: visual feature extraction, music generation frameworks, and\nconditioning mechanisms. We categorize existing approaches based on their\ndesigns for each component, clarifying the roles of different strategies.\nPreceding this, we provide a fine-grained classification of video and music\nmodalities, illustrating how different categories influence the design of\ncomponents within the generation pipelines. Furthermore, we summarize available\nmultimodal datasets and evaluation metrics while highlighting ongoing\nchallenges in the field.\n","authors":["Shulei Ji","Songruoyao Wu","Zihao Wang","Shuyu Li","Kejun Zhang"],"pdf_url":"https://arxiv.org/pdf/2502.12489v1.pdf","comment":null}]},"2025-02-17T00:00:00Z":{"Information Retrieval":[{"id":"http://arxiv.org/abs/2502.08826v2","updated":"2025-02-17T23:26:44Z","published":"2025-02-12T22:33:41Z","title":"Ask in Any Modality: A Comprehensive Survey on Multimodal\n  Retrieval-Augmented Generation","summary":"  Large Language Models (LLMs) struggle with hallucinations and outdated\nknowledge due to their reliance on static training data. Retrieval-Augmented\nGeneration (RAG) mitigates these issues by integrating external dynamic\ninformation enhancing factual and updated grounding. Recent advances in\nmultimodal learning have led to the development of Multimodal RAG,\nincorporating multiple modalities such as text, images, audio, and video to\nenhance the generated outputs. However, cross-modal alignment and reasoning\nintroduce unique challenges to Multimodal RAG, distinguishing it from\ntraditional unimodal RAG. This survey offers a structured and comprehensive\nanalysis of Multimodal RAG systems, covering datasets, metrics, benchmarks,\nevaluation, methodologies, and innovations in retrieval, fusion, augmentation,\nand generation. We precisely review training strategies, robustness\nenhancements, and loss functions, while also exploring the diverse Multimodal\nRAG scenarios. Furthermore, we discuss open challenges and future research\ndirections to support advancements in this evolving field. This survey lays the\nfoundation for developing more capable and reliable AI systems that effectively\nleverage multimodal dynamic external knowledge bases. Resources are available\nat https://github.com/llm-lab-org/Multimodal-RAG-Survey.\n","authors":["Mohammad Mahdi Abootorabi","Amirhosein Zobeiri","Mahdi Dehghani","Mohammadali Mohammadkhani","Bardia Mohammadi","Omid Ghahroodi","Mahdieh Soleymani Baghshah","Ehsaneddin Asgari"],"pdf_url":"https://arxiv.org/pdf/2502.08826v2.pdf","comment":"GitHub repository:\n  https://github.com/llm-lab-org/Multimodal-RAG-Survey"},{"id":"http://arxiv.org/abs/2502.12342v1","updated":"2025-02-17T22:10:47Z","published":"2025-02-17T22:10:47Z","title":"REAL-MM-RAG: A Real-World Multi-Modal Retrieval Benchmark","summary":"  Accurate multi-modal document retrieval is crucial for Retrieval-Augmented\nGeneration (RAG), yet existing benchmarks do not fully capture real-world\nchallenges with their current design. We introduce REAL-MM-RAG, an\nautomatically generated benchmark designed to address four key properties\nessential for real-world retrieval: (i) multi-modal documents, (ii) enhanced\ndifficulty, (iii) Realistic-RAG queries and (iv) accurate labeling.\nAdditionally, we propose a multi-difficulty-level scheme based on query\nrephrasing to evaluate models' semantic understanding beyond keyword matching.\nOur benchmark reveals significant model weaknesses, particularly in handling\ntable-heavy documents and robustness to query rephrasing. To mitigate these\nshortcomings, we curate a rephrased training set and introduce a new\nfinance-focused, table-heavy dataset. Fine-tuning on these datasets enables\nmodels to achieve state-of-the-art retrieval performance on REAL-MM-RAG\nbenchmark. Our work offers a better way to evaluate and improve retrieval in\nmulti-modal RAG systems while also providing training data and models that\naddress current limitations.\n","authors":["Navve Wasserman","Roi Pony","Oshri Naparstek","Adi Raz Goldfarb","Eli Schwartz","Udi Barzelay","Leonid Karlinsky"],"pdf_url":"https://arxiv.org/pdf/2502.12342v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.12145v1","updated":"2025-02-17T18:56:20Z","published":"2025-02-17T18:56:20Z","title":"Fast or Better? Balancing Accuracy and Cost in Retrieval-Augmented\n  Generation with Flexible User Control","summary":"  Retrieval-Augmented Generation (RAG) has emerged as a powerful approach to\nmitigate large language model (LLM) hallucinations by incorporating external\nknowledge retrieval. However, existing RAG frameworks often apply retrieval\nindiscriminately,leading to inefficiencies-over-retrieving when unnecessary or\nfailing to retrieve iteratively when required for complex reasoning. Recent\nadaptive retrieval strategies, though adaptively navigates these retrieval\nstrategies, predict only based on query complexity and lacks user-driven\nflexibility, making them infeasible for diverse user application needs. In this\npaper, we introduce a novel user-controllable RAG framework that enables\ndynamic adjustment of the accuracy-cost trade-off. Our approach leverages two\nclassifiers: one trained to prioritize accuracy and another to prioritize\nretrieval efficiency. Via an interpretable control parameter $\\alpha$, users\ncan seamlessly navigate between minimal-cost retrieval and high-accuracy\nretrieval based on their specific requirements. We empirically demonstrate that\nour approach effectively balances accuracy, retrieval cost, and user\ncontrollability, making it a practical and adaptable solution for real-world\napplications.\n","authors":["Jinyan Su","Jennifer Healey","Preslav Nakov","Claire Cardie"],"pdf_url":"https://arxiv.org/pdf/2502.12145v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.12137v1","updated":"2025-02-17T18:53:42Z","published":"2025-02-17T18:53:42Z","title":"REVERSUM: A Multi-staged Retrieval-Augmented Generation Method to\n  Enhance Wikipedia Tail Biographies through Personal Narratives","summary":"  Wikipedia is an invaluable resource for factual information about a wide\nrange of entities. However, the quality of articles on less-known entities\noften lags behind that of the well-known ones. This study proposes a novel\napproach to enhancing Wikipedia's B and C category biography articles by\nleveraging personal narratives such as autobiographies and biographies. By\nutilizing a multi-staged retrieval-augmented generation technique -- REVerSum\n-- we aim to enrich the informational content of these lesser-known articles.\nOur study reveals that personal narratives can significantly improve the\nquality of Wikipedia articles, providing a rich source of reliable information\nthat has been underutilized in previous studies. Based on crowd-based\nevaluation, REVerSum generated content outperforms the best performing baseline\nby 17% in terms of integrability to the original Wikipedia article and 28.5\\%\nin terms of informativeness. Code and Data are available at:\nhttps://github.com/sayantan11995/wikipedia_enrichment\n","authors":["Sayantan Adak","Pauras Mangesh Meher","Paramita Das","Animesh Mukherjee"],"pdf_url":"https://arxiv.org/pdf/2502.12137v1.pdf","comment":"Accepted at COLING2025 Industry Track"},{"id":"http://arxiv.org/abs/2410.21745v4","updated":"2025-02-17T16:26:20Z","published":"2024-10-29T05:18:34Z","title":"RDSA: A Robust Deep Graph Clustering Framework via Dual Soft Assignment","summary":"  Graph clustering is an essential aspect of network analysis that involves\ngrouping nodes into separate clusters. Recent developments in deep learning\nhave resulted in graph clustering, which has proven effective in many\napplications. Nonetheless, these methods often encounter difficulties when\ndealing with real-world graphs, particularly in the presence of noisy edges.\nAdditionally, many denoising graph clustering methods tend to suffer from lower\nperformance, training instability, and challenges in scaling to large datasets\ncompared to non-denoised models. To tackle these issues, we introduce a new\nframework called the Robust Deep Graph Clustering Framework via Dual Soft\nAssignment (RDSA). RDSA consists of three key components: (i) a node embedding\nmodule that effectively integrates the graph's topological features and node\nattributes; (ii) a structure-based soft assignment module that improves graph\nmodularity by utilizing an affinity matrix for node assignments; and (iii) a\nnode-based soft assignment module that identifies community landmarks and\nrefines node assignments to enhance the model's robustness. We assess RDSA on\nvarious real-world datasets, demonstrating its superior performance relative to\nexisting state-of-the-art methods. Our findings indicate that RDSA provides\nrobust clustering across different graph types, excelling in clustering\neffectiveness and robustness, including adaptability to noise, stability, and\nscalability.\n","authors":["Yang Xiang","Li Fan","Tulika Saha","Xiaoying Pang","Yushan Pan","Haiyang Zhang","Chengtao Ji"],"pdf_url":"https://arxiv.org/pdf/2410.21745v4.pdf","comment":"Accepted by DASFAA 2025; Complete version"},{"id":"http://arxiv.org/abs/2502.11921v1","updated":"2025-02-17T15:33:28Z","published":"2025-02-17T15:33:28Z","title":"Joint Evaluation of Fairness and Relevance in Recommender Systems with\n  Pareto Frontier","summary":"  Fairness and relevance are two important aspects of recommender systems\n(RSs). Typically, they are evaluated either (i) separately by individual\nmeasures of fairness and relevance, or (ii) jointly using a single measure that\naccounts for fairness with respect to relevance. However, approach (i) often\ndoes not provide a reliable joint estimate of the goodness of the models, as it\nhas two different best models: one for fairness and another for relevance.\nApproach (ii) is also problematic because these measures tend to be ad-hoc and\ndo not relate well to traditional relevance measures, like NDCG. Motivated by\nthis, we present a new approach for jointly evaluating fairness and relevance\nin RSs: Distance to Pareto Frontier (DPFR). Given some user-item interaction\ndata, we compute their Pareto frontier for a pair of existing relevance and\nfairness measures, and then use the distance from the frontier as a measure of\nthe jointly achievable fairness and relevance. Our approach is modular and\nintuitive as it can be computed with existing measures. Experiments with 4 RS\nmodels, 3 re-ranking strategies, and 6 datasets show that existing metrics have\ninconsistent associations with our Pareto-optimal solution, making DPFR a more\nrobust and theoretically well-founded joint measure for assessing fairness and\nrelevance. Our code: https://github.com/theresiavr/DPFR-recsys-evaluation\n","authors":["Theresia Veronika Rampisela","Tuukka Ruotsalo","Maria Maistro","Christina Lioma"],"pdf_url":"https://arxiv.org/pdf/2502.11921v1.pdf","comment":"Accepted to TheWebConf/WWW 2025 (Oral)"},{"id":"http://arxiv.org/abs/2502.11883v1","updated":"2025-02-17T15:11:09Z","published":"2025-02-17T15:11:09Z","title":"FairDiverse: A Comprehensive Toolkit for Fair and Diverse Information\n  Retrieval Algorithms","summary":"  In modern information retrieval (IR). achieving more than just accuracy is\nessential to sustaining a healthy ecosystem, especially when addressing\nfairness and diversity considerations. To meet these needs, various datasets,\nalgorithms, and evaluation frameworks have been introduced. However, these\nalgorithms are often tested across diverse metrics, datasets, and experimental\nsetups, leading to inconsistencies and difficulties in direct comparisons. This\nhighlights the need for a comprehensive IR toolkit that enables standardized\nevaluation of fairness- and diversity-aware algorithms across different IR\ntasks. To address this challenge, we present FairDiverse, an open-source and\nstandardized toolkit. FairDiverse offers a framework for integrating fair and\ndiverse methods, including pre-processing, in-processing, and post-processing\ntechniques, at different stages of the IR pipeline. The toolkit supports the\nevaluation of 28 fairness and diversity algorithms across 16 base models,\ncovering two core IR tasks (search and recommendation) thereby establishing a\ncomprehensive benchmark. Moreover, FairDiverse is highly extensible, providing\nmultiple APIs that empower IR researchers to swiftly develop and evaluate their\nown fairness and diversity aware models, while ensuring fair comparisons with\nexisting baselines. The project is open-sourced and available on\nhttps://github.com/XuChen0427/FairDiverse.\n","authors":["Chen Xu","Zhirui Deng","Clara Rus","Xiaopeng Ye","Yuanna Liu","Jun Xu","Zhicheng Dou","Ji-Rong Wen","Maarten de Rijke"],"pdf_url":"https://arxiv.org/pdf/2502.11883v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.11840v1","updated":"2025-02-17T14:35:16Z","published":"2025-02-17T14:35:16Z","title":"ChordFormer: A Conformer-Based Architecture for Large-Vocabulary Audio\n  Chord Recognition","summary":"  Chord recognition serves as a critical task in music information retrieval\ndue to the abstract and descriptive nature of chords in music analysis. While\naudio chord recognition systems have achieved significant accuracy for small\nvocabularies (e.g., major/minor chords), large-vocabulary chord recognition\nremains a challenging problem. This complexity also arises from the inherent\nlong-tail distribution of chords, where rare chord types are underrepresented\nin most datasets, leading to insufficient training samples. Effective chord\nrecognition requires leveraging contextual information from audio sequences,\nyet existing models, such as combinations of convolutional neural networks,\nbidirectional long short-term memory networks, and bidirectional transformers,\nface limitations in capturing long-term dependencies and exhibit suboptimal\nperformance on large-vocabulary chord recognition tasks. This work proposes\nChordFormer, a novel conformer-based architecture designed to tackle structural\nchord recognition (e.g., triads, bass, sevenths) for large vocabularies.\nChordFormer leverages conformer blocks that integrate convolutional neural\nnetworks with transformers, thus enabling the model to capture both local\npatterns and global dependencies effectively. By addressing challenges such as\nclass imbalance through a reweighted loss function and structured chord\nrepresentations, ChordFormer outperforms state-of-the-art models, achieving a\n2% improvement in frame-wise accuracy and a 6% increase in class-wise accuracy\non large-vocabulary chord datasets. Furthermore, ChordFormer excels in handling\nclass imbalance, providing robust and balanced recognition across chord types.\nThis approach bridges the gap between theoretical music knowledge and practical\napplications, advancing the field of large-vocabulary chord recognition.\n","authors":["Muhammad Waseem Akram","Stefano Dettori","Valentina Colla","Giorgio Carlo Buttazzo"],"pdf_url":"https://arxiv.org/pdf/2502.11840v1.pdf","comment":"13 pages, 4 figures"},{"id":"http://arxiv.org/abs/2502.11721v1","updated":"2025-02-17T12:08:18Z","published":"2025-02-17T12:08:18Z","title":"Enhancing Recommendation Explanations through User-Centric Refinement","summary":"  Generating natural language explanations for recommendations has become\nincreasingly important in recommender systems. Traditional approaches typically\ntreat user reviews as ground truth for explanations and focus on improving\nreview prediction accuracy by designing various model architectures. However,\ndue to limitations in data scale and model capability, these explanations often\nfail to meet key user-centric aspects such as factuality, personalization, and\nsentiment coherence, significantly reducing their overall helpfulness to users.\nIn this paper, we propose a novel paradigm that refines initial explanations\ngenerated by existing explainable recommender models during the inference stage\nto enhance their quality in multiple aspects. Specifically, we introduce a\nmulti-agent collaborative refinement framework based on large language models.\nTo ensure alignment between the refinement process and user demands, we employ\na plan-then-refine pattern to perform targeted modifications. To enable\ncontinuous improvements, we design a hierarchical reflection mechanism that\nprovides feedback on the refinement process from both strategic and content\nperspectives. Extensive experiments on three datasets demonstrate the\neffectiveness of our framework.\n","authors":["Jingsen Zhang","Zihang Tian","Xueyang Feng","Xu Chen"],"pdf_url":"https://arxiv.org/pdf/2502.11721v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.05558v3","updated":"2025-02-17T11:34:41Z","published":"2025-02-08T13:08:11Z","title":"Large Memory Network for Recommendation","summary":"  Modeling user behavior sequences in recommender systems is essential for\nunderstanding user preferences over time, enabling personalized and accurate\nrecommendations for improving user retention and enhancing business values.\nDespite its significance, there are two challenges for current sequential\nmodeling approaches. From the spatial dimension, it is difficult to mutually\nperceive similar users' interests for a generalized intention understanding;\nfrom the temporal dimension, current methods are generally prone to forgetting\nlong-term interests due to the fixed-length input sequence. In this paper, we\npresent Large Memory Network (LMN), providing a novel idea by compressing and\nstoring user history behavior information in a large-scale memory block. With\nthe elaborated online deployment strategy, the memory block can be easily\nscaled up to million-scale in the industry. Extensive offline comparison\nexperiments, memory scaling up experiments, and online A/B test on Douyin\nE-Commerce Search (ECS) are performed, validating the superior performance of\nLMN. Currently, LMN has been fully deployed in Douyin ECS, serving millions of\nusers each day.\n","authors":["Hui Lu","Zheng Chai","Yuchao Zheng","Zhe Chen","Deping Xie","Peng Xu","Xun Zhou","Di Wu"],"pdf_url":"https://arxiv.org/pdf/2502.05558v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.02061v2","updated":"2025-02-17T11:22:58Z","published":"2025-02-04T07:17:54Z","title":"Reason4Rec: Large Language Models for Recommendation with Deliberative\n  User Preference Alignment","summary":"  While recent advancements in aligning Large Language Models (LLMs) with\nrecommendation tasks have shown great potential and promising performance\noverall, these aligned recommendation LLMs still face challenges in complex\nscenarios. This is primarily due to the current alignment approach focusing on\noptimizing LLMs to generate user feedback directly, without incorporating\ndeliberation. To overcome this limitation and develop more reliable LLMs for\nrecommendations, we propose a new Deliberative Recommendation task, which\nincorporates explicit reasoning about user preferences as an additional\nalignment goal. We then introduce the Reasoning-powered Recommender framework\nfor deliberative user preference alignment, designed to enhance reasoning\ncapabilities by utilizing verbalized user feedback in a step-wise manner to\ntackle this task. The framework employs collaborative step-wise experts and\ntailored training strategies for each expert. Experimental results across three\nreal-world datasets demonstrate the rationality of the deliberative task\nformulation and the superior performance of the proposed framework in improving\nboth prediction accuracy and reasoning quality.\n","authors":["Yi Fang","Wenjie Wang","Yang Zhang","Fengbin Zhu","Qifan Wang","Fuli Feng","Xiangnan He"],"pdf_url":"https://arxiv.org/pdf/2502.02061v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.11610v1","updated":"2025-02-17T09:54:46Z","published":"2025-02-17T09:54:46Z","title":"Accuracy Assessment of OpenAlex and Clarivate Scholar ID with an\n  LLM-Assisted Benchmark","summary":"  In quantitative SciSci (science of science) studies, accurately identifying\nindividual scholars is paramount for scientific data analysis. However, the\nvariability in how names are represented-due to commonality, abbreviations, and\ndifferent spelling conventions-complicates this task. While identifier systems\nlike ORCID are being developed, many scholars remain unregistered, and numerous\npublications are not included. Scholarly databases such as Clarivate and\nOpenAlex have introduced their own ID systems as preliminary name\ndisambiguation solutions. This study evaluates the effectiveness of these\nsystems across different groups to determine their suitability for various\napplication scenarios. We sampled authors from the top quartile (Q1) of Web of\nScience (WOS) journals based on country, discipline, and number of\ncorresponding author papers. For each group, we selected 100 scholars and\nmeticulously annotated all their papers using a Search-enhanced Large Language\nModel method. Using these annotations, we identified the corresponding IDs in\nOpenAlex and Clarivate, extracted all associated papers, filtered for Q1 WOS\njournals, and calculated precision and recall by comparing against the\nannotated dataset.\n","authors":["Renyu Zhao","Yunxin Chen"],"pdf_url":"https://arxiv.org/pdf/2502.11610v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.11624v4","updated":"2025-02-17T09:26:15Z","published":"2024-03-18T09:56:00Z","title":"Dual-Channel Multiplex Graph Neural Networks for Recommendation","summary":"  Effective recommender systems play a crucial role in accurately capturing\nuser and item attributes that mirror individual preferences. Some existing\nrecommendation techniques have started to shift their focus towards modeling\nvarious types of interactive relations between users and items in real-world\nrecommendation scenarios, such as clicks, marking favorites, and purchases on\nonline shopping platforms. Nevertheless, these approaches still grapple with\ntwo significant challenges: (1) Insufficient modeling and exploitation of the\nimpact of various behavior patterns formed by multiplex relations between users\nand items on representation learning, and (2) ignoring the effect of different\nrelations within behavior patterns on the target relation in recommender system\nscenarios. In this work, we introduce a novel recommendation framework,\nDual-Channel Multiplex Graph Neural Network (DCMGNN), which addresses the\naforementioned challenges. It incorporates an explicit behavior pattern\nrepresentation learner to capture the behavior patterns composed of multiplex\nuser-item interactive relations, and includes a relation chain representation\nlearner and a relation chain-aware encoder to discover the impact of various\nauxiliary relations on the target relation, the dependencies between different\nrelations, and mine the appropriate order of relations in a behavior pattern.\nExtensive experiments on three real-world datasets demonstrate that our \\model\nsurpasses various state-of-the-art recommendation methods. It outperforms the\nbest baselines by 10.06% and 12.15% on average across all datasets in terms of\nRecall@10 and NDCG@10 respectively.\n","authors":["Xiang Li","Chaofan Fu","Zhongying Zhao","Guanjie Zheng","Chao Huang","Yanwei Yu","Junyu Dong"],"pdf_url":"https://arxiv.org/pdf/2403.11624v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.11571v1","updated":"2025-02-17T09:05:21Z","published":"2025-02-17T09:05:21Z","title":"FaMTEB: Massive Text Embedding Benchmark in Persian Language","summary":"  In this paper, we introduce a comprehensive benchmark for Persian (Farsi)\ntext embeddings, built upon the Massive Text Embedding Benchmark (MTEB). Our\nbenchmark includes 63 datasets spanning seven different tasks: classification,\nclustering, pair classification, reranking, retrieval, summary retrieval, and\nsemantic textual similarity. The datasets are formed as a combination of\nexisting, translated, and newly generated data, offering a diverse evaluation\nframework for Persian language models. Given the increasing use of text\nembedding models in chatbots, evaluation datasets are becoming inseparable\ningredients in chatbot challenges and Retrieval-Augmented Generation systems.\nAs a contribution, we include chatbot evaluation datasets in the MTEB benchmark\nfor the first time. In addition, in this paper, we introduce the new task of\nsummary retrieval which is not part of the tasks included in standard MTEB.\nAnother contribution of this paper is the introduction of a substantial number\nof new Persian language NLP datasets suitable for training and evaluation, some\nof which have no previous counterparts in Persian. We evaluate the performance\nof several Persian and multilingual embedding models in a range of tasks. This\nwork introduces an open-source benchmark with datasets, code and a public\nleaderboard.\n","authors":["Erfan Zinvandi","Morteza Alikhani","Mehran Sarmadi","Zahra Pourbahman","Sepehr Arvin","Reza Kazemi","Arash Amini"],"pdf_url":"https://arxiv.org/pdf/2502.11571v1.pdf","comment":"to appear in ACL 2025"},{"id":"http://arxiv.org/abs/2410.23166v2","updated":"2025-02-17T08:59:45Z","published":"2024-10-30T16:18:22Z","title":"SciPIP: An LLM-based Scientific Paper Idea Proposer","summary":"  The rapid advancement of large language models (LLMs) has opened new\npossibilities for automating the proposal of innovative scientific ideas. This\nprocess involves two key phases: literature retrieval and idea generation.\nHowever, existing approaches often fall short due to their reliance on\nkeyword-based search tools during the retrieval phase, which neglects crucial\nsemantic information and frequently results in incomplete retrieval outcomes.\nSimilarly, in the idea generation phase, current methodologies tend to depend\nsolely on the internal knowledge of LLMs or metadata from retrieved papers,\nthereby overlooking significant valuable insights contained within the full\ntexts. To address these limitations, we introduce SciPIP, an innovative\nframework designed to enhance the LLM-based proposal of scientific ideas\nthrough improvements in both literature retrieval and idea generation. Our\napproach begins with the construction of a comprehensive literature database\nthat supports advanced retrieval based not only on keywords but also on\nsemantics and citation relationships. This is complemented by the introduction\nof a multi-granularity retrieval algorithm aimed at ensuring more thorough and\nexhaustive retrieval results. For the idea generation phase, we propose a\ndual-path framework that effectively integrates both the content of retrieved\npapers and the extensive internal knowledge of LLMs. This integration\nsignificantly boosts the novelty, feasibility, and practical value of proposed\nideas. Our experiments, conducted across various domains such as natural\nlanguage processing and computer vision, demonstrate SciPIP's capability to\ngenerate a multitude of innovative and useful ideas. These findings underscore\nSciPIP's potential as a valuable tool for researchers seeking to advance their\nfields with groundbreaking concepts.\n","authors":["Wenxiao Wang","Lihui Gu","Liye Zhang","Yunxiang Luo","Yi Dai","Chen Shen","Liang Xie","Binbin Lin","Xiaofei He","Jieping Ye"],"pdf_url":"https://arxiv.org/pdf/2410.23166v2.pdf","comment":"20 pages, 5 figures, 12 tables. The code has been availabel:\n  https://github.com/cheerss/SciPIP"},{"id":"http://arxiv.org/abs/2306.13887v3","updated":"2025-02-17T08:44:49Z","published":"2023-06-24T07:27:43Z","title":"Cross-domain Recommender Systems via Multimodal Domain Adaptation","summary":"  Collaborative Filtering (CF) has emerged as one of the most prominent\nimplementation strategies for building recommender systems. The key idea is to\nexploit the usage patterns of individuals to generate personalized\nrecommendations. CF techniques, especially for newly launched platforms, often\nface a critical issue known as the data sparsity problem, which greatly limits\ntheir performance. Cross-domain CF alleviates the problem of data sparsity by\nfinding a common set of entities (users or items) across the domains, which\nthen act as a conduit for knowledge transfer. Nevertheless, most real-world\ndatasets are collected from different domains, so they often lack information\nabout anchor points or reference information for entity alignment. This paper\nintroduces a domain adaptation technique to align the embeddings of entities\nacross domains. Our approach first exploits the available textual and visual\ninformation to independently learn a multi-view latent representation for each\nentity in the auxiliary and target domains. The different representations of\nthe entity are then fused to generate the corresponding unified representation.\nA domain classifier is then trained to learn the embedding for the domain\nalignment by fixing the unified features as the anchor points. Experiments on\n\\AS{four} publicly available benchmark datasets indicate the effectiveness of\nour proposed approach.\n","authors":["Adamya Shyam","Ramya Kamani","Venkateswara Rao Kagita","Vikas Kumar"],"pdf_url":"https://arxiv.org/pdf/2306.13887v3.pdf","comment":"20 pages"},{"id":"http://arxiv.org/abs/2501.07365v3","updated":"2025-02-17T08:40:10Z","published":"2025-01-13T14:34:26Z","title":"Multimodal semantic retrieval for product search","summary":"  Semantic retrieval (also known as dense retrieval) based on textual data has\nbeen extensively studied for both web search and product search application\nfields, where the relevance of a query and a potential target document is\ncomputed by their dense vector representation comparison. Product image is\ncrucial for e-commerce search interactions and is a key factor for customers at\nproduct explorations. However, its impact on semantic retrieval has not been\nwell studied yet. In this research, we build a multimodal representation for\nproduct items in e-commerce search in contrast to pure-text representation of\nproducts, and investigate the impact of such representations. The models are\ndeveloped and evaluated on e-commerce datasets. We demonstrate that a\nmultimodal representation scheme for a product can show improvement either on\npurchase recall or relevance accuracy in semantic retrieval. Additionally, we\nprovide numerical analysis for exclusive matches retrieved by a multimodal\nsemantic retrieval model versus a text-only semantic retrieval model, to\ndemonstrate the validation of multimodal solutions.\n","authors":["Dong Liu","Esther Lopez Ramos"],"pdf_url":"https://arxiv.org/pdf/2501.07365v3.pdf","comment":"Accepted at EReL@MIR WWW 2025"},{"id":"http://arxiv.org/abs/2502.09827v2","updated":"2025-02-17T08:34:43Z","published":"2025-02-13T23:53:40Z","title":"Data and Decision Traceability for SDA TAP Lab's Prototype Battle\n  Management System","summary":"  Space Protocol is applying the principles derived from MITRE and NIST's\nSupply Chain Traceability: Manufacturing Meta-Framework (NIST IR 8536) to a\ncomplex multi party system to achieve introspection, auditing, and replay of\ndata and decisions that ultimately lead to a end decision. The core goal of\ndecision traceability is to ensure transparency, accountability, and integrity\nwithin the WA system. This is accomplished by providing a clear, auditable path\nfrom the system's inputs all the way to the final decision. This traceability\nenables the system to track the various algorithms and data flows that have\ninfluenced a particular outcome.\n","authors":["Latha Pratti","Samya Bagchi","Yasir Latif"],"pdf_url":"https://arxiv.org/pdf/2502.09827v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.11490v1","updated":"2025-02-17T06:49:34Z","published":"2025-02-17T06:49:34Z","title":"GPU-accelerated Multi-relational Parallel Graph Retrieval for Web-scale\n  Recommendations","summary":"  Web recommendations provide personalized items from massive catalogs for\nusers, which rely heavily on retrieval stages to trade off the effectiveness\nand efficiency of selecting a small relevant set from billion-scale candidates\nin online digital platforms. As one of the largest Chinese search engine and\nnews feed providers, Baidu resorts to Deep Neural Network (DNN) and graph-based\nApproximate Nearest Neighbor Search (ANNS) algorithms for accurate relevance\nestimation and efficient search for relevant items. However, current retrieval\nat Baidu fails in comprehensive user-item relational understanding due to\ndissected interaction modeling, and performs inefficiently in large-scale\ngraph-based ANNS because of suboptimal traversal navigation and the GPU\ncomputational bottleneck under high concurrency. To this end, we propose a\nGPU-accelerated Multi-relational Parallel Graph Retrieval (GMP-GR) framework to\nachieve effective yet efficient retrieval in web-scale recommendations. First,\nwe propose a multi-relational user-item relevance metric learning method that\nunifies diverse user behaviors through multi-objective optimization and employs\na self-covariant loss to enhance pathfinding performance. Second, we develop a\nhierarchical parallel graph-based ANNS to boost graph retrieval throughput,\nwhich conducts breadth-depth-balanced searches on a large-scale item graph and\ncost-effectively handles irregular neural computation via adaptive aggregation\non GPUs. In addition, we integrate system optimization strategies in the\ndeployment of GMP-GR in Baidu. Extensive experiments demonstrate the\nsuperiority of GMP-GR in retrieval accuracy and efficiency. Deployed across\nmore than twenty applications at Baidu, GMP-GR serves hundreds of millions of\nusers with a throughput exceeding one hundred million requests per second.\n","authors":["Zhuoning Guo","Guangxing Chen","Qian Gao","Xiaochao Liao","Jianjia Zheng","Lu Shen","Hao Liu"],"pdf_url":"https://arxiv.org/pdf/2502.11490v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.11471v1","updated":"2025-02-17T06:02:59Z","published":"2025-02-17T06:02:59Z","title":"GLTW: Joint Improved Graph Transformer and LLM via Three-Word Language\n  for Knowledge Graph Completion","summary":"  Knowledge Graph Completion (KGC), which aims to infer missing or incomplete\nfacts, is a crucial task for KGs. However, integrating the vital structural\ninformation of KGs into Large Language Models (LLMs) and outputting predictions\ndeterministically remains challenging. To address this, we propose a new method\ncalled GLTW, which encodes the structural information of KGs and merges it with\nLLMs to enhance KGC performance. Specifically, we introduce an improved Graph\nTransformer (iGT) that effectively encodes subgraphs with both local and global\nstructural information and inherits the characteristics of language model,\nbypassing training from scratch. Also, we develop a subgraph-based\nmulti-classification training objective, using all entities within KG as\nclassification objects, to boost learning efficiency.Importantly, we combine\niGT with an LLM that takes KG language prompts as input.Our extensive\nexperiments on various KG datasets show that GLTW achieves significant\nperformance gains compared to SOTA baselines.\n","authors":["Kangyang Luo","Yuzhuo Bai","Cheng Gao","Shuzheng Si","Yingli Shen","Zhu Liu","Zhitong Wang","Cunliang Kong","Wenhao Li","Yufei Huang","Ye Tian","Xuantang Xiong","Lei Han","Maosong Sun"],"pdf_url":"https://arxiv.org/pdf/2502.11471v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.11442v1","updated":"2025-02-17T04:58:14Z","published":"2025-02-17T04:58:14Z","title":"Multi-Turn Multi-Modal Question Clarification for Enhanced\n  Conversational Understanding","summary":"  Conversational query clarification enables users to refine their search\nqueries through interactive dialogue, improving search effectiveness.\nTraditional approaches rely on text-based clarifying questions, which often\nfail to capture complex user preferences, particularly those involving visual\nattributes. While recent work has explored single-turn multi-modal\nclarification with images alongside text, such methods do not fully support the\nprogressive nature of user intent refinement over multiple turns. Motivated by\nthis, we introduce the Multi-turn Multi-modal Clarifying Questions (MMCQ) task,\nwhich combines text and visual modalities to refine user queries in a\nmulti-turn conversation. To facilitate this task, we create a large-scale\ndataset named ClariMM comprising over 13k multi-turn interactions and 33k\nquestion-answer pairs containing multi-modal clarifying questions. We propose\nMario, a retrieval framework that employs a two-phase ranking strategy: initial\nretrieval with BM25, followed by a multi-modal generative re-ranking model that\nintegrates textual and visual information from conversational history. Our\nexperiments show that multi-turn multi-modal clarification outperforms\nuni-modal and single-turn approaches, improving MRR by 12.88%. The gains are\nmost significant in longer interactions, demonstrating the value of progressive\nrefinement for complex queries.\n","authors":["Kimia Ramezan","Alireza Amiri Bavandpour","Yifei Yuan","Clemencia Siro","Mohammad Aliannejadi"],"pdf_url":"https://arxiv.org/pdf/2502.11442v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.14483v4","updated":"2025-02-17T03:10:44Z","published":"2023-10-23T01:29:18Z","title":"Chain-of-Factors Paper-Reviewer Matching","summary":"  With the rapid increase in paper submissions to academic conferences, the\nneed for automated and accurate paper-reviewer matching is more critical than\never. Previous efforts in this area have considered various factors to assess\nthe relevance of a reviewer's expertise to a paper, such as the semantic\nsimilarity, shared topics, and citation connections between the paper and the\nreviewer's previous works. However, most of these studies focus on only one\nfactor, resulting in an incomplete evaluation of the paper-reviewer relevance.\nTo address this issue, we propose a unified model for paper-reviewer matching\nthat jointly considers semantic, topic, and citation factors. To be specific,\nduring training, we instruction-tune a contextualized language model shared\nacross all factors to capture their commonalities and characteristics; during\ninference, we chain the three factors to enable step-by-step, coarse-to-fine\nsearch for qualified reviewers given a submission. Experiments on four datasets\n(one of which is newly contributed by us) spanning various fields such as\nmachine learning, computer vision, information retrieval, and data mining\nconsistently demonstrate the effectiveness of our proposed Chain-of-Factors\nmodel in comparison with state-of-the-art paper-reviewer matching methods and\nscientific pre-trained language models.\n","authors":["Yu Zhang","Yanzhen Shen","SeongKu Kang","Xiusi Chen","Bowen Jin","Jiawei Han"],"pdf_url":"https://arxiv.org/pdf/2310.14483v4.pdf","comment":"10 pages; Accepted to WWW 2025 (Code:\n  https://github.com/yuzhimanhua/CoF)"},{"id":"http://arxiv.org/abs/2410.10293v3","updated":"2025-02-17T02:51:14Z","published":"2024-10-14T08:47:21Z","title":"FunnelRAG: A Coarse-to-Fine Progressive Retrieval Paradigm for RAG","summary":"  Retrieval-Augmented Generation (RAG) prevails in Large Language Models. It\nmainly consists of retrieval and generation. The retrieval modules (a.k.a.\nretrievers) aim to find useful information used to facilitate the generation\nmodules (a.k.a. generators). As such, generators' performance largely depends\non the effectiveness and efficiency of retrievers. However, the widely used\nretrieval paradigm remains flat. It treats retrieval procedures as a one-off\ndeal with constant granularity. Despite effectiveness, we argue that they\nsuffer from two limitations: (1) flat retrieval exerts a significant burden on\none retriever; (2) constant granularity limits the ceiling of retrieval\nperformance. In this work, we propose a progressive retrieval paradigm with\ncoarse-to-fine granularity for RAG, termed FunnelRAG, so as to balance\neffectiveness and efficiency. Specifically, FunnelRAG establishes a progressive\nretrieval pipeline by collaborating coarse-to-fine granularity, large-to-small\nquantity, and low-to-high capacity, which can relieve the burden on one\nretriever and also promote the ceiling of retrieval performance. Extensive\nexperiments manifest that FunnelRAG achieves comparable retrieval performance\nwhile the time overhead is reduced by nearly 40 percent.\n","authors":["Xinping Zhao","Yan Zhong","Zetian Sun","Xinshuo Hu","Zhenyu Liu","Dongfang Li","Baotian Hu","Min Zhang"],"pdf_url":"https://arxiv.org/pdf/2410.10293v3.pdf","comment":"18 pages, 6 figures, 13 tables. Accepted by NAACL 2025"},{"id":"http://arxiv.org/abs/2502.08346v3","updated":"2025-02-17T02:47:18Z","published":"2025-02-12T12:13:51Z","title":"Graph Foundation Models for Recommendation: A Comprehensive Survey","summary":"  Recommender systems (RS) serve as a fundamental tool for navigating the vast\nexpanse of online information, with deep learning advancements playing an\nincreasingly important role in improving ranking accuracy. Among these, graph\nneural networks (GNNs) excel at extracting higher-order structural information,\nwhile large language models (LLMs) are designed to process and comprehend\nnatural language, making both approaches highly effective and widely adopted.\nRecent research has focused on graph foundation models (GFMs), which integrate\nthe strengths of GNNs and LLMs to model complex RS problems more efficiently by\nleveraging the graph-based structure of user-item relationships alongside\ntextual understanding. In this survey, we provide a comprehensive overview of\nGFM-based RS technologies by introducing a clear taxonomy of current\napproaches, diving into methodological details, and highlighting key challenges\nand future directions. By synthesizing recent advancements, we aim to offer\nvaluable insights into the evolving landscape of GFM-based recommender systems.\n","authors":["Bin Wu","Yihang Wang","Yuanhao Zeng","Jiawei Liu","Jiashu Zhao","Cheng Yang","Yawen Li","Long Xia","Dawei Yin","Chuan Shi"],"pdf_url":"https://arxiv.org/pdf/2502.08346v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.11371v1","updated":"2025-02-17T02:36:30Z","published":"2025-02-17T02:36:30Z","title":"RAG vs. GraphRAG: A Systematic Evaluation and Key Insights","summary":"  Retrieval-Augmented Generation (RAG) enhances the performance of LLMs across\nvarious tasks by retrieving relevant information from external sources,\nparticularly on text-based data. For structured data, such as knowledge graphs,\nGraphRAG has been widely used to retrieve relevant information. However, recent\nstudies have revealed that structuring implicit knowledge from text into graphs\ncan benefit certain tasks, extending the application of GraphRAG from graph\ndata to general text-based data. Despite their successful extensions, most\napplications of GraphRAG for text data have been designed for specific tasks\nand datasets, lacking a systematic evaluation and comparison between RAG and\nGraphRAG on widely used text-based benchmarks. In this paper, we systematically\nevaluate RAG and GraphRAG on well-established benchmark tasks, such as Question\nAnswering and Query-based Summarization. Our results highlight the distinct\nstrengths of RAG and GraphRAG across different tasks and evaluation\nperspectives. Inspired by these observations, we investigate strategies to\nintegrate their strengths to improve downstream tasks. Additionally, we provide\nan in-depth discussion of the shortcomings of current GraphRAG approaches and\noutline directions for future research.\n","authors":["Haoyu Han","Harry Shomer","Yu Wang","Yongjia Lei","Kai Guo","Zhigang Hua","Bo Long","Hui Liu","Jiliang Tang"],"pdf_url":"https://arxiv.org/pdf/2502.11371v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.15005v5","updated":"2025-02-17T01:53:43Z","published":"2024-11-22T15:29:05Z","title":"Multi-granularity Interest Retrieval and Refinement Network for\n  Long-Term User Behavior Modeling in CTR Prediction","summary":"  Click-through Rate (CTR) prediction is crucial for online personalization\nplatforms. Recent advancements have shown that modeling rich user behaviors can\nsignificantly improve the performance of CTR prediction. Current long-term user\nbehavior modeling algorithms predominantly follow two cascading stages. The\nfirst stage retrieves subsequence related to the target item from the long-term\nbehavior sequence, while the second stage models the relationship between the\nsubsequence and the target item. Despite significant progress, these methods\nhave two critical flaws. First, the retrieval query typically includes only\ntarget item information, limiting the ability to capture the user's diverse\ninterests. Second, relational information, such as sequential and interactive\ninformation within the subsequence, is frequently overlooked. Therefore, it\nrequires to be further mined to more accurately model user interests.\n  To this end, we propose Multi-granularity Interest Retrieval and Refinement\nNetwork (MIRRN). Specifically, we first construct queries based on behaviors\nobserved at different time scales to obtain subsequences, each capturing users'\ninterest at various granularities. We then introduce an noval multi-head\nFourier transformer to efficiently learn sequential and interactive information\nwithin the subsequences, leading to more accurate modeling of user interests.\nFinally, we employ multi-head target attention to adaptively assess the impact\nof these multi-granularity interests on the target item. Extensive experiments\nhave demonstrated that MIRRN significantly outperforms state-of-the-art\nbaselines. Furthermore, an A/B test shows that MIRRN increases the average\nnumber of listening songs by 1.32% and the average time of listening songs by\n0.55% on the Huawei Music App. The implementation code is publicly available at\nhttps://github.com/USTC-StarTeam/MIRRN.\n","authors":["Xiang Xu","Hao Wang","Wei Guo","Luankang Zhang","Wanshan Yang","Runlong Yu","Yong Liu","Defu Lian","Enhong Chen"],"pdf_url":"https://arxiv.org/pdf/2411.15005v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.08334v2","updated":"2025-02-17T01:49:01Z","published":"2024-11-13T04:32:58Z","title":"MIRe: Enhancing Multimodal Queries Representation via Fusion-Free\n  Modality Interaction for Multimodal Retrieval","summary":"  Recent multimodal retrieval methods have endowed text-based retrievers with\nmultimodal capabilities by utilizing pre-training strategies for visual-text\nalignment. They often directly fuse the two modalities for cross-reference\nduring the alignment to understand multimodal queries. However, existing\nmethods often overlook crucial visual information due to a text-dominant issue,\nwhich overly depends on text-driven signals. In this paper, we introduce MIRe,\na retrieval framework that achieves modality interaction without fusing textual\nfeatures during the alignment. Our method allows the textual query to attend to\nvisual embeddings while not feeding text-driven signals back into the visual\nrepresentations. Additionally, we construct a pre-training dataset for\nmultimodal query retrieval by transforming concise question-answer pairs into\nextended passages. Our experiments demonstrate that our pre-training strategy\nsignificantly enhances the understanding of multimodal queries, resulting in\nstrong performance across four multimodal retrieval benchmarks under zero-shot\nsettings. Our code is publicly available: https://github.com/yeongjoonJu/MIRe.\n","authors":["Yeong-Joon Ju","Ho-Joong Kim","Seong-Whan Lee"],"pdf_url":"https://arxiv.org/pdf/2411.08334v2.pdf","comment":"preprint"},{"id":"http://arxiv.org/abs/2312.15490v4","updated":"2025-02-17T01:36:07Z","published":"2023-12-24T14:23:15Z","title":"Diffusion-EXR: Controllable Review Generation for Explainable\n  Recommendation via Diffusion Models","summary":"  Denoising Diffusion Probabilistic Model (DDPM) has shown great competence in\nimage and audio generation tasks. However, there exist few attempts to employ\nDDPM in the text generation, especially review generation under recommendation\nsystems. Fueled by the predicted reviews explainability that justifies\nrecommendations could assist users better understand the recommended items and\nincrease the transparency of recommendation system, we propose a Diffusion\nModel-based Review Generation towards EXplainable Recommendation named\nDiffusion-EXR. Diffusion-EXR corrupts the sequence of review embeddings by\nincrementally introducing varied levels of Gaussian noise to the sequence of\nword embeddings and learns to reconstruct the original word representations in\nthe reverse process. The nature of DDPM enables our lightweight Transformer\nbackbone to perform excellently in the recommendation review generation task.\nExtensive experimental results have demonstrated that Diffusion-EXR can achieve\nstate-of-the-art review generation for recommendation on two publicly available\nbenchmark datasets.\n","authors":["Ling Li","Shaohua Li","Winda Marantika","Alex C. Kot","Huijing Zhan"],"pdf_url":"https://arxiv.org/pdf/2312.15490v4.pdf","comment":"We request to withdraw our paper from the archive due to significant\n  errors identified in the analysis and conclusions. Upon further review, we\n  realized that these errors undermine the validity of our findings. We plan to\n  conduct additional research to correct these issues and resubmit a revised\n  version in the future"},{"id":"http://arxiv.org/abs/2502.11335v1","updated":"2025-02-17T01:13:45Z","published":"2025-02-17T01:13:45Z","title":"Personalized Ranking on Cascading Behavior Graphs for Accurate\n  Multi-Behavior Recommendation","summary":"  Multi-behavior recommendation predicts items a user may purchase by analyzing\ndiverse behaviors like viewing, adding to a cart, and purchasing. Existing\nmethods fall into two categories: representation learning and graph ranking.\nRepresentation learning generates user and item embeddings to capture latent\ninteraction patterns, leveraging multi-behavior properties for better\ngeneralization. However, these methods often suffer from over-smoothing and\nbias toward frequent interactions, limiting their expressiveness. Graph ranking\nmethods, on the other hand, directly compute personalized ranking scores,\ncapturing user preferences more effectively. Despite their potential, graph\nranking approaches have been primarily explored in single-behavior settings and\nremain underutilized for multi-behavior recommendation. In this paper, we\npropose CascadingRank, a novel graph ranking method for multi-behavior\nrecommendation. It models the natural sequence of user behaviors (e.g.,\nviewing, adding to cart, and purchasing) through a cascading behavior graph. An\niterative algorithm computes ranking scores, ensuring smoothness, query\nfitting, and cascading alignment. Experiments on three real-world datasets\ndemonstrate that CascadingRank outperforms state-of-the-art methods, with up to\n9.56% and 7.16% improvements in HR@10 and NDCG@10, respectively. Furthermore,\nwe provide theoretical analysis highlighting its effectiveness, convergence,\nand scalability, showcasing the advantages of graph ranking in multi-behavior\nrecommendation.\n","authors":["Geonwoo Ko","Minseo Jeon","Jinhong Jung"],"pdf_url":"https://arxiv.org/pdf/2502.11335v1.pdf","comment":"26 pages"}],"Multimedia":[{"id":"http://arxiv.org/abs/2411.03823v2","updated":"2025-02-17T18:29:13Z","published":"2024-11-06T10:44:15Z","title":"Both Text and Images Leaked! A Systematic Analysis of Multimodal LLM\n  Data Contamination","summary":"  The rapid progression of multimodal large language models (MLLMs) has\ndemonstrated superior performance on various multimodal benchmarks. However,\nthe issue of data contamination during training creates challenges in\nperformance evaluation and comparison. While numerous methods exist for\ndetecting models' contamination in large language models (LLMs), they are less\neffective for MLLMs due to their various modalities and multiple training\nphases. In this study, we introduce a multimodal data contamination detection\nframework, MM-Detect, designed for MLLMs. Our experimental results indicate\nthat MM-Detect is quite effective and sensitive in identifying varying degrees\nof contamination, and can highlight significant performance improvements due to\nthe leakage of multimodal benchmark training sets. Furthermore, we explore\nwhether the contamination originates from the base LLMs used by MLLMs or the\nmultimodal training phase, providing new insights into the stages at which\ncontamination may be introduced.\n","authors":["Dingjie Song","Sicheng Lai","Shunian Chen","Lichao Sun","Benyou Wang"],"pdf_url":"https://arxiv.org/pdf/2411.03823v2.pdf","comment":"Code Available: https://github.com/MLLM-Data-Contamination/MM-Detect"},{"id":"http://arxiv.org/abs/2502.12096v1","updated":"2025-02-17T18:14:18Z","published":"2025-02-17T18:14:18Z","title":"Token Communications: A Unified Framework for Cross-modal Context-aware\n  Semantic Communications","summary":"  In this paper, we introduce token communications (TokCom), a unified\nframework to leverage cross-modal context information in generative semantic\ncommunications (GenSC). TokCom is a new paradigm, motivated by the recent\nsuccess of generative foundation models and multimodal large language models\n(GFM/MLLMs), where the communication units are tokens, enabling efficient\ntransformer-based token processing at the transmitter and receiver. In this\npaper, we introduce the potential opportunities and challenges of leveraging\ncontext in GenSC, explore how to integrate GFM/MLLMs-based token processing\ninto semantic communication systems to leverage cross-modal context\neffectively, present the key principles for efficient TokCom at various layers\nin future wireless networks. We demonstrate the corresponding TokCom benefits\nin a GenSC setup for image, leveraging cross-modal context information, which\nincreases the bandwidth efficiency by 70.8% with negligible loss of\nsemantic/perceptual quality. Finally, the potential research directions are\nidentified to facilitate adoption of TokCom in future wireless networks.\n","authors":["Li Qiao","Mahdi Boloursaz Mashhadi","Zhen Gao","Rahim Tafazolli","Mehdi Bennis","Dusit Niyato"],"pdf_url":"https://arxiv.org/pdf/2502.12096v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.19651v2","updated":"2025-02-17T15:29:40Z","published":"2024-07-29T02:32:44Z","title":"Bridging Compressed Image Latents and Multimodal Large Language Models","summary":"  This paper presents the first-ever study of adapting compressed image latents\nto suit the needs of downstream vision tasks that adopt Multimodal Large\nLanguage Models (MLLMs). MLLMs have extended the success of large language\nmodels to modalities (e.g. images) beyond text, but their billion scale hinders\ndeployment on resource-constrained end devices. While cloud-hosted MLLMs could\nbe available, transmitting raw, uncompressed images captured by end devices to\nthe cloud requires an efficient image compression system. To address this, we\nfocus on emerging neural image compression and propose a novel framework with a\nlightweight transform-neck and a surrogate loss to adapt compressed image\nlatents for MLLM-based vision tasks. Given the huge scale of MLLMs, our\nframework excludes the entire downstream MLLM except part of its visual encoder\nfrom training our system. This stands out from most existing coding for machine\napproaches that involve downstream networks in training and thus could be\nimpractical when the networks are MLLMs. The proposed framework is general in\nthat it is applicable to various MLLMs, neural image codecs, and multiple\napplication scenarios, where the neural image codec can be (1) pre-trained for\nhuman perception without updating, (2) fully updated for joint human and\nmachine perception, or (3) fully updated for only machine perception. Extensive\nexperiments on different neural image codecs and various MLLMs show that our\nmethod achieves great rate-accuracy performance with much less complexity.\n","authors":["Chia-Hao Kao","Cheng Chien","Yu-Jen Tseng","Yi-Hsin Chen","Alessandro Gnutti","Shao-Yuan Lo","Wen-Hsiao Peng","Riccardo Leonardi"],"pdf_url":"https://arxiv.org/pdf/2407.19651v2.pdf","comment":"Accepted by ICLR 2025"},{"id":"http://arxiv.org/abs/2406.10469v2","updated":"2025-02-17T12:12:25Z","published":"2024-06-15T02:19:31Z","title":"Object-Attribute-Relation Representation Based Video Semantic\n  Communication","summary":"  With the rapid growth of multimedia data volume, there is an increasing need\nfor efficient video transmission in applications such as virtual reality and\nfuture video streaming services. Semantic communication is emerging as a vital\ntechnique for ensuring efficient and reliable transmission in low-bandwidth,\nhigh-noise settings. However, most current approaches focus on joint\nsource-channel coding (JSCC) that depends on end-to-end training. These methods\noften lack an interpretable semantic representation and struggle with\nadaptability to various downstream tasks. In this paper, we introduce the use\nof object-attribute-relation (OAR) as a semantic framework for videos to\nfacilitate low bit-rate coding and enhance the JSCC process for more effective\nvideo transmission. We utilize OAR sequences for both low bit-rate\nrepresentation and generative video reconstruction. Additionally, we\nincorporate OAR into the image JSCC model to prioritize communication resources\nfor areas more critical to downstream tasks. Our experiments on traffic\nsurveillance video datasets assess the effectiveness of our approach in terms\nof video transmission performance. The empirical findings demonstrate that our\nOAR-based video coding method not only outperforms H.265 coding at lower\nbit-rates but also synergizes with JSCC to deliver robust and efficient video\ntransmission.\n","authors":["Qiyuan Du","Yiping Duan","Qianqian Yang","Xiaoming Tao","Mérouane Debbah"],"pdf_url":"https://arxiv.org/pdf/2406.10469v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.08334v2","updated":"2025-02-17T01:49:01Z","published":"2024-11-13T04:32:58Z","title":"MIRe: Enhancing Multimodal Queries Representation via Fusion-Free\n  Modality Interaction for Multimodal Retrieval","summary":"  Recent multimodal retrieval methods have endowed text-based retrievers with\nmultimodal capabilities by utilizing pre-training strategies for visual-text\nalignment. They often directly fuse the two modalities for cross-reference\nduring the alignment to understand multimodal queries. However, existing\nmethods often overlook crucial visual information due to a text-dominant issue,\nwhich overly depends on text-driven signals. In this paper, we introduce MIRe,\na retrieval framework that achieves modality interaction without fusing textual\nfeatures during the alignment. Our method allows the textual query to attend to\nvisual embeddings while not feeding text-driven signals back into the visual\nrepresentations. Additionally, we construct a pre-training dataset for\nmultimodal query retrieval by transforming concise question-answer pairs into\nextended passages. Our experiments demonstrate that our pre-training strategy\nsignificantly enhances the understanding of multimodal queries, resulting in\nstrong performance across four multimodal retrieval benchmarks under zero-shot\nsettings. Our code is publicly available: https://github.com/yeongjoonJu/MIRe.\n","authors":["Yeong-Joon Ju","Ho-Joong Kim","Seong-Whan Lee"],"pdf_url":"https://arxiv.org/pdf/2411.08334v2.pdf","comment":"preprint"}]}}