<!DOCTYPE html>
<html lang="en">

<head>
    <title>MyArxiv</title>
    <meta charset="utf-8"/>
    <meta http-equiv="X-UA-Compatible" content="IE=edge"/>
    <meta name="robots" content="noindex, nofollow"/>
    <meta name="viewport" content="width=device-width, initial-scale=1"/>
    <link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
    <link href="index.css" rel="stylesheet"/>
    <link href="https://cdn.jsdelivr.net/npm/remixicon@2.5.0/fonts/remixicon.css" rel="stylesheet">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css"
          integrity="sha384-R4558gYOUz8mP9YWpZJjofhk+zx0AS11p36HnD2ZKj/6JR5z27gSSULCNHIRReVs" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css"
          integrity="sha384-R4558gYOUz8mP9YWpZJjofhk+zx0AS11p36HnD2ZKj/6JR5z27gSSULCNHIRReVs" crossorigin="anonymous">
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.js"
            integrity="sha384-z1fJDqw8ZApjGO3/unPWUPsIymfsJmyrDVWC8Tv/a1HeOtGmkwNd/7xUS0Xcnvsx"
            crossorigin="anonymous"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/contrib/auto-render.min.js"
            integrity="sha384-+XBljXPPiv+OzfbB3cVmLHf4hdUFHlWNZN5spNQ7rmHTXpd7WvJum6fIACpNNfIR"
            crossorigin="anonymous"></script>
    <script>
        document.addEventListener("DOMContentLoaded", function () {
            renderMathInElement(document.body, {
                // customised options
                // • auto-render specific keys, e.g.:
                delimiters: [
                    {left: '$$', right: '$$', display: true},
                    {left: '$', right: '$', display: false},
                    {left: '\\(', right: '\\)', display: false},
                    {left: '\\[', right: '\\]', display: true},
                    {left: "\\begin{equation}", right: "\\end{equation}", display: true},
                    {left: "\\begin{align}", right: "\\end{align}", display: true},
                    {left: "\\begin{alignat}", right: "\\end{alignat}", display: true},
                    {left: "\\begin{gather}", right: "\\end{gather}", display: true},
                    {left: "\\begin{CD}", right: "\\end{CD}", display: true},
                ],
                // • rendering keys, e.g.:
                throwOnError: false
            });
        });
    </script>
</head>

<body>
<section class="header-container">
    <div style="display:flex; justify-content:space-between; align-items:flex-end;">
        <div>
            <div class="header-title">
                MyArxiv
            </div>
        </div>

        <div class=icons>
            <label class="theme-switch" for="checkbox">
                <input type="checkbox" id="checkbox"/>
                <i id="theme-icon" class="ri-moon-line" style="font-size: 32px" rel="noopener noreferrer"></i>
            </label>
        </div>
    </div>
</section>

    <section class="day-container">
        <div class="date">
            <time datetime="2025-02-24T00:00:00Z">2025-02-24</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Computation and Language <span class="chip" style="font-size: 60%">7</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Conditional [MASK] Discrete Diffusion Language Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.06438v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.06438v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hyukhun Koh, Minha Jhang, Dohyung Kim, Sangmook Lee, Kyomin Jung
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Although auto-regressive models excel in natural language processing, they
often struggle to generate diverse text and provide limited controllability.
Non-auto-regressive methods could be an alternative but often produce
degenerate outputs and exhibit shortcomings in conditional generation. To
address these challenges, we propose Diffusion-EAGS, a novel framework that
integrates conditional masked language models into diffusion language models
through the theoretical lens of a conditional Markov Random Field. In doing so,
we propose entropy-adaptive Gibbs sampling and entropy-based noise scheduling
to counterbalance each model's shortcomings. Experimental results show that
Diffusion-EAGS outperforms baselines and achieves the best quality-diversity
tradeoff, demonstrating its effectiveness in non-autoregressive text
generation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Enhancing RWKV-based Language Models for Long-Sequence Text Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.15485v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.15485v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinghan Pan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces an enhanced RWKV architecture with adaptive temporal
gating mechanisms for improved long-context language modeling. We propose two
principal innovations: (1) a position-aware convolutional shift operator that
captures local syntactic patterns while preserving global coherence, and (2) a
neurally-gated information routing mechanism that dynamically regulates
inter-token information flow. Through comprehensive experiments on text
generation tasks, our enhanced model demonstrates superior performance compared
to the baseline RWKV, achieving 96.5 relative improvement in ROUGE-L scores
with only 2.95 increased inference latency. Ablation studies validate the
individual contributions of each component, while linguistic analysis reveals
the model's adaptive attention to syntactic boundaries and entity coherence.
The proposed modifications maintain RWKV's linear computational complexity
while significantly enhancing its contextual modeling capabilities,
establishing new state-of-the-art performance for recurrent-style architectures
in long-form text generation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 2 tables, 3 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ HiFi-KPI: A <span class="highlight-title">Dataset</span> for Hierarchical KPI Extraction from Earnings
  Filings 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.15411v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.15411v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rasmus Aavang, Giovanni Rizzi, Rasmus Bøggild, Alexandre Iolov, Mike Zhang, Johannes Bjerva
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The U.S. Securities and Exchange Commission (SEC) requires that public
companies file financial reports tagging numbers with the machine readable
inline eXtensible Business Reporting Language (iXBRL) standard. However, the
highly complex and highly granular taxonomy defined by iXBRL limits label
transferability across domains. In this paper, we introduce the Hierarchical
Financial Key Performance Indicator (HiFi-KPI) dataset, designed to facilitate
numerical KPI extraction at specified levels of granularity from unstructured
financial text. Our approach organizes a 218,126-label hierarchy using a
taxonomy based grouping method, investigating which taxonomy layer provides the
most meaningful structure. HiFi-KPI comprises ~1.8M paragraphs and ~5M
entities, each linked to a label in the iXBRL-specific calculation and
presentation taxonomies. We provide baselines using encoder-based approaches
and structured extraction using Large Language Models (LLMs). To simplify LLM
inference and evaluation, we additionally release HiFi-KPI Lite, a manually
curated subset with four expert-mapped labels. We publicly release all
artifacts.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Round Attention: A Novel Round-Level Attention Mechanism to Accelerate
  LLM Inference 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.15294v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.15294v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yaohua Tang, Zhicheng Hu, Kun Cheng, Fan Mo, Qiheng Lv, Hua Wang, Zhi Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The increasing context window size in large language models (LLMs) has
improved their ability to handle complex, long-text tasks. However, as the
conversation rounds continue, it is required to store a large amount of KV
cache in GPU memory, which significantly affects the efficiency and even
availability of the model serving systems. This paper analyzes dialogue data
from real users and discovers that the LLM inference manifests a watershed
layer, after which the distribution of round-level attention shows notable
similarity. We propose Round Attention, a novel round-level attention mechanism
that only recalls and computes the KV cache of the most relevant rounds. The
experiments show that our method saves 55\% memory usage without compromising
model performance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ESPnet-SpeechLM: An Open Speech Language Model Toolkit 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.15218v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.15218v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jinchuan Tian, Jiatong Shi, William Chen, Siddhant Arora, Yoshiki Masuyama, Takashi Maekaku, Yihan Wu, Junyi Peng, Shikhar Bharadwaj, Yiwen Zhao, Samuele Cornell, Yifan Peng, Xiang Yue, Chao-Han Huck Yang, Graham Neubig, Shinji Watanabe
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present ESPnet-SpeechLM, an open toolkit designed to democratize the
development of speech language models (SpeechLMs) and voice-driven agentic
applications. The toolkit standardizes speech processing tasks by framing them
as universal sequential modeling problems, encompassing a cohesive workflow of
data preprocessing, pre-training, inference, and task evaluation. With
ESPnet-SpeechLM, users can easily define task templates and configure key
settings, enabling seamless and streamlined SpeechLM development. The toolkit
ensures flexibility, efficiency, and scalability by offering highly
configurable modules for every stage of the workflow. To illustrate its
capabilities, we provide multiple use cases demonstrating how competitive
SpeechLMs can be constructed with ESPnet-SpeechLM, including a 1.7B-parameter
model pre-trained on both text and speech tasks, across diverse benchmarks. The
toolkit and its recipes are fully transparent and reproducible at:
https://github.com/espnet/espnet/tree/speechlm.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PairBench: A Systematic Framework for Selecting Reliable Judge VLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.15210v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.15210v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aarash Feizi, Sai Rajeswar, Adriana Romero-Soriano, Reihaneh Rabbany, Spandana Gella, Valentina Zantedeschi, João Monteiro
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As large vision language models (VLMs) are increasingly used as automated
evaluators, understanding their ability to effectively compare data pairs as
instructed in the prompt becomes essential. To address this, we present
PairBench, a low-cost framework that systematically evaluates VLMs as
customizable similarity tools across various modalities and scenarios. Through
PairBench, we introduce four metrics that represent key desiderata of
similarity scores: alignment with human annotations, consistency for data pairs
irrespective of their order, smoothness of similarity distributions, and
controllability through prompting. Our analysis demonstrates that no model,
whether closed- or open-source, is superior on all metrics; the optimal choice
depends on an auto evaluator's desired behavior (e.g., a smooth vs. a sharp
judge), highlighting risks of widespread adoption of VLMs as evaluators without
thorough assessment. For instance, the majority of VLMs struggle with
maintaining symmetric similarity scores regardless of order. Additionally, our
results show that the performance of VLMs on the metrics in PairBench closely
correlates with popular benchmarks, showcasing its predictive power in ranking
models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Aligned at the Start: Conceptual Groupings in LLM Embeddings 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.05315v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.05315v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mehrdad Khatir, Sanchit Kabra, Chandan K. Reddy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper shifts focus to the often-overlooked input embeddings - the
initial representations fed into transformer blocks. Using fuzzy graph,
k-nearest neighbor (k-NN), and community detection, we analyze embeddings from
diverse LLMs, finding significant categorical community structure aligned with
predefined concepts and categories aligned with humans. We observe these
groupings exhibit within-cluster organization (such as hierarchies, topological
ordering, etc.), hypothesizing a fundamental structure that precedes contextual
processing. To further investigate the conceptual nature of these groupings, we
explore cross-model alignments across different LLM categories within their
input embeddings, observing a medium to high degree of alignment. Furthermore,
provide evidence that manipulating these groupings can play a functional role
in mitigating ethnicity bias in LLM tasks.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Computer Vision and Pattern Recognition <span class="chip" style="font-size: 60%">4</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Para-Lane: Multi-Lane <span class="highlight-title">Dataset</span> Registering Parallel Scans for
  Benchmarking Novel View Synthesis <span class="chip">3DV</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.15635v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.15635v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziqian Ni, Sicong Du, Zhenghua Hou, Chenming Wu, Sheng Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  To evaluate end-to-end autonomous driving systems, a simulation environment
based on Novel View Synthesis (NVS) techniques is essential, which synthesizes
photo-realistic images and point clouds from previously recorded sequences
under new vehicle poses, particularly in cross-lane scenarios. Therefore, the
development of a multi-lane dataset and benchmark is necessary. While recent
synthetic scene-based NVS datasets have been prepared for cross-lane
benchmarking, they still lack the realism of captured images and point clouds.
To further assess the performance of existing methods based on NeRF and 3DGS,
we present the first multi-lane dataset registering parallel scans specifically
for novel driving view synthesis dataset derived from real-world scans,
comprising 25 groups of associated sequences, including 16,000 front-view
images, 64,000 surround-view images, and 16,000 LiDAR frames. All frames are
labeled to differentiate moving objects from static elements. Using this
dataset, we evaluate the performance of existing approaches in various testing
scenarios at different lanes and distances. Additionally, our method provides
the solution for solving and assessing the quality of multi-sensor poses for
multi-modal data alignment for curating such a dataset in real-world. We plan
to continually add new sequences to test the generalization of existing methods
across different scenarios. The dataset is released publicly at the project
page: https://nizqleo.github.io/paralane-dataset/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by International Conference on 3D Vision (3DV) 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PFSD: A Multi-Modal Pedestrian-Focus Scene <span class="highlight-title">Dataset</span> for Rich Tasks in
  Semi-Structured Environments 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.15342v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.15342v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yueting Liu, Hanshi Wang, Yunfei Lei, Zhengjun Zha, Weiming Hu, Jin Gao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in autonomous driving perception have revealed
exceptional capabilities within structured environments dominated by vehicular
traffic. However, current perception models exhibit significant limitations in
semi-structured environments, where dynamic pedestrians with more diverse
irregular movement and occlusion prevail. We attribute this shortcoming to the
scarcity of high-quality datasets in semi-structured scenes, particularly
concerning pedestrian perception and prediction. In this work, we present the
multi-modal Pedestrian-Focused Scene Dataset(PFSD), rigorously annotated in
semi-structured scenes with the format of nuScenes. PFSD provides comprehensive
multi-modal data annotations with point cloud segmentation, detection, and
object IDs for tracking. It encompasses over 130,000 pedestrian instances
captured across various scenarios with varying densities, movement patterns,
and occlusions. Furthermore, to demonstrate the importance of addressing the
challenges posed by more diverse and complex semi-structured environments, we
propose a novel Hybrid Multi-Scale Fusion Network (HMFN). Specifically, to
detect pedestrians in densely populated and occluded scenarios, our method
effectively captures and fuses multi-scale features using a meticulously
designed hybrid framework that integrates sparse and vanilla convolutions.
Extensive experiments on PFSD demonstrate that HMFN attains improvement in mean
Average Precision (mAP) over existing methods, thereby underscoring its
efficacy in addressing the challenges of 3D pedestrian detection in complex
semi-structured environments. Coding and benchmark are available.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ An ocean front detection and tracking algorithm 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.15250v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.15250v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yishuo Wang, Feng Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Existing ocean front detection methods--including histogram-based variance
analysis, Lyapunov exponent, gradient thresholding, and machine
learning--suffer from critical limitations: discontinuous outputs,
over-detection, reliance on single-threshold decisions, and lack of open-source
implementations. To address these challenges, this paper proposes the Bayesian
Front Detection and Tracking framework with Metric Space Analysis (BFDT-MSA).
The framework introduces three innovations: (1) a Bayesian decision mechanism
that integrates gradient priors and field operators to eliminate manual
threshold sensitivity; (2) morphological refinement algorithms for merging
fragmented fronts, deleting spurious rings, and thinning frontal zones to
pixel-level accuracy; and (3) a novel metric space definition for temporal
front tracking, enabling systematic analysis of front evolution. Validated on
global SST data (2022--2024), BFDT-MSA reduces over-detection by $73\%$
compared to histogram-based methods while achieving superior intensity
($0.16^\circ$C/km), continuity, and spatiotemporal coherence. The open-source
release bridges a critical gap in reproducible oceanographic research.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Image Translation-Based Unsupervised Cross-Modality Domain Adaptation
  for Medical Image Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.15193v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.15193v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tao Yang, Lisheng Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Supervised deep learning usually faces more challenges in medical images than
in natural images. Since annotations in medical images require the expertise of
doctors and are more time-consuming and expensive. Thus, some researchers turn
to unsupervised learning methods, which usually face inevitable performance
drops. In addition, medical images may have been acquired at different medical
centers with different scanners and under different image acquisition
protocols, so the modalities of the medical images are often inconsistent. This
modality difference (domain shift) also reduces the applicability of deep
learning methods. In this regard, we propose an unsupervised crossmodality
domain adaptation method based on image translation by transforming the source
modality image with annotation into the unannotated target modality and using
its annotation to achieve supervised learning of the target modality. In
addition, the subtle differences between translated pseudo images and real
images are overcome by self-training methods to further improve the task
performance of deep learning. The proposed method showed mean Dice Similarity
Coefficient (DSC) and Average Symmetric Surface Distance (ASSD) of $0.8351 \pm
0.1152$ and $1.6712 \pm 2.1948$ for vestibular schwannoma (VS), $0.8098 \pm
0.0233$ and $0.2317 \pm 0.1577$ for cochlea on the VS and cochlea segmentation
task of the Cross-Modality Domain Adaptation (crossMoDA 2022) challenge
validation phase leaderboard.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages, 1 figure</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Information Retrieval <span class="chip" style="font-size: 60%">2</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CKnowEdit: A New Chinese Knowledge Editing <span class="highlight-title">Dataset</span> for Linguistics,
  Facts, and Logic Error Correction in LLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.05806v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.05806v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jizhan Fang, Tianhe Lu, Yunzhi Yao, Ziyan Jiang, Xin Xu, Ningyu Zhang, Huajun Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Chinese, as a linguistic system rich in depth and complexity, is
characterized by distinctive elements such as ancient poetry, proverbs, idioms,
and other cultural constructs. However, current Large Language Models (LLMs)
face limitations in these specialized domains, highlighting the need for the
development of comprehensive datasets that can assess, continuously update, and
progressively improve these culturally-grounded linguistic competencies through
targeted training optimizations. To address this gap, we introduce CKnowEdit,
the first-ever Chinese knowledge editing dataset designed to correct
linguistic, factual, and logical errors in LLMs. We collect seven types of
knowledge from a wide range of sources, including classical texts, idioms, and
content from Baidu Tieba Ruozhiba, taking into account the unique polyphony,
antithesis, and logical structures inherent in the Chinese language. By
analyzing this dataset, we highlight the challenges current LLMs face in
mastering Chinese. Furthermore, our evaluation of state-of-the-art knowledge
editing techniques reveals opportunities to advance the correction of Chinese
knowledge. Code and dataset are available at
https://github.com/zjunlp/EasyEdit.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Ongoing work; project website is available at
  https://zjunlp.github.io/project/CKnowEdit code and dataset are available at
  https://github.com/zjunlp/EasyEdit</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ RAPTOR: Refined Approach for Product Table Object Recognition <span class="chip">WACV</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.14918v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.14918v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Eliott Thomas, Mickael Coustaty, Aurelie Joseph, Gaspar Deloin, Elodie Carel, Vincent Poulain D'Andecy, Jean-Marc Ogier
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Extracting tables from documents is a critical task across various
industries, especially on business documents like invoices and reports.
Existing systems based on DEtection TRansformer (DETR) such as TAble
TRansformer (TATR), offer solutions for Table Detection (TD) and Table
Structure Recognition (TSR) but face challenges with diverse table formats and
common errors like incorrect area detection and overlapping columns. This
research introduces RAPTOR, a modular post-processing system designed to
enhance state-of-the-art models for improved table extraction, particularly for
product tables. RAPTOR addresses recurrent TD and TSR issues, improving both
precision and structural predictions. For TD, we use DETR (trained on ICDAR
2019) and TATR (trained on PubTables-1M and FinTabNet), while TSR only relies
on TATR. A Genetic Algorithm is incorporated to optimize RAPTOR's module
parameters, using a private dataset of product tables to align with industrial
needs. We evaluate our method on two private datasets of product tables, the
public DOCILE dataset (which contains tables similar to our target product
tables), and the ICDAR 2013 and ICDAR 2019 datasets. The results demonstrate
that while our approach excels at product tables, it also maintains reasonable
performance across diverse table formats. An ablation study further validates
the contribution of each module in our system.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for WACVW 2025 (VisionDocs)</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Machine Learning <span class="chip" style="font-size: 60%">7</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ <span class="highlight-title">★</span> Superintelligent Agents Pose Catastrophic Risks: Can Scientist AI Offer
  a Safer Path? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.15657v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.15657v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        <span class="highlight-author">Yoshua Bengio</span>, Michael Cohen, Damiano Fornasiere, Joumana Ghosn, Pietro Greiner, Matt MacDermott, Sören Mindermann, Adam Oberman, Jesse Richardson, Oliver Richardson, Marc-Antoine Rondeau, Pierre-Luc St-Charles, David Williams-King
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The leading AI companies are increasingly focused on building generalist AI
agents -- systems that can autonomously plan, act, and pursue goals across
almost all tasks that humans can perform. Despite how useful these systems
might be, unchecked AI agency poses significant risks to public safety and
security, ranging from misuse by malicious actors to a potentially irreversible
loss of human control. We discuss how these risks arise from current AI
training methods. Indeed, various scenarios and experiments have demonstrated
the possibility of AI agents engaging in deception or pursuing goals that were
not specified by human operators and that conflict with human interests, such
as self-preservation. Following the precautionary principle, we see a strong
need for safer, yet still useful, alternatives to the current agency-driven
trajectory. Accordingly, we propose as a core building block for further
advances the development of a non-agentic AI system that is trustworthy and
safe by design, which we call Scientist AI. This system is designed to explain
the world from observations, as opposed to taking actions in it to imitate or
please humans. It comprises a world model that generates theories to explain
data and a question-answering inference machine. Both components operate with
an explicit notion of uncertainty to mitigate the risks of overconfident
predictions. In light of these considerations, a Scientist AI could be used to
assist human researchers in accelerating scientific progress, including in AI
safety. In particular, our system can be employed as a guardrail against AI
agents that might be created despite the risks involved. Ultimately, focusing
on non-agentic AI may enable the benefits of AI innovation while avoiding the
risks associated with the current trajectory. We hope these arguments will
motivate researchers, developers, and policymakers to favor this safer path.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>v2 with fixed formatting for URLs and hyperlinks</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Generalization of the Gibbs algorithm with high probability at low
  temperatures 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11071v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11071v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Andreas Maurer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The paper gives a bound on the generalization error of the Gibbs algorithm,
which recovers known data-independent bounds for the high temperature range and
extends to the low-temperature range, where generalization depends critically
on the data-dependent loss-landscape. It is shown, that with high probability
the generalization error of a single hypothesis drawn from the Gibbs posterior
decreases with the total prior volume of all hypotheses with similar or smaller
empirical error. This gives theoretical support to the belief in the benefit of
flat minima. The zero temperature limit is discussed and the bound is extended
to a class of similar stochastic algorithms.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ TAG: A Decentralized Framework for Multi-Agent Hierarchical
  Reinforcement Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.15425v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.15425v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Giuseppe Paolo, Abdelhakim Benechehab, Hamza Cherkaoui, Albert Thomas, Balázs Kégl
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Hierarchical organization is fundamental to biological systems and human
societies, yet artificial intelligence systems often rely on monolithic
architectures that limit adaptability and scalability. Current hierarchical
reinforcement learning (HRL) approaches typically restrict hierarchies to two
levels or require centralized training, which limits their practical
applicability. We introduce TAME Agent Framework (TAG), a framework for
constructing fully decentralized hierarchical multi-agent systems.TAG enables
hierarchies of arbitrary depth through a novel LevelEnv concept, which
abstracts each hierarchy level as the environment for the agents above it. This
approach standardizes information flow between levels while preserving loose
coupling, allowing for seamless integration of diverse agent types. We
demonstrate the effectiveness of TAG by implementing hierarchical architectures
that combine different RL agents across multiple levels, achieving improved
performance over classical multi-agent RL baselines on standard benchmarks. Our
results show that decentralized hierarchical organization enhances both
learning speed and final performance, positioning TAG as a promising direction
for scalable multi-agent systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Comparative Analysis of Black Hole Mass Estimation in Type-2 AGNs:
  Classical vs. Quantum Machine Learning and Deep Learning Approaches 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.15297v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.15297v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sathwik Narkedimilli, Venkata Sriram Amballa, N V Saran Kumar, R Arun Kumar, R Praneeth Reddy, Satvik Raghav, Manish M, Aswath Babu H
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the case of Type-2 AGNs, estimating the mass of the black hole is
challenging. Understanding how galaxies form and evolve requires considerable
insight into the mass of black holes. This work compared different classical
and quantum machine learning (QML) algorithms for black hole mass estimation,
wherein the classical algorithms are Linear Regression, XGBoost Regression,
Random Forest Regressor, Support Vector Regressor (SVR), Lasso Regression,
Ridge Regression, Elastic Net Regression, Bayesian Regression, Decision Tree
Regressor, Gradient Booster Regressor, Classical Neural Networks, Gated
Recurrent Unit (GRU), LSTM, Deep Residual Networks (ResNets) and
Transformer-Based Regression. On the other hand, quantum algorithms including
Hybrid Quantum Neural Networks (QNN), Quantum Long Short-Term Memory (Q-LSTM),
Sampler-QNN, Estimator-QNN, Variational Quantum Regressor (VQR), Quantum Linear
Regression(Q-LR), QML with JAX optimization were also tested. The results
revealed that classical algorithms gave better R^2, MAE, MSE, and RMSE results
than the quantum models. Among the classical models, LSTM has the best result
with an accuracy of 99.77%. Estimator-QNN has the highest accuracy for quantum
algorithms with an MSE of 0.0124 and an accuracy of 99.75%. This study
ascertains both the strengths and weaknesses of the classical and the quantum
approaches. As far as our knowledge goes, this work could pave the way for the
future application of quantum algorithms in astrophysical data analysis.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>29 pages, 12 Figures, 6 Tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Tensor Product Neural Networks for Functional ANOVA Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.15215v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.15215v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Seokhun Park, Insung Kong, Yongchan Choi, Chanmoo Park, Yongdai Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Interpretability for machine learning models is becoming more and more
important as machine learning models become more complex. The functional ANOVA
model, which decomposes a high-dimensional function into a sum of lower
dimensional functions so called components, is one of the most popular tools
for interpretable AI, and recently, various neural network models have been
developed for estimating each component in the functional ANOVA model. However,
such neural networks are highly unstable when estimating components since the
components themselves are not uniquely defined. That is, there are multiple
functional ANOVA decompositions for a given function. In this paper, we propose
a novel interpretable model which guarantees a unique functional ANOVA
decomposition and thus is able to estimate each component stably. We call our
proposed model ANOVA-NODE since it is a modification of Neural Oblivious
Decision Ensembles (NODE) for the functional ANOVA model. Theoretically, we
prove that ANOVA-NODE can approximate a smooth function well. Additionally, we
experimentally show that ANOVA-NODE provides much more stable estimation of
each component and thus much more stable interpretation when training data and
initial values of the model parameters vary than existing neural network models
do.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>45 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PairBench: A Systematic Framework for Selecting Reliable Judge VLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.15210v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.15210v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aarash Feizi, Sai Rajeswar, Adriana Romero-Soriano, Reihaneh Rabbany, Spandana Gella, Valentina Zantedeschi, João Monteiro
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As large vision language models (VLMs) are increasingly used as automated
evaluators, understanding their ability to effectively compare data pairs as
instructed in the prompt becomes essential. To address this, we present
PairBench, a low-cost framework that systematically evaluates VLMs as
customizable similarity tools across various modalities and scenarios. Through
PairBench, we introduce four metrics that represent key desiderata of
similarity scores: alignment with human annotations, consistency for data pairs
irrespective of their order, smoothness of similarity distributions, and
controllability through prompting. Our analysis demonstrates that no model,
whether closed- or open-source, is superior on all metrics; the optimal choice
depends on an auto evaluator's desired behavior (e.g., a smooth vs. a sharp
judge), highlighting risks of widespread adoption of VLMs as evaluators without
thorough assessment. For instance, the majority of VLMs struggle with
maintaining symmetric similarity scores regardless of order. Additionally, our
results show that the performance of VLMs on the metrics in PairBench closely
correlates with popular benchmarks, showcasing its predictive power in ranking
models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Projection Optimization: A General Framework for Multi-Objective and
  Multi-Group RLHF 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.15145v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.15145v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nuoya Xiong, Aarti Singh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reinforcement Learning with Human Feedback (RLHF) is a widely used
fine-tuning approach that aligns machine learning model, particularly Language
Model (LM) with human preferences. There are typically multiple objectives
driving the preference, hence humans find it easier to express per-objective
comparisons rather than a global preference between two choices.
Multi-Objective RLHF (MORLHF) aims to use per-objective preference feedback and
achieve Pareto optimality among these objectives by aggregating them into a
single unified objective for optimization. However, nearly all prior works rely
on linear aggregation, which rules out policies that favor specific objectives
such as the worst one. The only existing approach using non-linear aggregation
is computationally expensive due to its reward-based nature and the need for
retraining whenever the aggregation parameters change. In this work, we address
this limitation by transforming the non-linear aggregation maximization problem
into a series of sub-problems. Each sub-problem involves only linear
aggregation, making it computationally efficient to solve. We further extend
our framework to handle multi-group scenarios, where each group has distinct
weights for the objectives. Our method enables achieving consensus or
maximizing the aggregated objective across all groups. Theoretically, we
demonstrate that our algorithmic framework achieves sublinear regret and can be
easily adapted to a reward-free algorithm. Empirically, leveraging our
theoretical insights, we propose a nearly training-free algorithm once the
optimal policies for individual objectives are obtained.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2025-02-23T00:00:00Z">2025-02-23</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Information Retrieval <span class="chip" style="font-size: 60%">1</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Agentic Information Retrieval 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.09713v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.09713v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Weinan Zhang, Junwei Liao, Ning Li, Kounianhua Du, Jianghao Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Since the 1970s, information retrieval (IR) has long been defined as the
process of acquiring relevant information items from a pre-defined corpus to
satisfy user information needs. Traditional IR systems, while effective in
domains like web search, are constrained by their reliance on static,
pre-defined information items. To this end, this paper introduces agentic
information retrieval (Agentic IR), a transformative next-generation paradigm
for IR driven by large language models (LLMs) and AI agents. The central shift
in agentic IR is the evolving definition of ``information'' from static,
pre-defined information items to dynamic, context-dependent information states.
Information state refers to a particular information context that the user is
right in within a dynamic environment, encompassing not only the acquired
information items but also real-time user preferences, contextual factors, and
decision-making processes. In such a way, traditional information retrieval,
focused on acquiring relevant information items based on user queries, can be
naturally extended to achieving the target information state given the user
instruction, which thereby defines the agentic information retrieval. We
systematically discuss agentic IR from various aspects, i.e., task formulation,
architecture, evaluation, case studies, as well as challenges and future
prospects. We believe that the concept of agentic IR introduced in this paper
not only broadens the scope of information retrieval research but also lays the
foundation for a more adaptive, interactive, and intelligent next-generation IR
paradigm.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, perspective paper</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2025-02-22T00:00:00Z">2025-02-22</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Information Retrieval <span class="chip" style="font-size: 60%">4</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Evaluating Sakana's AI Scientist for Autonomous Research: Wishful
  Thinking or an Emerging Reality Towards 'Artificial Research Intelligence'
  (ARI)? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.14297v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.14297v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Joeran Beel, Min-Yen Kan, Moritz Baumgart
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A major step toward Artificial General Intelligence (AGI) and Super
Intelligence is AI's ability to autonomously conduct research - what we term
Artificial Research Intelligence (ARI). If machines could generate hypotheses,
conduct experiments, and write research papers without human intervention, it
would transform science. Sakana recently introduced the 'AI Scientist',
claiming to conduct research autonomously, i.e. they imply to have achieved
what we term Artificial Research Intelligence (ARI). The AI Scientist gained
much attention, but a thorough independent evaluation has yet to be conducted.
  Our evaluation of the AI Scientist reveals critical shortcomings. The
system's literature reviews produced poor novelty assessments, often
misclassifying established concepts (e.g., micro-batching for stochastic
gradient descent) as novel. It also struggles with experiment execution: 42% of
experiments failed due to coding errors, while others produced flawed or
misleading results. Code modifications were minimal, averaging 8% more
characters per iteration, suggesting limited adaptability. Generated
manuscripts were poorly substantiated, with a median of five citations, most
outdated (only five of 34 from 2020 or later). Structural errors were frequent,
including missing figures, repeated sections, and placeholder text like
'Conclusions Here'. Some papers contained hallucinated numerical results.
  Despite these flaws, the AI Scientist represents a leap forward in research
automation. It generates full research manuscripts with minimal human input,
challenging expectations of AI-driven science. Many reviewers might struggle to
distinguish its work from human researchers. While its quality resembles a
rushed undergraduate paper, its speed and cost efficiency are unprecedented,
producing a full paper for USD 6 to 15 with 3.5 hours of human involvement, far
outpacing traditional researchers.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Towards Context-Robust LLMs: A Gated Representation Fine-tuning Approach 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.14100v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.14100v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shenglai Zeng, Pengfei He, Kai Guo, Tianqi Zheng, Hanqing Lu, Yue Xing, Hui Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) enhanced with external contexts, such as through
retrieval-augmented generation (RAG), often face challenges in handling
imperfect evidence. They tend to over-rely on external knowledge, making them
vulnerable to misleading and unhelpful contexts. To address this, we propose
the concept of context-robust LLMs, which can effectively balance internal
knowledge with external context, similar to human cognitive processes.
Specifically, context-robust LLMs should rely on external context only when
lacking internal knowledge, identify contradictions between internal and
external knowledge, and disregard unhelpful contexts. To achieve this goal, we
introduce Grft, a lightweight and plug-and-play gated representation
fine-tuning approach. Grft consists of two key components: a gating mechanism
to detect and filter problematic inputs, and low-rank representation adapters
to adjust hidden representations. By training a lightweight intervention
function with only 0.0004\% of model size on fewer than 200 examples, Grft can
effectively adapt LLMs towards context-robust behaviors.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Generating with Fairness: A Modality-Diffused Counterfactual Framework
  for Incomplete Multimodal Recommendations <span class="chip">WWW 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.11916v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.11916v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jin Li, Shoujin Wang, Qi Zhang, Shui Yu, Fang Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Incomplete scenario is a prevalent, practical, yet challenging setting in
Multimodal Recommendations (MMRec), where some item modalities are missing due
to various factors. Recently, a few efforts have sought to improve the
recommendation accuracy by exploring generic structures from incomplete data.
However, two significant gaps persist: 1) the difficulty in accurately
generating missing data due to the limited ability to capture modality
distributions; and 2) the critical but overlooked visibility bias, where items
with missing modalities are more likely to be disregarded due to the
prioritization of items' multimodal data over user preference alignment. This
bias raises serious concerns about the fair treatment of items. To bridge these
two gaps, we propose a novel Modality-Diffused Counterfactual (MoDiCF)
framework for incomplete multimodal recommendations. MoDiCF features two key
modules: a novel modality-diffused data completion module and a new
counterfactual multimodal recommendation module. The former, equipped with a
particularly designed multimodal generative framework, accurately generates and
iteratively refines missing data from learned modality-specific distribution
spaces. The latter, grounded in the causal perspective, effectively mitigates
the negative causal effects of visibility bias and thus assures fairness in
recommendations. Both modules work collaboratively to address the two
aforementioned significant gaps for generating more accurate and fair results.
Extensive experiments on three real-world datasets demonstrate the superior
performance of MoDiCF in terms of both recommendation accuracy and fairness.
The code and processed datasets are released at
https://github.com/JinLi-i/MoDiCF.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by WWW 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Leave No One Behind: Enhancing Diversity While Maintaining Accuracy in
  Social Recommendation <span class="chip">DASFAA2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11374v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11374v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lei Li, Xiao Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Social recommendation, which incorporates social connections into recommender
systems, has proven effective in improving recommendation accuracy. However,
beyond accuracy, diversity is also crucial for enhancing user engagement.
Despite its importance, the impact of social recommendation models on diversity
remains largely unexplored. In this study, we systematically examine the dual
performance of existing social recommendation algorithms in terms of both
accuracy and diversity. Our empirical analysis reveals a concerning trend:
while social recommendation models enhance accuracy, they often reduce
diversity. To address this issue, we propose Diversified Social Recommendation
(DivSR), a novel approach that employs relational knowledge distillation to
transfer high-diversity structured knowledge from non-social recommendation
models to social recommendation models. DivSR is a lightweight, model-agnostic
framework that seamlessly integrates with existing social recommendation
architectures. Experiments on three benchmark datasets demonstrate that DivSR
significantly enhances diversity while maintaining competitive accuracy,
achieving a superior accuracy-diversity trade-off. Our code and data are
publicly available at: https://github.com/ll0ruc/DivSR.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by DASFAA2025</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Multimedia <span class="chip" style="font-size: 60%">1</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Llasa: Scaling Train-Time and Inference-Time Compute for Llama-based
  Speech Synthesis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.04128v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.04128v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhen Ye, Xinfa Zhu, Chi-Min Chan, Xinsheng Wang, Xu Tan, Jiahe Lei, Yi Peng, Haohe Liu, Yizhu Jin, Zheqi Dai, Hongzhan Lin, Jianyi Chen, Xingjian Du, Liumeng Xue, Yunlin Chen, Zhifei Li, Lei Xie, Qiuqiang Kong, Yike Guo, Wei Xue
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances in text-based large language models (LLMs), particularly in
the GPT series and the o1 model, have demonstrated the effectiveness of scaling
both training-time and inference-time compute. However, current
state-of-the-art TTS systems leveraging LLMs are often multi-stage, requiring
separate models (e.g., diffusion models after LLM), complicating the decision
of whether to scale a particular model during training or testing. This work
makes the following contributions: First, we explore the scaling of train-time
and inference-time compute for speech synthesis. Second, we propose a simple
framework Llasa for speech synthesis that employs a single-layer vector
quantizer (VQ) codec and a single Transformer architecture to fully align with
standard LLMs such as Llama. Our experiments reveal that scaling train-time
compute for Llasa consistently improves the naturalness of synthesized speech
and enables the generation of more complex and accurate prosody patterns.
Furthermore, from the perspective of scaling inference-time compute, we employ
speech understanding models as verifiers during the search, finding that
scaling inference-time compute shifts the sampling modes toward the preferences
of specific verifiers, thereby improving emotional expressiveness, timbre
consistency, and content accuracy. In addition, we released the checkpoint and
training code for our TTS model (1B, 3B, 8B) and codec model publicly
available.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2025-02-21T00:00:00Z">2025-02-21</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Computation and Language <span class="chip" style="font-size: 60%">133</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Privacy Ripple Effects from Adding or Removing Personal Information in
  Language Model Training 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.15680v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.15680v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jaydeep Borkar, Matthew Jagielski, Katherine Lee, Niloofar Mireshghallah, David A. Smith, Christopher A. Choquette-Choo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Due to the sensitive nature of personally identifiable information (PII), its
owners may have the authority to control its inclusion or request its removal
from large-language model (LLM) training. Beyond this, PII may be added or
removed from training datasets due to evolving dataset curation techniques,
because they were newly scraped for retraining, or because they were included
in a new downstream fine-tuning stage. We find that the amount and ease of PII
memorization is a dynamic property of a model that evolves throughout training
pipelines and depends on commonly altered design choices. We characterize three
such novel phenomena: (1) similar-appearing PII seen later in training can
elicit memorization of earlier-seen sequences in what we call assisted
memorization, and this is a significant factor (in our settings, up to 1/3);
(2) adding PII can increase memorization of other PII significantly (in our
settings, as much as $\approx\!7.5\times$); and (3) removing PII can lead to
other PII being memorized. Model creators should consider these first- and
second-order privacy risks when training models to avoid the risk of new PII
regurgitation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>23 pages, 26 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FLEKE: Federated Locate-then-Edit Knowledge Editing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.15677v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.15677v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zongkai Zhao, Guozeng Xu, Xiuhua Li, Kaiwen Wei, Jiang Zhong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Locate-then-Edit Knowledge Editing (LEKE) is a key technique for updating
large language models (LLMs) without full retraining. However, existing methods
assume a single-user setting and become inefficient in real-world multi-client
scenarios, where decentralized organizations (e.g., hospitals, financial
institutions) independently update overlapping knowledge, leading to redundant
mediator knowledge vector (MKV) computations and privacy concerns. To address
these challenges, we introduce Federated Locate-then-Edit Knowledge Editing
(FLEKE), a novel task that enables multiple clients to collaboratively perform
LEKE while preserving privacy and reducing computational overhead. To achieve
this, we propose FedEdit, a two-stage framework that optimizes MKV selection
and reuse. In the first stage, clients locally apply LEKE and upload the
computed MKVs. In the second stage, rather than relying solely on server-based
MKV sharing, FLEKE allows clients retrieve relevant MKVs based on cosine
similarity, enabling knowledge re-edit and minimizing redundant computations.
Experimental results on two benchmark datasets demonstrate that FedEdit retains
over 96% of the performance of non-federated LEKE while significantly
outperforming a FedAvg-based baseline by approximately twofold. Besides, we
find that MEMIT performs more consistently than PMET in the FLEKE task with our
FedEdit framework. Our code is available at https://github.com/zongkaiz/FLEKE.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AutoToM: Automated Bayesian Inverse Planning and Model Discovery for
  Open-ended Theory of Mind 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.15676v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.15676v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhining Zhang, Chuanyang Jin, Mung Yao Jia, Tianmin Shu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Theory of Mind (ToM), the ability to understand people's mental variables
based on their behavior, is key to developing socially intelligent agents.
Current approaches to Theory of Mind reasoning either rely on prompting Large
Language Models (LLMs), which are prone to systematic errors, or use rigid,
handcrafted Bayesian Theory of Mind (BToM) models, which are more robust but
cannot generalize across different domains. In this work, we introduce AutoToM,
an automated Bayesian Theory of Mind method for achieving open-ended machine
Theory of Mind. AutoToM can operate in any domain, infer any mental variable,
and conduct robust Theory of Mind reasoning of any order. Given a Theory of
Mind inference problem, AutoToM first proposes an initial BToM model. It then
conducts automated Bayesian inverse planning based on the proposed model,
leveraging an LLM as the backend. Based on the uncertainty of the inference, it
iteratively refines the model, by introducing additional mental variables
and/or incorporating more timesteps in the context. Empirical evaluations
across multiple Theory of Mind benchmarks demonstrate that AutoToM consistently
achieves state-of-the-art performance, offering a scalable, robust, and
interpretable approach to machine Theory of Mind.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>23 pages, 6 figures, 11 tables. Website at
  https://chuanyangjin.com/AutoToM/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Almost AI, Almost Human: The Challenge of Detecting AI-Polished Writing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.15666v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.15666v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shoumik Saha, Soheil Feizi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The growing use of large language models (LLMs) for text generation has led
to widespread concerns about AI-generated content detection. However, an
overlooked challenge is AI-polished text, where human-written content undergoes
subtle refinements using AI tools. This raises a critical question: should
minimally polished text be classified as AI-generated? Misclassification can
lead to false plagiarism accusations and misleading claims about AI prevalence
in online content. In this study, we systematically evaluate eleven
state-of-the-art AI-text detectors using our AI-Polished-Text Evaluation
(APT-Eval) dataset, which contains $11.7K$ samples refined at varying
AI-involvement levels. Our findings reveal that detectors frequently
misclassify even minimally polished text as AI-generated, struggle to
differentiate between degrees of AI involvement, and exhibit biases against
older and smaller models. These limitations highlight the urgent need for more
nuanced detection methodologies.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages, 17 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Machine-generated text detection prevents language model collapse 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.15654v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.15654v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        George Drayson, Vasileios Lampos
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As Large Language Models (LLMs) become increasingly prevalent, their
generated outputs are proliferating across the web, risking a future where
machine-generated content dilutes human-authored text. Since web data is the
primary resource for LLM pretraining, future models will be trained on an
unknown portion of synthetic data. This will lead to model collapse, a
degenerative process which causes models to reinforce their own errors and
experience a drop in model performance. In this study, we investigate the
impact of decoding strategy on model collapse, where we analyse the
characteristics of the generated data during recursive training, its similarity
to human references and the resulting model performance. Using the decoding
strategies that lead to the most significant model degradation, we tackle the
question: how to avoid model collapse when the origin (human or synthetic) of
the training data is unknown. We design a novel methodology based on resampling
the data distribution using importance weights from our machine-generated text
detector. Our method is validated on two LLM variants (GPT-2 and SmolLM2) on
the open-ended text generation task, demonstrating that we can successfully
prevent model collapse and when there is enough human-authored data in the
training dataset, our method improves model performance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Empowering LLMs with Logical Reasoning: A Comprehensive <span class="highlight-title">Survey</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.15652v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.15652v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fengxiang Cheng, Haoxuan Li, Fenrong Liu, Robert van Rooij, Kun Zhang, Zhouchen Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have achieved remarkable successes on various
natural language tasks. However, recent studies have found that there are still
significant challenges to the logical reasoning abilities of LLMs. This paper
summarizes and categorizes the main challenges into two aspects: (1) Logical
question answering, LLMs often fail to generate the correct answer within
complex logical problem which requires sophisticated deductive, inductive or
abductive reasoning given a collection of premises and constrains. (2) Logical
consistency, LLMs are prone to producing responses contradicting themselves
across different questions. For example, a state-of-the-art Macaw
question-answering LLM answers Yes to both questions Is a magpie a bird? and
Does a bird have wings? but answers No to Does a magpie have wings?. To
facilitate this research direction, we comprehensively investigate the most
cutting-edge methods and propose detailed taxonomies of these methods.
Specifically, to accurately answer complex logic questions, previous methods
can be categorized based on reliance on external solvers, prompts, pretraining,
and fine-tuning. To avoid logical contradictions, we discuss concepts and
solutions of various logical consistencies, including implication, negation,
transitivity, factuality consistency, and their composites. In addition, we
review commonly used benchmark datasets and evaluation metrics, and discuss
promising research directions, such as extensions to modal logic to account for
uncertainty, and efficient algorithms satisfying multiple logical consistencies
simultaneously.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Steering into New Embedding Spaces: Analyzing Cross-Lingual Alignment
  Induced by Model Interventions in Multilingual Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.15639v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.15639v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anirudh Sundar, Sinead Williamson, Katherine Metcalf, Barry-John Theobald, Skyler Seto, Masha Fedzechkina
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Aligned representations across languages is a desired property in
multilingual large language models (mLLMs), as alignment can improve
performance in cross-lingual tasks. Typically alignment requires fine-tuning a
model, which is computationally expensive, and sizable language data, which
often may not be available. A data-efficient alternative to fine-tuning is
model interventions -- a method for manipulating model activations to steer
generation into the desired direction. We analyze the effect of a popular
intervention (finding experts) on the alignment of cross-lingual
representations in mLLMs. We identify the neurons to manipulate for a given
language and introspect the embedding space of mLLMs pre- and
post-manipulation. We show that modifying the mLLM's activations changes its
embedding space such that cross-lingual alignment is enhanced. Further, we show
that the changes to the embedding space translate into improved downstream
performance on retrieval tasks, with up to 2x improvements in top-1 accuracy on
cross-lingual retrieval.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>34 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Extraction multi-étiquettes de relations en utilisant des couches de
  <span class="highlight-title">Transformer</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.15619v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.15619v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ngoc Luyen Le, Gildas Tagny Ngompé
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this article, we present the BTransformer18 model, a deep learning
architecture designed for multi-label relation extraction in French texts. Our
approach combines the contextual representation capabilities of pre-trained
language models from the BERT family - such as BERT, RoBERTa, and their French
counterparts CamemBERT and FlauBERT - with the power of Transformer encoders to
capture long-term dependencies between tokens. Experiments conducted on the
dataset from the TextMine'25 challenge show that our model achieves superior
performance, particularly when using CamemBERT-Large, with a macro F1 score of
0.654, surpassing the results obtained with FlauBERT-Large. These results
demonstrate the effectiveness of our approach for the automatic extraction of
complex relations in intelligence reports.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>in French language</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Probe Pruning: Accelerating LLMs through Dynamic Pruning via
  Model-Probing <span class="chip">ICLR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.15618v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.15618v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qi Le, Enmao Diao, Ziyan Wang, Xinran Wang, Jie Ding, Li Yang, Ali Anwar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce Probe Pruning (PP), a novel framework for online, dynamic,
structured pruning of Large Language Models (LLMs) applied in a batch-wise
manner. PP leverages the insight that not all samples and tokens contribute
equally to the model's output, and probing a small portion of each batch
effectively identifies crucial weights, enabling tailored dynamic pruning for
different batches. It comprises three main stages: probing, history-informed
pruning, and full inference. In the probing stage, PP selects a small yet
crucial set of hidden states, based on residual importance, to run a few model
layers ahead. During the history-informed pruning stage, PP strategically
integrates the probing states with historical states. Subsequently, it
structurally prunes weights based on the integrated states and the PP
importance score, a metric developed specifically to assess the importance of
each weight channel in maintaining performance. In the final stage, full
inference is conducted on the remaining weights. A major advantage of PP is its
compatibility with existing models, as it operates without requiring additional
neural network modules or fine-tuning. Comprehensive evaluations of PP on
LLaMA-2/3 and OPT models reveal that even minimal probing-using just 1.5% of
FLOPs-can substantially enhance the efficiency of structured pruning of LLMs.
For instance, when evaluated on LLaMA-2-7B with WikiText2, PP achieves a 2.56
times lower ratio of performance degradation per unit of runtime reduction
compared to the state-of-the-art method at a 40% pruning ratio. Our code is
available at https://github.com/Qi-Le1/Probe_Pruning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Pastiche Novel Generation Creating: Fan Fiction You Love in Your
  Favorite Author's Style 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.15616v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.15616v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xueran Han, Yuhan Liu, Mingzhe Li, Wei Liu, Sen Hu, Rui Yan, Zhiqiang Xu, Xiuying Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Great novels create immersive worlds with rich character arcs,
well-structured plots, and nuanced writing styles. However, current novel
generation methods often rely on brief, simplistic story outlines and generate
details using plain, generic language. To bridge this gap, we introduce the
task of Pastiche Novel Generation, which requires the generated novels to
imitate the distinctive features of the original work, including understanding
character profiles, predicting plausible plot developments, and writing
concrete details using vivid, expressive language. To achieve this, we propose
WriterAgent, a novel generation system designed to master the core aspects of
literary pastiche. WriterAgent is trained through a curriculum learning
paradigm, progressing from low-level stylistic mastery to high-level narrative
coherence. Its key tasks include language style learning, character modeling,
plot planning, and stylish writing, ensuring comprehensive narrative control.
To support this, WriterAgent leverages the WriterLoRA framework, an extension
of LoRA with hierarchical and cumulative task-specific modules, each
specializing in a different narrative aspect. We evaluate WriterAgent on
multilingual classics like Harry Potter and Dream of the Red Chamber,
demonstrating its superiority over baselines in capturing the target author's
settings, character dynamics, and writing style to produce coherent, faithful
narratives.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LaTIM: Measuring Latent Token-to-Token Interactions in Mamba Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.15612v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.15612v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hugo Pitorro, Marcos Treviso
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  State space models (SSMs), such as Mamba, have emerged as an efficient
alternative to transformers for long-context sequence modeling. However,
despite their growing adoption, SSMs lack the interpretability tools that have
been crucial for understanding and improving attention-based architectures.
While recent efforts provide insights into Mamba's internal mechanisms, they do
not explicitly decompose token-wise contributions, leaving gaps in
understanding how Mamba selectively processes sequences across layers. In this
work, we introduce LaTIM, a novel token-level decomposition method for both
Mamba-1 and Mamba-2 that enables fine-grained interpretability. We extensively
evaluate our method across diverse tasks, including machine translation,
copying, and retrieval-based generation, demonstrating its effectiveness in
revealing Mamba's token-to-token interaction patterns.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 10 figures in the main paper</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ On the Robustness of <span class="highlight-title">Transformer</span>s against Context Hijacking for Linear
  Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.15609v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.15609v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tianle Li, Chenyang Zhang, Xingwu Chen, Yuan Cao, Difan Zou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Transformer-based Large Language Models (LLMs) have demonstrated powerful
in-context learning capabilities. However, their predictions can be disrupted
by factually correct context, a phenomenon known as context hijacking,
revealing a significant robustness issue. To understand this phenomenon
theoretically, we explore an in-context linear classification problem based on
recent advances in linear transformers. In our setup, context tokens are
designed as factually correct query-answer pairs, where the queries are similar
to the final query but have opposite labels. Then, we develop a general
theoretical analysis on the robustness of the linear transformers, which is
formulated as a function of the model depth, training context lengths, and
number of hijacking context tokens. A key finding is that a well-trained deeper
transformer can achieve higher robustness, which aligns with empirical
observations. We show that this improvement arises because deeper layers enable
more fine-grained optimization steps, effectively mitigating interference from
context hijacking. This is also well supported by our numerical experiments.
Our findings provide theoretical insights into the benefits of deeper
architectures and contribute to enhancing the understanding of transformer
architectures.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Do Multilingual LLMs Think In English? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.15603v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.15603v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lisa Schut, Yarin Gal, Sebastian Farquhar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have multilingual capabilities and can solve
tasks across various languages. However, we show that current LLMs make key
decisions in a representation space closest to English, regardless of their
input and output languages. Exploring the internal representations with a logit
lens for sentences in French, German, Dutch, and Mandarin, we show that the LLM
first emits representations close to English for semantically-loaded words
before translating them into the target language. We further show that
activation steering in these LLMs is more effective when the steering vectors
are computed in English rather than in the language of the inputs and outputs.
This suggests that multilingual LLMs perform key reasoning steps in a
representation that is heavily shaped by English in a way that is not
transparent to system users.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Main paper 9 pages; including appendix 48 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Robust Bias Detection in MLMs and its Application to Human Trait Ratings <span class="chip">NAACL 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.15600v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.15600v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ingroj Shrestha, Louis Tay, Padmini Srinivasan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  There has been significant prior work using templates to study bias against
demographic attributes in MLMs. However, these have limitations: they overlook
random variability of templates and target concepts analyzed, assume equality
amongst templates, and overlook bias quantification. Addressing these, we
propose a systematic statistical approach to assess bias in MLMs, using mixed
models to account for random effects, pseudo-perplexity weights for sentences
derived from templates and quantify bias using statistical effect sizes.
Replicating prior studies, we match on bias scores in magnitude and direction
with small to medium effect sizes. Next, we explore the novel problem of gender
bias in the context of $\textit{personality}$ and $\textit{character}$ traits,
across seven MLMs (base and large). We find that MLMs vary; ALBERT is unbiased
for binary gender but the most biased for non-binary $\textit{neo}$, while
RoBERTa-large is the most biased for binary gender but shows small to no bias
for $\textit{neo}$. There is some alignment of MLM bias and findings in
psychology (human perspective) - in $\textit{agreeableness}$ with RoBERTa-large
and $\textit{emotional stability}$ with BERT-large. There is general agreement
for the remaining 3 personality dimensions: both sides observe at most small
differences across gender. For character traits, human studies on gender bias
are limited thus comparisons are not feasible.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To appear at Findings of NAACL 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SafeInt: Shielding Large Language Models from Jailbreak Attacks via
  Safety-Aware Representation Intervention 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.15594v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.15594v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiaqi Wu, Chen Chen, Chunyan Hou, Xiaojie Yuan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the widespread real-world deployment of large language models (LLMs),
ensuring their behavior complies with safety standards has become crucial.
Jailbreak attacks exploit vulnerabilities in LLMs to induce undesirable
behavior, posing a significant threat to LLM safety. Previous defenses often
fail to achieve both effectiveness and efficiency simultaneously. Defenses from
a representation perspective offer new insights, but existing interventions
cannot dynamically adjust representations based on the harmfulness of the
queries. To address this limitation while ensuring both effectiveness and
efficiency, we propose SafeIntervention (SafeInt), a novel defense method that
shields LLMs from jailbreak attacks through safety-aware representation
intervention. SafeInt is built on our analysis of the representations of
jailbreak samples. It adjusts representation distributions of jailbreak samples
through intervention to align them with the representations of unsafe samples
while minimizing unnecessary perturbations to jailbreak-irrelevant
representations. We conduct comprehensive experiments covering six jailbreak
attacks, two jailbreak datasets, and two utility benchmarks. Experimental
results demonstrate that SafeInt outperforms all baselines in defending LLMs
against jailbreak attacks while largely maintaining utility. Additionally, we
evaluate SafeInt against adaptive attacks and verify its effectiveness in
mitigating real-time attacks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Generalizing From Short to Long: Effective Data Synthesis for
  Long-Context Instruction Tuning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.15592v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.15592v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenhao Zhu, Pinzhen Chen, Hanxu Hu, Shujian Huang, Fei Yuan, Jiajun Chen, Alexandra Birch
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Long-context modelling for large language models (LLMs) has been a key area
of recent research because many real world use cases require reasoning over
longer inputs such as documents. The focus of research into modelling long
context has been on how to model position and there has been little
investigation into other important aspects of language modelling such as
instruction tuning. Long context training examples are challenging and
expensive to create and use. In this paper, we investigate how to design
instruction data for the post-training phase of a long context pre-trained
model: how much and what type of context is needed for optimal and efficient
post-training. Our controlled study reveals that models instruction-tuned on
short contexts can effectively generalize to longer ones, while also
identifying other critical factors such as instruction difficulty and context
composition. Based on these findings, we propose context synthesis, a novel
data synthesis framework that leverages off-the-shelf LLMs to generate extended
background contexts for high-quality instruction-answer pairs. Experiment
results on the document-level benchmark (LongBench) demonstrate that our
proposed approach outperforms previous instruction synthesis approaches and
comes close to the performance of human-annotated long-context instruction
data. The project will be available at:
https://github.com/NJUNLP/context-synthesis.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LightThinker: Thinking Step-by-Step Compression 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.15589v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.15589v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jintian Zhang, Yuqi Zhu, Mengshu Sun, Yujie Luo, Shuofei Qiao, Lun Du, Da Zheng, Huajun Chen, Ningyu Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have shown remarkable performance in complex
reasoning tasks, but their efficiency is hindered by the substantial memory and
computational costs associated with generating lengthy tokens. In this paper,
we propose LightThinker, a novel method that enables LLMs to dynamically
compress intermediate thoughts during reasoning. Inspired by human cognitive
processes, LightThinker compresses verbose thought steps into compact
representations and discards the original reasoning chains, thereby
significantly reducing the number of tokens stored in the context window. This
is achieved by training the model on when and how to perform compression
through data construction, mapping hidden states to condensed gist tokens, and
creating specialized attention masks. Additionally, we introduce the Dependency
(Dep) metric to quantify the degree of compression by measuring the reliance on
historical tokens during generation. Extensive experiments on four datasets and
two models show that LightThinker reduces peak memory usage and inference time,
while maintaining competitive accuracy. Our work provides a new direction for
improving the efficiency of LLMs in complex reasoning tasks without sacrificing
performance. Code will be released at https://github.com/zjunlp/LightThinker.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Chats-Grid: An Iterative Retrieval Q&A Optimization Scheme Leveraging
  Large Model and Retrieval Enhancement Generation in smart grid 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.15583v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.15583v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yunfeng Li, Jiqun Zhang, Guofu Liao, Xue Shi, Junhong Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With rapid advancements in artificial intelligence, question-answering (Q&A)
systems have become essential in intelligent search engines, virtual
assistants, and customer service platforms. However, in dynamic domains like
smart grids, conventional retrieval-augmented generation(RAG) Q&A systems face
challenges such as inadequate retrieval quality, irrelevant responses, and
inefficiencies in handling large-scale, real-time data streams. This paper
proposes an optimized iterative retrieval-based Q&A framework called Chats-Grid
tailored for smart grid environments. In the pre-retrieval phase, Chats-Grid
advanced query expansion ensures comprehensive coverage of diverse data
sources, including sensor readings, meter records, and control system
parameters. During retrieval, Best Matching 25(BM25) sparse retrieval and BAAI
General Embedding(BGE) dense retrieval in Chats-Grid are combined to process
vast, heterogeneous datasets effectively. Post-retrieval, a fine-tuned large
language model uses prompt engineering to assess relevance, filter irrelevant
results, and reorder documents based on contextual accuracy. The model further
generates precise, context-aware answers, adhering to quality criteria and
employing a self-checking mechanism for enhanced reliability. Experimental
results demonstrate Chats-Grid's superiority over state-of-the-art methods in
fidelity, contextual recall, relevance, and accuracy by 2.37%, 2.19%, and 3.58%
respectively. This framework advances smart grid management by improving
decision-making and user interactions, fostering resilient and adaptive smart
grid infrastructures.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 10 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Interpreting and Steering LLMs with Mutual Information-based
  Explanations on Sparse Autoencoders 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.15576v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.15576v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xuansheng Wu, Jiayi Yuan, Wenlin Yao, Xiaoming Zhai, Ninghao Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) excel at handling human queries, but they can
occasionally generate flawed or unexpected responses. Understanding their
internal states is crucial for understanding their successes, diagnosing their
failures, and refining their capabilities. Although sparse autoencoders (SAEs)
have shown promise for interpreting LLM internal representations, limited
research has explored how to better explain SAE features, i.e., understanding
the semantic meaning of features learned by SAE. Our theoretical analysis
reveals that existing explanation methods suffer from the frequency bias issue,
where they emphasize linguistic patterns over semantic concepts, while the
latter is more critical to steer LLM behaviors. To address this, we propose
using a fixed vocabulary set for feature interpretations and designing a mutual
information-based objective, aiming to better capture the semantic meaning
behind these features. We further propose two runtime steering strategies that
adjust the learned feature activations based on their corresponding
explanations. Empirical results show that, compared to baselines, our method
provides more discourse-level explanations and effectively steers LLM behaviors
to defend against jailbreak attacks. These findings highlight the value of
explanations for steering LLM behaviors in downstream applications. We will
release our code and data once accepted.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Pre-print. 20 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DReSD: Dense Retrieval for Speculative Decoding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.15572v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.15572v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Milan Gritta, Huiyin Xue, Gerasimos Lampouras
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Speculative decoding (SD) accelerates Large Language Model (LLM) generation
by using an efficient draft model to propose the next few tokens, which are
verified by the LLM in a single forward call, reducing latency while preserving
its outputs. We focus on retrieval-based SD where the draft model retrieves the
next tokens from a non-parametric datastore. Sparse retrieval (REST), which
operates on the surface form of strings, is currently the dominant paradigm due
to its simplicity and scalability. However, its effectiveness is limited due to
the usage of short contexts and exact string matching. Instead, we introduce
Dense Retrieval for Speculative Decoding (DReSD), a novel framework that uses
approximate nearest neighbour search with contextualised token embeddings to
retrieve the most semantically relevant token sequences for SD. Extensive
experiments show that DReSD achieves (on average) 87% higher acceptance rates,
65% longer accepted tokens and 19% faster generation speeds compared to sparse
retrieval (REST).
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under Review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Bridging vision language model (VLM) evaluation gaps with a framework
  for scalable and cost-effective benchmark generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.15563v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.15563v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tim Rädsch, Leon Mayer, Simon Pavicic, A. Emre Kavur, Marcel Knopp, Barış Öztürk, Klaus Maier-Hein, Paul F. Jaeger, Fabian Isensee, Annika Reinke, Lena Maier-Hein
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reliable evaluation of AI models is critical for scientific progress and
practical application. While existing VLM benchmarks provide general insights
into model capabilities, their heterogeneous designs and limited focus on a few
imaging domains pose significant challenges for both cross-domain performance
comparison and targeted domain-specific evaluation. To address this, we propose
three key contributions: (1) a framework for the resource-efficient creation of
domain-specific VLM benchmarks enabled by task augmentation for creating
multiple diverse tasks from a single existing task, (2) the release of new VLM
benchmarks for seven domains, created according to the same homogeneous
protocol and including 162,946 thoroughly human-validated answers, and (3) an
extensive benchmarking of 22 state-of-the-art VLMs on a total of 37,171 tasks,
revealing performance variances across domains and tasks, thereby supporting
the need for tailored VLM benchmarks. Adoption of our methodology will pave the
way for the resource-efficient domain-specific selection of models and guide
future research efforts toward addressing core open questions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PIP-KAG: Mitigating Knowledge Conflicts in Knowledge-Augmented
  Generation via Parametric Pruning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.15543v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.15543v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pengcheng Huang, Zhenghao Liu, Yukun Yan, Xiaoyuan Yi, Hao Chen, Zhiyuan Liu, Maosong Sun, Tong Xiao, Ge Yu, Chenyan Xiong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Knowledge-Augmented Generation (KAG) has shown great promise in updating the
internal memory of Large Language Models (LLMs) by integrating external
knowledge. However, KAG inevitably faces knowledge conflicts when the internal
memory contradicts external information. Current approaches to mitigating these
conflicts mainly focus on improving external knowledge utilization. However,
these methods have shown only limited effectiveness in mitigating the knowledge
conflict problem, as internal knowledge continues to influence the generation
process of LLMs. In this paper, we propose a ParametrIc Pruning-based
Knowledge-Augmented Generation (PIP-KAG) approach, which prunes internal
knowledge of LLMs and incorporates a plug-and-play adaptation module to help
LLMs better leverage external sources. Additionally, we construct the
CoConflictQA benchmark based on the hallucination of LLMs to better evaluate
contextual faithfulness during answering questions. Experimental results on
CoConflictQA demonstrate that PIP-KAG significantly reduces knowledge conflicts
and improves context fidelity. Notably, PIP-KAG reduces LLM's parameters by
13%, enhancing parameter efficiency in LLMs within the KAG framework. All codes
are available at https://github.com/OpenBMB/PIP-KAG.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>20 pages, 7 figures, 7 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SOTOPIA-Ω: Dynamic Strategy Injection Learning and Social
  Instrucion Following Evaluation for Social Agents 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.15538v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.15538v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenyuan Zhang, Tianyun Liu, Mengxiao Song, Xiaodong Li, Tingwen Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite the abundance of prior social strategies possessed by humans, there
remains a paucity of research dedicated to their transfer and integration into
social agents. Our proposed SOTOPIA-{\Omega} framework aims to address and
bridge this gap, with a particular focus on enhancing the social capabilities
of language agents. This framework dynamically injects multi-step reasoning
strategies inspired by negotiation theory, along with two simple direct
strategies, into expert agents, thereby automating the construction of
high-quality social dialogue training corpus. Additionally, we introduce the
concept of Social Instruction Following (S-IF) and propose two new S-IF
evaluation metrics that are complementary to social capability. We demonstrate
that several 7B models trained on high-quality corpus not only significantly
surpass the expert agent (GPT-4) in achieving social goals but also enhance
S-IF performance. Analysis and variant experiments validate the advantages of
dynamic construction, which can especially break the agent's prolonged
deadlock.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>26 pages, 5 figures, 23 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Activation Steering in Neural Theorem Provers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.15507v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.15507v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shashank Kirtania
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have shown promise in proving formal theorems
using proof assistants like Lean. However, current state of the art language
models struggles to predict next step in proofs leading practitioners to use
different sampling techniques to improve LLMs capabilities. We observe that the
LLM is capable of predicting the correct tactic; however, it faces challenges
in ranking it appropriately within the set of candidate tactics, affecting the
overall selection process. To overcome this hurdle, we use activation steering
to guide LLMs responses to improve the generations at the time of inference.
Our results suggest that activation steering offers a promising lightweight
alternative to specialized fine-tuning for enhancing theorem proving
capabilities in LLMs, particularly valuable in resource-constrained
environments.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Scale-Distribution Decoupling: Enabling Stable and Effective Training of
  Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.15499v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.15499v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ya Wang, Zhijian Zhuo, Yutao Zeng, Xun Zhou, Jian Yang, Xiaoqing Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Training stability is a persistent challenge in the pre-training of large
language models (LLMs), particularly for architectures such as Post-Norm
Transformers, which are prone to gradient explosion and dissipation. In this
paper, we propose Scale-Distribution Decoupling (SDD), a novel approach that
stabilizes training by explicitly decoupling the scale and distribution of the
weight matrix in fully-connected layers. SDD applies a normalization mechanism
to regulate activations and a learnable scaling vector to maintain
well-conditioned gradients, effectively preventing $\textbf{gradient explosion
and dissipation}$. This separation improves optimization efficiency,
particularly in deep networks, by ensuring stable gradient propagation.
Experimental results demonstrate that our method stabilizes training across
various LLM architectures and outperforms existing techniques in different
normalization configurations. Furthermore, the proposed method is lightweight
and compatible with existing frameworks, making it a practical solution for
stabilizing LLM training. Code is available at https://github.com/kaihemo/SDD.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ExpliCa: Evaluating Explicit Causal Reasoning in Large Language Models <span class="chip">ACL 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.15487v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.15487v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Martina Miliani, Serenna Auriemma, Alessandro Bondielli, Emmanuele Chersoni, Lucia Passaro, Irene Sucameli, Alessandro Lenci
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) are increasingly used in tasks requiring
interpretive and inferential accuracy. In this paper, we introduce ExpliCa, a
new dataset for evaluating LLMs in explicit causal reasoning. ExpliCa uniquely
integrates both causal and temporal relations presented in different linguistic
orders and explicitly expressed by linguistic connectives. The dataset is
enriched with crowdsourced human acceptability ratings. We tested LLMs on
ExpliCa through prompting and perplexity-based metrics. We assessed seven
commercial and open-source LLMs, revealing that even top models struggle to
reach 0.80 accuracy. Interestingly, models tend to confound temporal relations
with causal ones, and their performance is also strongly influenced by the
linguistic order of the events. Finally, perplexity-based scores and prompting
performance are differently affected by model size.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to ACL 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A fast convergence algorithm based on binary integer programming for
  expert load balancing in MoE LLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.15451v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.15451v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuan Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  MoE (Mixture-of-Expert) architectures appear frequently in large language
models, and the number of experts can be over one hundred recently. However,
the expert load imbalance problem always happens in MoE model pre-training,
which will cause routing collapse or increased computational overhead. In order
to balance loads on experts, we propose BIP-Based Balancing, an expert load
balancing algorithm based on binary integer programming (BIP). The algorithm
maintains an additional vector q that can help change the top-K order of s by
solving a binary integer programming with very small time costs. In simulation
experiments, we observe that BIP-Based Balancing make imbalance disappoint very
fast, while the final sum of routine scores decreases very little. Our
algorithm achieves nearly perfect trade-off between expert load balance and
pre-training efficiency under the simulation view.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ When Compression Meets Model Compression: Memory-Efficient Double
  Compression for Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.15443v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.15443v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Weilan Wang, Yu Mao, Dongdong Tang, Hongchao Du, Nan Guan, Chun Jason Xue
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) exhibit excellent performance in various tasks.
However, the memory requirements of LLMs present a great challenge when
deploying on memory-limited devices, even for quantized LLMs. This paper
introduces a framework to compress LLM after quantization further, achieving
about 2.2x compression ratio. A compression-aware quantization is first
proposed to enhance model weight compressibility by re-scaling the model
parameters before quantization, followed by a pruning method to improve
further. Upon this, we notice that decompression can be a bottleneck during
practical scenarios. We then give a detailed analysis of the trade-off between
memory usage and latency brought by the proposed method. A speed-adaptive
method is proposed to overcome it. The experimental results show inference with
the compressed model can achieve a 40% reduction in memory size with negligible
loss in accuracy and inference speed.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Fed-SB: A Silver Bullet for Extreme Communication Efficiency and
  Performance in (Private) Federated LoRA Fine-Tuning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.15436v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.15436v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Raghav Singhal, Kaustubh Ponkshe, Rohit Vartak, Lav R. Varshney, Praneeth Vepakomma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Low-Rank Adaptation (LoRA) has become ubiquitous for efficiently fine-tuning
foundation models. However, federated fine-tuning using LoRA is challenging due
to suboptimal updates arising from traditional federated averaging of
individual adapters. Existing solutions either incur prohibitively high
communication cost that scales linearly with the number of clients or suffer
from performance degradation due to limited expressivity. We introduce
Federated Silver Bullet (Fed-SB), a novel approach for federated fine-tuning of
LLMs using LoRA-SB, a recently proposed low-rank adaptation method. LoRA-SB
optimally aligns the optimization trajectory with the ideal low-rank full
fine-tuning projection by learning a small square matrix (R) between adapters B
and A, keeping other components fixed. Direct averaging of R guarantees exact
updates, substantially reducing communication cost, which remains independent
of the number of clients, and enables scalability. Fed-SB achieves
state-of-the-art performance across commonsense reasoning, arithmetic
reasoning, and language inference tasks while reducing communication costs by
up to 230x. In private settings, Fed-SB further improves performance by (1)
reducing trainable parameters, thereby lowering the noise required for
differential privacy and (2) avoiding noise amplification introduced by other
methods. Overall, Fed-SB establishes a new Pareto frontier in the tradeoff
between communication and performance, offering an efficient and scalable
solution for both private and non-private federated fine-tuning. Our code is
publicly available at https://github.com/CERT-Lab/fed-sb.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Raghav Singhal and Kaustubh Ponkshe contributed equally to this work</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Single-pass Detection of Jailbreaking Input in Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.15435v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.15435v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Leyla Naz Candogan, Yongtao Wu, Elias Abad Rocamora, Grigorios G. Chrysos, Volkan Cevher
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Defending aligned Large Language Models (LLMs) against jailbreaking attacks
is a challenging problem, with existing approaches requiring multiple requests
or even queries to auxiliary LLMs, making them computationally heavy. Instead,
we focus on detecting jailbreaking input in a single forward pass. Our method,
called Single Pass Detection SPD, leverages the information carried by the
logits to predict whether the output sentence will be harmful. This allows us
to defend in just one forward pass. SPD can not only detect attacks effectively
on open-source models, but also minimizes the misclassification of harmless
inputs. Furthermore, we show that SPD remains effective even without complete
logit access in GPT-3.5 and GPT-4. We believe that our proposed method offers a
promising approach to efficiently safeguard LLMs against adversarial attacks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted in TMLR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Mixup Model Merge: Enhancing Model Merging Performance through
  Randomized Linear Interpolation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.15434v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.15434v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yue Zhou, Yi Chang, Yuan Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Model merging integrates the parameters of multiple models into a unified
model, combining their diverse capabilities. Existing model merging methods are
often constrained by fixed parameter merging ratios. In this study, we propose
Mixup Model Merge (M$^3$), an innovative approach inspired by the Mixup data
augmentation technique. This method merges the parameters of two large language
models (LLMs) by randomly generating linear interpolation ratios, allowing for
a more flexible and comprehensive exploration of the parameter space. Extensive
experiments demonstrate the superiority of our proposed M$^3$ method in merging
fine-tuned LLMs: (1) it significantly improves performance across multiple
tasks, (2) it enhances LLMs' out-of-distribution (OOD) robustness and
adversarial robustness, (3) it achieves superior results when combined with
sparsification techniques such as DARE, and (4) it offers a simple yet
efficient solution that does not require additional computational resources. In
conclusion, M$^3$ is a simple yet effective model merging method that
significantly enhances the performance of the merged model by randomly
generating contribution ratios for two fine-tuned LLMs. The code is available
at https://github.com/MLGroupJLU/MixupModelMerge.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Pub-Guard-LLM: Detecting Fraudulent Biomedical Articles with Reliable
  Explanations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.15429v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.15429v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lihu Chen, Shuojie Fu, Gabriel Freedman, Cemre Zor, Guy Martin, James Kinross, Uddhav Vaghela, Ovidiu Serban, Francesca Toni
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A significant and growing number of published scientific articles is found to
involve fraudulent practices, posing a serious threat to the credibility and
safety of research in fields such as medicine. We propose Pub-Guard-LLM, the
first large language model-based system tailored to fraud detection of
biomedical scientific articles. We provide three application modes for
deploying Pub-Guard-LLM: vanilla reasoning, retrieval-augmented generation, and
multi-agent debate. Each mode allows for textual explanations of predictions.
To assess the performance of our system, we introduce an open-source benchmark,
PubMed Retraction, comprising over 11K real-world biomedical articles,
including metadata and retraction labels. We show that, across all modes,
Pub-Guard-LLM consistently surpasses the performance of various baselines and
provides more reliable explanations, namely explanations which are deemed more
relevant and coherent than those generated by the baselines when evaluated by
multiple assessment methods. By enhancing both detection performance and
explainability in scientific fraud detection, Pub-Guard-LLM contributes to
safeguarding research integrity with a novel, effective, open-source tool.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>long paper under review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Evaluating Multimodal Generative AI with Korean Educational Standards <span class="chip">NAACL 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.15422v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.15422v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sanghee Park, Geewook Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents the Korean National Educational Test Benchmark (KoNET), a
new benchmark designed to evaluate Multimodal Generative AI Systems using
Korean national educational tests. KoNET comprises four exams: the Korean
Elementary General Educational Development Test (KoEGED), Middle (KoMGED), High
(KoHGED), and College Scholastic Ability Test (KoCSAT). These exams are
renowned for their rigorous standards and diverse questions, facilitating a
comprehensive analysis of AI performance across different educational levels.
By focusing on Korean, KoNET provides insights into model performance in
less-explored languages. We assess a range of models - open-source,
open-access, and closed APIs - by examining difficulties, subject diversity,
and human error rates. The code and dataset builder will be made fully
open-sourced at https://github.com/naver-ai/KoNET.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>18 pages; To appear at NAACL 2025 Main Conference (Project page:
  https://github.com/naver-ai/KoNET )</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Beyond Translation: LLM-Based Data Generation for Multilingual
  Fact-Checking 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.15419v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.15419v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yi-Ling Chung, Aurora Cobo, Pablo Serna
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Robust automatic fact-checking systems have the potential to combat online
misinformation at scale. However, most existing research primarily focuses on
English. In this paper, we introduce MultiSynFact, the first large-scale
multilingual fact-checking dataset containing 2.2M claim-source pairs designed
to support Spanish, German, English, and other low-resource languages. Our
dataset generation pipeline leverages Large Language Models (LLMs), integrating
external knowledge from Wikipedia and incorporating rigorous claim validation
steps to ensure data quality. We evaluate the effectiveness of MultiSynFact
across multiple models and experimental settings. Additionally, we open-source
a user-friendly framework to facilitate further research in multilingual
fact-checking and dataset generation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages, 1 figure, 18 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MHQA: A Diverse, Knowledge Intensive Mental Health Question Answering
  Challenge for Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.15418v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.15418v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Suraj Racha, Prashant Joshi, Anshika Raman, Nikita Jangid, Mridul Sharma, Ganesh Ramakrishnan, Nirmal Punjabi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Mental health remains a challenging problem all over the world, with issues
like depression, anxiety becoming increasingly common. Large Language Models
(LLMs) have seen a vast application in healthcare, specifically in answering
medical questions. However, there is a lack of standard benchmarking datasets
for question answering (QA) in mental health. Our work presents a novel
multiple choice dataset, MHQA (Mental Health Question Answering), for
benchmarking Language models (LMs). Previous mental health datasets have
focused primarily on text classification into specific labels or disorders.
MHQA, on the other hand, presents question-answering for mental health focused
on four key domains: anxiety, depression, trauma, and obsessive/compulsive
issues, with diverse question types, namely, factoid, diagnostic, prognostic,
and preventive. We use PubMed abstracts as the primary source for QA. We
develop a rigorous pipeline for LLM-based identification of information from
abstracts based on various selection criteria and converting it into QA pairs.
Further, valid QA pairs are extracted based on post-hoc validation criteria.
Overall, our MHQA dataset consists of 2,475 expert-verified gold standard
instances called MHQA-gold and ~56.1k pairs pseudo labeled using external
medical references. We report F1 scores on different LLMs along with few-shot
and supervised fine-tuning experiments, further discussing the insights for the
scores.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Textual-to-Visual Iterative Self-Verification for Slide Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.15412v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.15412v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yunqing Xu, Xinbei Ma, Jiyang Qiu, Hai Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generating presentation slides is a time-consuming task that urgently
requires automation. Due to their limited flexibility and lack of automated
refinement mechanisms, existing autonomous LLM-based agents face constraints in
real-world applicability. We decompose the task of generating missing
presentation slides into two key components: content generation and layout
generation, aligning with the typical process of creating academic slides.
First, we introduce a content generation approach that enhances coherence and
relevance by incorporating context from surrounding slides and leveraging
section retrieval strategies. For layout generation, we propose a
textual-to-visual self-verification process using a LLM-based Reviewer +
Refiner workflow, transforming complex textual layouts into intuitive visual
formats. This modality transformation simplifies the task, enabling accurate
and human-like review and refinement. Experiments show that our approach
significantly outperforms baseline methods in terms of alignment, logical flow,
visual appeal, and readability.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Work in progress</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Problem-Solving Logic Guided Curriculum In-Context Learning for LLMs
  Complex Reasoning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.15401v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.15401v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xuetao Ma, Wenbin Jiang, Hua Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In-context learning (ICL) can significantly enhance the complex reasoning
capabilities of large language models (LLMs), with the key lying in the
selection and ordering of demonstration examples. Previous methods typically
relied on simple features to measure the relevance between examples. We argue
that these features are not sufficient to reflect the intrinsic connections
between examples. In this study, we propose a curriculum ICL strategy guided by
problem-solving logic. We select demonstration examples by analyzing the
problem-solving logic and order them based on curriculum learning.
Specifically, we constructed a problem-solving logic instruction set based on
the BREAK dataset and fine-tuned a language model to analyze the
problem-solving logic of examples. Subsequently, we selected appropriate
demonstration examples based on problem-solving logic and assessed their
difficulty according to the number of problem-solving steps. In accordance with
the principles of curriculum learning, we ordered the examples from easy to
hard to serve as contextual prompts. Experimental results on multiple
benchmarks indicate that our method outperforms previous ICL approaches in
terms of performance and efficiency, effectively enhancing the complex
reasoning capabilities of LLMs. Our project will be publicly available
subsequently.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Chitrarth: Bridging Vision and Language for a Billion People 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.15392v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.15392v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shaharukh Khan, Ayush Tarun, Abhinav Ravi, Ali Faraz, Akshat Patidar, Praveen Kumar Pokala, Anagha Bhangare, Raja Kolla, Chandra Khatri, Shubham Agarwal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent multimodal foundation models are primarily trained on English or high
resource European language data, which hinders their applicability to other
medium and low-resource languages. To address this limitation, we introduce
Chitrarth (Chitra: Image; Artha: Meaning), an inclusive Vision-Language Model
(VLM), specifically targeting the rich linguistic diversity and visual
reasoning across 10 prominent Indian languages. Our model effectively
integrates a state-of-the-art (SOTA) multilingual Large Language Model (LLM)
with a vision module, primarily trained on multilingual image-text data.
Furthermore, we also introduce BharatBench, a comprehensive framework for
evaluating VLMs across various Indian languages, ultimately contributing to
more diverse and effective AI systems. Our model achieves SOTA results for
benchmarks across low resource languages while retaining its efficiency in
English. Through our research, we aim to set new benchmarks in
multilingual-multimodal capabilities, offering substantial improvements over
existing models and establishing a foundation to facilitate future advancements
in this arena.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Identifying Features that Shape Perceived Consciousness in Large
  Language Model-based AI: A Quantitative Study of Human Responses 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.15365v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.15365v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kang Bongsu, Kim Jundong, Yun Tae-Rim, Bae Hyojin, Kim Chang-Eop
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This study quantitively examines which features of AI-generated text lead
humans to perceive subjective consciousness in large language model (LLM)-based
AI systems. Drawing on 99 passages from conversations with Claude 3 Opus and
focusing on eight features -- metacognitive self-reflection, logical reasoning,
empathy, emotionality, knowledge, fluency, unexpectedness, and subjective
expressiveness -- we conducted a survey with 123 participants. Using regression
and clustering analyses, we investigated how these features influence
participants' perceptions of AI consciousness. The results reveal that
metacognitive self-reflection and the AI's expression of its own emotions
significantly increased perceived consciousness, while a heavy emphasis on
knowledge reduced it. Participants clustered into seven subgroups, each showing
distinct feature-weighting patterns. Additionally, higher prior knowledge of
LLMs and more frequent usage of LLM-based chatbots were associated with greater
overall likelihood assessments of AI consciousness. This study underscores the
multidimensional and individualized nature of perceived AI consciousness and
provides a foundation for better understanding the psychosocial implications of
human-AI interaction.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 3 figures, 4 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Evaluating Social Biases in LLM Reasoning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.15361v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.15361v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xuyang Wu, Jinming Nian, Zhiqiang Tao, Yi Fang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the recent development of AI reasoning, large language models (LLMs) are
trained to automatically generate chain-of-thought reasoning steps, which have
demonstrated compelling performance on math and coding tasks. However, when
bias is mixed within the reasoning process to form strong logical arguments, it
could cause even more harmful results and further induce hallucinations. In
this paper, we have evaluated the 8B and 32B variants of DeepSeek-R1 against
their instruction tuned counterparts on the BBQ dataset, and investigated the
bias that is elicited out and being amplified through reasoning steps. To the
best of our knowledge, this empirical study is the first to assess bias issues
in LLM reasoning.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ARS: Automatic Routing Solver with Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.15359v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.15359v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kai Li, Fei Liu, Zhenkun Wang, Xialiang Tong, Xiongwei Han, Mingxuan Yuan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Real-world Vehicle Routing Problems (VRPs) are characterized by a variety of
practical constraints, making manual solver design both knowledge-intensive and
time-consuming. Although there is increasing interest in automating the design
of routing algorithms, existing research has explored only a limited array of
VRP variants and fails to adequately address the complex and prevalent
constraints encountered in real-world situations. To fill this gap, this paper
introduces RoutBench, a benchmark of 1,000 VRP variants derived from 24
attributes, for evaluating the effectiveness of automatic routing solvers in
addressing complex constraints. Along with RoutBench, we present the Automatic
Routing Solver (ARS), which employs Large Language Model (LLM) agents to
enhance a backbone algorithm framework by automatically generating
constraint-aware heuristic code, based on problem descriptions and several
representative constraints selected from a database. Our experiments show that
ARS outperforms state-of-the-art LLM-based methods and commonly used solvers,
automatically solving 91.67% of common VRPs and achieving at least a 30%
improvement across all benchmarks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AttentionEngine: A Versatile Framework for Efficient Attention
  Mechanisms on Diverse Hardware Platforms 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.15349v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.15349v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Feiyang Chen, Yu Cheng, Lei Wang, Yuqing Xia, Ziming Miao, Lingxiao Ma, Fan Yang, Jilong Xue, Zhi Yang, Mao Yang, Haibo Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Transformers and large language models (LLMs) have revolutionized machine
learning, with attention mechanisms at the core of their success. As the
landscape of attention variants expands, so too do the challenges of optimizing
their performance, particularly across different hardware platforms. Current
optimization strategies are often narrowly focused, requiring extensive manual
intervention to accommodate changes in model configurations or hardware
environments. In this paper, we introduce AttentionEngine, a comprehensive
framework designed to streamline the optimization of attention mechanisms
across heterogeneous hardware backends. By decomposing attention computation
into modular operations with customizable components, AttentionEngine enables
flexible adaptation to diverse algorithmic requirements. The framework further
automates kernel optimization through a combination of programmable templates
and a robust cross-platform scheduling strategy. Empirical results reveal
performance gains of up to 10x on configurations beyond the reach of existing
methods. AttentionEngine offers a scalable, efficient foundation for developing
and deploying attention mechanisms with minimal manual tuning. Our code has
been open-sourced and is available at
https://github.com/microsoft/AttentionEngine.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Constructing a Norm for Children's Scientific Drawing: Distribution
  Features Based on Semantic Similarity of Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.15348v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.15348v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yi Zhang, Fan Wei, Jingyi Li, Yan Wang, Yanyan Yu, Jianli Chen, Zipo Cai, Xinyu Liu, Wei Wang, Peng Wang, Zhong Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The use of children's drawings to examining their conceptual understanding
has been proven to be an effective method, but there are two major problems
with previous research: 1. The content of the drawings heavily relies on the
task, and the ecological validity of the conclusions is low; 2. The
interpretation of drawings relies too much on the subjective feelings of the
researchers. To address this issue, this study uses the Large Language Model
(LLM) to identify 1420 children's scientific drawings (covering 9 scientific
themes/concepts), and uses the word2vec algorithm to calculate their semantic
similarity. The study explores whether there are consistent drawing
representations for children on the same theme, and attempts to establish a
norm for children's scientific drawings, providing a baseline reference for
follow-up children's drawing research. The results show that the representation
of most drawings has consistency, manifested as most semantic similarity
greater than 0.8. At the same time, it was found that the consistency of the
representation is independent of the accuracy (of LLM's recognition),
indicating the existence of consistency bias. In the subsequent exploration of
influencing factors, we used Kendall rank correlation coefficient to
investigate the effects of Sample Size, Abstract Degree, and Focus Points on
drawings, and used word frequency statistics to explore whether children
represented abstract themes/concepts by reproducing what was taught in class.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Tokenization is Sensitive to Language Variation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.15343v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.15343v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anna Wegmann, Dong Nguyen, David Jurgens
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Variation in language is ubiquitous and often systematically linked to
regional, social, and contextual factors. Tokenizers split texts into smaller
units and might behave differently for less common linguistic forms. This might
affect downstream LLM performance differently on two types of tasks: Tasks
where the model should be robust to language variation (e.g., for semantic
tasks like NLI, labels do not depend on whether a text uses British or American
spelling) and tasks where the model should be sensitive to language variation
(e.g., for form-based tasks like authorship verification, labels depend on
whether a text uses British or American spelling). We pre-train BERT base
models for the popular Byte-Pair Encoding algorithm to investigate how key
algorithmic design choices impact downstream models' performances: fitting
corpus, pre-tokenizer and vocabulary size. We find that the best tokenizer
varies on the two task types -- with the pre-tokenizer having the biggest
impact on performance. Further, we introduce a new approach to estimate
tokenizer impact on downstream LLM performance, showing significant improvement
over techniques like R\'enyi efficiency. We encourage more work on language
variation and its relation to tokenizers and thus LLM performance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Stepwise Informativeness Search for Improving LLM Reasoning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.15335v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.15335v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Siyuan Wang, Enda Zhao, Zhongyu Wei, Xiang Ren
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Advances in Large Language Models (LLMs) have significantly improved
multi-step reasoning through generating free-text rationales. However, recent
studies show that LLMs tend to lose focus over the middle of long contexts.
This raises concerns that as reasoning progresses, LLMs may overlook
information in earlier steps when decoding subsequent steps, leading to
generate unreliable and redundant rationales. To address this, we propose
guiding LLMs to generate more accurate and concise step-by-step rationales by
(1) proactively referencing information from underutilized prior steps, and (2)
minimizing redundant information between new and existing steps. We introduce
stepwise informativeness search, an inference-time tree search framework
incorporating two selection heuristics: grounding-guided selection which
prioritizes steps paying higher attention over underutilized steps; and
novelty-guided selection which encourages steps with novel conclusions. During
rationale generation, we use a self-grounding strategy that prompts LLMs to
explicitly reference relevant prior steps to provide premises before deduction
at each step. Experimental results on four reasoning datasets demonstrate that
our approach improves reasoning accuracy by generating higher-quality
rationales with reduced errors and redundancy.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Detecting Future-related Contexts of Entity Mentions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.15332v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.15332v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Puneet Prashar, Krishna Mohan Shukla, Adam Jatowt
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The ability to automatically identify whether an entity is referenced in a
future context can have multiple applications including decision making,
planning and trend forecasting. This paper focuses on detecting implicit future
references in entity-centric texts, addressing the growing need for automated
temporal analysis in information processing. We first present a novel dataset
of 19,540 sentences built around popular entities sourced from Wikipedia, which
consists of future-related and non-future-related contexts in which those
entities appear. As a second contribution, we evaluate the performance of
several Language Models including also Large Language Models (LLMs) on the task
of distinguishing future-oriented content in the absence of explicit temporal
references.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SVDq: 1.25-bit and 410x Key Cache Compression for LLM Attention 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.15304v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.15304v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hong Yankun, Li Xing, Zhen Hui-Ling, Yu Xianzhi, Liu Wulong, Yuan Mingxuan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  For the efficient inference of Large Language Models (LLMs), the effective
compression of key-value (KV) cache is essential. Three main types of KV cache
compression techniques, namely sparsity, channel compression, and quantization,
have been identified. This study presents SVDq, a Singular Value Decomposition
(SVD) - based mixed precision quantization method for K cache. Initially, K
cache is transformed into latent channels using SVD basis representations.
Since the values in latent channels decay rapidly and become negligible after
only a few latent channels, our method then incorporates importance-aware
quantization and compression for latent channels. This enables the effective
allocation of higher precision to more significant channels. Theoretically, we
prove that SVDq results in quantization errors (x0.1 or even lower) that are
much lower than those of per-channel key quantization in the original space.
Our findings based on RULER and LongBench benchmarks demonstrate that SVDq can
achieve an equivalent key cache precision as low as 1.25-bit. When combined
with key sparsity, it can reach a key compression ratio of up to 410x for
attention computation, all while maintaining comparable model performance.
Notably, our method is nearly lossless for LongBench datasets. This indicates
that SVDq enables high-precision low-bit quantization, providing a more
efficient solution for KV cache compression in LLMs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Analyzing the Inner Workings of <span class="highlight-title">Transformer</span>s in Compositional
  Generalization <span class="chip">NAACL 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.15277v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.15277v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ryoma Kumon, Hitomi Yanaka
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The compositional generalization abilities of neural models have been sought
after for human-like linguistic competence. The popular method to evaluate such
abilities is to assess the models' input-output behavior. However, that does
not reveal the internal mechanisms, and the underlying competence of such
models in compositional generalization remains unclear. To address this
problem, we explore the inner workings of a Transformer model by finding an
existing subnetwork that contributes to the generalization performance and by
performing causal analyses on how the model utilizes syntactic features. We
find that the model depends on syntactic features to output the correct answer,
but that the subnetwork with much better generalization performance than the
whole model relies on a non-compositional algorithm in addition to the
syntactic features. We also show that the subnetwork improves its
generalization performance relatively slowly during the training compared to
the in-distribution one, and the non-compositional solution is acquired in the
early stages of the training.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to NAACL 2025 main</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Training-free LLM-based Approach to General Chinese Character Error
  Correction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.15266v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.15266v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Houquan Zhou, Bo Zhang, Zhenghua Li, Ming Yan, Min Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Chinese spelling correction (CSC) is a crucial task that aims to correct
character errors in Chinese text. While conventional CSC focuses on character
substitution errors caused by mistyping, two other common types of character
errors, missing and redundant characters, have received less attention. These
errors are often excluded from CSC datasets during the annotation process or
ignored during evaluation, even when they have been annotated. This issue
limits the practicality of the CSC task. To address this issue, we introduce
the task of General Chinese Character Error Correction (C2EC), which focuses on
all three types of character errors. We construct a high-quality C2EC benchmark
by combining and manually verifying data from CCTC and Lemon datasets. We
extend the training-free prompt-free CSC method to C2EC by using Levenshtein
distance for handling length changes and leveraging an additional prompt-based
large language model (LLM) to improve performance. Experiments show that our
method enables a 14B-parameter LLM to be on par with models nearly 50 times
larger on both conventional CSC and C2EC tasks, without any fine-tuning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>25 pages, 12 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Retrieval-Augmented Speech Recognition Approach for Domain Challenges 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.15264v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.15264v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Peng Shen, Xugang Lu, Hisashi Kawai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Speech recognition systems often face challenges due to domain mismatch,
particularly in real-world applications where domain-specific data is
unavailable because of data accessibility and confidentiality constraints.
Inspired by Retrieval-Augmented Generation (RAG) techniques for large language
models (LLMs), this paper introduces a LLM-based retrieval-augmented speech
recognition method that incorporates domain-specific textual data at the
inference stage to enhance recognition performance. Rather than relying on
domain-specific textual data during the training phase, our model is trained to
learn how to utilize textual information provided in prompts for LLM decoder to
improve speech recognition performance. Benefiting from the advantages of the
RAG retrieval mechanism, our approach efficiently accesses locally available
domain-specific documents, ensuring a convenient and effective process for
solving domain mismatch problems. Experiments conducted on the CSJ database
demonstrate that the proposed method significantly improves speech recognition
accuracy and achieves state-of-the-art results on the CSJ dataset, even without
relying on the full training data.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Corrections Meet Explanations: A Unified Framework for Explainable
  Grammatical Error Correction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.15261v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.15261v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jingheng Ye, Shang Qin, Yinghui Li, Hai-Tao Zheng, Shen Wang, Qingsong Wen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Grammatical Error Correction (GEC) faces a critical challenge concerning
explainability, notably when GEC systems are designed for language learners.
Existing research predominantly focuses on explaining grammatical errors
extracted in advance, thus neglecting the relationship between explanations and
corrections. To address this gap, we introduce EXGEC, a unified explainable GEC
framework that integrates explanation and correction tasks in a generative
manner, advocating that these tasks mutually reinforce each other. Experiments
have been conducted on EXPECT, a recent human-labeled dataset for explainable
GEC, comprising around 20k samples. Moreover, we detect significant noise
within EXPECT, potentially compromising model training and evaluation.
Therefore, we introduce an alternative dataset named EXPECT-denoised, ensuring
a more objective framework for training and evaluation. Results on various NLP
models (BART, T5, and Llama3) show that EXGEC models surpass single-task
baselines in both tasks, demonstrating the effectiveness of our approach.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>19 pages, 2 figures, and 9 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LightMamba: Efficient Mamba Acceleration on FPGA with Quantization and
  Hardware Co-design <span class="chip">DATE 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.15260v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.15260v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Renjie Wei, Songqiang Xu, Linfeng Zhong, Zebin Yang, Qingyu Guo, Yuan Wang, Runsheng Wang, Meng Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  State space models (SSMs) like Mamba have recently attracted much attention.
Compared to Transformer-based large language models (LLMs), Mamba achieves
linear computation complexity with the sequence length and demonstrates
superior performance. However, Mamba is hard to accelerate due to the scattered
activation outliers and the complex computation dependency, rendering existing
LLM accelerators inefficient. In this paper, we propose LightMamba that
co-designs the quantization algorithm and FPGA accelerator architecture for
efficient Mamba inference. We first propose an FPGA-friendly post-training
quantization algorithm that features rotation-assisted quantization and
power-of-two SSM quantization to reduce the majority of computation to 4-bit.
We further design an FPGA accelerator that partially unrolls the Mamba
computation to balance the efficiency and hardware costs. Through computation
reordering as well as fine-grained tiling and fusion, the hardware utilization
and memory efficiency of the accelerator get drastically improved. We implement
LightMamba on Xilinx Versal VCK190 FPGA and achieve 4.65x to 6.06x higher
energy efficiency over the GPU baseline. When evaluated on Alveo U280 FPGA,
LightMamba reaches 93 tokens/s, which is 1.43x that of the GPU baseline.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by DATE 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A General Pseudonymization Framework for Cloud-Based LLMs: Replacing
  Privacy Information in Controlled Text Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.15233v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.15233v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shilong Hou, Ruilin Shang, Zi Long, Xianghua Fu, Yin Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  An increasing number of companies have begun providing services that leverage
cloud-based large language models (LLMs), such as ChatGPT. However, this
development raises substantial privacy concerns, as users' prompts are
transmitted to and processed by the model providers. Among the various privacy
protection methods for LLMs, those implemented during the pre-training and
fine-tuning phrases fail to mitigate the privacy risks associated with the
remote use of cloud-based LLMs by users. On the other hand, methods applied
during the inference phrase are primarily effective in scenarios where the
LLM's inference does not rely on privacy-sensitive information. In this paper,
we outline the process of remote user interaction with LLMs and, for the first
time, propose a detailed definition of a general pseudonymization framework
applicable to cloud-based LLMs. The experimental results demonstrate that the
proposed framework strikes an optimal balance between privacy protection and
utility. The code for our method is available to the public at
https://github.com/Mebymeby/Pseudonymization-Framework.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>under review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Understand User Opinions of Large Language Models via LLM-Powered
  In-the-Moment User Experience Interviews 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.15226v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.15226v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mengqiao Liu, Tevin Wang, Cassandra A. Cohen, Sarah Li, Chenyan Xiong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Which large language model (LLM) is better? Every evaluation tells a story,
but what do users really think about current LLMs? This paper presents CLUE, an
LLM-powered interviewer that conducts in-the-moment user experience interviews,
right after users interacted with LLMs, and automatically gathers insights
about user opinions from massive interview logs. We conduct a study with
thousands of users to understand user opinions on mainstream LLMs, recruiting
users to first chat with a target LLM and then interviewed by CLUE. Our
experiments demonstrate that CLUE captures interesting user opinions, for
example, the bipolar views on the displayed reasoning process of DeepSeek-R1
and demands for information freshness and multi-modality. Our collected
chat-and-interview logs will be released.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A <span class="highlight-title">BERT</span> Based Hybrid Recommendation System For Academic Collaboration 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.15223v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.15223v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sangeetha N, Harish Thangaraj, Varun Vashisht, Eshaan Joshi, Kanishka Verma, Diya Katariya
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Universities serve as a hub for academic collaboration, promoting the
exchange of diverse ideas and perspectives among students and faculty through
interdisciplinary dialogue. However, as universities expand in size,
conventional networking approaches via student chapters, class groups, and
faculty committees become cumbersome. To address this challenge, an
academia-specific profile recommendation system is proposed to connect
like-minded stakeholders within any university community. This study evaluates
three techniques: Term Frequency-Inverse Document Frequency (TF-IDF),
Bidirectional Encoder Representations from Transformers (BERT), and a hybrid
approach to generate effective recommendations. Due to the unlabelled nature of
the dataset, Affinity Propagation cluster-based relabelling is performed to
understand the grouping of similar profiles. The hybrid model demonstrated
superior performance, evidenced by its similarity score, Silhouette score,
Davies-Bouldin index, and Normalized Discounted Cumulative Gain (NDCG),
achieving an optimal balance between diversity and relevance in
recommendations. Furthermore, the optimal model has been implemented as a
mobile application, which dynamically suggests relevant profiles based on
users' skills and collaboration interests, incorporating contextual
understanding. The potential impact of this application is significant, as it
promises to enhance networking opportunities within large academic institutions
through the deployment of intelligent recommendation systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>International Conference on Intelligent Systems and Security - 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The Evolving Landscape of LLM- and VLM-Integrated Reinforcement Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.15214v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.15214v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sheila Schoepp, Masoud Jafaripour, Yingyue Cao, Tianpei Yang, Fatemeh Abdollahi, Shadan Golestan, Zahin Sufiyan, Osmar R. Zaiane, Matthew E. Taylor
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reinforcement learning (RL) has shown impressive results in sequential
decision-making tasks. Meanwhile, Large Language Models (LLMs) and
Vision-Language Models (VLMs) have emerged, exhibiting impressive capabilities
in multimodal understanding and reasoning. These advances have led to a surge
of research integrating LLMs and VLMs into RL. In this survey, we review
representative works in which LLMs and VLMs are used to overcome key challenges
in RL, such as lack of prior knowledge, long-horizon planning, and reward
design. We present a taxonomy that categorizes these LLM/VLM-assisted RL
approaches into three roles: agent, planner, and reward. We conclude by
exploring open problems, including grounding, bias mitigation, improved
representations, and action advice. By consolidating existing research and
identifying future directions, this survey establishes a framework for
integrating LLMs and VLMs into RL, advancing approaches that unify natural
language and visual understanding with sequential decision-making.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Unveiling Attractor Cycles in Large Language Models: A Dynamical Systems
  View of Successive Paraphrasing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.15208v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.15208v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhilin Wang, Yafu Li, Jianhao Yan, Yu Cheng, Yue Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Dynamical systems theory provides a framework for analyzing iterative
processes and evolution over time. Within such systems, repetitive
transformations can lead to stable configurations, known as attractors,
including fixed points and limit cycles. Applying this perspective to large
language models (LLMs), which iteratively map input text to output text,
provides a principled approach to characterizing long-term behaviors.
Successive paraphrasing serves as a compelling testbed for exploring such
dynamics, as paraphrases re-express the same underlying meaning with linguistic
variation. Although LLMs are expected to explore a diverse set of paraphrases
in the text space, our study reveals that successive paraphrasing converges to
stable periodic states, such as 2-period attractor cycles, limiting linguistic
diversity. This phenomenon is attributed to the self-reinforcing nature of
LLMs, as they iteratively favour and amplify certain textual forms over others.
This pattern persists with increasing generation randomness or alternating
prompts and LLMs. These findings underscore inherent constraints in LLM
generative capability, while offering a novel dynamical systems perspective for
studying their expressive potential.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ TETRIS: Optimal Draft Token Selection for Batch Speculative Decoding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.15197v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.15197v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhaoxuan Wu, Zijian Zhou, Arun Verma, Alok Prakash, Daniela Rus, Bryan Kian Hsiang Low
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose TETRIS, a novel method that optimizes the total throughput of
batch speculative decoding in multi-request settings. Unlike existing methods
that optimize for a single request or a group of requests as a whole, TETRIS
actively selects the most promising draft tokens (for every request in a batch)
to be accepted when verified in parallel, resulting in fewer rejected tokens
and hence less wasted computing resources. Such an effective resource
utilization to achieve fast inference in large language models (LLMs) is
especially important to service providers with limited inference capacity.
Compared to baseline speculative decoding, TETRIS yields a consistently higher
acceptance rate and more effective utilization of the limited inference
capacity. We show theoretically and empirically that TETRIS outperforms
baseline speculative decoding and existing methods that dynamically select
draft tokens, leading to a more efficient batch inference in LLMs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages, 10 figures, 5 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Scale-Free Graph-Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.15189v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.15189v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jianglin Lu, Yixuan Liu, Yitian Zhang, Yun Fu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Graph-language models (GLMs) have demonstrated great potential in graph-based
semi-supervised learning. A typical GLM consists of two key stages: graph
generation and text embedding, which are usually implemented by inferring a
latent graph and finetuning a language model (LM), respectively. However, the
former often relies on artificial assumptions about the underlying edge
distribution, while the latter requires extensive data annotations. To tackle
these challenges, this paper introduces a novel GLM that integrates graph
generation and text embedding within a unified framework. Specifically, for
graph generation, we leverage an inherent characteristic of real edge
distribution--the scale-free property--as a structural prior. We unexpectedly
find that this natural property can be effectively approximated by a simple
k-nearest neighbor (KNN) graph. For text embedding, we develop a graph-based
pseudo-labeler that utilizes scale-free graphs to provide complementary
supervision for improved LM finetuning. Extensive experiments on representative
datasets validate our findings on the scale-free structural approximation of
KNN graphs and demonstrate the effectiveness of integrating graph generation
and text embedding with a real structural prior. Our code is available at
https://github.com/Jianglin954/SFGL.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ BP-<span class="highlight-title">GPT</span>: Auditory Neural Decoding Using fMRI-<span class="highlight-title">prompt</span>ed LLM 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.15172v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.15172v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaoyu Chen, Changde Du, Che Liu, Yizhe Wang, Huiguang He
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Decoding language information from brain signals represents a vital research
area within brain-computer interfaces, particularly in the context of
deciphering the semantic information from the fMRI signal. Although existing
work uses LLM to achieve this goal, their method does not use an end-to-end
approach and avoids the LLM in the mapping of fMRI-to-text, leaving space for
the exploration of the LLM in auditory decoding. In this paper, we introduce a
novel method, the Brain Prompt GPT (BP-GPT). By using the brain representation
that is extracted from the fMRI as a prompt, our method can utilize GPT-2 to
decode fMRI signals into stimulus text. Further, we introduce the text prompt
and align the fMRI prompt to it. By introducing the text prompt, our BP-GPT can
extract a more robust brain prompt and promote the decoding of pre-trained LLM.
We evaluate our BP-GPT on the open-source auditory semantic decoding dataset
and achieve a significant improvement up to 4.61 on METEOR and 2.43 on
BERTScore across all the subjects compared to the state-of-the-art method. The
experimental results demonstrate that using brain representation as a prompt to
further drive LLM for auditory neural decoding is feasible and effective. The
code is available at https://github.com/1994cxy/BP-GPT.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>arXiv admin note: substantial text overlap with arXiv:2405.07840</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ mStyleDistance: Multilingual Style Embeddings and their Evaluation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.15168v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.15168v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Justin Qiu, Jiacheng Zhu, Ajay Patel, Marianna Apidianaki, Chris Callison-Burch
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Style embeddings are useful for stylistic analysis and style transfer;
however, only English style embeddings have been made available. We introduce
Multilingual StyleDistance (mStyleDistance), a multilingual style embedding
model trained using synthetic data and contrastive learning. We train the model
on data from nine languages and create a multilingual STEL-or-Content benchmark
(Wegmann et al., 2022) that serves to assess the embeddings' quality. We also
employ our embeddings in an authorship verification task involving different
languages. Our results show that mStyleDistance embeddings outperform existing
models on these multilingual style benchmarks and generalize well to unseen
features and languages. We make our model publicly available at
https://huggingface.co/StyleDistance/mstyledistance .
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>arXiv admin note: substantial text overlap with arXiv:2410.12757</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Extreme Speech Classification in the Era of LLMs: Exploring Open-Source
  and Proprietary Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.15155v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.15155v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sarthak Mahajan, Nimmi Rangaswamy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, widespread internet adoption and the growth in userbase of
various social media platforms have led to an increase in the proliferation of
extreme speech online. While traditional language models have demonstrated
proficiency in distinguishing between neutral text and non-neutral text (i.e.
extreme speech), categorizing the diverse types of extreme speech presents
significant challenges. The task of extreme speech classification is
particularly nuanced, as it requires a deep understanding of socio-cultural
contexts to accurately interpret the intent of the language used by the
speaker. Even human annotators often disagree on the appropriate classification
of such content, emphasizing the complex and subjective nature of this task.
The use of human moderators also presents a scaling issue, necessitating the
need for automated systems for extreme speech classification. The recent launch
of ChatGPT has drawn global attention to the potential applications of Large
Language Models (LLMs) across a diverse variety of tasks. Trained on vast and
diverse corpora, and demonstrating the ability to effectively capture and
encode contextual information, LLMs emerge as highly promising tools for
tackling this specific task of extreme speech classification. In this paper, we
leverage the Indian subset of the extreme speech dataset from Maronikolakis et
al. (2022) to develop an effective classification framework using LLMs. We
evaluate open-source Llama models against closed-source OpenAI models, finding
that while pre-trained LLMs show moderate efficacy, fine-tuning with
domain-specific data significantly enhances performance, highlighting their
adaptability to linguistic and contextual nuances. Although GPT-based models
outperform Llama models in zero-shot settings, the performance gap disappears
after fine-tuning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to 7th International Conference on information systems and
  management science (ISMS), 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Investigating the Adaptive Robustness with Knowledge Conflicts in
  LLM-based Multi-Agent Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.15153v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.15153v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tianjie Ju, Bowen Wang, Hao Fei, Mong-Li Lee, Wynne Hsu, Yun Li, Qianren Wang, Pengzhou Cheng, Zongru Wu, Zhuosheng Zhang, Gongshen Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances in Large Language Models (LLMs) have upgraded them from
sophisticated text generators to autonomous agents capable of corporation and
tool use in multi-agent systems (MASs). However, the robustness of these
LLM-based MASs, especially under knowledge conflicts, remains unclear. In this
paper, we design four comprehensive metrics to investigate the robustness of
MASs when facing mild or task-critical knowledge conflicts. We first analyze
mild knowledge conflicts introduced by heterogeneous agents and find that they
do not harm system robustness but instead improve collaborative
decision-making. Next, we investigate task-critical knowledge conflicts by
synthesizing knowledge conflicts and embedding them into one of the agents. Our
results show that these conflicts have surprisingly little to no impact on MAS
robustness. Furthermore, we observe that MASs demonstrate certain
self-repairing capabilities by reducing their reliance on knowledge conflicts
and adopting alternative solution paths to maintain stability. Finally, we
conduct ablation studies on the knowledge conflict number, agent number, and
interaction rounds, finding that the self-repairing capability of MASs has
intrinsic limits, and all findings hold consistently across various factors.
Our code is publicly available at
https://github.com/wbw625/MultiAgentRobustness.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Working in progress</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Latent Factor Models Meets Instructions:Goal-conditioned Latent Factor
  Discovery without Task Supervision <span class="chip">NAACL 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.15147v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.15147v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhouhang Xie, Tushar Khot, Bhavana Dalvi Mishra, Harshit Surana, Julian McAuley, Peter Clark, Bodhisattwa Prasad Majumder
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Instruction-following LLMs have recently allowed systems to discover hidden
concepts from a collection of unstructured documents based on a natural
language description of the purpose of the discovery (i.e., goal). Still, the
quality of the discovered concepts remains mixed, as it depends heavily on
LLM's reasoning ability and drops when the data is noisy or beyond LLM's
knowledge. We present Instruct-LF, a goal-oriented latent factor discovery
system that integrates LLM's instruction-following ability with statistical
models to handle large, noisy datasets where LLM reasoning alone falls short.
  Instruct-LF uses LLMs to propose fine-grained, goal-related properties from
documents, estimates their presence across the dataset, and applies
gradient-based optimization to uncover hidden factors, where each factor is
represented by a cluster of co-occurring properties. We evaluate latent factors
produced by Instruct-LF on movie recommendation, text-world navigation, and
legal document categorization tasks. These interpretable representations
improve downstream task performance by 5-52% than the best baselines and were
preferred 1.8 times as often as the best alternative, on average, in human
evaluation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NAACL 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Do LLMs Make Mistakes Like Students? Exploring Natural Alignment between
  Language Models and Human Error Patterns 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.15140v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.15140v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Naiming Liu, Shashank Sonkar, Richard G. Baraniuk
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have demonstrated remarkable capabilities in
various educational tasks, yet their alignment with human learning patterns,
particularly in predicting which incorrect options students are most likely to
select in multiple-choice questions (MCQs), remains underexplored. Our work
investigates the relationship between LLM generation likelihood and student
response distributions in MCQs with a specific focus on distractor selections.
We collect a comprehensive dataset of MCQs with real-world student response
distributions to explore two fundamental research questions: (1). RQ1 - Do the
distractors that students more frequently select correspond to those that LLMs
assign higher generation likelihood to? (2). RQ2 - When an LLM selects a
incorrect choice, does it choose the same distractor that most students pick?
Our experiments reveals moderate correlations between LLM-assigned
probabilities and student selection patterns for distractors in MCQs.
Additionally, when LLMs make mistakes, they are more likley to select the same
incorrect answers that commonly mislead students, which is a pattern consistent
across both small and large language models. Our work provides empirical
evidence that despite LLMs' strong performance on generating educational
content, there remains a gap between LLM's underlying reasoning process and
human cognitive processes in identifying confusing distractors. Our findings
also have significant implications for educational assessment development. The
smaller language models could be efficiently utilized for automated distractor
generation as they demonstrate similar patterns in identifying confusing answer
choices as larger language models. This observed alignment between LLMs and
student misconception patterns opens new opportunities for generating
high-quality distractors that complement traditional human-designed
distractors.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Chain-of-Rank: Enhancing Large Language Models for Domain-Specific RAG
  in Edge Device <span class="chip">NAACL 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.15134v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.15134v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Juntae Lee, Jihwan Bang, Seunghan Yang, Kyuhong Shim, Simyung Chang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Retrieval-augmented generation (RAG) with large language models (LLMs) is
especially valuable in specialized domains, where precision is critical. To
more specialize the LLMs into a target domain, domain-specific RAG has recently
been developed by allowing the LLM to access the target domain early via
finetuning. The domain-specific RAG makes more sense in resource-constrained
environments like edge devices, as they should perform a specific task (e.g.
personalization) reliably using only small-scale LLMs. While the
domain-specific RAG is well-aligned with edge devices in this respect, it often
relies on widely-used reasoning techniques like chain-of-thought (CoT). The
reasoning step is useful to understand the given external knowledge, and yet it
is computationally expensive and difficult for small-scale LLMs to learn it.
Tackling this, we propose the Chain of Rank (CoR) which shifts the focus from
intricate lengthy reasoning to simple ranking of the reliability of input
external documents. Then, CoR reduces computational complexity while
maintaining high accuracy, making it particularly suited for
resource-constrained environments. We attain the state-of-the-art (SOTA)
results in benchmarks, and analyze its efficacy.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NAACL 2025 (Findings)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CoT-ICL Lab: A Petri Dish for Studying Chain-of-Thought Learning from
  In-Context Demonstrations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.15132v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.15132v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vignesh Kothapalli, Hamed Firooz, Maziar Sanjabi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce CoT-ICL Lab, a framework and methodology to generate synthetic
tokenized datasets and systematically study chain-of-thought (CoT) in-context
learning (ICL) in language models. CoT-ICL Lab allows fine grained control over
the complexity of in-context examples by decoupling (1) the causal structure
involved in chain token generation from (2) the underlying token processing
functions. We train decoder-only transformers (up to 700M parameters) on these
datasets and show that CoT accelerates the accuracy transition to higher values
across model sizes. In particular, we find that model depth is crucial for
leveraging CoT with limited in-context examples, while more examples help
shallow models match deeper model performance. Additionally, limiting the
diversity of token processing functions throughout training improves causal
structure learning via ICL. We also interpret these transitions by analyzing
transformer embeddings and attention maps. Overall, CoT-ICL Lab serves as a
simple yet powerful testbed for theoretical and empirical insights into ICL and
CoT in language models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>22 pages, 27 figures, 3 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Unveiling Reasoning Thresholds in Language Models: Scaling, Fine-Tuning,
  and Interpretability through Attention Maps 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.15120v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.15120v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yen-Che Hsiao, Abhishek Dutta
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This study investigates the in-context learning capabilities of various
decoder-only transformer-based language models with different model sizes and
training data, including GPT2, SmolLM2, OpenELM, TinyLlama, Stable LM, and
Gemma 2. We identify a critical parameter threshold (~1.6 billion), beyond
which reasoning performance improves significantly in tasks such as commonsense
reasoning in multiple-choice question answering and deductive reasoning.
Specifically, models above this threshold achieve better success rates in
chain-of-thought (CoT) prompting for deductive reasoning tasks, especially
those requiring longer reasoning chains, such as proof by contradiction and
disjunction elimination. To address limitations in sub-threshold models, we
demonstrate that fine-tuning with task-specific exemplars substantially
enhances reasoning performance, enabling accurate CoT generation even without
additional exemplars in the prompt for tasks with shorter reasoning chains.
Finally, our analysis of attention maps reveals that models capable of
generating correct CoTs exhibit higher token-level attention scores on
subsequent correct tokens and the correct parts of speech, providing
interpretability insights into reasoning processes. These findings collectively
advance understanding of reasoning capabilities in decoder-only
transformer-based models. The code is available at:
https://github.com/AnnonymousForPapers/CoT_Reasoning_Test.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Social Genome: Grounded Social Reasoning Abilities of Multimodal Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.15109v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.15109v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Leena Mathur, Marian Qian, Paul Pu Liang, Louis-Philippe Morency
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Social reasoning abilities are crucial for AI systems to effectively
interpret and respond to multimodal human communication and interaction within
social contexts. We introduce Social Genome, the first benchmark for
fine-grained, grounded social reasoning abilities of multimodal models. Social
Genome contains 272 videos of interactions and 1,486 human-annotated reasoning
traces related to inferences about these interactions. These traces contain
5,777 reasoning steps that reference evidence from visual cues, verbal cues,
vocal cues, and external knowledge (contextual knowledge external to videos).
Social Genome is also the first modeling challenge to study external knowledge
in social reasoning. Social Genome computes metrics to holistically evaluate
semantic and structural qualities of model-generated social reasoning traces.
We demonstrate the utility of Social Genome through experiments with
state-of-the-art models, identifying performance gaps and opportunities for
future research to improve the grounded social reasoning abilities of
multimodal models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under Review, 22 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Schema Augmentation for Zero-Shot Domain Adaptation in Dialogue State
  Tracking 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.00150v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.00150v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Christopher Richardson, Roshan Sharma, Neeraj Gaur, Parisa Haghani, Anirudh Sundar, Bhuvana Ramabhadran
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Zero-shot domain adaptation for dialogue state tracking (DST) remains a
challenging problem in task-oriented dialogue (TOD) systems, where models must
generalize to target domains unseen at training time. Current large language
model approaches for zero-shot domain adaptation rely on prompting to introduce
knowledge pertaining to the target domains. However, their efficacy strongly
depends on prompt engineering, as well as the zero-shot ability of the
underlying language model. In this work, we devise a novel data augmentation
approach, Schema Augmentation, that improves the zero-shot domain adaptation of
language models through fine-tuning. Schema Augmentation is a simple but
effective technique that enhances generalization by introducing variations of
slot names within the schema provided in the prompt. Experiments on MultiWOZ
and SpokenWOZ showed that the proposed approach resulted in a substantial
improvement over the baseline, in some experiments achieving over a twofold
accuracy gain over unseen domains while maintaining equal or superior
performance over all domains.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>short paper (4 pages) submitted to ARR</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ELMI: Interactive and Intelligent Sign Language Translation of Lyrics
  for Song Signing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.09760v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.09760v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Suhyeon Yoo, Khai N. Truong, Young-Ho Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  d/Deaf and hearing song-signers have become prevalent across video-sharing
platforms, but translating songs into sign language remains cumbersome and
inaccessible. Our formative study revealed the challenges song-signers face,
including semantic, syntactic, expressive, and rhythmic considerations in
translations. We present ELMI, an accessible song-signing tool that assists in
translating lyrics into sign language. ELMI enables users to edit glosses
line-by-line, with real-time synced lyric and music video snippets. Users can
also chat with a large language model-driven AI to discuss meaning, glossing,
emoting, and timing. Through an exploratory study with 13 song-signers, we
examined how ELMI facilitates their workflows and how song-signers leverage and
receive an LLM-driven chat for translation. Participants successfully adopted
ELMI to song-signing, with active discussions throughout. They also reported
improved confidence and independence in their translations, finding ELMI
encouraging, constructive, and informative. We discuss research and design
implications for accessible and culturally sensitive song-signing translation
tools.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages excluding reference and appendix. Accepted at ACM CHI 2025.
  https://naver-ai.github.io/elmi</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Adapting Large Language Models for Character-based Augmentative and
  Alternative Communication 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.10582v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.10582v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dylan Gaines, Keith Vertanen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Users of Augmentative and Alternative Communication (AAC) may write
letter-by-letter via an interface that uses a character language model.
However, most state-of-the-art large pretrained language models predict subword
tokens of variable length. We investigate how to practically use such models to
make accurate and efficient character predictions. We fine-tune models using a
large dataset of sentences we curated in which each sentence is rated according
to how useful it might be for spoken or written AAC communication. We find that
using an algorithm to produce character predictions from a subword large
language model provides more accurate predictions than adding a classification
layer or using a byte-level model. We also find that our domain adaptation
procedure is effective at improving model performance on simple, conversational
text.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SWEPO: Simultaneous Weighted Preference Optimization for Group
  Contrastive Alignment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.04628v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.04628v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Taneesh Gupta, Rahul Madhavan, Xuchao Zhang, Chetan Bansal, Saravan Rajmohan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Direct Preference Optimization (DPO) has proven effective in aligning large
language models with human preferences but is often constrained to pairwise
comparisons -- overlooking additional positive and negative responses that are
commonly available in real-world settings. We propose Simultaneous Weighted
Preference Optimization (SWEPO), which incorporates multiple responses per
query and prioritizes those that deviate most from the average reward. This
deviation-based weighting focuses training on the most informative outliers,
akin to a built-in curriculum. Theoretically, we prove that such
multi-preference sampling lowers alignment bias, bounding the expected
deviation from the true acceptable-response distribution at a rate of
$\mathcal{O}(\tfrac{1}{\sqrt{k}})$. Empirically, SWEPO outperforms
state-of-the-art baselines on the Ultra-Feedback dataset and demonstrates
substantial improvements over DPO and InfoNCA, yielding boosts of up to $\sim
4$% on length-controlled win-rate on AlpacaEval.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ProjectTest: A Project-level LLM Unit Test Generation Benchmark and
  Impact of Error Fixing Mechanisms 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.06556v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.06556v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yibo Wang, Congying Xia, Wenting Zhao, Jiangshu Du, Chunyu Miao, Zhongfen Deng, Philip S. Yu, Chen Xing
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Unit test generation has become a promising and important use case of LLMs.
However, existing evaluation benchmarks for assessing LLM unit test generation
capabilities focus on function- or class-level code rather than more practical
and challenging project-level codebases. To address such limitation, we propose
ProjectTest, a project-level benchmark for unit test generation covering
Python, Java, and JavaScript. ProjectTest features 20 moderate-sized and
high-quality projects per language. We evaluate nine frontier LLMs on
ProjectTest and the results show that all frontier LLMs tested exhibit moderate
performance on ProjectTest on Python and Java, highlighting the difficulty of
ProjectTest. We also conduct a thorough error analysis, which shows that even
frontier LLMs, such as Claude-3.5-Sonnet, have significant basic yet critical
errors, including compilation and cascade errors. Motivated by this
observation, we further evaluate all frontier LLMs under manual error-fixing
and self-error-fixing scenarios to assess their potential when equipped with
error-fixing mechanisms. Our code and dataset is available at
\href{https://github.com/YiboWANG214/ProjectTest}{ProjectTest}.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Training Turn-by-Turn Verifiers for Dialogue Tutoring Agents: The
  Curious Case of LLMs as Your Coding Tutors 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.13311v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.13311v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jian Wang, Yinpei Dai, Yichi Zhang, Ziqiao Ma, Wenjie Li, Joyce Chai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Intelligent tutoring agents powered by large language models (LLMs) have been
increasingly explored to deliver personalized guidance in areas such as
language learning and science education. However, their capabilities in guiding
users to solve complex real-world tasks remain underexplored. To address this
limitation, in this work, we focus on coding tutoring, a challenging problem
that requires tutors to proactively guide students toward completing predefined
coding tasks. We propose a novel agent workflow, Trace-and-Verify (TRAVER),
which combines knowledge tracing to estimate a student's knowledge state and
turn-by-turn verification to ensure effective guidance toward task completion.
We introduce DICT, an automatic evaluation protocol that assesses tutor agents
holistically using controlled student simulation and code generation tests.
Extensive experiments reveal the challenges of coding tutoring and demonstrate
that TRAVER achieves a significantly higher success rate. Although we use code
tutoring as an example in this paper, our results and findings can be extended
beyond coding, providing valuable insights into advancing tutoring agents for a
variety of tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Everyday Speech in the Indian Subcontinent 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.10508v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.10508v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Utkarsh P
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  India has 1369 languages of which 22 are official. About 13 different scripts
are used to represent these languages. A Common Label Set (CLS) was developed
based on phonetics to address the issue of large vocabulary of units required
in the End-to-End (E2E) framework for multilingual synthesis. The Indian
language text is first converted to CLS. This approach enables seamless code
switching across 13 Indian languages and English in a given native speaker's
voice, which corresponds to everyday speech in the Indian subcontinent, where
the population is multilingual.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>3 Pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Data-Constrained Synthesis of Training Data for De-Identification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.14677v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.14677v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Thomas Vakili, Aron Henriksson, Hercules Dalianis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Many sensitive domains -- such as the clinical domain -- lack widely
available datasets due to privacy risks. The increasing generative capabilities
of large language models (LLMs) have made synthetic datasets a viable path
forward. In this study, we domain-adapt LLMs to the clinical domain and
generate synthetic clinical texts that are machine-annotated with tags for
personally identifiable information using capable encoder-based NER models. The
synthetic corpora are then used to train synthetic NER models. The results show
that training NER models using synthetic corpora incurs only a small drop in
predictive performance. The limits of this process are investigated in a
systematic ablation study -- using both Swedish and Spanish data. Our analysis
shows that smaller datasets can be sufficient for domain-adapting LLMs for data
synthesis. Instead, the effectiveness of this process is almost entirely
contingent on the performance of the machine-annotating NER models trained
using the original data.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Reasoning based on symbolic and parametric knowledge bases: a <span class="highlight-title">survey</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.01030v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.01030v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mayi Xu, Yunfeng Ning, Yongqi Li, Jianhao Chen, Jintao Wen, Yao Xiao, Shen Zhou, Birong Pan, Zepeng Bao, Xin Miao, Hankun Kang, Ke Sun, Tieyun Qian
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reasoning is fundamental to human intelligence, and critical for
problem-solving, decision-making, and critical thinking. Reasoning refers to
drawing new conclusions based on existing knowledge, which can support various
applications like clinical diagnosis, basic education, and financial analysis.
Though a good number of surveys have been proposed for reviewing
reasoning-related methods, none of them has systematically investigated these
methods from the viewpoint of their dependent knowledge base. Both the
scenarios to which the knowledge bases are applied and their storage formats
are significantly different. Hence, investigating reasoning methods from the
knowledge base perspective helps us better understand the challenges and future
directions. To fill this gap, this paper first classifies the knowledge base
into symbolic and parametric ones. The former explicitly stores information in
human-readable symbols, and the latter implicitly encodes knowledge within
parameters. Then, we provide a comprehensive overview of reasoning methods
using symbolic knowledge bases, parametric knowledge bases, and both of them.
Finally, we identify the future direction toward enhancing reasoning
capabilities to bridge the gap between human and machine intelligence.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>There are imperfections in some parts of the paper, which may lead to
  misunderstandings among readers. To be rigorous, we apply for the withdrawal
  of this paper.</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Unveiling Scoring Processes: Dissecting the Differences between LLMs and
  Human Graders in Automatic Scoring 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.18328v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.18328v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xuansheng Wu, Padmaja Pravin Saraf, Gyeonggeon Lee, Ehsan Latif, Ninghao Liu, Xiaoming Zhai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have demonstrated strong potential in performing
automatic scoring for constructed response assessments. While constructed
responses graded by humans are usually based on given grading rubrics, the
methods by which LLMs assign scores remain largely unclear. It is also
uncertain how closely AI's scoring process mirrors that of humans or if it
adheres to the same grading criteria. To address this gap, this paper uncovers
the grading rubrics that LLMs used to score students' written responses to
science tasks and their alignment with human scores. We also examine whether
enhancing the alignments can improve scoring accuracy. Specifically, we prompt
LLMs to generate analytic rubrics that they use to assign scores and study the
alignment gap with human grading rubrics. Based on a series of experiments with
various configurations of LLM settings, we reveal a notable alignment gap
between human and LLM graders. While LLMs can adapt quickly to scoring tasks,
they often resort to shortcuts, bypassing deeper logical reasoning expected in
human grading. We found that incorporating high-quality analytical rubrics
designed to reflect human grading logic can mitigate this gap and enhance LLMs'
scoring accuracy. These results underscore the need for a nuanced approach when
applying LLMs in science education and highlight the importance of aligning LLM
outputs with human expectations to ensure efficient and accurate automatic
scoring.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by Technology, Knowledge, and Learning (TKNL)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ From Priest to Doctor: Domain Adaptation for Low-Resource Neural Machine
  Translation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.00966v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.00966v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ali Marashian, Enora Rice, Luke Gessler, Alexis Palmer, Katharina von der Wense
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Many of the world's languages have insufficient data to train high-performing
general neural machine translation (NMT) models, let alone domain-specific
models, and often the only available parallel data are small amounts of
religious texts. Hence, domain adaptation (DA) is a crucial issue faced by
contemporary NMT and has, so far, been underexplored for low-resource
languages. In this paper, we evaluate a set of methods from both low-resource
NMT and DA in a realistic setting, in which we aim to translate between a
high-resource and a low-resource language with access to only: a) parallel
Bible data, b) a bilingual dictionary, and c) a monolingual target-domain
corpus in the high-resource language. Our results show that the effectiveness
of the tested methods varies, with the simplest one, DALI, being most
effective. We follow up with a small human evaluation of DALI, which shows that
there is still a need for more careful investigation of how to accomplish DA
for low-resource NMT.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Flow of Reasoning:Training LLMs for Divergent Problem Solving with
  Minimal Examples 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.05673v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.05673v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fangxu Yu, Lai Jiang, Haoqiang Kang, Shibo Hao, Lianhui Qin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The ability to generate diverse solutions to a given problem is a hallmark of
human creativity. This divergent reasoning is also crucial for machines,
enhancing their robustness and enabling them to assist humans in many
applications such as scientific discovery. However, existing approaches to
multi-step reasoning with large language models (LLMs) have mostly focused only
on reasoning accuracy, without further discovering more diverse valid
solutions. For example, supervised fine-tuning can improve LLM reasoning
quality, but requires extensive supervised data to capture the full range of
possible solutions. Reward-maximization reinforcement learning aims to find
limited highest-reward solutions while neglecting the solution diversity. To
fill this gap, we propose Flow of Reasoning (FoR), an efficient
diversity-seeking LLM finetuning method aimed at improving reasoning quality
and diversity with minimal data. FoR formulates multi-step LLM reasoning as a
Markovian flow on a DAG-structured reasoning graph. This formulation allows us
to incorporate and adapt principled GFlowNet approaches, for finetuning LLMs to
sample divergent paths with probabilities proportional to the (unnormalized)
reward of target problems. Extensive experiments show that, with limited
training examples (e.g., 15 examples), FoR enables the discovery of diverse,
creative, high-quality solutions, greatly outperforming a wide range of
existing inference and training methods across six challenging reasoning tasks,
including BlocksWorld (embodied reasoning), Game24 (math puzzle solving),
Rubik's Cube (spatial reasoning), 1D-ARC (abstraction reasoning), GSM8k (math
reasoning), and ProntoQA (logical reasoning). Code is available at
https://github.com/Yu-Fangxu/FoR.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Reverb: Open-Source ASR and Diarization from Rev 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.03930v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.03930v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nishchal Bhandari, Danny Chen, Miguel Ángel del Río Fernández, Natalie Delworth, Jennifer Drexler Fox, Migüel Jetté, Quinten McNamara, Corey Miller, Ondřej Novotný, Ján Profant, Nan Qin, Martin Ratajczak, Jean-Philippe Robichaud
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Today, we are open-sourcing our core speech recognition and diarization
models for non-commercial use. We are releasing both a full production pipeline
for developers as well as pared-down research models for experimentation. Rev
hopes that these releases will spur research and innovation in the fast-moving
domain of voice technology. The speech recognition models released today
outperform all existing open source speech recognition models across a variety
of long-form speech recognition domains.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ NEAT: Nonlinear Parameter-efficient Adaptation of <span class="highlight-title">Pre-train</span>ed Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01870v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01870v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yibo Zhong, Haoxiang Jiang, Lincan Li, Ryumei Nakada, Tianci Liu, Linjun Zhang, Huaxiu Yao, Haoyu Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Fine-tuning pre-trained models often yields state-of-the-art performance but
is computationally expensive when updating all parameters. Parameter-efficient
fine-tuning (PEFT) methods, such as Low-Rank Adaptation (LoRA), address this by
freezing pre-trained weights and introducing low-rank matrices. However,
because LoRA relies on low-rank decomposition, it struggles to capture complex
nonlinear dynamics and optimal optimization trajectories, resulting in a
performance gap relative to full fine-tuning and inefficient parameter
utilization. To overcome these issues, we propose NEAT, a nonlinear PEFT
approach that employs a lightweight neural network to learn a nonlinear
transformation of the pre-trained weights, thereby better approximating
cumulative weight updates. Our theoretical analysis shows that NEAT achieves
greater efficiency than LoRA while maintaining equivalent expressivity.
Extensive experiments on four benchmarks and over twenty datasets demonstrate
that NEAT significantly outperforms state-of-the-art baselines in both vision
and text tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ NaturalReasoning: Reasoning in the Wild with 2.8M Challenging Questions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.13124v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.13124v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Weizhe Yuan, Jane Yu, Song Jiang, Karthik Padthe, Yang Li, Dong Wang, Ilia Kulikov, Kyunghyun Cho, Yuandong Tian, Jason E Weston, Xian Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Scaling reasoning capabilities beyond traditional domains such as math and
coding is hindered by the lack of diverse and high-quality questions. To
overcome this limitation, we introduce a scalable approach for generating
diverse and challenging reasoning questions, accompanied by reference answers.
We present NaturalReasoning, a comprehensive dataset comprising 2.8 million
questions that span multiple domains, including STEM fields (e.g., Physics,
Computer Science), Economics, Social Sciences, and more. We demonstrate the
utility of the questions in NaturalReasoning through knowledge distillation
experiments which show that NaturalReasoning can effectively elicit and
transfer reasoning capabilities from a strong teacher model. Furthermore, we
demonstrate that NaturalReasoning is also effective for unsupervised
self-training using external reward models or self-rewarding.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Dataset at https://huggingface.co/datasets/facebook/natural_reasoning</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Exploring and Controlling Diversity in LLM-Agent Conversation <span class="chip">AAAI 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.21102v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.21102v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        KuanChao Chu, Yi-Pei Chen, Hideki Nakayama
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Controlling diversity in LLM-agent world simulations is essential for
maintaining stability in structured tasks while enabling variation where
creativity is needed. However, we observe that dialogue diversity declines
significantly over long-term simulation. To investigate the role of prompt
design in conversational diversity, we modularized the utterance generation
prompt and found that reducing the given information leads to more diverse
outputs. Based on this insight, we propose Adaptive Prompt Pruning (APP), a
novel method that allows users to control diversity through a single parameter,
lambda. APP dynamically prunes the utterance generation prompt based on their
attention weights and is compatible with traditional diversity control
techniques. We demonstrate that APP effectively controls output diversity
through extensive experiments, and propose a method to balance the control
trade-offs. Additionally, we provide an in-depth analysis to offer insights
into optimizing diversity control in multi-agent simulation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for the AAAI 2025 Workshop on Advancing LLM-Based
  Multi-Agent Collaboration (v1); updated version (v2)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Fully automatic extraction of morphological traits from the Web: utopia
  or reality? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.17179v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.17179v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Diego Marcos, Robert van de Vlasakker, Ioannis N. Athanasiadis, Pierre Bonnet, Hervé Goeau, Alexis Joly, W. Daniel Kissling, César Leblanc, André S. J. van Proosdij, Konstantinos P. Panousis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Plant morphological traits, their observable characteristics, are fundamental
to understand the role played by each species within their ecosystem. However,
compiling trait information for even a moderate number of species is a
demanding task that may take experts years to accomplish. At the same time,
massive amounts of information about species descriptions is available online
in the form of text, although the lack of structure makes this source of data
impossible to use at scale. To overcome this, we propose to leverage recent
advances in large language models (LLMs) and devise a mechanism for gathering
and processing information on plant traits in the form of unstructured textual
descriptions, without manual curation. We evaluate our approach by
automatically replicating three manually created species-trait matrices. Our
method managed to find values for over half of all species-trait pairs, with an
F1-score of over 75%. Our results suggest that large-scale creation of
structured trait databases from unstructured online text is currently feasible
thanks to the information extraction capabilities of LLMs, being limited by the
availability of textual descriptions covering all the traits of interest.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Giving AI Personalities Leads to More Human-Like Reasoning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.14155v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.14155v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Animesh Nighojkar, Bekhzodbek Moydinboyev, My Duong, John Licato
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In computational cognitive modeling, capturing the full spectrum of human
judgment and decision-making processes, beyond just optimal behaviors, is a
significant challenge. This study explores whether Large Language Models (LLMs)
can emulate the breadth of human reasoning by predicting both intuitive, fast
System 1 and deliberate, slow System 2 processes. We investigate the potential
of AI to mimic diverse reasoning behaviors across a human population,
addressing what we call the "full reasoning spectrum problem". We designed
reasoning tasks using a novel generalization of the Natural Language Inference
(NLI) format to evaluate LLMs' ability to replicate human reasoning. The
questions were crafted to elicit both System 1 and System 2 responses. Human
responses were collected through crowd-sourcing and the entire distribution was
modeled, rather than just the majority of the answers. We used
personality-based prompting inspired by the Big Five personality model to
elicit AI responses reflecting specific personality traits, capturing the
diversity of human reasoning, and exploring how personality traits influence
LLM outputs. Combined with genetic algorithms to optimize the weighting of
these prompts, this method was tested alongside traditional machine learning
models. The results show that LLMs can mimic human response distributions, with
open-source models like Llama and Mistral outperforming proprietary GPT models.
Personality-based prompting, especially when optimized with genetic algorithms,
significantly enhanced LLMs' ability to predict human response distributions,
suggesting that capturing suboptimal, naturalistic reasoning may require
modeling techniques incorporating diverse reasoning styles and psychological
profiles. The study concludes that personality-based prompting combined with
genetic algorithms is promising for enhancing AI's 'human-ness' in reasoning.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Is Free Self-Alignment Possible? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.03642v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.03642v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dyah Adila, Changho Shin, Yijing Zhang, Frederic Sala
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Aligning pretrained language models (LMs) often requires large-scale
preference data and substantial computational resources. These costs become
even more prohibitive for multi-objective or pluralistic alignment. Is this
truly necessary? Can we perform efficient alignment using only internal model
capabilities, and without additional training? To answer this question, we
propose AlignEZ, a novel approach that leverages (1) self-generated preference
data and (2) representation editing to achieve cost-effective, efficient
alignment. By operating directly on learned representations, AlignEZ
independently targets different behavioral aspects without the overhead of
traditional alignment methods. Our experiments reveal that this cost-efficient
procedure improves performance across diverse tasks: up to 19.9% on general
alignment and 1.9% on challenging mathematical reasoning tasks, even when
starting from a strong base model. AlignEZ can also align models to multiple
objectives simultaneously, granting fine-grained control over multiple
preference axes. Finally, we show that AlignEZ can accelerate more expensive
alignment procedures--such as DPO--even under limited availability of
ground-truth preference data.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DarwinLM: Evolutionary Structured Pruning of Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.07780v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.07780v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shengkun Tang, Oliver Sieberling, Eldar Kurtic, Zhiqiang Shen, Dan Alistarh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have achieved significant success across various
NLP tasks. However, their massive computational costs limit their widespread
use, particularly in real-time applications. Structured pruning offers an
effective solution by compressing models and directly providing end-to-end
speed improvements, regardless of the hardware environment. Meanwhile,
different components of the model exhibit varying sensitivities towards
pruning, calling for \emph{non-uniform} model compression. However, a pruning
method should not only identify a capable substructure, but also account for
post-compression training. To this end, we propose \sysname, a method for
\emph{training-aware} structured pruning. \sysname builds upon an evolutionary
search process, generating multiple offspring models in each generation through
mutation, and selecting the fittest for survival. To assess the effect of
post-training, we incorporate a lightweight, multistep training process within
the offspring population, progressively increasing the number of tokens and
eliminating poorly performing models in each selection stage. We validate our
method through extensive experiments on Llama-2-7B, Llama-3.1-8B and
Qwen-2.5-14B-Instruct, achieving state-of-the-art performance for structured
pruning. For instance, \sysname surpasses ShearedLlama while requiring
$5\times$ less training data during post-compression training. Code is at:
https://github.com/IST-DASLab/DarwinLM
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code: https://github.com/IST-DASLab/DarwinLM</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Evolving Hate Speech Online: An Adaptive Framework for Detection and
  Mitigation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.10921v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.10921v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shiza Ali, Jeremy Blackburn, Gianluca Stringhini
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The proliferation of social media platforms has led to an increase in the
spread of hate speech, particularly targeting vulnerable communities.
Unfortunately, existing methods for automatically identifying and blocking
toxic language rely on pre-constructed lexicons, making them reactive rather
than adaptive. As such, these approaches become less effective over time,
especially when new communities are targeted with slurs not included in the
original datasets. To address this issue, we present an adaptive approach that
uses word embeddings to update lexicons and develop a hybrid model that adjusts
to emerging slurs and new linguistic patterns. This approach can effectively
detect toxic language, including intentional spelling mistakes employed by
aggressors to avoid detection. Our hybrid model, which combines BERT with
lexicon-based techniques, achieves an accuracy of 95% for most state-of-the-art
datasets. Our work has significant implications for creating safer online
environments by improving the detection of toxic content and proactively
updating the lexicon. Content Warning: This paper contains examples of hate
speech that may be triggering.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ How does a Language-Specific Tokenizer affect LLMs? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.12560v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.12560v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jean Seo, Jaeyoon Kim, SungJoo Byun, Hyopil Shin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The necessity of language-specific tokenizers intuitively appears crucial for
effective natural language processing, yet empirical analyses on their
significance and underlying reasons are lacking. This study explores how
language-specific tokenizers influence the behavior of Large Language Models
predominantly trained with English text data, through the case study of Korean.
The research unfolds in two main stages: (1) the development of a
Korean-specific extended tokenizer and (2) experiments to compare models with
the basic tokenizer and the extended tokenizer through various Next Token
Prediction tasks. Our in-depth analysis reveals that the extended tokenizer
decreases confidence in incorrect predictions during generation and reduces
cross-entropy in complex tasks, indicating a tendency to produce less
nonsensical outputs. Consequently, the extended tokenizer provides stability
during generation, potentially leading to higher performance in downstream
tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Iterative Repair with Weak Verifiers for Few-shot Transfer in KBQA with
  Unanswerability 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.14313v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.14313v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Riya Sawhney, Samrat Yadav, Indrajit Bhattacharya,  Mausam
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Real-world applications of KBQA require models to handle unanswerable
questions with a limited volume of in-domain labeled training data. We propose
the novel task of few-shot transfer for KBQA with unanswerable questions and
contribute two new datasets for performance evaluation. We present FUn-FuSIC -
a novel solution for our task that extends FuSIC KBQA, the state-of-the-art
few-shot transfer model for answerable-only KBQA. We first note that
FuSIC-KBQA's iterative repair makes a strong assumption that all questions are
unanswerable. As a remedy, we propose Feedback for Unanswerability (FUn), which
uses iterative repair using feedback from a suite of strong and weak verifiers,
and an adaptation of self consistency for unanswerabilty to better assess the
answerability of a question. Our experiments show that FUn-FuSIC significantly
outperforms suitable adaptations of multiple LLM based and supervised SoTA
models on our task, while establishing a new SoTA for answerable few-shot
transfer as well.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Do Large Language Models Truly Understand Geometric Structures? <span class="chip">ICLR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.13773v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.13773v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaofeng Wang, Yiming Wang, Wenhong Zhu, Rui Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Geometric ability is a significant challenge for large language models (LLMs)
due to the need for advanced spatial comprehension and abstract thinking.
Existing datasets primarily evaluate LLMs on their final answers, but they
cannot truly measure their true understanding of geometric structures, as LLMs
can arrive at correct answers by coincidence. To fill this gap, we introduce
the GeomRel dataset, designed to evaluate LLMs' understanding of geometric
structures by isolating the core step of geometric relationship identification
in problem-solving. Using this benchmark, we conduct thorough evaluations of
diverse LLMs and identify key limitations in understanding geometric
structures. We further propose the Geometry Chain-of-Thought (GeoCoT) method,
which enhances LLMs' ability to identify geometric relationships, resulting in
significant performance improvements.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ICLR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Comparing zero-shot self-explanations with human rationales in text
  classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.03296v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.03296v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Stephanie Brandl, Oliver Eberle
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Instruction-tuned LLMs are able to provide an explanation about their output
to users by generating self-explanations. These do not require gradient
computations or the application of possibly complex XAI methods. In this paper,
we analyse whether this ability results in a good explanation. We evaluate
self-explanations in the form of input rationales with respect to their
plausibility to humans as well as their faithfulness to models. We study two
text classification tasks: sentiment classification and forced labour
detection, i.e., identifying pre-defined risk indicators of forced labour. In
addition to English, we include Danish and Italian translations of the
sentiment classification task and compare self-explanations to human
annotations for all samples. To allow for direct comparisons, we also compute
post-hoc feature attribution, i.e., layer-wise relevance propagation (LRP) and
analyse 4 LLMs. We show that self-explanations align more closely with human
annotations compared to LRP, while maintaining a comparable level of
faithfulness. This finding suggests that self-explanations indeed provide good
explanations for text classification.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>preprint</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ <span class="highlight-title">BERT</span>time Stories: Investigating the Role of Synthetic Story Data in
  Language <span class="highlight-title">Pre-train</span>ing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.15365v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.15365v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nikitas Theodoropoulos, Giorgos Filandrianos, Vassilis Lyberatos, Maria Lymperaiou, Giorgos Stamou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We describe our contribution to the Strict and Strict-Small tracks of the 2nd
iteration of the BabyLM Challenge. The shared task is centered around efficient
pre-training given data constraints motivated by human development. In
response, we study the effect of synthetic story data in language pre-training
using TinyStories: a recently introduced dataset of short stories. Initially,
we train GPT-Neo models on subsets of TinyStories, while varying the amount of
available data. We find that, even with access to less than 100M words, the
models are able to generate high-quality, original completions to a given
story, and acquire substantial linguistic knowledge. To measure the effect of
synthetic story data, we train LTG-BERT encoder models on a combined dataset
of: a subset of TinyStories, story completions generated by GPT-Neo, and a
subset of the BabyLM dataset. Our experimentation reveals that synthetic data
can occasionally offer modest gains, but overall have a negative influence on
linguistic understanding. Our work offers an initial study on synthesizing
story data in low resource settings and underscores their potential for
augmentation in data-constrained language modeling. We publicly release our
models and implementation on our GitHub.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CHBench: A Chinese <span class="highlight-title">Dataset</span> for Evaluating Health in Large Language
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.15766v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.15766v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chenlu Guo, Nuo Xu, Yi Chang, Yuan Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the rapid development of large language models (LLMs), assessing their
performance on health-related inquiries has become increasingly essential. The
use of these models in real-world contexts-where misinformation can lead to
serious consequences for individuals seeking medical advice and
support-necessitates a rigorous focus on safety and trustworthiness. In this
work, we introduce CHBench, the first comprehensive safety-oriented Chinese
health-related benchmark designed to evaluate LLMs' capabilities in
understanding and addressing physical and mental health issues with a safety
perspective across diverse scenarios. CHBench comprises 6,493 entries on mental
health and 2,999 entries on physical health, spanning a wide range of topics.
Our extensive evaluations of four popular Chinese LLMs highlight significant
gaps in their capacity to deliver safe and accurate health information,
underscoring the urgent need for further advancements in this critical domain.
The code is available at https://github.com/TracyGuo2001/CHBench.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Adaptive Retrieval Without Self-Knowledge? Bringing Uncertainty Back
  Home 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.12835v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.12835v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Viktor Moskvoretskii, Maria Lysyuk, Mikhail Salnikov, Nikolay Ivanov, Sergey Pletenev, Daria Galimzianova, Nikita Krayko, Vasily Konovalov, Irina Nikishina, Alexander Panchenko
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Retrieval Augmented Generation (RAG) improves correctness of Question
Answering (QA) and addresses hallucinations in Large Language Models (LLMs),
yet greatly increase computational costs. Besides, RAG is not always needed as
may introduce irrelevant information. Recent adaptive retrieval methods
integrate LLMs' intrinsic knowledge with external information appealing to LLM
self-knowledge, but they often neglect efficiency evaluations and comparisons
with uncertainty estimation techniques. We bridge this gap by conducting a
comprehensive analysis of 35 adaptive retrieval methods, including 8 recent
approaches and 27 uncertainty estimation techniques, across 6 datasets using 10
metrics for QA performance, self-knowledge, and efficiency. Our findings show
that uncertainty estimation techniques often outperform complex pipelines in
terms of efficiency and self-knowledge, while maintaining comparable QA
performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The code and data are at https://github.com/s-nlp/AdaRAGUE</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Post-edits Are Preferences Too 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02320v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02320v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nathaniel Berger, Miriam Exel, Matthias Huck, Stefan Riezler
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Preference Optimization (PO) techniques are currently one of the state of the
art techniques for fine-tuning large language models (LLMs) on pairwise
preference feedback from human annotators. However, in machine translation,
this sort of feedback can be difficult to solicit. Additionally, Kreutzer et
al. (2018) have shown that, for machine translation, pairwise preferences are
less reliable than other forms of human feedback, such as 5-point ratings.
  We examine post-edits to see if they can be a source of reliable human
preferences by construction. In PO, a human annotator is shown sequences $s_1$
and $s_2$ and asked for a preference judgment, %$s_1 > s_2$; while for
post-editing, editors create $s_1$ and know that it should be better than
$s_2$. We attempt to use these implicit preferences for PO and show that it
helps the model move towards post-edit-like hypotheses and away from machine
translation-like hypotheses. Furthermore, we show that best results are
obtained by pre-training the model with supervised fine-tuning (SFT) on
post-edits in order to promote post-edit-like hypotheses to the top output
ranks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To appear at the Ninth Conference on Machine Translation (WMT24)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Less is More: <span class="highlight-title">Pre-Train</span>ing Cross-Lingual Small-Scale Language Models
  with Cognitively-Plausible Curriculum Learning Strategies <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.22886v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.22886v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Suchir Salhan, Richard Diehl Martinez, Zébulon Goriely, Paula Buttery
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Curriculum Learning has been a popular strategy to improve the cognitive
plausibility of Small-Scale Language Models (SSLMs) in the BabyLM Challenge.
However, it has not led to considerable improvements over non-curriculum
models. We assess whether theoretical linguistic acquisition theories can be
used to specify more fine-grained curriculum learning strategies, creating
age-ordered corpora of Child-Directed Speech for four typologically distant
language families to implement SSLMs and acquisition-inspired curricula
cross-lingually. Comparing the success of three objective curricula (Growing,
Inwards and MMM) that precisely replicate the predictions of acquisition
theories on a standard SSLM architecture, we find fine-grained
acquisition-inspired curricula can outperform non-curriculum baselines and
performance benefits of curricula strategies in SSLMs can be derived by
specifying fine-grained language-specific curricula that precisely replicate
language acquisition theories.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>BabyLM Shared Task 2024 (Accepted, Poster), co-located in EMNLP 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Tailored Design of Audio-Visual Speech Recognition Models using
  Branchformers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.06606v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.06606v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        David Gimeno-Gómez, Carlos-D. Martínez-Hinarejos
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances in Audio-Visual Speech Recognition (AVSR) have led to
unprecedented achievements in the field, improving the robustness of this type
of system in adverse, noisy environments. In most cases, this task has been
addressed through the design of models composed of two independent encoders,
each dedicated to a specific modality. However, while recent works have
explored unified audio-visual encoders, determining the optimal cross-modal
architecture remains an ongoing challenge. Furthermore, such approaches often
rely on models comprising vast amounts of parameters and high computational
cost training processes. In this paper, we aim to bridge this research gap by
introducing a novel audio-visual framework. Our proposed method constitutes, to
the best of our knowledge, the first attempt to harness the flexibility and
interpretability offered by encoder architectures, such as the Branchformer, in
the design of parameter-efficient AVSR systems. To be more precise, the
proposed framework consists of two steps: first, estimating audio- and
video-only systems, and then designing a tailored audio-visual unified encoder
based on the layer-level branch scores provided by the modality-specific
models. Extensive experiments on English and Spanish AVSR benchmarks covering
multiple data conditions and scenarios demonstrated the effectiveness of our
proposed method. Even when trained on a moderate scale of data, our models
achieve competitive word error rates (WER) of approximately 2.5\% for English
and surpass existing approaches for Spanish, establishing a new benchmark with
an average WER of around 9.1\%. These results reflect how our tailored AVSR
system is able to reach state-of-the-art recognition rates while significantly
reducing the model complexity w.r.t. the prevalent approach in the field. Code
and pre-trained models are available at
https://github.com/david-gimeno/tailored-avsr.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted and under review for the Computer Speech and Language
  journal of Elsevier</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ TituLLMs: A Family of Bangla LLMs with Comprehensive Benchmarking 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11187v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11187v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shahriar Kabir Nahin, Rabindra Nath Nandi, Sagor Sarker, Quazi Sarwar Muhtaseem, Md Kowsher, Apu Chandraw Shill, Md Ibrahim, Mehadi Hasan Menon, Tareq Al Muntasir, Firoj Alam
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we present TituLLMs, the first large pretrained Bangla LLMs,
available in 1b and 3b parameter sizes. Due to computational constraints during
both training and inference, we focused on smaller models. To train TituLLMs,
we collected a pretraining dataset of approximately ~37 billion tokens. We
extended the Llama-3.2 tokenizer to incorporate language- and culture-specific
knowledge, which also enables faster training and inference. There was a lack
of benchmarking datasets to benchmark LLMs for Bangla. To address this gap, we
developed five benchmarking datasets. We benchmarked various LLMs, including
TituLLMs, and demonstrated that TituLLMs outperforms its initial multilingual
versions. However, this is not always the case, highlighting the complexities
of language adaptation. Our work lays the groundwork for adapting existing
multilingual open models to other low-resource languages. To facilitate broader
adoption and further research, we have made the TituLLMs models and
benchmarking datasets publicly available
(https://huggingface.co/collections/hishab/titulm-llama-family-6718d31fc1b83529276f490a).
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>LLMs, Benchmarking, Large Language Models, Bangla</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ AlphaMaze: Enhancing Large Language Models' Spatial Intelligence via
  GRPO 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.14669v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.14669v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alan Dao, Dinh Bach Vu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have demonstrated impressive capabilities in
language processing, yet they often struggle with tasks requiring genuine
visual spatial reasoning. In this paper, we introduce a novel two-stage
training framework designed to equip standard LLMs with visual reasoning
abilities for maze navigation. First, we leverage Supervised Fine Tuning (SFT)
on a curated dataset of tokenized maze representations to teach the model to
predict step-by-step movement commands. Next, we apply Group Relative Policy
Optimization (GRPO)-a technique used in DeepSeekR1-with a carefully crafted
reward function to refine the model's sequential decision-making and encourage
emergent chain-of-thought behaviors. Experimental results on synthetically
generated mazes show that while a baseline model fails to navigate the maze,
the SFT-trained model achieves 86% accuracy, and further GRPO fine-tuning
boosts accuracy to 93%. Qualitative analyses reveal that GRPO fosters more
robust and self-corrective reasoning, highlighting the potential of our
approach to bridge the gap between language models and visual spatial tasks.
These findings offer promising implications for applications in robotics,
autonomous navigation, and other domains that require integrated visual and
sequential reasoning.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Magnifier <span class="highlight-title">Prompt</span>: Tackling Multimodal Hallucination via Extremely Simple
  Instructions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.11701v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.11701v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuhan Fu, Ruobing Xie, Jiazhen Liu, Bangxiang Lan, Xingwu Sun, Zhanhui Kang, Xirong Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Hallucinations in multimodal large language models (MLLMs) hinder their
practical applications. To address this, we propose a Magnifier Prompt
(MagPrompt), a simple yet effective method to tackle hallucinations in MLLMs
via extremely simple instructions. MagPrompt is based on the following two key
principles, which guide the design of various effective prompts, demonstrating
robustness: (1) MLLMs should focus more on the image. (2) When there are
conflicts between the image and the model's inner knowledge, MLLMs should
prioritize the image. MagPrompt is training-free and can be applied to
open-source and closed-source models, such as GPT-4o and Gemini-pro. It
performs well across many datasets and its effectiveness is comparable or even
better than more complex methods like VCD. Furthermore, our prompt design
principles and experimental analyses provide valuable insights into multimodal
hallucination.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The proposed method does not work for up-to-date MLLMs.</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Chain-of-Action: Faithful and Multimodal Question Answering through
  Large Language Models <span class="chip">ICLR</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17359v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17359v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhenyu Pan, Haozheng Luo, Manling Li, Han Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a Chain-of-Action (CoA) framework for multimodal and
retrieval-augmented Question-Answering (QA). Compared to the literature, CoA
overcomes two major challenges of current QA applications: (i) unfaithful
hallucination that is inconsistent with real-time or domain facts and (ii) weak
reasoning performance over compositional information. Our key contribution is a
novel reasoning-retrieval mechanism that decomposes a complex question into a
reasoning chain via systematic prompting and pre-designed actions.
Methodologically, we propose three types of domain-adaptable `Plug-and-Play'
actions for retrieving real-time information from heterogeneous sources. We
also propose a multi-reference faith score (MRFS) to verify and resolve
conflicts in the answers. Empirically, we exploit both public benchmarks and a
Web3 case study to demonstrate the capability of CoA over other methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>International Conference on Learning Representations (ICLR) 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ From Text to Trajectory: Exploring Complex Constraint Representation and
  Decomposition in Safe Reinforcement Learning <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.08920v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.08920v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pusen Dong, Tianchen Zhu, Yue Qiu, Haoyi Zhou, Jianxin Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Safe reinforcement learning (RL) requires the agent to finish a given task
while obeying specific constraints. Giving constraints in natural language form
has great potential for practical scenarios due to its flexible transfer
capability and accessibility. Previous safe RL methods with natural language
constraints typically need to design cost functions manually for each
constraint, which requires domain expertise and lacks flexibility. In this
paper, we harness the dual role of text in this task, using it not only to
provide constraint but also as a training signal. We introduce the
Trajectory-level Textual Constraints Translator (TTCT) to replace the manually
designed cost function. Our empirical results demonstrate that TTCT effectively
comprehends textual constraint and trajectory, and the policies trained by TTCT
can achieve a lower violation rate than the standard cost function. Extra
studies are conducted to demonstrate that the TTCT has zero-shot transfer
capability to adapt to constraint-shift environments.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Mitigating Tail Narrowing in LLM Self-Improvement via Socratic-Guided
  Sampling <span class="chip">NAACL 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.00750v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.00750v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yiwen Ding, Zhiheng Xi, Wei He, Zhuoyuan Li, Yitao Zhai, Xiaowei Shi, Xunliang Cai, Tao Gui, Qi Zhang, Xuanjing Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Self-improvement methods enable large language models (LLMs) to generate
solutions themselves and iteratively train on filtered, high-quality
rationales. This process proves effective and reduces the reliance on human
supervision in LLMs' reasoning, but the performance soon plateaus. We delve
into the process and find that models tend to over-sample on easy queries and
under-sample on queries they have yet to master. As iterations proceed, this
imbalance in sampling is exacerbated, leading to a long-tail distribution where
solutions to difficult queries almost diminish. This phenomenon limits the
performance gain of self-improving models. A straightforward solution is
brute-force sampling to balance the distribution, which significantly raises
computational costs. In this paper, we introduce Guided Self-Improvement (GSI),
a strategy aimed at improving the efficiency of sampling challenging
heavy-tailed data. It leverages Socratic-style guidance signals to help LLM
reasoning with complex queries, reducing the exploration effort and minimizing
computational overhead. Experiments on four models across diverse mathematical
tasks show that GSI strikes a balance between performance and efficiency, while
also being effective on held-out tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to NAACL 2025 Main Conference. Codes are publicly available
  at https://github.com/Yiwen-Ding/Guided-Self-Improvement</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PPTAgent: Generating and Evaluating Presentations Beyond Text-to-Slides 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.03936v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.03936v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hao Zheng, Xinyan Guan, Hao Kong, Jia Zheng, Weixiang Zhou, Hongyu Lin, Yaojie Lu, Ben He, Xianpei Han, Le Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Automatically generating presentations from documents is a challenging task
that requires accommodating content quality, visual appeal, and structural
coherence. Existing methods primarily focus on improving and evaluating the
content quality in isolation, overlooking visual appeal and structural
coherence, which limits their practical applicability. To address these
limitations, we propose PPTAgent, which comprehensively improves presentation
generation through a two-stage, edit-based approach inspired by human
workflows. PPTAgent first analyzes reference presentations to extract
slide-level functional types and content schemas, then drafts an outline and
iteratively generates editing actions based on selected reference slides to
create new slides. To comprehensively evaluate the quality of generated
presentations, we further introduce PPTEval, an evaluation framework that
assesses presentations across three dimensions: Content, Design, and Coherence.
Results demonstrate that PPTAgent significantly outperforms existing automatic
presentation generation methods across all three dimensions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 23 figures, see https://github.com/icip-cas/PPTAgent for
  details</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ From Sub-Ability Diagnosis to Human-Aligned Generation: Bridging the Gap
  for Text Length Control via MARKERGEN 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.13544v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.13544v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Peiwen Yuan, Chuyi Tan, Shaoxiong Feng, Yiwei Li, Xinglin Wang, Yueqi Zhang, Jiayi Shi, Boyuan Pan, Yao Hu, Kan Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite the rapid progress of large language models (LLMs), their
length-controllable text generation (LCTG) ability remains below expectations,
posing a major limitation for practical applications. Existing methods mainly
focus on end-to-end training to reinforce adherence to length constraints.
However, the lack of decomposition and targeted enhancement of LCTG
sub-abilities restricts further progress. To bridge this gap, we conduct a
bottom-up decomposition of LCTG sub-abilities with human patterns as reference
and perform a detailed error analysis. On this basis, we propose MarkerGen, a
simple-yet-effective plug-and-play approach that:(1) mitigates LLM fundamental
deficiencies via external tool integration;(2) conducts explicit length
modeling with dynamically inserted markers;(3) employs a three-stage generation
scheme to better align length constraints while maintaining content quality.
Comprehensive experiments demonstrate that MarkerGen significantly improves
LCTG across various settings, exhibiting outstanding effectiveness and
generalizability.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Explanations of Deep Language Models Explain Language Representations in
  the Brain 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.14671v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.14671v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Maryam Rahimi, Yadollah Yaghoobzadeh, Mohammad Reza Daliri
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances in artificial intelligence have given rise to large language
models (LLMs) that not only achieve human-like performance but also share
computational principles with the brain's language processing mechanisms. While
previous research has primarily focused on aligning LLMs' internal
representations with neural activity, we introduce a novel approach that
leverages explainable AI (XAI) methods to forge deeper connections between the
two domains. Using attribution methods, we quantified how preceding words
contribute to an LLM's next-word predictions and employed these explanations to
predict fMRI recordings from participants listening to the same narratives. Our
findings demonstrate that attribution methods robustly predict brain activity
across the language network, surpassing traditional internal representations in
early language areas. This alignment is hierarchical: early-layer explanations
correspond to the initial stages of language processing in the brain, while
later layers align with more advanced stages. Moreover, the layers more
influential on LLM next-word prediction$\unicode{x2014}$those with higher
attribution scores$\unicode{x2014}$exhibited stronger alignment with neural
activity. This work establishes a bidirectional bridge between AI and
neuroscience. First, we demonstrate that attribution methods offer a powerful
lens for investigating the neural mechanisms of language comprehension,
revealing how meaning emerges from preceding context. Second, we propose using
brain alignment as a metric to evaluate the validity of attribution methods,
providing a framework for assessing their biological plausibility.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PiCO: Peer <span class="highlight-title">Review</span> in LLMs based on the Consistency Optimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.01830v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.01830v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kun-Peng Ning, Shuo Yang, Yu-Yang Liu, Jia-Yu Yao, Zhen-Hui Liu, Yong-Hong Tian, Yibing Song, Li Yuan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Existing large language models (LLMs) evaluation methods typically focus on
testing the performance on some closed-environment and domain-specific
benchmarks with human annotations. In this paper, we explore a novel
unsupervised evaluation direction, utilizing peer-review mechanisms to measure
LLMs automatically. In this setting, both open-source and closed-source LLMs
lie in the same environment, capable of answering unlabeled questions and
evaluating each other, where each LLM's response score is jointly determined by
other anonymous ones. To obtain the ability hierarchy among these models, we
assign each LLM a learnable capability parameter to adjust the final ranking.
We formalize it as a constrained optimization problem, intending to maximize
the consistency of each LLM's capabilities and scores. The key assumption
behind is that high-level LLM can evaluate others' answers more accurately than
low-level ones, while higher-level LLM can also achieve higher response scores.
Moreover, we propose three metrics called PEN, CIN, and LIS to evaluate the gap
in aligning human rankings. We perform experiments on multiple datasets with
these metrics, validating the effectiveness of the proposed approach.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PersonaMath: Boosting Mathematical Reasoning via Persona-Driven Data
  Augmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01504v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01504v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jing Luo, Longze Chen, Run Luo, Liang Zhu, Chang Ao, Jiaming Li, Yukun Chen, Xin Cheng, Wen Yang, Jiayuan Su, Ahmadreza Argha, Hamid Alinejad-Rokny, Chengming Li, Shiwen Ni, Min Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While closed-source Large Language Models (LLMs) demonstrate strong
mathematical problem-solving abilities, open-source models still face
challenges with such tasks. To bridge this gap, we propose a data augmentation
approach and introduce PersonaMathQA, a dataset derived from MATH and GSM8K, on
which we train the PersonaMath models. Our approach consists of two stages: the
first stage focuses on learning from Persona Diversification, and the second
stage emphasizes learning from Reflection. In the first stage, we regenerate
detailed chain-of-thought (CoT) solutions as instructions using a closed-source
LLM and introduce a persona-driven data augmentation technique. This technique
innovatively classifies personas based on occupations, significantly enhancing
the dataset's diversity and quality. In the second stage, we incorporate
reflection to fully leverage more challenging and valuable questions.
Evaluation of our PersonaMath models on MATH and GSM8K reveals that the
PersonaMath-7B model (based on Qwen2.5-7B) achieves an accuracy of 61.2% on
MATH and 87.8% on GSM8K, surpassing all baseline methods and achieving
state-of-the-art performance. Notably, our dataset contains only 128.9K data
points-merely 32.6% of MetaMathQA and 49.5% of MathInstruct-yet our model
outperforms these baselines, demonstrating the high quality and diversity of
our dataset, which enables more efficient model training. We open-source the
PersonaMathQA dataset, PersonaMath models, and our code for public usage.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Skill over Scale: The Case for Medium, Domain-Specific Models for SE 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.03268v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.03268v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Manisha Mukherjee, Vincent J. Hellendoorn
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in AI have sparked a trend in constructing large,
generalist language models that handle a multitude of tasks, including many
code-related ones. While these models are expensive to train and are often
closed-source, they have enjoyed broad adoption because they tend to outperform
smaller, domain-specific models of code. In this work, we argue that this is
not a foregone conclusion. We show that modestly sized domain-specific models
can outperform much larger ones on code labeling tasks, provided they are
trained to the same standards. Concretely, we focus on StackOverflow (SO),
which offers large volumes of aligned code and text data. We align established
best-practices for pre-training large language models with properties of SO as
a data source, especially using a large context window (2,048 tokens), coupled
with a powerful toolkit (Megatron-LM) to train two models: SOBertBase (125M
parameters) and SOBertLarge (762M parameters), at a budget of just $374 and
$1600 each. We compare the performance of our models with a prior
domain-specific model which did not adopt many of these practices
(BERTOverflow), as well two general-purpose BERT models and two models in
OpenAI's GPT series (GPT-3.5 and GPT-4). We study four labeling tasks: question
quality prediction, closed question prediction, NER and obsoletion prediction.
The final task is a new benchmark we introduce, on which we additionally
compare SOBert with a fine-tuned CodeLlama and StackLlama (models with 10x more
parameters than SOBertLarge). Our models consistently outperform all baselines.
In contrast, BertOverflow is outperformed by generalist models in most tasks.
These results demonstrate that pre-training both extensively and properly on
in-domain data can yield a powerful and affordable alternative to leveraging
closed-source general-purpose models. Both models are released to the public on
Hugging Face.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ RIDE: Enhancing Large Language Model Alignment through Restyled
  In-Context Learning Demonstration Exemplars 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11681v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11681v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuncheng Hua, Lizhen Qu, Zhuang Li, Hao Xue, Flora D. Salim, Gholamreza Haffari
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Alignment tuning is crucial for ensuring large language models (LLMs) behave
ethically and helpfully. Current alignment approaches require high-quality
annotations and significant training resources. This paper proposes a low-cost,
tuning-free method using in-context learning (ICL) to enhance LLM alignment.
Through an analysis of high-quality ICL demos, we identified style as a key
factor influencing LLM alignment capabilities and explicitly restyled ICL
exemplars based on this stylistic framework. Additionally, we combined the
restyled demos to achieve a balance between the two conflicting aspects of LLM
alignment--factuality and safety. We packaged the restyled examples as prompts
to trigger few-shot learning, improving LLM alignment. Compared to the best
baseline approach, with an average score of 5.00 as the maximum, our method
achieves a maximum 0.10 increase on the Alpaca task (from 4.50 to 4.60), a 0.22
enhancement on the Just-eval benchmark (from 4.34 to 4.56), and a maximum
improvement of 0.32 (from 3.53 to 3.85) on the MT-Bench dataset. We release the
code and data at https://github.com/AnonymousCode-ComputerScience/RIDE.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>38 pages, 2 figures, 20 tables; The paper is under review in ARR</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning to Keep a Promise: Scaling Language Model Decoding Parallelism
  with Learned Asynchronous Decoding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11517v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11517v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tian Jin, Ellie Y. Cheng, Zack Ankner, Nikunj Saunshi, Blake M. Elias, Amir Yazdanbakhsh, Jonathan Ragan-Kelley, Suvinay Subramanian, Michael Carbin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Decoding with autoregressive large language models (LLMs) traditionally
occurs sequentially, generating one token after another. An emerging line of
work explored parallel decoding by identifying and simultaneously generating
semantically independent chunks of LLM responses. However, these techniques
rely on hand-crafted heuristics tied to syntactic structures like lists and
paragraphs, making them rigid and imprecise. We present PASTA, a learning-based
system that teaches LLMs to identify semantic independence and express parallel
decoding opportunities in their own responses. At its core are PASTA-LANG and
its interpreter: PASTA-LANG is an annotation language that enables LLMs to
express semantic independence in their own responses; the language interpreter
acts on these annotations to orchestrate parallel decoding on-the-fly at
inference time. Through a two-stage finetuning process, we train LLMs to
generate PASTA-LANG annotations that optimize both response quality and
decoding speed. Evaluation on AlpacaEval, an instruction following benchmark,
shows that our approach Pareto-dominates existing methods in terms of decoding
speed and response quality; our results demonstrate geometric mean speedups
ranging from 1.21x to 1.93x with corresponding quality changes of +2.2% to
-7.1%, measured by length-controlled win rates against sequential decoding
baseline.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Investigating the (De)Composition Capabilities of Large Language Models
  in Natural-to-Formal Language Conversion <span class="chip">NAACL 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.14649v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.14649v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziyao Xu, Houfeng Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  To achieve generalized and robust natural-to-formal language conversion
(N2F), large language models (LLMs) need to have strong capabilities of
decomposition and composition in N2F when faced with an unfamiliar formal
language and be able to cope with compositional gaps and counter-intuitive
symbolic names. To investigate whether LLMs have this set of basic capabilities
in N2F, we propose the DEDC framework. This framework semi-automatically
performs sample and task construction, allowing decoupled evaluation of the set
of decomposition and composition capabilities of LLMs in N2F. Based on this
framework, we evaluate and analyze the most advanced LLMs, and the main
findings include that: (1) the LLMs are deficient in both decomposition and
composition; (2) the LLMs show a wide coverage of error types that can be
attributed to deficiencies in natural language understanding and the learning
and use of symbolic systems; (3) compositional gaps and counter-intuitive
symbolic names both affect the decomposition and composition of the LLMs. Our
work provides a new perspective for investigating the basic capabilities of
decomposition and composition of LLMs in N2F. The detailed analysis of
deficiencies and attributions can help subsequent improvements of LLMs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at NAACL 2025 main conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Revisiting Jailbreaking for Large Language Models: A Representation
  Engineering Perspective <span class="chip">COLING 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.06824v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.06824v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tianlong Li, Zhenghua Wang, Wenhao Liu, Muling Wu, Shihan Dou, Changze Lv, Xiaohua Wang, Xiaoqing Zheng, Xuanjing Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The recent surge in jailbreaking attacks has revealed significant
vulnerabilities in Large Language Models (LLMs) when exposed to malicious
inputs. While various defense strategies have been proposed to mitigate these
threats, there has been limited research into the underlying mechanisms that
make LLMs vulnerable to such attacks. In this study, we suggest that the
self-safeguarding capability of LLMs is linked to specific activity patterns
within their representation space. Although these patterns have little impact
on the semantic content of the generated text, they play a crucial role in
shaping LLM behavior under jailbreaking attacks. Our findings demonstrate that
these patterns can be detected with just a few pairs of contrastive queries.
Extensive experimentation shows that the robustness of LLMs against
jailbreaking can be manipulated by weakening or strengthening these patterns.
Further visual analysis provides additional evidence for our conclusions,
providing new insights into the jailbreaking phenomenon. These findings
highlight the importance of addressing the potential misuse of open-source LLMs
within the community.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by COLING 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Question-to-Question Retrieval for Hallucination-Free Knowledge Access:
  An Approach for Wikipedia and Wikidata Question Answering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.11301v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.11301v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Santhosh Thottingal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces an approach to question answering over knowledge bases
like Wikipedia and Wikidata by performing "question-to-question" matching and
retrieval from a dense vector embedding store. Instead of embedding document
content, we generate a comprehensive set of questions for each logical content
unit using an instruction-tuned LLM. These questions are vector-embedded and
stored, mapping to the corresponding content. Vector embedding of user queries
are then matched against this question vector store. The highest similarity
score leads to direct retrieval of the associated article content, eliminating
the need for answer generation. Our method achieves high cosine similarity ( >
0.9 ) for relevant question pairs, enabling highly precise retrieval. This
approach offers several advantages including computational efficiency, rapid
response times, and increased scalability. We demonstrate its effectiveness on
Wikipedia and Wikidata, including multimedia content through structured fact
retrieval from Wikidata, opening up new pathways for multimodal question
answering.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ BA-LoRA: Bias-Alleviating Low-Rank Adaptation to Mitigate Catastrophic
  Inheritance in Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.04556v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.04556v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yupeng Chang, Yi Chang, Yuan Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have demonstrated remarkable proficiency across
various natural language processing (NLP) tasks. However, adapting LLMs to
downstream applications requires computationally intensive and memory-demanding
fine-tuning procedures. To alleviate these burdens, parameter-efficient
fine-tuning (PEFT) techniques have emerged as a promising approach to tailor
LLMs with minimal computational overhead. While PEFT methods offer substantial
advantages, they do not fully address the pervasive issue of bias propagation
from pre-training data. This work introduces Bias-Alleviating Low-Rank
Adaptation (BA-LoRA), a novel PEFT method designed to counteract bias
inheritance. BA-LoRA incorporates three distinct regularization terms: (1) a
consistency regularizer, (2) a diversity regularizer, and (3) a singular value
decomposition regularizer. These regularizers aim to enhance the models'
consistency, diversity, and generalization capabilities during fine-tuning. We
conduct extensive experiments on natural language understanding (NLU) and
natural language generation (NLG) tasks using prominent LLMs such as LLaMA,
Mistral, and Gemma. The results demonstrate that BA-LoRA outperforms LoRA and
its state-of-the-art variants. Moreover, our method effectively mitigates the
adverse effects of pre-training bias, leading to more reliable and robust model
outputs. The code is available at https://github.com/cyp-jlu-ai/BA-LoRA.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>25 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Towards Efficient and Multifaceted Computer-assisted Pronunciation
  Training Leveraging Hierarchical Selective State Space Model and Decoupled
  Cross-entropy Loss <span class="chip">NAACL 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.07575v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.07575v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fu-An Chao, Berlin Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Prior efforts in building computer-assisted pronunciation training (CAPT)
systems often treat automatic pronunciation assessment (APA) and
mispronunciation detection and diagnosis (MDD) as separate fronts: the former
aims to provide multiple pronunciation aspect scores across diverse linguistic
levels, while the latter focuses instead on pinpointing the precise phonetic
pronunciation errors made by non-native language learners. However, it is
generally expected that a full-fledged CAPT system should perform both
functionalities simultaneously and efficiently. In response to this surging
demand, we in this work first propose HMamba, a novel CAPT approach that
seamlessly integrates APA and MDD tasks in parallel. In addition, we introduce
a novel loss function, decoupled cross-entropy loss (deXent), specifically
tailored for MDD to facilitate better-supervised learning for detecting
mispronounced phones, thereby enhancing overall performance. A comprehensive
set of empirical results on the speechocean762 benchmark dataset demonstrates
the effectiveness of our approach on APA. Notably, our proposed approach also
yields a considerable improvement in MDD performance over a strong baseline,
achieving an F1-score of 63.85%. Our codes are made available at
https://github.com/Fuann/hmamba
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to NAACL 2025 main conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Who Speaks Next? Multi-party AI Discussion Leveraging the Systematics of
  Turn-taking in Murder Mystery Games 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.04937v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.04937v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ryota Nonomura, Hiroki Mori
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-agent systems utilizing large language models (LLMs) have shown great
promise in achieving natural dialogue. However, smooth dialogue control and
autonomous decision making among agents still remain challenges. In this study,
we focus on conversational norms such as adjacency pairs and turn-taking found
in conversation analysis and propose a new framework called "Murder Mystery
Agents" that applies these norms to AI agents' dialogue control. As an
evaluation target, we employed the "Murder Mystery" game, a reasoning-type
table-top role-playing game that requires complex social reasoning and
information manipulation. In this game, players need to unravel the truth of
the case based on fragmentary information through cooperation and bargaining.
The proposed framework integrates next speaker selection based on adjacency
pairs and a self-selection mechanism that takes agents' internal states into
account to achieve more natural and strategic dialogue. To verify the
effectiveness of this new approach, we analyzed utterances that led to dialogue
breakdowns and conducted automatic evaluation using LLMs, as well as human
evaluation using evaluation criteria developed for the Murder Mystery game.
Experimental results showed that the implementation of the next speaker
selection mechanism significantly reduced dialogue breakdowns and improved the
ability of agents to share information and perform logical reasoning. The
results of this study demonstrate that the systematics of turn-taking in human
conversation are also effective in controlling dialogue among AI agents, and
provide design guidelines for more advanced multi-agent dialogue systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Knowledge Pyramid Construction for Multi-Level Retrieval-Augmented
  Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.21276v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.21276v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rubing Chen, Xulu Zhang, Jiaxin Wu, Wenqi Fan, Xiao-Yong Wei, Qing Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper addresses the need for improved precision in existing
knowledge-enhanced question-answering frameworks, specifically
Retrieval-Augmented Generation (RAG) methods that primarily focus on enhancing
recall. We propose a multi-layer knowledge pyramid approach within the RAG
framework to achieve a better balance between precision and recall. The
knowledge pyramid consists of three layers: Ontologies, Knowledge Graphs (KGs),
and chunk-based raw text. We employ cross-layer augmentation techniques for
comprehensive knowledge coverage and dynamic updates of the Ontology schema and
instances. To ensure compactness, we utilize cross-layer filtering methods for
knowledge condensation in KGs. Our approach, named PolyRAG, follows a waterfall
model for retrieval, starting from the top of the pyramid and progressing down
until a confident answer is obtained. We introduce two benchmarks for
domain-specific knowledge retrieval, one in the academic domain and the other
in the financial domain. The effectiveness of the methods has been validated
through comprehensive experiments by outperforming 19 SOTA methods. An
encouraging observation is that the proposed method has augmented the GPT-4,
providing 395% F1 gain by improving its performance from 0.1636 to 0.8109.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MIA-Bench: Towards Better Instruction Following Evaluation of Multimodal
  LLMs <span class="chip">ICLR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.01509v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.01509v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yusu Qian, Hanrong Ye, Jean-Philippe Fauconnier, Peter Grasch, Yinfei Yang, Zhe Gan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce MIA-Bench, a new benchmark designed to evaluate multimodal large
language models (MLLMs) on their ability to strictly adhere to complex
instructions. Our benchmark comprises a diverse set of 400 image-prompt pairs,
each crafted to challenge the models' compliance with layered instructions in
generating accurate responses that satisfy specific requested patterns.
Evaluation results from a wide array of state-of-the-art MLLMs reveal
significant variations in performance, highlighting areas for improvement in
instruction fidelity. Additionally, we create extra training data and explore
supervised fine-tuning to enhance the models' ability to strictly follow
instructions without compromising performance on other tasks. We hope this
benchmark not only serves as a tool for measuring MLLM adherence to
instructions, but also guides future developments in MLLM training methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at ICLR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ QUBE: Enhancing Automatic Heuristic Design via Quality-Uncertainty
  Balanced Evolution 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.20694v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.20694v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zijie Chen, Zhanchao Zhou, Yu Lu, Renjun Xu, Lili Pan, Zhenzhong Lan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Solving NP-hard problems traditionally relies on heuristics, yet manually
designing effective heuristics for complex problems remains a significant
challenge. While recent advancements like FunSearch have shown that large
language models (LLMs) can be integrated into evolutionary algorithms (EAs) for
heuristic design, their potential is hindered by limitations in balancing
exploitation and exploration. We introduce Quality-Uncertainty Balanced
Evolution (QUBE), a novel approach that enhances LLM+EA methods by redefining
the priority criterion within the FunSearch framework. QUBE employs the
Quality-Uncertainty Trade-off Criterion (QUTC), based on our proposed
Uncertainty-Inclusive Quality metric, to evaluate and guide the evolutionary
process. Through extensive experiments on challenging NP-complete problems,
QUBE demonstrates significant performance improvements over FunSearch and
baseline methods. Our code are available at
https://github.com/zzjchen/QUBE_code.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ I-MCTS: Enhancing Agentic AutoML via Introspective Monte Carlo Tree
  Search 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.14693v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.14693v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zujie Liang, Feng Wei, Wujiang Xu, Lin Chen, Yuxi Qian, Xinhui Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in large language models (LLMs) have shown remarkable
potential in automating machine learning tasks. However, existing LLM-based
agents often struggle with low-diversity and suboptimal code generation. While
recent work has introduced Monte Carlo Tree Search (MCTS) to address these
issues, limitations persist in the quality and diversity of thoughts generated,
as well as in the scalar value feedback mechanisms used for node selection. In
this study, we introduce Introspective Monte Carlo Tree Search (I-MCTS), a
novel approach that iteratively expands tree nodes through an introspective
process that meticulously analyzes solutions and results from parent and
sibling nodes. This facilitates a continuous refinement of the node in the
search tree, thereby enhancing the overall decision-making process.
Furthermore, we integrate a Large Language Model (LLM)-based value model to
facilitate direct evaluation of each node's solution prior to conducting
comprehensive computational rollouts. A hybrid rewarding mechanism is
implemented to seamlessly transition the Q-value from LLM-estimated scores to
actual performance scores. This allows higher-quality nodes to be traversed
earlier. Applied to the various ML tasks, our approach demonstrates a 6%
absolute improvement in performance compared to the strong open-source AutoML
agents, showcasing its effectiveness in enhancing agentic AutoML systems.
Resource available at https://github.com/jokieleung/I-MCTS
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Are Smarter LLMs Safer? Exploring Safety-Reasoning Trade-offs in
  <span class="highlight-title">Prompt</span>ing and Fine-Tuning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.09673v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.09673v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ang Li, Yichuan Mo, Mingjie Li, Yifei Wang, Yisen Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have demonstrated remarkable success across
various NLP benchmarks. However, excelling in complex tasks that require
nuanced reasoning and precise decision-making demands more than raw language
proficiency--LLMs must reason, i.e., think logically, draw from past
experiences, and synthesize information to reach conclusions and take action.
To enhance reasoning abilities, approaches such as prompting and fine-tuning
have been widely explored. While these methods have led to clear improvements
in reasoning, their impact on LLM safety remains less understood. In this work,
we investigate the interplay between reasoning and safety in LLMs. We highlight
the latent safety risks that arise as reasoning capabilities improve, shedding
light on previously overlooked vulnerabilities. At the same time, we explore
how reasoning itself can be leveraged to enhance safety, uncovering potential
mitigation strategies. By examining both the risks and opportunities in
reasoning-driven LLM safety, our study provides valuable insights for
developing models that are not only more capable but also more trustworthy in
real-world deployments.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ HiddenDetect: Detecting Jailbreak Attacks against Large Vision-Language
  Models via Monitoring Hidden States 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.14744v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.14744v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yilei Jiang, Xinyan Gao, Tianshuo Peng, Yingshui Tan, Xiaoyong Zhu, Bo Zheng, Xiangyu Yue
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The integration of additional modalities increases the susceptibility of
large vision-language models (LVLMs) to safety risks, such as jailbreak
attacks, compared to their language-only counterparts. While existing research
primarily focuses on post-hoc alignment techniques, the underlying safety
mechanisms within LVLMs remain largely unexplored. In this work , we
investigate whether LVLMs inherently encode safety-relevant signals within
their internal activations during inference. Our findings reveal that LVLMs
exhibit distinct activation patterns when processing unsafe prompts, which can
be leveraged to detect and mitigate adversarial inputs without requiring
extensive fine-tuning. Building on this insight, we introduce HiddenDetect, a
novel tuning-free framework that harnesses internal model activations to
enhance safety. Experimental results show that {HiddenDetect} surpasses
state-of-the-art methods in detecting jailbreak attacks against LVLMs. By
utilizing intrinsic safety-aware patterns, our method provides an efficient and
scalable solution for strengthening LVLM robustness against multimodal threats.
Our code will be released publicly at
https://github.com/leigest519/HiddenDetect.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Selective <span class="highlight-title">Prompt</span> Anchoring for Code Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.09121v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.09121v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuan Tian, Tianyi Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances in large language models (LLMs) have transformed software
development by automatically generating code from natural language. Yet
challenges remain in generating fully correct code that aligns with user
intent. Our study reveals that LLMs tend to pay less attention to user prompts
as more code tokens are generated. We hypothesize that this attention dilution
issue is an important reason for code generation errors. To mitigate this
issue, we propose Selective Prompt Anchoring (SPA) to guide code LLMs to pay
more attention to user intent when generating code. We evaluate SPA using six
base LLMs across six benchmarks. Our results demonstrate that SPA enhances
Pass@1 by up to 12.9%, consistently outperforming SOTA code generation methods
in all settings. Our code is available at
https://github.com/magic-YuanTian/Selective-Prompt-Anchoring.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ RLTHF: Targeted Human Feedback for LLM Alignment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.13417v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.13417v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yifei Xu, Tusher Chakraborty, Emre Kıcıman, Bibek Aryal, Eduardo Rodrigues, Srinagesh Sharma, Roberto Estevao, Maria Angels de Luis Balaguer, Jessica Wolk, Rafael Padilha, Leonardo Nunes, Shobana Balakrishnan, Songwu Lu, Ranveer Chandra
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Fine-tuning large language models (LLMs) to align with user preferences is
challenging due to the high cost of quality human annotations in Reinforcement
Learning from Human Feedback (RLHF) and the generalizability limitations of AI
Feedback. To address these challenges, we propose RLTHF, a human-AI hybrid
framework that combines LLM-based initial alignment with selective human
annotations to achieve full-human annotation alignment with minimal effort.
RLTHF identifies hard-to-annotate samples mislabeled by LLMs using a reward
model's reward distribution and iteratively enhances alignment by integrating
strategic human corrections while leveraging LLM's correctly labeled samples.
Evaluations on HH-RLHF and TL;DR datasets show that RLTHF reaches full-human
annotation-level alignment with only 6-7% of the human annotation effort.
Furthermore, models trained on RLTHF's curated datasets for downstream tasks
outperform those trained on fully human-annotated datasets, underscoring the
effectiveness of RLTHF's strategic data curation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Drift: Decoding-time Personalized Alignments with Implicit User
  Preferences 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.14289v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.14289v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Minbeom Kim, Kang-il Lee, Seongho Joo, Hwaran Lee, Kyomin Jung
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Personalized alignments for individual users have been a long-standing goal
in large language models (LLMs). We introduce Drift, a novel framework that
personalizes LLMs at decoding time with implicit user preferences. Traditional
Reinforcement Learning from Human Feedback (RLHF) requires thousands of
annotated examples and expensive gradient updates. In contrast, Drift
personalizes LLMs in a training-free manner, using only a few dozen examples to
steer a frozen model through efficient preference modeling. Our approach models
user preferences as a composition of predefined, interpretable attributes and
aligns them at decoding time to enable personalized generation. Experiments on
both a synthetic persona dataset (Perspective) and a real human-annotated
dataset (PRISM) demonstrate that Drift significantly outperforms RLHF baselines
while using only 50-100 examples. Our results and analysis show that Drift is
both computationally efficient and interpretable.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>19 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Aligning the Objective of LLM-based Program Repair <span class="chip">ICSE'25</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.08877v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.08877v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junjielong Xu, Ying Fu, Shin Hwei Tan, Pinjia He
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have achieved decent results on automated
program repair (APR). However, the next token prediction training objective of
decoder-only LLMs (e.g., GPT-4) is misaligned with the masked span prediction
objective of current infilling-style methods, which impedes LLMs from fully
leveraging pre-trained knowledge for program repair. In addition, while some
LLMs can locate and repair bugs in certain functions using the related
artifacts (e.g., test cases), existing methods still depend on statement-level
fault localization methods to provide a list of buggy hunks for repair. This
restriction hinders LLMs from exploring potential patches beyond the given
locations.
  In this paper, we investigate a new approach to adapt LLMs to program repair.
Our core insight is that LLM's APR capability can be greatly improved by simply
aligning the output to their training objective and allowing them to refine the
whole program without first identifying faulty statements. Based on this
insight, we designed D4C, a straightforward prompting framework for APR. D4C
can repair 180 bugs correctly in Defects4J, with each patch being sampled only
10 times. This surpasses the SOTA APR methods with perfect fault localization
by 10% and reduces the patch sampling number by 90%. Our findings reveal that
(1) objective alignment is crucial for fully exploiting LLM's pre-trained
capability, and (2) replacing the traditional localize-buggy-hunks-then-repair
workflow with direct debugging is more effective for LLM-based APR methods.
Thus, we believe this paper introduces a new mindset for harnessing LLMs in
APR.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICSE'25</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Self-Updatable Large Language Models by Integrating Context into Model
  Parameters 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.00487v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.00487v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yu Wang, Xinshuang Liu, Xiusi Chen, Sean O'Brien, Junda Wu, Julian McAuley
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite significant advancements in large language models (LLMs), the rapid
and frequent integration of small-scale experiences, such as interactions with
surrounding objects, remains a substantial challenge. Two critical factors in
assimilating these experiences are (1) Efficacy: the ability to accurately
remember recent events; (2) Retention: the capacity to recall long-past
experiences. Current methods either embed experiences within model parameters
using continual learning, model editing, or knowledge distillation techniques,
which often struggle with rapid updates and complex interactions, or rely on
external storage to achieve long-term retention, thereby increasing storage
requirements. In this paper, we propose SELF-PARAM (Self-Updatable Large
Language Models with Parameter Integration). SELF-PARAM requires no extra
parameters while ensuring near-optimal efficacy and long-term retention. Our
method employs a training objective that minimizes the Kullback-Leibler (KL)
divergence between the predictions of an original model (with access to
contextual information) and a target model (without such access). By generating
diverse question-answer pairs related to the knowledge and minimizing the KL
divergence across this dataset, we update the target model to internalize the
knowledge seamlessly within its parameters. Evaluations on question-answering
and conversational recommendation tasks demonstrate that SELF-PARAM
significantly outperforms existing methods, even when accounting for non-zero
storage requirements. This advancement paves the way for more efficient and
scalable integration of experiences in large language models by embedding
knowledge directly into model parameters.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Humanity's Last Exam 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.14249v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.14249v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Long Phan, Alice Gatti, Ziwen Han, Nathaniel Li, Josephina Hu, Hugh Zhang, Chen Bo Calvin Zhang, Mohamed Shaaban, John Ling, Sean Shi, Michael Choi, Anish Agrawal, Arnav Chopra, Adam Khoja, Ryan Kim, Richard Ren, Jason Hausenloy, Oliver Zhang, Mantas Mazeika, Tung Nguyen, Daron Anderson, Imad Ali Shah, Mikhail Doroshenko, Alun Cennyth Stokes, Mobeen Mahmood, Jaeho Lee, Oleksandr Pokutnyi, Oleg Iskra, Jessica P. Wang, Robert Gerbicz, John-Clark Levin, Serguei Popov, Fiona Feng, Steven Y. Feng, Haoran Zhao, Michael Yu, Varun Gangal, Chelsea Zou, Zihan Wang, Mstyslav Kazakov, Geoff Galgon, Johannes Schmitt, Alvaro Sanchez, Yongki Lee, Will Yeadon, Scott Sauers, Marc Roth, Chidozie Agu, Søren Riis, Fabian Giska, Saiteja Utpala, Antrell Cheatom, Zachary Giboney, Gashaw M. Goshu, Sarah-Jane Crowson, Mohinder Maheshbhai Naiya, Noah Burns, Lennart Finke, Zerui Cheng, Hyunwoo Park, Francesco Fournier-Facio, Jennifer Zampese, John B. Wydallis, Ryan G. Hoerr, Mark Nandor, Tim Gehrunger, Jiaqi Cai, Ben McCarty, Jungbae Nam, Edwin Taylor, Jun Jin, Gautier Abou Loume, Hangrui Cao, Alexis C Garretson, Damien Sileo, Qiuyu Ren, Doru Cojoc, Pavel Arkhipov, Usman Qazi, Aras Bacho, Lianghui Li, Sumeet Motwani, Christian Schroeder de Witt, Alexei Kopylov, Johannes Veith, Eric Singer, Paolo Rissone, Jaehyeok Jin, Jack Wei Lun Shi, Chris G. Willcocks, Ameya Prabhu, Longke Tang, Kevin Zhou, Emily de Oliveira Santos, Andrey Pupasov Maksimov, Edward Vendrow, Kengo Zenitani, Joshua Robinson, Aleksandar Mikov, Julien Guillod, Yuqi Li, Ben Pageler, Joshua Vendrow, Vladyslav Kuchkin, Pierre Marion, Denis Efremov, Jayson Lynch, Kaiqu Liang, Andrew Gritsevskiy, Dakotah Martinez, Nick Crispino, Dimitri Zvonkine, Natanael Wildner Fraga, Saeed Soori, Ori Press, Henry Tang, Julian Salazar, Sean R. Green, Lina Brüssel, Moon Twayana, Aymeric Dieuleveut, T. Ryan Rogers, Wenjin Zhang, Ross Finocchio, Bikun Li, Jinzhou Yang, Arun Rao, Gabriel Loiseau, Mikhail Kalinin, Marco Lukas, Ciprian Manolescu, Nate Stambaugh, Subrata Mishra, Ariel Ghislain Kemogne Kamdoum, Tad Hogg, Alvin Jin, Carlo Bosio, Gongbo Sun, Brian P Coppola, Haline Heidinger, Rafael Sayous, Stefan Ivanov, Joseph M Cavanagh, Jiawei Shen, Joseph Marvin Imperial, Philippe Schwaller, Shaipranesh Senthilkuma, Andres M Bran, Andres Algaba, Brecht Verbeken, Kelsey Van den Houte, Lynn Van Der Sypt, David Noever, Lisa Schut, Ilia Sucholutsky, Evgenii Zheltonozhskii, Qiaochu Yuan, Derek Lim, Richard Stanley, Shankar Sivarajan, Tong Yang, John Maar, Julian Wykowski, Martí Oller, Jennifer Sandlin, Anmol Sahu, Cesare Giulio Ardito, Yuzheng Hu, Felipe Meneguitti Dias, Tobias Kreiman, Kaivalya Rawal, Tobias Garcia Vilchis, Yuexuan Zu, Martin Lackner, James Koppel, Jeremy Nguyen, Daniil S. Antonenko, Steffi Chern, Bingchen Zhao, Pierrot Arsene, Sergey Ivanov, Rafał Poświata, Chenguang Wang, Daofeng Li, Donato Crisostomi, Ali Dehghan, Andrea Achilleos, John Arnold Ambay, Benjamin Myklebust, Archan Sen, David Perrella, Nurdin Kaparov, Mark H Inlow, Allen Zang, Kalyan Ramakrishnan, Daniil Orel, Vladislav Poritski, Shalev Ben-David, Zachary Berger, Parker Whitfill, Michael Foster, Daniel Munro, Linh Ho, Dan Bar Hava, Aleksey Kuchkin, Robert Lauff, David Holmes, Frank Sommerhage, Anji Zhang, Richard Moat, Keith Schneider, Daniel Pyda, Zakayo Kazibwe, Mukhwinder Singh, Don Clarke, Dae Hyun Kim, Sara Fish, Veit Elser, Victor Efren Guadarrama Vilchis, Immo Klose, Christoph Demian, Ujjwala Anantheswaran, Adam Zweiger, Guglielmo Albani, Jeffery Li, Nicolas Daans, Maksim Radionov, Václav Rozhoň, Vincent Ginis, Ziqiao Ma, Christian Stump, Jacob Platnick, Volodymyr Nevirkovets, Luke Basler, Marco Piccardo, Niv Cohen, Virendra Singh, Josef Tkadlec, Paul Rosu, Alan Goldfarb, Piotr Padlewski, Stanislaw Barzowski, Kyle Montgomery, Aline Menezes, Arkil Patel, Zixuan Wang, Jamie Tucker-Foltz, Jack Stade, Declan Grabb, Tom Goertzen, Fereshteh Kazemi, Jeremiah Milbauer, Abhishek Shukla, Hossam Elgnainy, Yan Carlos Leyva Labrador, Hao He, Ling Zhang, Alan Givré, Hew Wolff, Gözdenur Demir, Muhammad Fayez Aziz, Younesse Kaddar, Ivar Ängquist, Yanxu Chen, Elliott Thornley, Robin Zhang, Jiayi Pan, Antonio Terpin, Niklas Muennighoff, Hailey Schoelkopf, Eric Zheng, Avishy Carmi, Jainam Shah, Ethan D. L. Brown, Kelin Zhu, Max Bartolo, Richard Wheeler, Andrew Ho, Shaul Barkan, Jiaqi Wang, Martin Stehberger, Egor Kretov, Peter Bradshaw, JP Heimonen, Kaustubh Sridhar, Zaki Hossain, Ido Akov, Yury Makarychev, Joanna Tam, Hieu Hoang, David M. Cunningham, Vladimir Goryachev, Demosthenes Patramanis, Michael Krause, Andrew Redenti, David Aldous, Jesyin Lai, Shannon Coleman, Jiangnan Xu, Sangwon Lee, Ilias Magoulas, Sandy Zhao, Ning Tang, Michael K. Cohen, Micah Carroll, Orr Paradise, Jan Hendrik Kirchner, Stefan Steinerberger, Maksym Ovchynnikov, Jason O. Matos, Adithya Shenoy, Michael Wang, Yuzhou Nie, Paolo Giordano, Philipp Petersen, Anna Sztyber-Betley, Paolo Faraboschi, Robin Riblet, Jonathan Crozier, Shiv Halasyamani, Antonella Pinto, Shreyas Verma, Prashant Joshi, Eli Meril, Zheng-Xin Yong, Allison Tee, Jérémy Andréoletti, Orion Weller, Raghav Singhal, Gang Zhang, Alexander Ivanov, Seri Khoury, Nils Gustafsson, Hamid Mostaghimi, Kunvar Thaman, Qijia Chen, Tran Quoc Khánh, Jacob Loader, Stefano Cavalleri, Hannah Szlyk, Zachary Brown, Himanshu Narayan, Jonathan Roberts, William Alley, Kunyang Sun, Ryan Stendall, Max Lamparth, Anka Reuel, Ting Wang, Hanmeng Xu, Pablo Hernández-Cámara, Freddie Martin, Thomas Preu, Tomek Korbak, Marcus Abramovitch, Dominic Williamson, Ida Bosio, Ziye Chen, Biró Bálint, Eve J. Y. Lo, Maria Inês S. Nunes, Yibo Jiang, M Saiful Bari, Peyman Kassani, Zihao Wang, Behzad Ansarinejad, Yewen Sun, Stephane Durand, Guillaume Douville, Daniel Tordera, George Balabanian, Earth Anderson, Lynna Kvistad, Alejandro José Moyano, Hsiaoyun Milliron, Ahmad Sakor, Murat Eron, Isaac C. McAlister, Andrew Favre D. O., Shailesh Shah, Xiaoxiang Zhou, Firuz Kamalov, Ronald Clark, Sherwin Abdoli, Tim Santens, Harrison K Wang, Evan Chen, Alessandro Tomasiello, G. Bruno De Luca, Shi-Zhuo Looi, Vinh-Kha Le, Noam Kolt, Niels Mündler, Avi Semler, Emma Rodman, Jacob Drori, Carl J Fossum, Luk Gloor, Milind Jagota, Ronak Pradeep, Honglu Fan, Tej Shah, Jonathan Eicher, Michael Chen, Kushal Thaman, William Merrill, Moritz Firsching, Carter Harris, Stefan Ciobâcă, Jason Gross, Rohan Pandey, Ilya Gusev, Adam Jones, Shashank Agnihotri, Pavel Zhelnov, Siranut Usawasutsakorn, Mohammadreza Mofayezi, Alexander Piperski, Marc Carauleanu, David K. Zhang, Kostiantyn Dobarskyi, Dylan Ler, Roman Leventov, Ignat Soroko, Thorben Jansen, Scott Creighton, Pascal Lauer, Joshua Duersch, Vage Taamazyan, Dario Bezzi, Wiktor Morak, Wenjie Ma, William Held, Tran Đuc Huy, Ruicheng Xian, Armel Randy Zebaze, Mohanad Mohamed, Julian Noah Leser, Michelle X Yuan, Laila Yacar, Johannes Lengler, Katarzyna Olszewska, Hossein Shahrtash, Edson Oliveira, Joseph W. Jackson, Daniel Espinosa Gonzalez, Andy Zou, Muthu Chidambaram, Timothy Manik, Hector Haffenden, Dashiell Stander, Ali Dasouqi, Alexander Shen, Emilien Duc, Bita Golshani, David Stap, Mikalai Uzhou, Alina Borisovna Zhidkovskaya, Lukas Lewark, Miguel Orbegozo Rodriguez, Mátyás Vincze, Dustin Wehr, Colin Tang, Shaun Phillips, Fortuna Samuele, Jiang Muzhen, Fredrik Ekström, Angela Hammon, Oam Patel, Faraz Farhidi, George Medley, Forough Mohammadzadeh, Madellene Peñaflor, Haile Kassahun, Alena Friedrich, Claire Sparrow, Rayner Hernandez Perez, Taom Sakal, Omkar Dhamane, Ali Khajegili Mirabadi, Eric Hallman, Kenchi Okutsu, Mike Battaglia, Mohammad Maghsoudimehrabani, Alon Amit, Dave Hulbert, Roberto Pereira, Simon Weber,  Handoko, Anton Peristyy, Stephen Malina, Samuel Albanie, Will Cai, Mustafa Mehkary, Rami Aly, Frank Reidegeld, Anna-Katharina Dick, Cary Friday, Jasdeep Sidhu, Hassan Shapourian, Wanyoung Kim, Mariana Costa, Hubeyb Gurdogan, Brian Weber, Harsh Kumar, Tong Jiang, Arunim Agarwal, Chiara Ceconello, Warren S. Vaz, Chao Zhuang, Haon Park, Andrew R. Tawfeek, Daattavya Aggarwal, Michael Kirchhof, Linjie Dai, Evan Kim, Johan Ferret, Yuzhou Wang, Minghao Yan, Krzysztof Burdzy, Lixin Zhang, Antonio Franca, Diana T. Pham, Kang Yong Loh, Joshua Robinson, Abram Jackson, Shreen Gul, Gunjan Chhablani, Zhehang Du, Adrian Cosma, Jesus Colino, Colin White, Jacob Votava, Vladimir Vinnikov, Ethan Delaney, Petr Spelda, Vit Stritecky, Syed M. Shahid, Jean-Christophe Mourrat, Lavr Vetoshkin, Koen Sponselee, Renas Bacho, Florencia de la Rosa, Xiuyu Li, Guillaume Malod, Leon Lang, Julien Laurendeau, Dmitry Kazakov, Fatimah Adesanya, Julien Portier, Lawrence Hollom, Victor Souza, Yuchen Anna Zhou, Julien Degorre, Yiğit Yalın, Gbenga Daniel Obikoya, Luca Arnaboldi,  Rai, Filippo Bigi, M. C. Boscá, Oleg Shumar, Kaniuar Bacho, Pierre Clavier, Gabriel Recchia, Mara Popescu, Nikita Shulga, Ngefor Mildred Tanwie, Denis Peskoff, Thomas C. H. Lux, Ben Rank, Colin Ni, Matthew Brooks, Alesia Yakimchyk,  Huanxu,  Liu, Olle Häggström, Emil Verkama, Hans Gundlach, Leonor Brito-Santana, Brian Amaro, Vivek Vajipey, Rynaa Grover, Yiyang Fan, Gabriel Poesia Reis e Silva, Linwei Xin, Yosi Kratish, Jakub Łucki, Wen-Ding Li, Sivakanth Gopi, Andrea Caciolai, Justin Xu, Kevin Joseph Scaria, Freddie Vargus, Farzad Habibi,  Long,  Lian, Emanuele Rodolà, Jules Robins, Vincent Cheng, Tony Fruhauff, Brad Raynor, Hao Qi, Xi Jiang, Ben Segev, Jingxuan Fan, Sarah Martinson, Erik Y. Wang, Kaylie Hausknecht, Michael P. Brenner, Mao Mao, Xinyu Zhang, David Avagian, Eshawn Jessica Scipio, Alon Ragoler, Justin Tan, Blake Sims, Rebeka Plecnik, Aaron Kirtland, Omer Faruk Bodur, D. P. Shinde, Zahra Adoul, Mohamed Zekry, Ali Karakoc, Tania C. B. Santos, Samir Shamseldeen, Loukmane Karim, Anna Liakhovitskaia, Nate Resman, Nicholas Farina, Juan Carlos Gonzalez, Gabe Maayan, Sarah Hoback, Rodrigo De Oliveira Pena, Glen Sherman, Elizabeth Kelley, Hodjat Mariji, Rasoul Pouriamanesh, Wentao Wu, Sandra Mendoza, Ismail Alarab, Joshua Cole, Danyelle Ferreira, Bryan Johnson, Mohammad Safdari, Liangti Dai, Siriphan Arthornthurasuk, Alexey Pronin, Jing Fan, Angel Ramirez-Trinidad, Ashley Cartwright, Daphiny Pottmaier, Omid Taheri, David Outevsky, Stanley Stepanic, Samuel Perry, Luke Askew, Raúl Adrián Huerta Rodríguez, Ali M. R. Minissi, Sam Ali, Ricardo Lorena, Krishnamurthy Iyer, Arshad Anil Fasiludeen, Sk Md Salauddin, Murat Islam, Juan Gonzalez, Josh Ducey, Maja Somrak, Vasilios Mavroudis, Eric Vergo, Juehang Qin, Benjámin Borbás, Eric Chu, Jack Lindsey, Anil Radhakrishnan, Antoine Jallon, I. M. J. McInnis, Pawan Kumar, Laxman Prasad Goswami, Daniel Bugas, Nasser Heydari, Ferenc Jeanplong, Archimedes Apronti, Abdallah Galal, Ng Ze-An, Ankit Singh, Joan of Arc Xavier, Kanu Priya Agarwal, Mohammed Berkani, Benedito Alves de Oliveira Junior, Dmitry Malishev, Nicolas Remy, Taylor D. Hartman, Tim Tarver, Stephen Mensah, Javier Gimenez, Roselynn Grace Montecillo, Russell Campbell, Asankhaya Sharma, Khalida Meer, Xavier Alapont, Deepakkumar Patil, Rajat Maheshwari, Abdelkader Dendane, Priti Shukla, Sergei Bogdanov, Sören Möller, Muhammad Rehan Siddiqi, Prajvi Saxena, Himanshu Gupta, Innocent Enyekwe, Ragavendran P V, Zienab EL-Wasif, Aleksandr Maksapetyan, Vivien Rossbach, Chris Harjadi, Mohsen Bahaloohoreh, Song Bian, John Lai, Justine Leon Uro, Greg Bateman, Mohamed Sayed, Ahmed Menshawy, Darling Duclosel, Yashaswini Jain, Ashley Aaron, Murat Tiryakioglu, Sheeshram Siddh, Keith Krenek, Alex Hoover, Joseph McGowan, Tejal Patwardhan, Summer Yue, Alexandr Wang, Dan Hendrycks
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Benchmarks are important tools for tracking the rapid advancements in large
language model (LLM) capabilities. However, benchmarks are not keeping pace in
difficulty: LLMs now achieve over 90\% accuracy on popular benchmarks like
MMLU, limiting informed measurement of state-of-the-art LLM capabilities. In
response, we introduce Humanity's Last Exam (HLE), a multi-modal benchmark at
the frontier of human knowledge, designed to be the final closed-ended academic
benchmark of its kind with broad subject coverage. HLE consists of 2,700
questions across dozens of subjects, including mathematics, humanities, and the
natural sciences. HLE is developed globally by subject-matter experts and
consists of multiple-choice and short-answer questions suitable for automated
grading. Each question has a known solution that is unambiguous and easily
verifiable, but cannot be quickly answered via internet retrieval.
State-of-the-art LLMs demonstrate low accuracy and calibration on HLE,
highlighting a significant gap between current LLM capabilities and the expert
human frontier on closed-ended academic questions. To inform research and
policymaking upon a clear understanding of model capabilities, we publicly
release HLE at https://lastexam.ai.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>27 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Structure-aware Generative Model for Biomedical Event Extraction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.06583v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.06583v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haohan Yuan, Siu Cheung Hui, Haopeng Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Biomedical Event Extraction (BEE) is a challenging task that involves
modeling complex relationships between fine-grained entities in biomedical
text. BEE has traditionally been formulated as a classification problem. With
recent advancements in large language models (LLMs), generation-based models
that cast event extraction as a sequence generation problem have attracted
attention in the NLP research community. However, current generative models
often overlook cross-instance information in complex event structures, such as
nested and overlapping events, which constitute over 20% of events in benchmark
datasets. In this paper, we propose GenBEE, an event structure-aware generative
model that captures complex event structures in biomedical text for biomedical
event extraction. GenBEE constructs event prompts that distill knowledge from
LLMs to incorporate both label semantics and argument dependency relationships.
In addition, GenBEE generates prefixes with event structural prompts to
incorporate structural features to improve the model's overall performance. We
have evaluated the proposed GenBEE model on three widely used BEE benchmark
datasets, namely MLEE, GE11, and PHEE. Experimental results show that GenBEE
has achieved state-of-the-art performance on the MLEE and GE11 datasets, and
achieved competitive results when compared to the state-of-the-art
classification-based models on the PHEE dataset.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 4 figures, 6 tables</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Computer Vision and Pattern Recognition <span class="chip" style="font-size: 60%">104</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ELIP: Enhanced Visual-Language Foundation Models for Image Retrieval 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.15682v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.15682v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guanqi Zhan, Yuanpei Liu, Kai Han, Weidi Xie, Andrew Zisserman
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The objective in this paper is to improve the performance of text-to-image
retrieval. To this end, we introduce a new framework that can boost the
performance of large-scale pre-trained vision-language models, so that they can
be used for text-to-image re-ranking. The approach, Enhanced Language-Image
Pre-training (ELIP), uses the text query to predict a set of visual prompts to
condition the ViT image encoding. ELIP can easily be applied to the commonly
used CLIP/SigLIP and the state-of-the-art BLIP-2 architectures. To train the
architecture with limited computing resources, we develop a 'student friendly'
best practice involving global hard sample mining, and selection and curation
of a large-scale dataset. On the evaluation side, we set up two new
out-of-distribution benchmarks, Occluded COCO and ImageNet-R, to assess the
zero-shot generalisation of the models to different domains. Benefiting from
the novel architecture and data curation, experiments show our enhanced network
significantly boosts CLIP/SigLIP performance and outperforms the
state-of-the-art BLIP-2 model on text-to-image retrieval.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ One-step Diffusion Models with $f$-Divergence Distribution Matching 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.15681v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.15681v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yilun Xu, Weili Nie, Arash Vahdat
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Sampling from diffusion models involves a slow iterative process that hinders
their practical deployment, especially for interactive applications. To
accelerate generation speed, recent approaches distill a multi-step diffusion
model into a single-step student generator via variational score distillation,
which matches the distribution of samples generated by the student to the
teacher's distribution. However, these approaches use the reverse
Kullback-Leibler (KL) divergence for distribution matching which is known to be
mode seeking. In this paper, we generalize the distribution matching approach
using a novel $f$-divergence minimization framework, termed $f$-distill, that
covers different divergences with different trade-offs in terms of mode
coverage and training variance. We derive the gradient of the $f$-divergence
between the teacher and student distributions and show that it is expressed as
the product of their score differences and a weighting function determined by
their density ratio. This weighting function naturally emphasizes samples with
higher density in the teacher distribution, when using a less mode-seeking
divergence. We observe that the popular variational score distillation approach
using the reverse-KL divergence is a special case within our framework.
Empirically, we demonstrate that alternative $f$-divergences, such as
forward-KL and Jensen-Shannon divergences, outperform the current best
variational score distillation methods across image generation tasks. In
particular, when using Jensen-Shannon divergence, $f$-distill achieves current
state-of-the-art one-step generation performance on ImageNet64 and zero-shot
text-to-image generation on MS-COCO. Project page:
https://research.nvidia.com/labs/genair/f-distill
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ BOSS: Benchmark for Observation Space Shift in Long-Horizon Task 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.15679v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.15679v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yue Yang, Linfeng Zhao, Mingyu Ding, Gedas Bertasius, Daniel Szafir
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Robotics has long sought to develop visual-servoing robots capable of
completing previously unseen long-horizon tasks. Hierarchical approaches offer
a pathway for achieving this goal by executing skill combinations arranged by a
task planner, with each visuomotor skill pre-trained using a specific imitation
learning (IL) algorithm. However, even in simple long-horizon tasks like skill
chaining, hierarchical approaches often struggle due to a problem we identify
as Observation Space Shift (OSS), where the sequential execution of preceding
skills causes shifts in the observation space, disrupting the performance of
subsequent individually trained skill policies. To validate OSS and evaluate
its impact on long-horizon tasks, we introduce BOSS (a Benchmark for
Observation Space Shift). BOSS comprises three distinct challenges: "Single
Predicate Shift", "Accumulated Predicate Shift", and "Skill Chaining", each
designed to assess a different aspect of OSS's negative effect. We evaluated
several recent popular IL algorithms on BOSS, including three Behavioral
Cloning methods and the Visual Language Action model OpenVLA. Even on the
simplest challenge, we observed average performance drops of 67%, 35%, 34%, and
54%, respectively, when comparing skill performance with and without OSS.
Additionally, we investigate a potential solution to OSS that scales up the
training data for each skill with a larger and more visually diverse set of
demonstrations, with our results showing it is not sufficient to resolve OSS.
The project page is: https://boss-benchmark.github.io/
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ VaViM and VaVAM: Autonomous Driving through Video Generative Modeling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.15672v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.15672v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Florent Bartoccioni, Elias Ramzi, Victor Besnier, Shashanka Venkataramanan, Tuan-Hung Vu, Yihong Xu, Loick Chambon, Spyros Gidaris, Serkan Odabas, David Hurych, Renaud Marlet, Alexandre Boulch, Mickael Chen, Éloi Zablocki, Andrei Bursuc, Eduardo Valle, Matthieu Cord
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We explore the potential of large-scale generative video models for
autonomous driving, introducing an open-source auto-regressive video model
(VaViM) and its companion video-action model (VaVAM) to investigate how video
pre-training transfers to real-world driving. VaViM is a simple auto-regressive
video model that predicts frames using spatio-temporal token sequences. We show
that it captures the semantics and dynamics of driving scenes. VaVAM, the
video-action model, leverages the learned representations of VaViM to generate
driving trajectories through imitation learning. Together, the models form a
complete perception-to-action pipeline. We evaluate our models in open- and
closed-loop driving scenarios, revealing that video-based pre-training holds
promise for autonomous driving. Key insights include the semantic richness of
the learned representations, the benefits of scaling for video synthesis, and
the complex relationship between model size, data, and safety metrics in
closed-loop evaluations. We release code and model weights at
https://github.com/valeoai/VideoActionModel
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code and model: https://github.com/valeoai/VideoActionModel, project
  page: https://valeoai.github.io/vavim-vavam/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Logit Disagreement: OoD Detection with Bayesian Neural Networks <span class="chip">ECCV 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.15648v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.15648v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kevin Raina
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Bayesian neural networks (BNNs), which estimate the full posterior
distribution over model parameters, are well-known for their role in
uncertainty quantification and its promising application in out-of-distribution
detection (OoD). Amongst other uncertainty measures, BNNs provide a
state-of-the art estimation of predictive entropy (total uncertainty) which can
be decomposed as the sum of mutual information and expected entropy. In the
context of OoD detection the estimation of predictive uncertainty in the form
of the predictive entropy score confounds aleatoric and epistemic uncertainty,
the latter being hypothesized to be high for OoD points. Despite these
justifications, the mutual information score has been shown to perform worse
than predictive entropy. Taking inspiration from Bayesian variational
autoencoder (BVAE) literature, this work proposes to measure the disagreement
between a corrected version of the pre-softmax quantities, otherwise known as
logits, as an estimate of epistemic uncertainty for Bayesian NNs under mean
field variational inference. The three proposed epistemic uncertainty scores
demonstrate marked improvements over mutual information on a range of OoD
experiments, with equal performance otherwise. Moreover, the epistemic
uncertainty scores perform on par with the Bayesian benchmark predictive
entropy on a range of MNIST and CIFAR10 experiments.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Presented at ECCV 2024 Workshop: 3rd Workshop on Uncertainty
  Quantification for Computer Vision</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ RGB-Only Gaussian Splatting SLAM for Unbounded Outdoor Scenes <span class="chip">ICRA 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.15633v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.15633v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sicheng Yu, Chong Cheng, Yifan Zhou, Xiaojun Yang, Hao Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  3D Gaussian Splatting (3DGS) has become a popular solution in SLAM, as it can
produce high-fidelity novel views. However, previous GS-based methods primarily
target indoor scenes and rely on RGB-D sensors or pre-trained depth estimation
models, hence underperforming in outdoor scenarios. To address this issue, we
propose a RGB-only gaussian splatting SLAM method for unbounded outdoor
scenes--OpenGS-SLAM. Technically, we first employ a pointmap regression network
to generate consistent pointmaps between frames for pose estimation. Compared
to commonly used depth maps, pointmaps include spatial relationships and scene
geometry across multiple views, enabling robust camera pose estimation. Then,
we propose integrating the estimated camera poses with 3DGS rendering as an
end-to-end differentiable pipeline. Our method achieves simultaneous
optimization of camera poses and 3DGS scene parameters, significantly enhancing
system tracking accuracy. Specifically, we also design an adaptive scale mapper
for the pointmap regression network, which provides more accurate pointmap
mapping to the 3DGS map representation. Our experiments on the Waymo dataset
demonstrate that OpenGS-SLAM reduces tracking error to 9.8\% of previous 3DGS
methods, and achieves state-of-the-art results in novel view synthesis. Project
Page: https://3dagentworld.github.io/opengs-slam/
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICRA 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Continual Person Identification using Footstep-Induced Floor Vibrations
  on Heterogeneous Floor Structures 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.15632v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.15632v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yiwen Dong, Hae Young Noh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Person identification is important for smart buildings to provide
personalized services such as health monitoring, activity tracking, and
personnel management. However, previous person identification relies on
pre-collected data from everyone, which is impractical in many buildings and
public facilities in which visitors are typically expected. This calls for a
continual person identification system that gradually learns people's
identities on the fly. Existing studies use cameras to achieve this goal, but
they require direct line-of-sight and also have raised privacy concerns in
public. Other modalities such as wearables and pressure mats are limited by the
requirement of device-carrying or dense deployment. Thus, prior studies
introduced footstep-induced structural vibration sensing, which is
non-intrusive and perceived as more privacy-friendly. However, this approach
has a significant challenge: the high variability of vibration data due to
structural heterogeneity and human gait variations, which makes online person
identification algorithms perform poorly. In this paper, we characterize the
variability in footstep-induced structural vibration data for accurate online
person identification. To achieve this, we quantify and decompose different
sources of variability and then design a feature transformation function to
reduce the variability within each person's data to make different people's
data more separable. We evaluate our approach through field experiments with 20
people. The results show a 70% variability reduction and a 90% accuracy for
online person identification.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ WorldCraft: Photo-Realistic 3D World Creation and Customization via LLM
  Agents 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.15601v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.15601v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinhang Liu, Chi-Keung Tang, Yu-Wing Tai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Constructing photorealistic virtual worlds has applications across various
fields, but it often requires the extensive labor of highly trained
professionals to operate conventional 3D modeling software. To democratize this
process, we introduce WorldCraft, a system where large language model (LLM)
agents leverage procedural generation to create indoor and outdoor scenes
populated with objects, allowing users to control individual object attributes
and the scene layout using intuitive natural language commands. In our
framework, a coordinator agent manages the overall process and works with two
specialized LLM agents to complete the scene creation: ForgeIt, which
integrates an ever-growing manual through auto-verification to enable precise
customization of individual objects, and ArrangeIt, which formulates
hierarchical optimization problems to achieve a layout that balances ergonomic
and aesthetic considerations. Additionally, our pipeline incorporates a
trajectory control agent, allowing users to animate the scene and operate the
camera through natural language interactions. Our system is also compatible
with off-the-shelf deep 3D generators to enrich scene assets. Through
evaluations and comparisons with state-of-the-art methods, we demonstrate the
versatility of WorldCraft, ranging from single-object customization to
intricate, large-scale interior and exterior scene designs. This system
empowers non-professionals to bring their creative visions to life.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Bridging vision language model (VLM) evaluation gaps with a framework
  for scalable and cost-effective benchmark generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.15563v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.15563v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tim Rädsch, Leon Mayer, Simon Pavicic, A. Emre Kavur, Marcel Knopp, Barış Öztürk, Klaus Maier-Hein, Paul F. Jaeger, Fabian Isensee, Annika Reinke, Lena Maier-Hein
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reliable evaluation of AI models is critical for scientific progress and
practical application. While existing VLM benchmarks provide general insights
into model capabilities, their heterogeneous designs and limited focus on a few
imaging domains pose significant challenges for both cross-domain performance
comparison and targeted domain-specific evaluation. To address this, we propose
three key contributions: (1) a framework for the resource-efficient creation of
domain-specific VLM benchmarks enabled by task augmentation for creating
multiple diverse tasks from a single existing task, (2) the release of new VLM
benchmarks for seven domains, created according to the same homogeneous
protocol and including 162,946 thoroughly human-validated answers, and (3) an
extensive benchmarking of 22 state-of-the-art VLMs on a total of 37,171 tasks,
revealing performance variances across domains and tasks, thereby supporting
the need for tailored VLM benchmarks. Adoption of our methodology will pave the
way for the resource-efficient domain-specific selection of models and guide
future research efforts toward addressing core open questions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Estimating Vehicle Speed on Roadways Using RNNs and <span class="highlight-title">Transformer</span>s: A
  Video-based Approach 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.15545v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.15545v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sai Krishna Reddy Mareddy, Dhanush Upplapati, Dhanush Kumar Antharam
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This project explores the application of advanced machine learning models,
specifically Long Short-Term Memory (LSTM), Gated Recurrent Units (GRU), and
Transformers, to the task of vehicle speed estimation using video data.
Traditional methods of speed estimation, such as radar and manual systems, are
often constrained by high costs, limited coverage, and potential disruptions.
In contrast, leveraging existing surveillance infrastructure and cutting-edge
neural network architectures presents a non-intrusive, scalable solution. Our
approach utilizes LSTM and GRU to effectively manage long-term dependencies
within the temporal sequence of video frames, while Transformers are employed
to harness their self-attention mechanisms, enabling the processing of entire
sequences in parallel and focusing on the most informative segments of the
data. This study demonstrates that both LSTM and GRU outperform basic Recurrent
Neural Networks (RNNs) due to their advanced gating mechanisms. Furthermore,
increasing the sequence length of input data consistently improves model
accuracy, highlighting the importance of contextual information in dynamic
environments. Transformers, in particular, show exceptional adaptability and
robustness across varied sequence lengths and complexities, making them highly
suitable for real-time applications in diverse traffic conditions. The findings
suggest that integrating these sophisticated neural network models can
significantly enhance the accuracy and reliability of automated speed detection
systems, thus promising to revolutionize traffic management and road safety.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Depth-aware Fusion Method based on Image and 4D Radar Spectrum for 3D
  Object Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.15516v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.15516v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yue Sun, Yeqiang Qian, Chunxiang Wang, Ming Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Safety and reliability are crucial for the public acceptance of autonomous
driving. To ensure accurate and reliable environmental perception, intelligent
vehicles must exhibit accuracy and robustness in various environments.
Millimeter-wave radar, known for its high penetration capability, can operate
effectively in adverse weather conditions such as rain, snow, and fog.
Traditional 3D millimeter-wave radars can only provide range, Doppler, and
azimuth information for objects. Although the recent emergence of 4D
millimeter-wave radars has added elevation resolution, the radar point clouds
remain sparse due to Constant False Alarm Rate (CFAR) operations. In contrast,
cameras offer rich semantic details but are sensitive to lighting and weather
conditions. Hence, this paper leverages these two highly complementary and
cost-effective sensors, 4D millimeter-wave radar and camera. By integrating 4D
radar spectra with depth-aware camera images and employing attention
mechanisms, we fuse texture-rich images with depth-rich radar data in the
Bird's Eye View (BEV) perspective, enhancing 3D object detection. Additionally,
we propose using GAN-based networks to generate depth images from radar spectra
in the absence of depth sensors, further improving detection accuracy.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Q-PETR: Quant-aware Position Embedding Transformation for Multi-View 3D
  Object Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.15488v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.15488v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiangyong Yu, Changyong Shu, Dawei Yang, Zichen Yu, Xing Hu, Yan Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  PETR-based methods have dominated benchmarks in 3D perception and are
increasingly becoming a key component in modern autonomous driving systems.
However, their quantization performance significantly degrades when INT8
inference is required, with a degradation of 58.2% in mAP and 36.9% in NDS on
the NuScenes dataset. To address this issue, we propose a quantization-aware
position embedding transformation for multi-view 3D object detection, termed
Q-PETR. Q-PETR offers a quantizationfriendly and deployment-friendly
architecture while preserving the original performance of PETR. It
substantially narrows the accuracy gap between INT8 and FP32 inference for
PETR-series methods. Without bells and whistles, our approach reduces the mAP
and NDS drop to within 1% under standard 8-bit per-tensor post-training
quantization. Furthermore, our method exceeds the performance of the original
PETR in terms of floating-point precision. Extensive experiments across a
variety of PETR-series models demonstrate its broad generalization.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Confidence-Based Annotation Of Brain Tumours In Ultrasound 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.15484v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.15484v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alistair Weld, Luke Dixon, Alfie Roddan, Giulio Anichini, Sophie Camp, Stamatia Giannarou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Purpose: An investigation of the challenge of annotating discrete
segmentations of brain tumours in ultrasound, with a focus on the issue of
aleatoric uncertainty along the tumour margin, particularly for diffuse
tumours. A segmentation protocol and method is proposed that incorporates this
margin-related uncertainty while minimising the interobserver variance through
reduced subjectivity, thereby diminishing annotator epistemic uncertainty.
Approach: A sparse confidence method for annotation is proposed, based on a
protocol designed using computer vision and radiology theory. Results: Output
annotations using the proposed method are compared with the corresponding
professional discrete annotation variance between the observers. A linear
relationship was measured within the tumour margin region, with a Pearson
correlation of 0.8. The downstream application was explored, comparing training
using confidence annotations as soft labels with using the best discrete
annotations as hard labels. In all evaluation folds, the Brier score was
superior for the soft-label trained network. Conclusion: A formal framework was
constructed to demonstrate the infeasibility of discrete annotation of brain
tumours in B-mode ultrasound. Subsequently, a method for sparse
confidence-based annotation is proposed and evaluated. Keywords: Brain tumours,
ultrasound, confidence, annotation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ On Neural BRDFs: A Thorough Comparison of State-of-the-Art Approaches <span class="chip">WACV</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.15480v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.15480v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Florian Hofherr, Bjoern Haefner, Daniel Cremers
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The bidirectional reflectance distribution function (BRDF) is an essential
tool to capture the complex interaction of light and matter. Recently, several
works have employed neural methods for BRDF modeling, following various
strategies, ranging from utilizing existing parametric models to purely neural
parametrizations. While all methods yield impressive results, a comprehensive
comparison of the different approaches is missing in the literature. In this
work, we present a thorough evaluation of several approaches, including results
for qualitative and quantitative reconstruction quality and an analysis of
reciprocity and energy conservation. Moreover, we propose two extensions that
can be added to existing approaches: A novel additive combination strategy for
neural BRDFs that split the reflectance into a diffuse and a specular part, and
an input mapping that ensures reciprocity exactly by construction, while
previous approaches only ensure it by soft constraints.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published in IEEE/CVF Winter Conference on Applications of Computer
  Vision (WACV) 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CondiQuant: Condition Number Based Low-Bit Quantization for Image
  Super-Resolution 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.15478v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.15478v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kai Liu, Dehui Wang, Zhiteng Li, Zheng Chen, Yong Guo, Wenbo Li, Linghe Kong, Yulun Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Low-bit model quantization for image super-resolution (SR) is a longstanding
task that is renowned for its surprising compression and acceleration ability.
However, accuracy degradation is inevitable when compressing the full-precision
(FP) model to ultra-low bit widths (2~4 bits). Experimentally, we observe that
the degradation of quantization is mainly attributed to the quantization of
activation instead of model weights. In numerical analysis, the condition
number of weights could measure how much the output value can change for a
small change in the input argument, inherently reflecting the quantization
error. Therefore, we propose CondiQuant, a condition number based low-bit
post-training quantization for image super-resolution. Specifically, we
formulate the quantization error as the condition number of weight metrics. By
decoupling the representation ability and the quantization sensitivity, we
design an efficient proximal gradient descent algorithm to iteratively minimize
the condition number and maintain the output still. With comprehensive
experiments, we demonstrate that CondiQuant outperforms existing
state-of-the-art post-training quantization methods in accuracy without
computation overhead and gains the theoretically optimal compression ratio in
model parameters. Our code and model are released at
https://github.com/Kai-Liu001/CondiQuant.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 5 figures. Code and models are released at
  https://github.com/Kai-Liu001/CondiQuant</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Aligning Task- and Reconstruction-Oriented Communications for Edge
  Intelligence 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.15472v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.15472v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yufeng Diao, Yichi Zhang, Changyang She, Philip Guodong Zhao, Emma Liying Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Existing communication systems aim to reconstruct the information at the
receiver side, and are known as reconstruction-oriented communications. This
approach often falls short in meeting the real-time, task-specific demands of
modern AI-driven applications such as autonomous driving and semantic
segmentation. As a new design principle, task-oriented communications have been
developed. However, it typically requires joint optimization of encoder,
decoder, and modified inference neural networks, resulting in extensive
cross-system redesigns and compatibility issues. This paper proposes a novel
communication framework that aligns reconstruction-oriented and task-oriented
communications for edge intelligence. The idea is to extend the Information
Bottleneck (IB) theory to optimize data transmission by minimizing
task-relevant loss function, while maintaining the structure of the original
data by an information reshaper. Such an approach integrates task-oriented
communications with reconstruction-oriented communications, where a variational
approach is designed to handle the intractability of mutual information in
high-dimensional neural network features. We also introduce a joint
source-channel coding (JSCC) modulation scheme compatible with classical
modulation techniques, enabling the deployment of AI technologies within
existing digital infrastructures. The proposed framework is particularly
effective in edge-based autonomous driving scenarios. Our evaluation in the Car
Learning to Act (CARLA) simulator demonstrates that the proposed framework
significantly reduces bits per service by 99.19% compared to existing methods,
such as JPEG, JPEG2000, and BPG, without compromising the effectiveness of task
execution.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for publication in IEEE Journal on Selected Areas in
  Communications (JSAC)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Game State and Spatio-temporal Action Detection in Soccer using Graph
  Neural Networks and 3D Convolutional Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.15462v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.15462v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jeremie Ochin, Guillaume Devineau, Bogdan Stanciulescu, Sotiris Manitsaris
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Soccer analytics rely on two data sources: the player positions on the pitch
and the sequences of events they perform. With around 2000 ball events per
game, their precise and exhaustive annotation based on a monocular video stream
remains a tedious and costly manual task. While state-of-the-art
spatio-temporal action detection methods show promise for automating this task,
they lack contextual understanding of the game. Assuming professional players'
behaviors are interdependent, we hypothesize that incorporating surrounding
players' information such as positions, velocity and team membership can
enhance purely visual predictions. We propose a spatio-temporal action
detection approach that combines visual and game state information via Graph
Neural Networks trained end-to-end with state-of-the-art 3D CNNs, demonstrating
improved metrics through game state integration.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Memory Helps, but Confabulation Misleads: Understanding Streaming Events
  in Videos with MLLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.15457v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.15457v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gengyuan Zhang, Mingcong Ding, Tong Liu, Yao Zhang, Volker Tresp
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multimodal large language models (MLLMs) have demonstrated strong performance
in understanding videos holistically, yet their ability to process streaming
videos-videos are treated as a sequence of visual events-remains underexplored.
Intuitively, leveraging past events as memory can enrich contextual and
temporal understanding of the current event. In this paper, we show that
leveraging memories as contexts helps MLLMs better understand video events.
However, because such memories rely on predictions of preceding events, they
may contain misinformation, leading to confabulation and degraded performance.
To address this, we propose a confabulation-aware memory modification method
that mitigates confabulated memory for memory-enhanced event understanding.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Short paper (5 pages)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MVIP -- A <span class="highlight-title">Dataset</span> and Methods for Application Oriented Multi-View and
  Multi-Modal Industrial Part Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.15448v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.15448v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Paul Koch, Marian Schlüter, Jörg Krüger
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present MVIP, a novel dataset for multi-modal and multi-view
application-oriented industrial part recognition. Here we are the first to
combine a calibrated RGBD multi-view dataset with additional object context
such as physical properties, natural language, and super-classes. The current
portfolio of available datasets offers a wide range of representations to
design and benchmark related methods. In contrast to existing classification
challenges, industrial recognition applications offer controlled multi-modal
environments but at the same time have different problems than traditional
2D/3D classification challenges. Frequently, industrial applications must deal
with a small amount or increased number of training data, visually similar
parts, and varying object sizes, while requiring a robust near 100% top 5
accuracy under cost and time constraints. Current methods tackle such
challenges individually, but direct adoption of these methods within industrial
applications is complex and requires further research. Our main goal with MVIP
is to study and push transferability of various state-of-the-art methods within
related downstream tasks towards an efficient deployment of industrial
classifiers. Additionally, we intend to push with MVIP research regarding
several modality fusion topics, (automated) synthetic data generation, and
complex data sampling -- combined in a single application-oriented benchmark.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to IMPROVE 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LEAP: Enhancing Vision-Based Occupancy Networks with Lightweight
  Spatio-Temporal Correlation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.15438v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.15438v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fengcheng Yu, Haoran Xu, Canming Xia, Guang Tan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vision-based occupancy networks provide an end-to-end solution for
reconstructing the surrounding environment using semantic occupied voxels
derived from multi-view images. This technique relies on effectively learning
the correlation between pixel-level visual information and voxels. Despite
recent advancements, occupancy results still suffer from limited accuracy due
to occlusions and sparse visual cues. To address this, we propose a Lightweight
Spatio-Temporal Correlation (LEAP)} method, which significantly enhances the
performance of existing occupancy networks with minimal computational overhead.
LEAP can be seamlessly integrated into various baseline networks, enabling a
plug-and-play application. LEAP operates in three stages: 1) it tokenizes
information from recent baseline and motion features into a shared, compact
latent space; 2) it establishes full correlation through a tri-stream fusion
architecture; 3) it generates occupancy results that strengthen the baseline's
output. Extensive experiments demonstrate the efficiency and effectiveness of
our method, outperforming the latest baseline models. The source code and
several demos are available in the supplementary material.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Anatomy-Informed Deep Learning and Radiomics for Automated Neurofibroma
  Segmentation in Whole-Body MRI 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.15424v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.15424v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Georgii Kolokolnikov, Marie-Lena Schmalhofer, Lennart Well, Said Farschtschi, Victor-Felix Mautner, Inka Ristow, Rene Werner
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neurofibromatosis Type 1 is a genetic disorder characterized by the
development of neurofibromas (NFs), which exhibit significant variability in
size, morphology, and anatomical location. Accurate and automated segmentation
of these tumors in whole-body magnetic resonance imaging (WB-MRI) is crucial to
assess tumor burden and monitor disease progression. In this study, we present
and analyze a fully automated pipeline for NF segmentation in fat-suppressed
T2-weighted WB-MRI, consisting of three stages: anatomy segmentation, NF
segmentation, and tumor candidate classification. In the first stage, we use
the MRSegmentator model to generate an anatomy segmentation mask, extended with
a high-risk zone for NFs. This mask is concatenated with the input image as
anatomical context information for NF segmentation. The second stage employs an
ensemble of 3D anisotropic anatomy-informed U-Nets to produce an NF
segmentation confidence mask. In the final stage, tumor candidates are
extracted from the confidence mask and classified based on radiomic features,
distinguishing tumors from non-tumor regions and reducing false positives. We
evaluate the proposed pipeline on three test sets representing different
conditions: in-domain data (test set 1), varying imaging protocols and field
strength (test set 2), and low tumor burden cases (test set 3). Experimental
results show a 68% improvement in per-scan Dice Similarity Coefficient (DSC), a
21% increase in per-tumor DSC, and a two-fold improvement in F1 score for tumor
detection in high tumor burden cases by integrating anatomy information. The
method is integrated into the 3D Slicer platform for practical clinical use,
with the code publicly accessible.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Evaluating Multimodal Generative AI with Korean Educational Standards <span class="chip">NAACL 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.15422v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.15422v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sanghee Park, Geewook Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents the Korean National Educational Test Benchmark (KoNET), a
new benchmark designed to evaluate Multimodal Generative AI Systems using
Korean national educational tests. KoNET comprises four exams: the Korean
Elementary General Educational Development Test (KoEGED), Middle (KoMGED), High
(KoHGED), and College Scholastic Ability Test (KoCSAT). These exams are
renowned for their rigorous standards and diverse questions, facilitating a
comprehensive analysis of AI performance across different educational levels.
By focusing on Korean, KoNET provides insights into model performance in
less-explored languages. We assess a range of models - open-source,
open-access, and closed APIs - by examining difficulties, subject diversity,
and human error rates. The code and dataset builder will be made fully
open-sourced at https://github.com/naver-ai/KoNET.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>18 pages; To appear at NAACL 2025 Main Conference (Project page:
  https://github.com/naver-ai/KoNET )</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Enhancing Vehicle Make and Model Recognition with 3D Attention Modules 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.15398v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.15398v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Narges Semiromizadeh, Omid Nejati Manzari, Shahriar B. Shokouhi, Sattar Mirzakuchaki
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vehicle make and model recognition (VMMR) is a crucial component of the
Intelligent Transport System, garnering significant attention in recent years.
VMMR has been widely utilized for detecting suspicious vehicles, monitoring
urban traffic, and autonomous driving systems. The complexity of VMMR arises
from the subtle visual distinctions among vehicle models and the wide variety
of classes produced by manufacturers. Convolutional Neural Networks (CNNs), a
prominent type of deep learning model, have been extensively employed in
various computer vision tasks, including VMMR, yielding remarkable results. As
VMMR is a fine-grained classification problem, it primarily faces inter-class
similarity and intra-class variation challenges. In this study, we implement an
attention module to address these challenges and enhance the model's focus on
critical areas containing distinguishing features. This module, which does not
increase the parameters of the original model, generates three-dimensional
(3-D) attention weights to refine the feature map. Our proposed model
integrates the attention module into two different locations within the middle
section of a convolutional model, where the feature maps from these sections
offer sufficient information about the input frames without being overly
detailed or overly coarse. The performance of our proposed model, along with
state-of-the-art (SOTA) convolutional and transformer-based models, was
evaluated using the Stanford Cars dataset. Our proposed model achieved the
highest accuracy, 90.69\%, among the compared models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LongCaptioning: Unlocking the Power of Long Caption Generation in Large
  Multimodal Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.15393v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.15393v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hongchen Wei, Zhihong Tan, Yaosi Hu, Changwen Chen, Zhenzhong Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large multimodal models (LMMs) have shown remarkable performance in video
understanding tasks and can even process videos longer than one hour. However,
despite their ability to handle long inputs, generating outputs with
corresponding levels of richness remains a challenge. In this paper, we explore
the issue of long outputs in LMMs using video captioning as a proxy task, and
we find that open-source LMMs struggle to consistently generate outputs
exceeding about 300 words. Through controlled experiments, we find that the
scarcity of paired examples with long-captions during training is the primary
factor limiting the model's output length. However, manually annotating
long-caption examples is time-consuming and expensive. To address this, we
propose the LongCaption-Agent, a framework that synthesizes long caption data
by aggregating multi-level descriptions. Using LongCaption-Agent, we curated a
new long-caption dataset, LongCaption-10K. We also develop LongCaption-Bench, a
benchmark designed to comprehensively evaluate the quality of long captions
generated by LMMs. By incorporating LongCaption-10K into training, we enable
LMMs to generate captions exceeding 1,000 words, while maintaining high output
quality. In LongCaption-Bench, our 8B parameter model achieved state-of-the-art
performance, even surpassing larger proprietary models. We will release the
dataset and code after publication.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Chitrarth: Bridging Vision and Language for a Billion People 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.15392v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.15392v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shaharukh Khan, Ayush Tarun, Abhinav Ravi, Ali Faraz, Akshat Patidar, Praveen Kumar Pokala, Anagha Bhangare, Raja Kolla, Chandra Khatri, Shubham Agarwal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent multimodal foundation models are primarily trained on English or high
resource European language data, which hinders their applicability to other
medium and low-resource languages. To address this limitation, we introduce
Chitrarth (Chitra: Image; Artha: Meaning), an inclusive Vision-Language Model
(VLM), specifically targeting the rich linguistic diversity and visual
reasoning across 10 prominent Indian languages. Our model effectively
integrates a state-of-the-art (SOTA) multilingual Large Language Model (LLM)
with a vision module, primarily trained on multilingual image-text data.
Furthermore, we also introduce BharatBench, a comprehensive framework for
evaluating VLMs across various Indian languages, ultimately contributing to
more diverse and effective AI systems. Our model achieves SOTA results for
benchmarks across low resource languages while retaining its efficiency in
English. Through our research, we aim to set new benchmarks in
multilingual-multimodal capabilities, offering substantial improvements over
existing models and establishing a foundation to facilitate future advancements
in this arena.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The Role of Background Information in Reducing Object Hallucination in
  Vision-Language Models: Insights from Cutoff API <span class="highlight-title">Prompt</span>ing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.15389v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.15389v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Masayo Tomita, Katsuhiko Hayashi, Tomoyuki Kaneko
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vision-Language Models (VLMs) occasionally generate outputs that contradict
input images, constraining their reliability in real-world applications. While
visual prompting is reported to suppress hallucinations by augmenting prompts
with relevant area inside an image, the effectiveness in terms of the area
remains uncertain. This study analyzes success and failure cases of
Attention-driven visual prompting in object hallucination, revealing that
preserving background context is crucial for mitigating object hallucination.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MOVE: A Mixture-of-Vision-Encoders Approach for Domain-Focused
  Vision-Language Processing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.15381v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.15381v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Matvey Skripkin, Elizaveta Goncharova, Dmitrii Tarasov, Andrey Kuznetsov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multimodal language models (MLMs) integrate visual and textual information by
coupling a vision encoder with a large language model through the specific
adapter. While existing approaches commonly rely on a single pre-trained vision
encoder, there is a great variability of specialized encoders that can boost
model's performance in distinct domains. In this work, we propose MOVE (Mixture
of Vision Encoders) a simple yet effective approach to leverage multiple
pre-trained encoders for specialized multimodal tasks. MOVE automatically
routes inputs to the most appropriate encoder among candidates such as Unichat,
InternViT, and Texify, thereby enhancing performance across a diverse set of
benchmarks, including ChartQA, MMBench, and MMMU. Experimental results
demonstrate that MOVE achieves competitive accuracy without incurring the
complexities of image slicing for high-resolution images.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 6 figures, 4 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Weakly Supervised Video Scene Graph Generation via Natural Language
  Supervision <span class="chip">ICLR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.15370v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.15370v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kibum Kim, Kanghoon Yoon, Yeonjun In, Jaehyeong Jeon, Jinyoung Moon, Donghyun Kim, Chanyoung Park
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Existing Video Scene Graph Generation (VidSGG) studies are trained in a fully
supervised manner, which requires all frames in a video to be annotated,
thereby incurring high annotation cost compared to Image Scene Graph Generation
(ImgSGG). Although the annotation cost of VidSGG can be alleviated by adopting
a weakly supervised approach commonly used for ImgSGG (WS-ImgSGG) that uses
image captions, there are two key reasons that hinder such a naive adoption: 1)
Temporality within video captions, i.e., unlike image captions, video captions
include temporal markers (e.g., before, while, then, after) that indicate time
related details, and 2) Variability in action duration, i.e., unlike human
actions in image captions, human actions in video captions unfold over varying
duration. To address these issues, we propose a Natural Language-based Video
Scene Graph Generation (NL-VSGG) framework that only utilizes the readily
available video captions for training a VidSGG model. NL-VSGG consists of two
key modules: Temporality-aware Caption Segmentation (TCS) module and Action
Duration Variability-aware caption-frame alignment (ADV) module. Specifically,
TCS segments the video captions into multiple sentences in a temporal order
based on a Large Language Model (LLM), and ADV aligns each segmented sentence
with appropriate frames considering the variability in action duration. Our
approach leads to a significant enhancement in performance compared to simply
applying the WS-ImgSGG pipeline to VidSGG on the Action Genome dataset. As a
further benefit of utilizing the video captions as weak supervision, we show
that the VidSGG model trained by NL-VSGG is able to predict a broader range of
action classes that are not included in the training data, which makes our
framework practical in reality.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, ICLR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ M2LADS Demo: A System for Generating Multimodal Learning Analytics
  Dashboards <span class="chip">AAAI 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.15363v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.15363v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alvaro Becerra, Roberto Daza, Ruth Cobos, Aythami Morales, Julian Fierrez
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a demonstration of a web-based system called M2LADS ("System for
Generating Multimodal Learning Analytics Dashboards"), designed to integrate,
synchronize, visualize, and analyze multimodal data recorded during
computer-based learning sessions with biosensors. This system presents a range
of biometric and behavioral data on web-based dashboards, providing detailed
insights into various physiological and activity-based metrics. The multimodal
data visualized include electroencephalogram (EEG) data for assessing attention
and brain activity, heart rate metrics, eye-tracking data to measure visual
attention, webcam video recordings, and activity logs of the monitored tasks.
M2LADS aims to assist data scientists in two key ways: (1) by providing a
comprehensive view of participants' experiences, displaying all data
categorized by the activities in which participants are engaged, and (2) by
synchronizing all biosignals and videos, facilitating easier data relabeling if
any activity information contains errors.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published in the Workshop on Innovation and Responsibility in
  AI-Supported Education (iRAISE25) at AAAI 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SentiFormer: Metadata Enhanced <span class="highlight-title">Transformer</span> for Image Sentiment Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.15322v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.15322v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bin Feng, Shulan Ruan, Mingzheng Yang, Dongxuan Han, Huijie Liu, Kai Zhang, Qi Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As more and more internet users post images online to express their daily
emotions, image sentiment analysis has attracted increasing attention.
Recently, researchers generally tend to design different neural networks to
extract visual features from images for sentiment analysis. Despite the
significant progress, metadata, the data (e.g., text descriptions and keyword
tags) for describing the image, has not been sufficiently explored in this
task. In this paper, we propose a novel Metadata Enhanced Transformer for
sentiment analysis (SentiFormer) to fuse multiple metadata and the
corresponding image into a unified framework. Specifically, we first obtain
multiple metadata of the image and unify the representations of diverse data.
To adaptively learn the appropriate weights for each metadata, we then design
an adaptive relevance learning module to highlight more effective information
while suppressing weaker ones. Moreover, we further develop a cross-modal
fusion module to fuse the adaptively learned representations and make the final
prediction. Extensive experiments on three publicly available datasets
demonstrate the superiority and rationality of our proposed method.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Research advances on fish feeding behavior recognition and intensity
  quantification methods in aquaculture 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.15311v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.15311v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shulong Zhang, Daoliang Li, Jiayin Zhao, Mingyuan Yao, Yingyi Chen, Yukang Huo, Xiao Liu, Haihua Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As a key part of aquaculture management, fish feeding behavior recognition
and intensity quantification has been a hot area of great concern to
researchers, and it plays a crucial role in monitoring fish health, guiding
baiting work and improving aquaculture efficiency. In order to better carry out
the related work in the future, this paper firstly reviews the research
advances of fish feeding behavior recognition and intensity quantification
methods based on computer vision, acoustics and sensors in a single modality.
Then the application of the current emerging multimodal fusion in fish feeding
behavior recognition and intensity quantification methods is expounded.
Finally, the advantages and disadvantages of various techniques are compared
and analyzed, and the future research directions are envisioned.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>22 pages, 4 figures,</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Road Traffic Sign Recognition method using Siamese network Combining
  Efficient-CNN based Encoder 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.15307v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.15307v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhenghao Xi, Yuchao Shao, Yang Zheng, Xiang Liu, Yaqi Liu, Yitong Cai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Traffic signs recognition (TSR) plays an essential role in assistant driving
and intelligent transportation system. However, the noise of complex
environment may lead to motion-blur or occlusion problems, which raise the
tough challenge to real-time recognition with high accuracy and robust. In this
article, we propose IECES-network which with improved encoders and Siamese net.
The three-stage approach of our method includes Efficient-CNN based encoders,
Siamese backbone and the fully-connected layers. We firstly use convolutional
encoders to extract and encode the traffic sign features of augmented training
samples and standard images. Then, we design the Siamese neural network with
Efficient-CNN based encoder and contrastive loss function, which can be trained
to improve the robustness of TSR problem when facing the samples of motion-blur
and occlusion by computing the distance between inputs and templates.
Additionally, the template branch of the proposed network can be stopped when
executing the recognition tasks after training to raise the process speed of
our real-time model, and alleviate the computational resource and parameter
scale. Finally, we recombined the feature code and a fully-connected layer with
SoftMax function to classify the codes of samples and recognize the category of
traffic signs. The results of experiments on the Tsinghua-Tencent 100K dataset
and the German Traffic Sign Recognition Benchmark dataset demonstrate the
performance of the proposed IECESnetwork. Compared with other state-of-the-art
methods, in the case of motion-blur and occluded environment, the proposed
method achieves competitive performance precision-recall and accuracy metric
average is 88.1%, 86.43% and 86.1% with a 2.9M lightweight scale, respectively.
Moreover, processing time of our model is 0.1s per frame, of which the speed is
increased by 1.5 times compared with existing methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Novel Riemannian Sparse Representation Learning Network for
  Polarimetric SAR Image Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.15302v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.15302v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junfei Shi, Mengmeng Nie, Weisi Lin, Haiyan Jin, Junhuai Li, Rui Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep learning is an effective end-to-end method for Polarimetric Synthetic
Aperture Radar(PolSAR) image classification, but it lacks the guidance of
related mathematical principle and is essentially a black-box model. In
addition, existing deep models learn features in Euclidean space, where PolSAR
complex matrix is commonly converted into a complex-valued vector as the
network input, distorting matrix structure and channel relationship. However,
the complex covariance matrix is Hermitian positive definite (HPD), and resides
on a Riemannian manifold instead of a Euclidean one. Existing methods cannot
measure the geometric distance of HPD matrices and easily cause some
misclassifications due to inappropriate Euclidean measures. To address these
issues, we propose a novel Riemannian Sparse Representation Learning Network
(SRSR CNN) for PolSAR images. Firstly, a superpixel-based Riemannian Sparse
Representation (SRSR) model is designed to learn the sparse features with
Riemannian metric. Then, the optimization procedure of the SRSR model is
inferred and further unfolded into an SRSRnet, which can automatically learn
the sparse coefficients and dictionary atoms. Furthermore, to learn contextual
high-level features, a CNN-enhanced module is added to improve classification
performance. The proposed network is a Sparse Representation (SR) guided deep
learning model, which can directly utilize the covariance matrix as the network
input, and utilize Riemannian metric to learn geometric structure and sparse
features of complex matrices in Riemannian space. Experiments on three real
PolSAR datasets demonstrate that the proposed method surpasses state-of-the-art
techniques in ensuring accurate edge details and correct region homogeneity for
classification.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 9 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Soybean pod and seed counting in both outdoor fields and indoor
  laboratories using unions of deep neural networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.15286v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.15286v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tianyou Jiang, Mingshun Shao, Tianyi Zhang, Xiaoyu Liu, Qun Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Automatic counting soybean pods and seeds in outdoor fields allows for rapid
yield estimation before harvesting, while indoor laboratory counting offers
greater accuracy. Both methods can significantly accelerate the breeding
process. However, it remains challenging for accurately counting pods and seeds
in outdoor fields, and there are still no accurate enough tools for counting
pods and seeds in laboratories. In this study, we developed efficient deep
learning models for counting soybean pods and seeds in both outdoor fields and
indoor laboratories. For outdoor fields, annotating not only visible seeds but
also occluded seeds makes YOLO have the ability to estimate the number of
soybean seeds that are occluded. Moreover, we enhanced YOLO architecture by
integrating it with HQ-SAM (YOLO-SAM), and domain adaptation techniques
(YOLO-DA), to improve model robustness and generalization across soybean images
taken in outdoor fields. Testing on soybean images from the outdoor field, we
achieved a mean absolute error (MAE) of 6.13 for pod counting and 10.05 for
seed counting. For the indoor setting, we utilized Mask-RCNN supplemented with
a Swin Transformer module (Mask-RCNN-Swin), models were trained exclusively on
synthetic training images generated from a small set of labeled data. This
approach resulted in near-perfect accuracy, with an MAE of 1.07 for pod
counting and 1.33 for seed counting across actual laboratory images from two
distinct studies.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CopyJudge: Automated Copyright Infringement Identification and
  Mitigation in Text-to-Image Diffusion Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.15278v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.15278v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shunchang Liu, Zhuan Shi, Lingjuan Lyu, Yaochu Jin, Boi Faltings
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Assessing whether AI-generated images are substantially similar to
copyrighted works is a crucial step in resolving copyright disputes. In this
paper, we propose CopyJudge, an automated copyright infringement identification
framework that leverages large vision-language models (LVLMs) to simulate
practical court processes for determining substantial similarity between
copyrighted images and those generated by text-to-image diffusion models.
Specifically, we employ an abstraction-filtration-comparison test framework
with multi-LVLM debate to assess the likelihood of infringement and provide
detailed judgment rationales. Based on the judgments, we further introduce a
general LVLM-based mitigation strategy that automatically optimizes infringing
prompts by avoiding sensitive expressions while preserving the non-infringing
content. Besides, our approach can be enhanced by exploring non-infringing
noise vectors within the diffusion latent space via reinforcement learning,
even without modifying the original prompts. Experimental results show that our
identification method achieves comparable state-of-the-art performance, while
offering superior generalization and interpretability across various forms of
infringement, and that our mitigation method could more effectively mitigate
memorization and IP infringement without losing non-infringing expressions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17pages, 8 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Omnidirectional Image Quality Captioning: A Large-scale Database and A
  New Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.15271v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.15271v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiebin Yan, Ziwen Tan, Yuming Fang, Junjie Chen, Wenhui Jiang, Zhou Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The fast growing application of omnidirectional images calls for effective
approaches for omnidirectional image quality assessment (OIQA). Existing OIQA
methods have been developed and tested on homogeneously distorted
omnidirectional images, but it is hard to transfer their success directly to
the heterogeneously distorted omnidirectional images. In this paper, we conduct
the largest study so far on OIQA, where we establish a large-scale database
called OIQ-10K containing 10,000 omnidirectional images with both homogeneous
and heterogeneous distortions. A comprehensive psychophysical study is
elaborated to collect human opinions for each omnidirectional image, together
with the spatial distributions (within local regions or globally) of
distortions, and the head and eye movements of the subjects. Furthermore, we
propose a novel multitask-derived adaptive feature-tailoring OIQA model named
IQCaption360, which is capable of generating a quality caption for an
omnidirectional image in a manner of textual template. Extensive experiments
demonstrate the effectiveness of IQCaption360, which outperforms
state-of-the-art methods by a significant margin on the proposed OIQ-10K
database. The OIQ-10K database and the related source codes are available at
https://github.com/WenJuing/IQCaption360.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Quantum autoencoders for image classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.15254v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.15254v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hinako Asaoka, Kazue Kudo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Classical machine learning often struggles with complex, high-dimensional
data. Quantum machine learning offers a potential solution, promising more
efficient processing. While the quantum convolutional neural network (QCNN), a
hybrid quantum-classical algorithm, is suitable for current noisy
intermediate-scale quantum-era hardware, its learning process relies heavily on
classical computation. Future large-scale, gate-based quantum computers could
unlock the full potential of quantum effects in machine learning. In contrast
to QCNNs, quantum autoencoders (QAEs) leverage classical optimization solely
for parameter tuning. Data compression and reconstruction are handled entirely
within quantum circuits, enabling purely quantum-based feature extraction. This
study introduces a novel image-classification approach using QAEs, achieving
classification without requiring additional qubits compared with conventional
QAE implementations. The quantum circuit structure significantly impacts
classification accuracy. Unlike hybrid methods such as QCNN, QAE-based
classification emphasizes quantum computation. Our experiments demonstrate high
accuracy in a four-class classification task, evaluating various quantum-gate
configurations to understand the impact of different parameterized quantum
circuit (ansatz) structures on classification performance. Our results reveal
that specific ansatz structures achieve superior accuracy, and we provide an
analysis of their effectiveness. Moreover, the proposed approach achieves
performance comparable to that of conventional machine-learning methods while
significantly reducing the number of parameters requiring optimization. These
findings indicate that QAEs can serve as efficient classification models with
fewer parameters and highlight the potential of utilizing quantum circuits for
complete end-to-end learning, a departure from hybrid approaches such as QCNN.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SiMHand: Mining Similar Hands for Large-Scale 3D Hand Pose <span class="highlight-title">Pre-train</span>ing <span class="chip">ICLR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.15251v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.15251v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nie Lin, Takehiko Ohkawa, Yifei Huang, Mingfang Zhang, Minjie Cai, Ming Li, Ryosuke Furuta, Yoichi Sato
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a framework for pre-training of 3D hand pose estimation from
in-the-wild hand images sharing with similar hand characteristics, dubbed
SimHand. Pre-training with large-scale images achieves promising results in
various tasks, but prior methods for 3D hand pose pre-training have not fully
utilized the potential of diverse hand images accessible from in-the-wild
videos. To facilitate scalable pre-training, we first prepare an extensive pool
of hand images from in-the-wild videos and design our pre-training method with
contrastive learning. Specifically, we collect over 2.0M hand images from
recent human-centric videos, such as 100DOH and Ego4D. To extract
discriminative information from these images, we focus on the similarity of
hands: pairs of non-identical samples with similar hand poses. We then propose
a novel contrastive learning method that embeds similar hand pairs closer in
the feature space. Our method not only learns from similar samples but also
adaptively weights the contrastive learning loss based on inter-sample
distance, leading to additional performance gains. Our experiments demonstrate
that our method outperforms conventional contrastive learning approaches that
produce positive pairs sorely from a single image with data augmentation. We
achieve significant improvements over the state-of-the-art method (PeCLR) in
various datasets, with gains of 15% on FreiHand, 10% on DexYCB, and 4% on
AssemblyHands.
  Our code is available at https://github.com/ut-vision/SiMHand.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR 2025. arXiv admin note: text overlap with arXiv:2409.09714</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AutoMR: A Universal Time Series Motion Recognition Pipeline 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.15228v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.15228v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Likun Zhang, Sicheng Yang, Zhuo Wang, Haining Liang, Junxiao Shen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we present an end-to-end automated motion recognition (AutoMR)
pipeline designed for multimodal datasets. The proposed framework seamlessly
integrates data preprocessing, model training, hyperparameter tuning, and
evaluation, enabling robust performance across diverse scenarios. Our approach
addresses two primary challenges: 1) variability in sensor data formats and
parameters across datasets, which traditionally requires task-specific machine
learning implementations, and 2) the complexity and time consumption of
hyperparameter tuning for optimal model performance. Our library features an
all-in-one solution incorporating QuartzNet as the core model, automated
hyperparameter tuning, and comprehensive metrics tracking. Extensive
experiments demonstrate its effectiveness on 10 diverse datasets, achieving
state-of-the-art performance. This work lays a solid foundation for deploying
motion-capture solutions across varied real-world applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Lung-DDPM: Semantic Layout-guided Diffusion Models for Thoracic CT Image
  Synthesis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.15204v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.15204v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yifan Jiang, Yannick Lemaréchal, Josée Bafaro, Jessica Abi-Rjeile, Philippe Joubert, Philippe Després, Venkata Manem
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the rapid development of artificial intelligence (AI), AI-assisted
medical imaging analysis demonstrates remarkable performance in early lung
cancer screening. However, the costly annotation process and privacy concerns
limit the construction of large-scale medical datasets, hampering the further
application of AI in healthcare. To address the data scarcity in lung cancer
screening, we propose Lung-DDPM, a thoracic CT image synthesis approach that
effectively generates high-fidelity 3D synthetic CT images, which prove helpful
in downstream lung nodule segmentation tasks. Our method is based on semantic
layout-guided denoising diffusion probabilistic models (DDPM), enabling
anatomically reasonable, seamless, and consistent sample generation even from
incomplete semantic layouts. Our results suggest that the proposed method
outperforms other state-of-the-art (SOTA) generative models in image quality
evaluation and downstream lung nodule segmentation tasks. Specifically,
Lung-DDPM achieved superior performance on our large validation cohort, with a
Fr\'echet inception distance (FID) of 0.0047, maximum mean discrepancy (MMD) of
0.0070, and mean squared error (MSE) of 0.0024. These results were 7.4$\times$,
3.1$\times$, and 29.5$\times$ better than the second-best competitors,
respectively. Furthermore, the lung nodule segmentation model, trained on a
dataset combining real and Lung-DDPM-generated synthetic samples, attained a
dice coefficient (Dice) of 0.3914 and sensitivity of 0.4393. This represents
8.8\% and 18.6\% improvements in DICE and sensitivity compared to the model
trained solely on real samples. The experimental results highlight Lung-DDPM's
potential for a broader range of medical imaging applications, such as general
tumor segmentation, cancer survival estimation, and risk prediction.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The code and pretrained models are available at
  https://github.com/Manem-Lab/Lung-DDPM</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FlipConcept: Tuning-Free Multi-Concept Personalization for Text-to-Image
  Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.15203v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.15203v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Young Beom Woo, Sun Eung Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, methods that integrate multiple personalized concepts into a single
image have garnered significant attention in the field of text-to-image (T2I)
generation. However, existing methods experience performance degradation in
complex scenes with multiple objects due to distortions in non-personalized
regions. To address this issue, we propose FlipConcept, a novel approach that
seamlessly integrates multiple personalized concepts into a single image
without requiring additional tuning. We introduce guided appearance attention
to accurately mimic the appearance of a personalized concept as intended.
Additionally, we introduce mask-guided noise mixing to protect non-personalized
regions during editing. Lastly, we apply background dilution to minimize
attribute leakage, which is the undesired blending of personalized concept
attributes with other objects in the image. In our experiments, we demonstrate
that the proposed method, despite not requiring tuning, outperforms existing
models in both single and multiple personalized concept inference.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ UrbanSAM: Learning Invariance-Inspired Adapters for Segment Anything
  Models in Urban Construction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.15199v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.15199v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chenyu Li, Danfeng Hong, Bing Zhang, Yuxuan Li, Gustau Camps-Valls, Xiao Xiang Zhu, Jocelyn Chanussot
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Object extraction and segmentation from remote sensing (RS) images is a
critical yet challenging task in urban environment monitoring. Urban morphology
is inherently complex, with irregular objects of diverse shapes and varying
scales. These challenges are amplified by heterogeneity and scale disparities
across RS data sources, including sensors, platforms, and modalities, making
accurate object segmentation particularly demanding. While the Segment Anything
Model (SAM) has shown significant potential in segmenting complex scenes, its
performance in handling form-varying objects remains limited due to
manual-interactive prompting. To this end, we propose UrbanSAM, a customized
version of SAM specifically designed to analyze complex urban environments
while tackling scaling effects from remotely sensed observations. Inspired by
multi-resolution analysis (MRA) theory, UrbanSAM incorporates a novel learnable
prompter equipped with a Uscaling-Adapter that adheres to the invariance
criterion, enabling the model to capture multiscale contextual information of
objects and adapt to arbitrary scale variations with theoretical guarantees.
Furthermore, features from the Uscaling-Adapter and the trunk encoder are
aligned through a masked cross-attention operation, allowing the trunk encoder
to inherit the adapter's multiscale aggregation capability. This synergy
enhances the segmentation performance, resulting in more powerful and accurate
outputs, supported by the learned adapter. Extensive experimental results
demonstrate the flexibility and superior segmentation performance of the
proposed UrbanSAM on a global-scale dataset, encompassing scale-varying urban
objects such as buildings, roads, and water.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Interleaved Block-based Learned Image Compression with Feature
  Enhancement and Quantization Error Compensation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.15188v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.15188v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shiqi Jiang, Hui Yuan, Shuai Li, Raouf Hamzaoui, Xu Wang, Junyan Huo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, learned image compression (LIC) methods have achieved
significant performance improvements. However, obtaining a more compact latent
representation and reducing the impact of quantization errors remain key
challenges in the field of LIC. To address these challenges, we propose a
feature extraction module, a feature refinement module, and a feature
enhancement module. Our feature extraction module shuffles the pixels in the
image, splits the resulting image into sub-images, and extracts coarse features
from the sub-images. Our feature refinement module stacks the coarse features
and uses an attention refinement block composed of concatenated
three-dimensional convolution residual blocks to learn more compact latent
features by exploiting correlations across channels, within sub-images
(intra-sub-image correlations), and across sub-images (inter-sub-image
correlations). Our feature enhancement module reduces information loss in the
decoded features following quantization. We also propose a quantization error
compensation module that mitigates the quantization mismatch between training
and testing. Our four modules can be readily integrated into state-of-the-art
LIC methods. Experiments show that combining our modules with Tiny-LIC
outperforms existing LIC methods and image compression standards in terms of
peak signal-to-noise ratio (PSNR) and multi-scale structural similarity
(MS-SSIM) on the Kodak dataset and the CLIC dataset.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LUMINA-Net: Low-light Upgrade through Multi-stage Illumination and Noise
  Adaptation Network for Image Enhancement 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.15186v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.15186v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Namrah Siddiqua, Kim Suneung
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Low-light image enhancement (LLIE) is a crucial task in computer vision aimed
to enhance the visual fidelity of images captured under low-illumination
conditions. Conventional methods frequently struggle to mitigate pervasive
shortcomings such as noise, over-exposure, and color distortion thereby
precipitating a pronounced degradation in image quality. To address these
challenges, we propose LUMINA-Net an advanced deep learning framework designed
specifically by integrating multi-stage illumination and reflectance modules.
First, the illumination module intelligently adjusts brightness and contrast
levels while meticulously preserving intricate textural details. Second, the
reflectance module incorporates a noise reduction mechanism that leverages
spatial attention and channel-wise feature refinement to mitigate noise
contamination. Through a comprehensive suite of experiments conducted on LOL
and SICE datasets using PSNR, SSIM and LPIPS metrics, surpassing
state-of-the-art methodologies and showcasing its efficacy in low-light image
enhancement.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Hierarchical Context <span class="highlight-title">Transformer</span> for Multi-level Semantic Scene
  Understanding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.15184v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.15184v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Luoying Hao, Yan Hu, Yang Yue, Li Wu, Huazhu Fu, Jinming Duan, Jiang Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A comprehensive and explicit understanding of surgical scenes plays a vital
role in developing context-aware computer-assisted systems in the operating
theatre. However, few works provide systematical analysis to enable
hierarchical surgical scene understanding. In this work, we propose to
represent the tasks set [phase recognition --> step recognition --> action and
instrument detection] as multi-level semantic scene understanding (MSSU). For
this target, we propose a novel hierarchical context transformer (HCT) network
and thoroughly explore the relations across the different level tasks.
Specifically, a hierarchical relation aggregation module (HRAM) is designed to
concurrently relate entries inside multi-level interaction information and then
augment task-specific features. To further boost the representation learning of
the different tasks, inter-task contrastive learning (ICL) is presented to
guide the model to learn task-wise features via absorbing complementary
information from other tasks. Furthermore, considering the computational costs
of the transformer, we propose HCT+ to integrate the spatial and temporal
adapter to access competitive performance on substantially fewer tunable
parameters. Extensive experiments on our cataract dataset and a publicly
available endoscopic PSI-AVA dataset demonstrate the outstanding performance of
our method, consistently exceeding the state-of-the-art methods by a large
margin. The code is available at https://github.com/Aurora-hao/HCT.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper has been accepted by the IEEE TCSVT</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ OccProphet: Pushing Efficiency Frontier of Camera-Only 4D Occupancy
  Forecasting with Observer-Forecaster-Refiner Framework <span class="chip">ICLR2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.15180v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.15180v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junliang Chen, Huaiyuan Xu, Yi Wang, Lap-Pui Chau
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Predicting variations in complex traffic environments is crucial for the
safety of autonomous driving. Recent advancements in occupancy forecasting have
enabled forecasting future 3D occupied status in driving environments by
observing historical 2D images. However, high computational demands make
occupancy forecasting less efficient during training and inference stages,
hindering its feasibility for deployment on edge agents. In this paper, we
propose a novel framework, i.e., OccProphet, to efficiently and effectively
learn occupancy forecasting with significantly lower computational requirements
while improving forecasting accuracy. OccProphet comprises three lightweight
components: Observer, Forecaster, and Refiner. The Observer extracts
spatio-temporal features from 3D multi-frame voxels using the proposed
Efficient 4D Aggregation with Tripling-Attention Fusion, while the Forecaster
and Refiner conditionally predict and refine future occupancy inferences.
Experimental results on nuScenes, Lyft-Level5, and nuScenes-Occupancy datasets
demonstrate that OccProphet is both training- and inference-friendly.
OccProphet reduces 58\%$\sim$78\% of the computational cost with a 2.6$\times$
speedup compared with the state-of-the-art Cam4DOcc. Moreover, it achieves
4\%$\sim$18\% relatively higher forecasting accuracy. Code and models are
publicly available at https://github.com/JLChen-C/OccProphet.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICLR2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Nonlinear Dynamical Systems for Automatic Face Annotation in Head
  Tracking and Pose Estimation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.15179v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.15179v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Thoa Thieu, Roderick Melnik
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Facial landmark tracking plays a vital role in applications such as facial
recognition, expression analysis, and medical diagnostics. In this paper, we
consider the performance of the Extended Kalman Filter (EKF) and Unscented
Kalman Filter (UKF) in tracking 3D facial motion in both deterministic and
stochastic settings. We first analyze a noise-free environment where the state
transition is purely deterministic, demonstrating that UKF outperforms EKF by
achieving lower mean squared error (MSE) due to its ability to capture
higher-order nonlinearities. However, when stochastic noise is introduced, EKF
exhibits superior robustness, maintaining lower mean square error (MSE)
compared to UKF, which becomes more sensitive to measurement noise and
occlusions. Our results highlight that UKF is preferable for high-precision
applications in controlled environments, whereas EKF is better suited for
real-world scenarios with unpredictable noise. These findings provide practical
insights for selecting the appropriate filtering technique in 3D facial
tracking applications, such as motion capture and facial recognition.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>25 pages, 10 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Methods and Trends in Detecting Generated Images: A Comprehensive <span class="highlight-title">Review</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.15176v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.15176v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Arpan Mahara, Naphtali Rishe
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The proliferation of generative models, such as Generative Adversarial
Networks (GANs), Diffusion Models, and Variational Autoencoders (VAEs), has
enabled the synthesis of high-quality multimedia data. However, these
advancements have also raised significant concerns regarding adversarial
attacks, unethical usage, and societal harm. Recognizing these challenges,
researchers have increasingly focused on developing methodologies to detect
synthesized data effectively, aiming to mitigate potential risks. Prior reviews
have primarily focused on deepfake detection and often lack coverage of recent
advancements in synthetic image detection, particularly methods leveraging
multimodal frameworks for improved forensic analysis. To address this gap, the
present survey provides a comprehensive review of state-of-the-art methods for
detecting and classifying synthetic images generated by advanced generative AI
models. This review systematically examines core detection methodologies,
identifies commonalities among approaches, and categorizes them into meaningful
taxonomies. Furthermore, given the crucial role of large-scale datasets in this
field, we present an overview of publicly available datasets that facilitate
further research and benchmarking in synthetic data detection.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>30 pages, 4 Figures, 10 Tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FD-LSCIC: Frequency Decomposition-based Learned Screen Content Image
  Compression 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.15174v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.15174v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shiqi Jiang, Hui Yuan, Shuai Li, Huanqiang Zeng, Sam Kwong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The learned image compression (LIC) methods have already surpassed
traditional techniques in compressing natural scene (NS) images. However,
directly applying these methods to screen content (SC) images, which possess
distinct characteristics such as sharp edges, repetitive patterns, embedded
text and graphics, yields suboptimal results. This paper addresses three key
challenges in SC image compression: learning compact latent features, adapting
quantization step sizes, and the lack of large SC datasets. To overcome these
challenges, we propose a novel compression method that employs a
multi-frequency two-stage octave residual block (MToRB) for feature extraction,
a cascaded triple-scale feature fusion residual block (CTSFRB) for multi-scale
feature integration and a multi-frequency context interaction module (MFCIM) to
reduce inter-frequency correlations. Additionally, we introduce an adaptive
quantization module that learns scaled uniform noise for each frequency
component, enabling flexible control over quantization granularity.
Furthermore, we construct a large SC image compression dataset (SDU-SCICD10K),
which includes over 10,000 images spanning basic SC images, computer-rendered
images, and mixed NS and SC images from both PC and mobile platforms.
Experimental results demonstrate that our approach significantly improves SC
image compression performance, outperforming traditional standards and
state-of-the-art learning-based methods in terms of peak signal-to-noise ratio
(PSNR) and multi-scale structural similarity (MS-SSIM).
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ M3-AGIQA: Multimodal, Multi-Round, Multi-Aspect AI-Generated Image
  Quality Assessment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.15167v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.15167v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chuan Cui, Kejiang Chen, Zhihua Wei, Wen Shen, Weiming Zhang, Nenghai Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rapid advancement of AI-generated image (AGI) models has introduced
significant challenges in evaluating their quality, which requires considering
multiple dimensions such as perceptual quality, prompt correspondence, and
authenticity. To address these challenges, we propose M3-AGIQA, a comprehensive
framework for AGI quality assessment that is Multimodal, Multi-Round, and
Multi-Aspect. Our approach leverages the capabilities of Multimodal Large
Language Models (MLLMs) as joint text and image encoders and distills advanced
captioning capabilities from online MLLMs into a local model via Low-Rank
Adaptation (LoRA) fine-tuning. The framework includes a structured multi-round
evaluation mechanism, where intermediate image descriptions are generated to
provide deeper insights into the quality, correspondence, and authenticity
aspects. To align predictions with human perceptual judgments, a predictor
constructed by an xLSTM and a regression head is incorporated to process
sequential logits and predict Mean Opinion Scores (MOSs). Extensive experiments
conducted on multiple benchmark datasets demonstrate that M3-AGIQA achieves
state-of-the-art performance, effectively capturing nuanced aspects of AGI
quality. Furthermore, cross-dataset validation confirms its strong
generalizability. The code is available at
https://github.com/strawhatboy/M3-AGIQA.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 5 figures. This work has been submitted to the IEEE for
  possible publication</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ HOpenCls: Training Hyperspectral Image Open-Set Classifiers in Their
  Living Environments 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.15163v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.15163v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hengwei Zhao, Xinyu Wang, Zhuo Zheng, Jingtao Li, Yanfei Zhong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Hyperspectral image (HSI) open-set classification is critical for HSI
classification models deployed in real-world environments, where classifiers
must simultaneously classify known classes and reject unknown classes. Recent
methods utilize auxiliary unknown classes data to improve classification
performance. However, the auxiliary unknown classes data is strongly assumed to
be completely separable from known classes and requires labor-intensive
annotation. To address this limitation, this paper proposes a novel framework,
HOpenCls, to leverage the unlabeled wild data-that is the mixture of known and
unknown classes. Such wild data is abundant and can be collected freely during
deploying classifiers in their living environments. The key insight is
reformulating the open-set HSI classification with unlabeled wild data as a
positive-unlabeled (PU) learning problem. Specifically, the multi-label
strategy is introduced to bridge the PU learning and open-set HSI
classification, and then the proposed gradient contraction and gradient
expansion module to make this PU learning problem tractable from the
observation of abnormal gradient weights associated with wild data. Extensive
experiment results demonstrate that incorporating wild data has the potential
to significantly enhance open-set HSI classification in complex real-world
scenarios.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Optimized Pap Smear Image Enhancement: Hybrid PMD Filter-CLAHE Using
  Spider Monkey Optimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.15156v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.15156v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ach Khozaimi, Isnani Darti, Syaiful Anam, Wuryansari Muharini Kusumawinahyu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Pap smear image quality is crucial for cervical cancer detection. This study
introduces an optimized hybrid approach that combines the Perona-Malik
Diffusion (PMD) filter with contrast-limited adaptive histogram equalization
(CLAHE) to enhance Pap smear image quality. The PMD filter reduces the image
noise, whereas CLAHE improves the image contrast. The hybrid method was
optimized using spider monkey optimization (SMO PMD-CLAHE). BRISQUE and CEIQ
are the new objective functions for the PMD filter and CLAHE optimization,
respectively. The simulations were conducted using the SIPaKMeD dataset. The
results indicate that SMO outperforms state-of-the-art methods in optimizing
the PMD filter and CLAHE. The proposed method achieved an average effective
measure of enhancement (EME) of 5.45, root mean square (RMS) contrast of 60.45,
Michelson's contrast (MC) of 0.995, and entropy of 6.80. This approach offers a
new perspective for improving Pap smear image quality.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Confidence-Weighted Boundary-Aware Learning for Semi-Supervised Semantic
  Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.15152v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.15152v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ebenezer Tarubinga, Jenifer Kalafatovich Espinoza
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Semi-supervised semantic segmentation (SSSS) aims to improve segmentation
performance by utilising unlabeled data alongside limited labeled samples.
Existing SSSS methods often face challenges such as coupling, where
over-reliance on initial labeled data leads to suboptimal learning;
confirmation bias, where incorrect predictions reinforce themselves repeatedly;
and boundary blur caused by insufficient boundary-awareness and ambiguous edge
information. To address these issues, we propose CW-BASS, a novel framework for
SSSS. In order to mitigate the impact of incorrect predictions, we assign
confidence weights to pseudo-labels. Additionally, we leverage
boundary-delineation techniques, which, despite being extensively explored in
weakly-supervised semantic segmentation (WSSS) remain under-explored in SSSS.
Specifically, our approach: (1) reduces coupling through a confidence-weighted
loss function that adjusts the influence of pseudo-labels based on their
predicted confidence scores, (2) mitigates confirmation bias with a dynamic
thresholding mechanism that learns to filter out pseudo-labels based on model
performance, (3) resolves boundary blur with a boundary-aware module that
enhances segmentation accuracy near object boundaries, and (4) reduces label
noise with a confidence decay strategy that progressively refines pseudo-labels
during training. Extensive experiments on the Pascal VOC 2012 and Cityscapes
demonstrate that our method achieves state-of-the-art performance. Moreover,
using only 1/8 or 12.5\% of labeled data, our method achieves a mIoU of 75.81
on Pascal VOC 2012, highlighting its effectiveness in limited-label settings.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ TransMamba: Fast Universal Architecture Adaption from <span class="highlight-title">Transformer</span>s to
  Mamba 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.15130v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.15130v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiuwei Chen, Sihao Lin, Xiao Dong, Zisheng Chen, Meng Cao, Jianhua Han, Hang Xu, Xiaodan Liang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Transformers have been favored in both uni-modal and multi-modal foundation
models for their flexible scalability in attention modules. Consequently, a
number of pre-trained Transformer models, e.g., LLaVA, CLIP, and DEIT, are
publicly available. Recent research has introduced subquadratic architectures
like Mamba, which enables global awareness with linear complexity.
Nevertheless, training specialized subquadratic architectures from scratch for
certain tasks is both resource-intensive and time-consuming. As a motivator, we
explore cross-architecture training to transfer the ready knowledge in existing
Transformer models to alternative architecture Mamba, termed TransMamba. Our
approach employs a two-stage strategy to expedite training new Mamba models,
ensuring effectiveness in across uni-modal and cross-modal tasks. Concerning
architecture disparities, we project the intermediate features into an aligned
latent space before transferring knowledge. On top of that, a Weight Subcloning
and Adaptive Bidirectional distillation method (WSAB) is introduced for
knowledge transfer without limitations on varying layer counts. For cross-modal
learning, we propose a cross-Mamba module that integrates language awareness
into Mamba's visual features, enhancing the cross-modal interaction
capabilities of Mamba architecture. Despite using less than 75% of the training
data typically required for training from scratch, TransMamba boasts
substantially stronger performance across various network architectures and
downstream tasks, including image classification, visual question answering,
and text-video retrieval. The code will be publicly available.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DAM-Seg: Anatomically accurate cardiac segmentation using Dense
  Associative Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.15128v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.15128v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zahid Ullah, Jihie Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep learning-based cardiac segmentation has seen significant advancements
over the years. Many studies have tackled the challenge of anatomically
incorrect segmentation predictions by introducing auxiliary modules. These
modules either post-process segmentation outputs or enforce consistency between
specific points to ensure anatomical correctness. However, such approaches
often increase network complexity, require separate training for these modules,
and may lack robustness in scenarios with poor visibility. To address these
limitations, we propose a novel transformer-based architecture that leverages
dense associative networks to learn and retain specific patterns inherent to
cardiac inputs. Unlike traditional methods, our approach restricts the network
to memorize a limited set of patterns. During forward propagation, a weighted
sum of these patterns is used to enforce anatomical correctness in the output.
Since these patterns are input-independent, the model demonstrates enhanced
robustness, even in cases with poor visibility. The proposed pipeline was
evaluated on two publicly available datasets, CAMUS and CardiacNet.
Experimental results indicate that our model consistently outperforms baseline
approaches across all metrics, highlighting its effectiveness and reliability
for cardiac segmentation tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 7 figures, 5 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CurricuVLM: Towards Safe Autonomous Driving via Personalized
  Safety-Critical Curriculum Learning with Vision-Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.15119v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.15119v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zihao Sheng, Zilin Huang, Yansong Qu, Yue Leng, Sruthi Bhavanam, Sikai Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Ensuring safety in autonomous driving systems remains a critical challenge,
particularly in handling rare but potentially catastrophic safety-critical
scenarios. While existing research has explored generating safety-critical
scenarios for autonomous vehicle (AV) testing, there is limited work on
effectively incorporating these scenarios into policy learning to enhance
safety. Furthermore, developing training curricula that adapt to an AV's
evolving behavioral patterns and performance bottlenecks remains largely
unexplored. To address these challenges, we propose CurricuVLM, a novel
framework that leverages Vision-Language Models (VLMs) to enable personalized
curriculum learning for autonomous driving agents. Our approach uniquely
exploits VLMs' multimodal understanding capabilities to analyze agent behavior,
identify performance weaknesses, and dynamically generate tailored training
scenarios for curriculum adaptation. Through comprehensive analysis of unsafe
driving situations with narrative descriptions, CurricuVLM performs in-depth
reasoning to evaluate the AV's capabilities and identify critical behavioral
patterns. The framework then synthesizes customized training scenarios
targeting these identified limitations, enabling effective and personalized
curriculum learning. Extensive experiments on the Waymo Open Motion Dataset
show that CurricuVLM outperforms state-of-the-art baselines across both regular
and safety-critical scenarios, achieving superior performance in terms of
navigation success, driving efficiency, and safety metrics. Further analysis
reveals that CurricuVLM serves as a general approach that can be integrated
with various RL algorithms to enhance autonomous driving systems. The code and
demo video are available at: https://zihaosheng.github.io/CurricuVLM/.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Assessing a Single Student's Concentration on Learning Platforms: A
  Machine Learning-Enhanced EEG-Based Framework 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.15107v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.15107v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zewen Zhuo, Mohamad Najafi, Hazem Zein, Amine Nait-Ali
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This study introduces a specialized pipeline designed to classify the
concentration state of an individual student during online learning sessions by
training a custom-tailored machine learning model. Detailed protocols for
acquiring and preprocessing EEG data are outlined, along with the extraction of
fifty statistical features from five EEG signal bands: alpha, beta, theta,
delta, and gamma. Following feature extraction, a thorough feature selection
process was conducted to optimize the data inputs for a personalized analysis.
The study also explores the benefits of hyperparameter fine-tuning to enhance
the classification accuracy of the student's concentration state. EEG signals
were captured from the student using a Muse headband (Gen 2), equipped with
five electrodes (TP9, AF7, AF8, TP10, and a reference electrode NZ), during
engagement with educational content on computer-based e-learning platforms.
Employing a random forest model customized to the student's data, we achieved
remarkable classification performance, with test accuracies of 97.6% in the
computer-based learning setting and 98% in the virtual reality setting. These
results underscore the effectiveness of our approach in delivering personalized
insights into student concentration during online educational activities.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Sparse Voxels Rasterization: Real-time High-fidelity Radiance Field
  Rendering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.04459v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.04459v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Cheng Sun, Jaesung Choe, Charles Loop, Wei-Chiu Ma, Yu-Chiang Frank Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose an efficient radiance field rendering algorithm that incorporates
a rasterization process on adaptive sparse voxels without neural networks or 3D
Gaussians. There are two key contributions coupled with the proposed system.
The first is to adaptively and explicitly allocate sparse voxels to different
levels of detail within scenes, faithfully reproducing scene details with
$65536^3$ grid resolution while achieving high rendering frame rates. Second,
we customize a rasterizer for efficient adaptive sparse voxels rendering. We
render voxels in the correct depth order by using ray direction-dependent
Morton ordering, which avoids the well-known popping artifact found in Gaussian
splatting. Our method improves the previous neural-free voxel model by over 4db
PSNR and more than 10x FPS speedup, achieving state-of-the-art comparable
novel-view synthesis results. Additionally, our voxel representation is
seamlessly compatible with grid-based 3D processing techniques such as Volume
Fusion, Voxel Pooling, and Marching Cubes, enabling a wide range of future
extensions and applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page at https://svraster.github.io/ Code at
  https://github.com/NVlabs/svraster</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ <span class="highlight-title">Self-Supervised</span> Diffusion MRI Denoising via Iterative and Stable
  Refinement 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.13514v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.13514v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chenxu Wu, Qingpeng Kong, Zihang Jiang, S. Kevin Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Magnetic Resonance Imaging (MRI), including diffusion MRI (dMRI), serves as a
``microscope'' for anatomical structures and routinely mitigates the influence
of low signal-to-noise ratio scans by compromising temporal or spatial
resolution. However, these compromises fail to meet clinical demands for both
efficiency and precision. Consequently, denoising is a vital preprocessing
step, particularly for dMRI, where clean data is unavailable. In this paper, we
introduce Di-Fusion, a fully self-supervised denoising method that leverages
the latter diffusion steps and an adaptive sampling process. Unlike previous
approaches, our single-stage framework achieves efficient and stable training
without extra noise model training and offers adaptive and controllable results
in the sampling process. Our thorough experiments on real and simulated data
demonstrate that Di-Fusion achieves state-of-the-art performance in
microstructure modeling, tractography tracking, and other downstream tasks.
Code is available at https://github.com/FouierL/Di-Fusion.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>39pages, 34figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Health<span class="highlight-title">GPT</span>: A Medical Large Vision-Language Model for Unifying
  Comprehension and Generation via Heterogeneous Knowledge Adaptation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.09838v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.09838v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tianwei Lin, Wenqiao Zhang, Sijing Li, Yuqian Yuan, Binhe Yu, Haoyuan Li, Wanggui He, Hao Jiang, Mengze Li, Xiaohui Song, Siliang Tang, Jun Xiao, Hui Lin, Yueting Zhuang, Beng Chin Ooi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present HealthGPT, a powerful Medical Large Vision-Language Model
(Med-LVLM) that integrates medical visual comprehension and generation
capabilities within a unified autoregressive paradigm. Our bootstrapping
philosophy is to progressively adapt heterogeneous comprehension and generation
knowledge to pre-trained large language models (LLMs). This is achieved through
a novel heterogeneous low-rank adaptation (H-LoRA) technique, which is
complemented by a tailored hierarchical visual perception approach and a
three-stage learning strategy. To effectively learn the HealthGPT, we devise a
comprehensive medical domain-specific comprehension and generation dataset
called VL-Health. Experimental results demonstrate exceptional performance and
scalability of HealthGPT in medical visual unified tasks. Our project can be
accessed at https://github.com/DCDmllm/HealthGPT.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Comments: added project page</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Tuning the Frequencies: Robust Training for Sinusoidal Neural Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.21121v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.21121v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tiago Novello, Diana Aldana, Andre Araujo, Luiz Velho
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Sinusoidal neural networks have been shown effective as implicit neural
representations (INRs) of low-dimensional signals, due to their smoothness and
high representation capacity. However, initializing and training them remain
empirical tasks which lack on deeper understanding to guide the learning
process. To fill this gap, our work introduces a theoretical framework that
explains the capacity property of sinusoidal networks and offers robust control
mechanisms for initialization and training. Our analysis is based on a novel
amplitude-phase expansion of the sinusoidal multilayer perceptron, showing how
its layer compositions produce a large number of new frequencies expressed as
integer combinations of the input frequencies. This relationship can be
directly used to initialize the input neurons, as a form of spectral sampling,
and to bound the network's spectrum while training. Our method, referred to as
TUNER (TUNing sinusoidal nEtwoRks), greatly improves the stability and
convergence of sinusoidal INR training, leading to detailed reconstructions,
while preventing overfitting.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ TexLiDAR: Automated Text Understanding for Panoramic LiDAR Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.04385v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.04385v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Naor Cohen, Roy Orfaig, Ben-Zion Bobrovsky
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Efforts to connect LiDAR data with text, such as LidarCLIP, have primarily
focused on embedding 3D point clouds into CLIP text-image space. However, these
approaches rely on 3D point clouds, which present challenges in encoding
efficiency and neural network processing. With the advent of advanced LiDAR
sensors like Ouster OS1, which, in addition to 3D point clouds, produce fixed
resolution depth, signal, and ambient panoramic 2D images, new opportunities
emerge for LiDAR based tasks. In this work, we propose an alternative approach
to connect LiDAR data with text by leveraging 2D imagery generated by the OS1
sensor instead of 3D point clouds. Using the Florence 2 large model in a
zero-shot setting, we perform image captioning and object detection. Our
experiments demonstrate that Florence 2 generates more informative captions and
achieves superior performance in object detection tasks compared to existing
methods like CLIP. By combining advanced LiDAR sensor data with a large
pre-trained model, our approach provides a robust and accurate solution for
challenging detection scenarios, including real-time applications requiring
high accuracy and robustness.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ AI and Entrepreneurship: Facial Recognition Technology Detects
  Entrepreneurs, Outperforming Human Experts 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.03765v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.03765v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Martin Obschonka, Christian Fisch, Tharindu Fernando, Clinton Fookes
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Occupational outcomes like entrepreneurship are generally considered personal
information that individuals should have the autonomy to disclose. With the
advancing capability of artificial intelligence (AI) to infer private details
from widely available human-centric data (e.g., social media), it is crucial to
investigate whether AI can accurately extract private occupational information
from such data. In this study, we demonstrate that deep neural networks can
classify individuals as entrepreneurs with high accuracy based on facial images
sourced from Crunchbase, a premier source for entrepreneurship data. Utilizing
a dataset comprising facial images of 40,728 individuals, including both
entrepreneurs and non-entrepreneurs, we train a Convolutional Neural Network
(CNN) using a contrastive learning approach based on pairs of facial images
(one entrepreneur and one non-entrepreneur per pair). While human experts
(n=650) and trained participants (n=133) were unable to classify entrepreneurs
with accuracy above chance levels (>50%), our AI model achieved a
classification accuracy of 79.51%. Several robustness tests indicate that this
high level of accuracy is maintained under various conditions. These results
indicate privacy risks for entrepreneurs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>46 pages, 2 tables, 11 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LayerPano3D: Layered 3D Panorama for Hyper-Immersive Scene Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.13252v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.13252v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shuai Yang, Jing Tan, Mengchen Zhang, Tong Wu, Yixuan Li, Gordon Wetzstein, Ziwei Liu, Dahua Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  3D immersive scene generation is a challenging yet critical task in computer
vision and graphics. A desired virtual 3D scene should 1) exhibit
omnidirectional view consistency, and 2) allow for free exploration in complex
scene hierarchies. Existing methods either rely on successive scene expansion
via inpainting or employ panorama representation to represent large FOV scene
environments. However, the generated scene suffers from semantic drift during
expansion and is unable to handle occlusion among scene hierarchies. To tackle
these challenges, we introduce Layerpano3D, a novel framework for full-view,
explorable panoramic 3D scene generation from a single text prompt. Our key
insight is to decompose a reference 2D panorama into multiple layers at
different depth levels, where each layer reveals the unseen space from the
reference views via diffusion prior. Layerpano3D comprises multiple dedicated
designs: 1) We introduce a new panorama dataset Upright360, comprising 9k
high-quality and upright panorama images, and finetune the advanced Flux model
on Upright360 for high-quality, upright and consistent panorama generation. 2)
We pioneer the Layered 3D Panorama as underlying representation to manage
complex scene hierarchies and lift it into 3D Gaussians to splat detailed
360-degree omnidirectional scenes with unconstrained viewing paths. Extensive
experiments demonstrate that our framework generates state-of-the-art 3D
panoramic scene in both full view consistency and immersive exploratory
experience. We believe that Layerpano3D holds promise for advancing 3D
panoramic scene creation with numerous applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://ys-imtech.github.io/projects/LayerPano3D/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ HumanGif: Single-View Human Diffusion with Generative Prior 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.12080v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.12080v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shoukang Hu, Takuya Narihira, Kazumi Fukuda, Ryosuke Sawata, Takashi Shibuya, Yuki Mitsufuji
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Previous 3D human creation methods have made significant progress in
synthesizing view-consistent and temporally aligned results from sparse-view
images or monocular videos. However, it remains challenging to produce
perpetually realistic, view-consistent, and temporally coherent human avatars
from a single image, as limited information is available in the single-view
input setting. Motivated by the success of 2D character animation, we propose
HumanGif, a single-view human diffusion model with generative prior.
Specifically, we formulate the single-view-based 3D human novel view and pose
synthesis as a single-view-conditioned human diffusion process, utilizing
generative priors from foundational diffusion models to complement the missing
information. To ensure fine-grained and consistent novel view and pose
synthesis, we introduce a Human NeRF module in HumanGif to learn spatially
aligned features from the input image, implicitly capturing the relative camera
and human pose transformation. Furthermore, we introduce an image-level loss
during optimization to bridge the gap between latent and image spaces in
diffusion models. Extensive experiments on RenderPeople and DNA-Rendering
datasets demonstrate that HumanGif achieves the best perceptual performance,
with better generalizability for novel view and pose synthesis.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://skhu101.github.io/HumanGif/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ UniDB: A Unified Diffusion Bridge Framework via Stochastic Optimal
  Control 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.05749v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.05749v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kaizhen Zhu, Mokai Pan, Yuexin Ma, Yanwei Fu, Jingyi Yu, Jingya Wang, Ye Shi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances in diffusion bridge models leverage Doob's $h$-transform to
establish fixed endpoints between distributions, demonstrating promising
results in image translation and restoration tasks. However, these approaches
frequently produce blurred or excessively smoothed image details and lack a
comprehensive theoretical foundation to explain these shortcomings. To address
these limitations, we propose UniDB, a unified framework for diffusion bridges
based on Stochastic Optimal Control (SOC). UniDB formulates the problem through
an SOC-based optimization and derives a closed-form solution for the optimal
controller, thereby unifying and generalizing existing diffusion bridge models.
We demonstrate that existing diffusion bridges employing Doob's $h$-transform
constitute a special case of our framework, emerging when the terminal penalty
coefficient in the SOC cost function tends to infinity. By incorporating a
tunable terminal penalty coefficient, UniDB achieves an optimal balance between
control costs and terminal penalties, substantially improving detail
preservation and output quality. Notably, UniDB seamlessly integrates with
existing diffusion bridge models, requiring only minimal code modifications.
Extensive experiments across diverse image restoration tasks validate the
superiority and adaptability of the proposed framework. Our code is available
at https://github.com/UniDB-SOC/UniDB/.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Going Beyond Feature Similarity: Effective <span class="highlight-title">Dataset</span> distillation based on
  Class-aware Conditional Mutual Information <span class="chip">ICLR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.09945v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.09945v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinhao Zhong, Bin Chen, Hao Fang, Xulin Gu, Shu-Tao Xia, En-Hui Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Dataset distillation (DD) aims to minimize the time and memory consumption
needed for training deep neural networks on large datasets, by creating a
smaller synthetic dataset that has similar performance to that of the full real
dataset. However, current dataset distillation methods often result in
synthetic datasets that are excessively difficult for networks to learn from,
due to the compression of a substantial amount of information from the original
data through metrics measuring feature similarity, e,g., distribution matching
(DM). In this work, we introduce conditional mutual information (CMI) to assess
the class-aware complexity of a dataset and propose a novel method by
minimizing CMI. Specifically, we minimize the distillation loss while
constraining the class-aware complexity of the synthetic dataset by minimizing
its empirical CMI from the feature space of pre-trained networks,
simultaneously. Conducting on a thorough set of experiments, we show that our
method can serve as a general regularization method to existing DD methods and
improve the performance and training efficiency.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ICLR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ HeRCULES: Heterogeneous Radar <span class="highlight-title">Dataset</span> in Complex Urban Environment for
  Multi-session Radar SLAM <span class="chip">ICRA
  2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.01946v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.01946v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hanjun Kim, Minwoo Jung, Chiyun Noh, Sangwoo Jung, Hyunho Song, Wooseong Yang, Hyesu Jang, Ayoung Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, radars have been widely featured in robotics for their robustness
in challenging weather conditions. Two commonly used radar types are spinning
radars and phased-array radars, each offering distinct sensor characteristics.
Existing datasets typically feature only a single type of radar, leading to the
development of algorithms limited to that specific kind. In this work, we
highlight that combining different radar types offers complementary advantages,
which can be leveraged through a heterogeneous radar dataset. Moreover, this
new dataset fosters research in multi-session and multi-robot scenarios where
robots are equipped with different types of radars. In this context, we
introduce the HeRCULES dataset, a comprehensive, multi-modal dataset with
heterogeneous radars, FMCW LiDAR, IMU, GPS, and cameras. This is the first
dataset to integrate 4D radar and spinning radar alongside FMCW LiDAR, offering
unparalleled localization, mapping, and place recognition capabilities. The
dataset covers diverse weather and lighting conditions and a range of urban
traffic scenarios, enabling a comprehensive analysis across various
environments. The sequence paths with multiple revisits and ground truth pose
for each sensor enhance its suitability for place recognition research. We
expect the HeRCULES dataset to facilitate odometry, mapping, place recognition,
and sensor fusion research. The dataset and development tools are available at
https://sites.google.com/view/herculesdataset.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>2025 IEEE International Conference on Robotics and Automation (ICRA
  2025)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LaRE$^2$: Latent Reconstruction Error Based Method for
  Diffusion-Generated Image Detection <span class="chip">CVPR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17465v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17465v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yunpeng Luo, Junlong Du, Ke Yan, Shouhong Ding
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The evolution of Diffusion Models has dramatically improved image generation
quality, making it increasingly difficult to differentiate between real and
generated images. This development, while impressive, also raises significant
privacy and security concerns. In response to this, we propose a novel Latent
REconstruction error guided feature REfinement method (LaRE^2) for detecting
the diffusion-generated images. We come up with the Latent Reconstruction Error
(LaRE), the first reconstruction-error based feature in the latent space for
generated image detection. LaRE surpasses existing methods in terms of feature
extraction efficiency while preserving crucial cues required to differentiate
between the real and the fake. To exploit LaRE, we propose an Error-Guided
feature REfinement module (EGRE), which can refine the image feature guided by
LaRE to enhance the discriminativeness of the feature. Our EGRE utilizes an
align-then-refine mechanism, which effectively refines the image feature for
generated-image detection from both spatial and channel perspectives. Extensive
experiments on the large-scale GenImage benchmark demonstrate the superiority
of our LaRE^2, which surpasses the best SoTA method by up to 11.9%/12.1%
average ACC/AP across 8 different image generators. LaRE also surpasses
existing methods in terms of feature extraction cost, delivering an impressive
speed enhancement of 8 times. Code is available.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR 2024. Code is available at https://github.com/luo3300612/LaRE</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Generative Video Diffusion for Unseen Novel Semantic Video Moment
  Retrieval <span class="chip">AAAI-25</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.13329v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.13329v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dezhao Luo, Shaogang Gong, Jiabo Huang, Hailin Jin, Yang Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Video moment retrieval (VMR) aims to locate the most likely video moment(s)
corresponding to a text query in untrimmed videos. Training of existing methods
is limited by the lack of diverse and generalisable VMR datasets, hindering
their ability to generalise moment-text associations to queries containing
novel semantic concepts (unseen both visually and textually in a training
source domain). For model generalisation to novel semantics, existing methods
rely heavily on assuming to have access to both video and text sentence pairs
from a target domain in addition to the source domain pair-wise training data.
This is neither practical nor scalable. In this work, we introduce a more
generalisable approach by assuming only text sentences describing new semantics
are available in model training without having seen any videos from a target
domain. To that end, we propose a Fine-grained Video Editing framework, termed
FVE, that explores generative video diffusion to facilitate fine-grained video
editing from the seen source concepts to the unseen target sentences consisting
of new concepts. This enables generative hypotheses of unseen video moments
corresponding to the novel concepts in the target domain. This fine-grained
generative video diffusion retains the original video structure and subject
specifics from the source domain while introducing semantic distinctions of
unseen novel vocabularies in the target domain. A critical challenge is how to
enable this generative fine-grained diffusion process to be meaningful in
optimising VMR, more than just synthesising visually pleasing videos. We solve
this problem by introducing a hybrid selection mechanism that integrates three
quantitative metrics to selectively incorporate synthetic video moments (novel
video hypotheses) as enlarged additions to the original source training data,
whilst minimising potential ...
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>AAAI-25</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DeepInteraction++: Multi-Modality Interaction for Autonomous Driving <span class="chip">NeurIPS 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.05075v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.05075v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zeyu Yang, Nan Song, Wei Li, Xiatian Zhu, Li Zhang, Philip H. S. Torr
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Existing top-performance autonomous driving systems typically rely on the
multi-modal fusion strategy for reliable scene understanding. This design is
however fundamentally restricted due to overlooking the modality-specific
strengths and finally hampering the model performance. To address this
limitation, in this work, we introduce a novel modality interaction strategy
that allows individual per-modality representations to be learned and
maintained throughout, enabling their unique characteristics to be exploited
during the whole perception pipeline. To demonstrate the effectiveness of the
proposed strategy, we design DeepInteraction++, a multi-modal interaction
framework characterized by a multi-modal representational interaction encoder
and a multi-modal predictive interaction decoder. Specifically, the encoder is
implemented as a dual-stream Transformer with specialized attention operation
for information exchange and integration between separate modality-specific
representations. Our multi-modal representational learning incorporates both
object-centric, precise sampling-based feature alignment and global dense
information spreading, essential for the more challenging planning task. The
decoder is designed to iteratively refine the predictions by alternately
aggregating information from separate representations in a unified
modality-agnostic manner, realizing multi-modal predictive interaction.
Extensive experiments demonstrate the superior performance of the proposed
framework on both 3D object detection and end-to-end autonomous driving tasks.
Our code is available at https://github.com/fudan-zvg/DeepInteraction.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Journal extension of NeurIPS 2022. arXiv admin note: text overlap
  with arXiv:2208.11112</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A large-scale multicenter breast cancer DCE-MRI benchmark <span class="highlight-title">dataset</span> with
  expert segmentations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.13844v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.13844v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lidia Garrucho, Kaisar Kushibar, Claire-Anne Reidel, Smriti Joshi, Richard Osuala, Apostolia Tsirikoglou, Maciej Bobowicz, Javier del Riego, Alessandro Catanese, Katarzyna Gwoździewicz, Maria-Laura Cosaka, Pasant M. Abo-Elhoda, Sara W. Tantawy, Shorouq S. Sakrana, Norhan O. Shawky-Abdelfatah, Amr Muhammad Abdo-Salem, Androniki Kozana, Eugen Divjak, Gordana Ivanac, Katerina Nikiforaki, Michail E. Klontzas, Rosa García-Dosdá, Meltem Gulsun-Akpinar, Oğuz Lafcı, Ritse Mann, Carlos Martín-Isla, Fred Prior, Kostas Marias, Martijn P. A. Starmans, Fredrik Strand, Oliver Díaz, Laura Igual, Karim Lekadir
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Artificial Intelligence (AI) research in breast cancer Magnetic Resonance
Imaging (MRI) faces challenges due to limited expert-labeled segmentations. To
address this, we present a multicenter dataset of 1506 pre-treatment
T1-weighted dynamic contrast-enhanced MRI cases, including expert annotations
of primary tumors and non-mass-enhanced regions. The dataset integrates imaging
data from four collections in The Cancer Imaging Archive (TCIA), where only 163
cases with expert segmentations were initially available. To facilitate the
annotation process, a deep learning model was trained to produce preliminary
segmentations for the remaining cases. These were subsequently corrected and
verified by 16 breast cancer experts (averaging 9 years of experience),
creating a fully annotated dataset. Additionally, the dataset includes 49
harmonized clinical and demographic variables, as well as pre-trained weights
for a baseline nnU-Net model trained on the annotated data. This resource
addresses a critical gap in publicly available breast cancer datasets, enabling
the development, validation, and benchmarking of advanced deep learning models,
thus driving progress in breast cancer diagnostics, treatment response
prediction, and personalized care.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 paes, 7 figures, 3 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Long Video Understanding with Learnable Retrieval in Video-Language
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.04931v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.04931v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiaqi Xu, Cuiling Lan, Wenxuan Xie, Xuejin Chen, Yan Lu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The remarkable natural language understanding, reasoning, and generation
capabilities of large language models (LLMs) have made them attractive for
application to video understanding, utilizing video tokens as contextual input.
However, employing LLMs for long video understanding presents significant
challenges. The extensive number of video tokens leads to considerable
computational costs for LLMs while using aggregated tokens results in loss of
vision details. Moreover, the presence of abundant question-irrelevant tokens
introduces noise to the video reasoning process. To address these issues, we
introduce a simple yet effective learnable retrieval-based video-language model
(R-VLM) for efficient long video understanding. Specifically, given a question
(query) and a long video, our model identifies and selects the most relevant K
video chunks and uses their associated visual tokens to serve as context for
the LLM inference. This effectively reduces the number of video tokens,
eliminates noise interference, and enhances system performance. We achieve this
by incorporating a learnable lightweight MLP block to facilitate the efficient
retrieval of question-relevant chunks, through the end-to-end training of our
video-language model with a proposed soft matching loss. Our experimental
results on multiple zero-shot video question answering datasets validate the
effectiveness of our framework for comprehending long videos.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 8 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Tailored Design of Audio-Visual Speech Recognition Models using
  Branchformers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.06606v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.06606v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        David Gimeno-Gómez, Carlos-D. Martínez-Hinarejos
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances in Audio-Visual Speech Recognition (AVSR) have led to
unprecedented achievements in the field, improving the robustness of this type
of system in adverse, noisy environments. In most cases, this task has been
addressed through the design of models composed of two independent encoders,
each dedicated to a specific modality. However, while recent works have
explored unified audio-visual encoders, determining the optimal cross-modal
architecture remains an ongoing challenge. Furthermore, such approaches often
rely on models comprising vast amounts of parameters and high computational
cost training processes. In this paper, we aim to bridge this research gap by
introducing a novel audio-visual framework. Our proposed method constitutes, to
the best of our knowledge, the first attempt to harness the flexibility and
interpretability offered by encoder architectures, such as the Branchformer, in
the design of parameter-efficient AVSR systems. To be more precise, the
proposed framework consists of two steps: first, estimating audio- and
video-only systems, and then designing a tailored audio-visual unified encoder
based on the layer-level branch scores provided by the modality-specific
models. Extensive experiments on English and Spanish AVSR benchmarks covering
multiple data conditions and scenarios demonstrated the effectiveness of our
proposed method. Even when trained on a moderate scale of data, our models
achieve competitive word error rates (WER) of approximately 2.5\% for English
and surpass existing approaches for Spanish, establishing a new benchmark with
an average WER of around 9.1\%. These results reflect how our tailored AVSR
system is able to reach state-of-the-art recognition rates while significantly
reducing the model complexity w.r.t. the prevalent approach in the field. Code
and pre-trained models are available at
https://github.com/david-gimeno/tailored-avsr.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted and under review for the Computer Speech and Language
  journal of Elsevier</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Towards Robust Probabilistic Modeling on SO(3) via Rotation Laplace
  Distribution <span class="chip">ICLR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.10465v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.10465v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yingda Yin, Jiangran Lyu, Yang Wang, Haoran Liu, He Wang, Baoquan Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Estimating the 3DoF rotation from a single RGB image is an important yet
challenging problem. As a popular approach, probabilistic rotation modeling
additionally carries prediction uncertainty information, compared to
single-prediction rotation regression. For modeling probabilistic distribution
over SO(3), it is natural to use Gaussian-like Bingham distribution and matrix
Fisher, however they are shown to be sensitive to outlier predictions, e.g.
$180^\circ$ error and thus are unlikely to converge with optimal performance.
In this paper, we draw inspiration from multivariate Laplace distribution and
propose a novel rotation Laplace distribution on SO(3). Our rotation Laplace
distribution is robust to the disturbance of outliers and enforces much
gradient to the low-error region that it can improve. In addition, we show that
our method also exhibits robustness to small noises and thus tolerates
imperfect annotations. With this benefit, we demonstrate its advantages in
semi-supervised rotation regression, where the pseudo labels are noisy. To
further capture the multi-modal rotation solution space for symmetric objects,
we extend our distribution to rotation Laplace mixture model and demonstrate
its effectiveness. Our extensive experiments show that our proposed
distribution and the mixture model achieve state-of-the-art performance in all
the rotation regression experiments over both probabilistic and
non-probabilistic baselines.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>TPAMI 2025. ICLR 2023 spotlight. arXiv admin note: substantial text
  overlap with arXiv:2303.01743</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ I2CKD : Intra- and Inter-Class Knowledge Distillation for Semantic
  Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18490v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18490v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ayoub Karine, Thibault Napoléon, Maher Jridi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper proposes a new knowledge distillation method tailored for image
semantic segmentation, termed Intra- and Inter-Class Knowledge Distillation
(I2CKD). The focus of this method is on capturing and transferring knowledge
between the intermediate layers of teacher (cumbersome model) and student
(compact model). For knowledge extraction, we exploit class prototypes derived
from feature maps. To facilitate knowledge transfer, we employ a triplet loss
in order to minimize intra-class variances and maximize inter-class variances
between teacher and student prototypes. Consequently, I2CKD enables the student
to better mimic the feature representation of the teacher for each class,
thereby enhancing the segmentation performance of the compact network.
Extensive experiments on three segmentation datasets, i.e., Cityscapes, Pascal
VOC and CamVid, using various teacher-student network pairs demonstrate the
effectiveness of the proposed method.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Adaptive <span class="highlight-title">Prompt</span>: Unlocking the Power of Visual <span class="highlight-title">Prompt</span> Tuning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.18936v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.18936v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Minh Le, Anh Nguyen, Huy Nguyen, Chau Nguyen, Nhat Ho
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Visual Prompt Tuning (VPT) has recently emerged as a powerful method for
adapting pre-trained vision models to downstream tasks. By introducing
learnable prompt tokens as task-specific instructions, VPT effectively guides
pre-trained transformer models with minimal overhead. Despite its empirical
success, a comprehensive theoretical understanding of VPT remains an active
area of research. Building on recent insights into the connection between
mixture of experts and prompt-based approaches, we identify a key limitation in
VPT: the restricted functional expressiveness in prompt formulation. To address
this limitation, we propose Visual Adaptive Prompt Tuning (VAPT), a new
generation of prompts that redefines prompts as adaptive functions of the
input. Our theoretical analysis shows that this simple yet intuitive approach
achieves optimal sample efficiency. Empirical results on VTAB-1K and FGVC
further demonstrate VAPT's effectiveness, with performance gains of 7.34% and
1.04% over fully fine-tuning baselines, respectively. Notably, VAPT also
surpasses VPT by a substantial margin while using fewer parameters. These
results highlight both the effectiveness and efficiency of our method and pave
the way for future research to explore the potential of adaptive prompts. Our
code is publicly available at https://github.com/Minhchuyentoancbn/VAPT
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>57 pages, 10 figures, 18 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ RelaCtrl: Relevance-Guided Efficient Control for Diffusion <span class="highlight-title">Transformer</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.14377v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.14377v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ke Cao, Jing Wang, Ao Ma, Jiasong Feng, Zhanjie Zhang, Xuanhua He, Shanyuan Liu, Bo Cheng, Dawei Leng, Yuhui Yin, Jie Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The Diffusion Transformer plays a pivotal role in advancing text-to-image and
text-to-video generation, owing primarily to its inherent scalability. However,
existing controlled diffusion transformer methods incur significant parameter
and computational overheads and suffer from inefficient resource allocation due
to their failure to account for the varying relevance of control information
across different transformer layers. To address this, we propose the
Relevance-Guided Efficient Controllable Generation framework, RelaCtrl,
enabling efficient and resource-optimized integration of control signals into
the Diffusion Transformer. First, we evaluate the relevance of each layer in
the Diffusion Transformer to the control information by assessing the
"ControlNet Relevance Score"-i.e., the impact of skipping each control layer on
both the quality of generation and the control effectiveness during inference.
Based on the strength of the relevance, we then tailor the positioning,
parameter scale, and modeling capacity of the control layers to reduce
unnecessary parameters and redundant computations. Additionally, to further
improve efficiency, we replace the self-attention and FFN in the commonly used
copy block with the carefully designed Two-Dimensional Shuffle Mixer (TDSM),
enabling efficient implementation of both the token mixer and channel mixer.
Both qualitative and quantitative experimental results demonstrate that our
approach achieves superior performance with only 15% of the parameters and
computational complexity compared to PixArt-delta.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Homepage: https://360cvgroup.github.io/RelaCtrl/ Github:
  https://github.com/360CVGroup/RelaCtrl</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Magnifier <span class="highlight-title">Prompt</span>: Tackling Multimodal Hallucination via Extremely Simple
  Instructions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.11701v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.11701v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuhan Fu, Ruobing Xie, Jiazhen Liu, Bangxiang Lan, Xingwu Sun, Zhanhui Kang, Xirong Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Hallucinations in multimodal large language models (MLLMs) hinder their
practical applications. To address this, we propose a Magnifier Prompt
(MagPrompt), a simple yet effective method to tackle hallucinations in MLLMs
via extremely simple instructions. MagPrompt is based on the following two key
principles, which guide the design of various effective prompts, demonstrating
robustness: (1) MLLMs should focus more on the image. (2) When there are
conflicts between the image and the model's inner knowledge, MLLMs should
prioritize the image. MagPrompt is training-free and can be applied to
open-source and closed-source models, such as GPT-4o and Gemini-pro. It
performs well across many datasets and its effectiveness is comparable or even
better than more complex methods like VCD. Furthermore, our prompt design
principles and experimental analyses provide valuable insights into multimodal
hallucination.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The proposed method does not work for up-to-date MLLMs.</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Temporal Misalignment in ANN-SNN Conversion and Its Mitigation via
  Probabilistic Spiking Neurons 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.14487v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.14487v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Velibor Bojković, Xiaofeng Wu, Bin Gu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Spiking Neural Networks (SNNs) offer a more energy-efficient alternative to
Artificial Neural Networks (ANNs) by mimicking biological neural principles,
establishing them as a promising approach to mitigate the increasing energy
demands of large-scale neural models. However, fully harnessing the
capabilities of SNNs remains challenging due to their discrete signal
processing and temporal dynamics. ANN-SNN conversion has emerged as a practical
approach, enabling SNNs to achieve competitive performance on complex machine
learning tasks. In this work, we identify a phenomenon in the ANN-SNN
conversion framework, termed temporal misalignment, in which random spike
rearrangement across SNN layers leads to performance improvements. Based on
this observation, we introduce biologically plausible two-phase probabilistic
(TPP) spiking neurons, further enhancing the conversion process. We demonstrate
the advantages of our proposed method both theoretically and empirically
through comprehensive experiments on CIFAR-10/100, CIFAR10-DVS, and ImageNet
across a variety of architectures, achieving state-of-the-art results.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Humanoid-VLA: Towards Universal Humanoid Control with Visual Integration 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.14795v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.14795v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pengxiang Ding, Jianfei Ma, Xinyang Tong, Binghong Zou, Xinxin Luo, Yiguo Fan, Ting Wang, Hongchao Lu, Panzhong Mo, Jinxin Liu, Yuefan Wang, Huaicheng Zhou, Wenshuo Feng, Jiacheng Liu, Siteng Huang, Donglin Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper addresses the limitations of current humanoid robot control
frameworks, which primarily rely on reactive mechanisms and lack autonomous
interaction capabilities due to data scarcity. We propose Humanoid-VLA, a novel
framework that integrates language understanding, egocentric scene perception,
and motion control, enabling universal humanoid control. Humanoid-VLA begins
with language-motion pre-alignment using non-egocentric human motion datasets
paired with textual descriptions, allowing the model to learn universal motion
patterns and action semantics. We then incorporate egocentric visual context
through a parameter efficient video-conditioned fine-tuning, enabling
context-aware motion generation. Furthermore, we introduce a self-supervised
data augmentation strategy that automatically generates pseudoannotations
directly derived from motion data. This process converts raw motion sequences
into informative question-answer pairs, facilitating the effective use of
large-scale unlabeled video data. Built upon whole-body control architectures,
extensive experiments show that Humanoid-VLA achieves object interaction and
environment exploration tasks with enhanced contextual awareness, demonstrating
a more human-like capacity for adaptive and intelligent engagement.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ZeroFlow: Overcoming Catastrophic Forgetting is Easier than You Think 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.01045v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.01045v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tao Feng, Wei Li, Didi Zhu, Hangjie Yuan, Wendi Zheng, Dan Zhang, Jie Tang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Backpropagation provides a generalized configuration for overcoming
catastrophic forgetting. Like, SGD and Adam are commonly used for weight
updates in continual learning and continual pre-training. In practice,
permission to access gradient information is not always granted (the gradient
ban), such as black-box APIs, hardware limitations, and non-differentiable
systems. To bridge this gap, we introduce the first benchmark ZeroFlow to
evaluate gradient-free optimization algorithms for overcoming forgetting. This
benchmark examines a suite of forward pass methods across multiple methods,
forgetting scenarios, and datasets. We find that forward passes alone are
enough to overcome forgetting. Our findings reveal new optimization principles
that highlight the potential of forward-pass in mitigating forgetting, managing
task conflicts, and reducing memory demands, alongside novel enhancements that
further mitigate forgetting with just one forward pass. This work provides
essential insights and tools for advancing forward pass methods to overcome
forgetting.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Backdoor Attacks against No-Reference Image Quality Assessment Models
  via a Scalable Trigger <span class="chip">AAAI 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.07277v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.07277v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yi Yu, Song Xia, Xun Lin, Wenhan Yang, Shijian Lu, Yap-peng Tan, Alex Kot
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  No-Reference Image Quality Assessment (NR-IQA), responsible for assessing the
quality of a single input image without using any reference, plays a critical
role in evaluating and optimizing computer vision systems, e.g., low-light
enhancement. Recent research indicates that NR-IQA models are susceptible to
adversarial attacks, which can significantly alter predicted scores with
visually imperceptible perturbations. Despite revealing vulnerabilities, these
attack methods have limitations, including high computational demands,
untargeted manipulation, limited practical utility in white-box scenarios, and
reduced effectiveness in black-box scenarios. To address these challenges, we
shift our focus to another significant threat and present a novel
poisoning-based backdoor attack against NR-IQA (BAIQA), allowing the attacker
to manipulate the IQA model's output to any desired target value by simply
adjusting a scaling coefficient $\alpha$ for the trigger. We propose to inject
the trigger in the discrete cosine transform (DCT) domain to improve the local
invariance of the trigger for countering trigger diminishment in NR-IQA models
due to widely adopted data augmentations. Furthermore, the universal
adversarial perturbations (UAP) in the DCT space are designed as the trigger,
to increase IQA model susceptibility to manipulation and improve attack
effectiveness. In addition to the heuristic method for poison-label BAIQA
(P-BAIQA), we explore the design of clean-label BAIQA (C-BAIQA), focusing on
$\alpha$ sampling and image data refinement, driven by theoretical insights we
reveal. Extensive experiments on diverse datasets and various NR-IQA models
demonstrate the effectiveness of our attacks. Code can be found at
https://github.com/yuyi-sd/BAIQA.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accept by AAAI 2025 (Also fix the typo mistakes in line 9 of the
  Algorithm 2 in the AAAI camera-ready version)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ChatVLA: Unified Multimodal Understanding and Robot Control with
  Vision-Language-Action Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.14420v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.14420v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhongyi Zhou, Yichen Zhu, Minjie Zhu, Junjie Wen, Ning Liu, Zhiyuan Xu, Weibin Meng, Ran Cheng, Yaxin Peng, Chaomin Shen, Feifei Feng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Humans possess a unified cognitive ability to perceive, comprehend, and
interact with the physical world. Why can't large language models replicate
this holistic understanding? Through a systematic analysis of existing training
paradigms in vision-language-action models (VLA), we identify two key
challenges: spurious forgetting, where robot training overwrites crucial
visual-text alignments, and task interference, where competing control and
understanding tasks degrade performance when trained jointly. To overcome these
limitations, we propose ChatVLA, a novel framework featuring Phased Alignment
Training, which incrementally integrates multimodal data after initial control
mastery, and a Mixture-of-Experts architecture to minimize task interference.
ChatVLA demonstrates competitive performance on visual question-answering
datasets and significantly surpasses state-of-the-art vision-language-action
(VLA) methods on multimodal understanding benchmarks. Notably, it achieves a
six times higher performance on MMMU and scores 47.2% on MMStar with a more
parameter-efficient design than ECoT. Furthermore, ChatVLA demonstrates
superior performance on 25 real-world robot manipulation tasks compared to
existing VLA methods like OpenVLA. Our findings highlight the potential of our
unified framework for achieving both robust multimodal understanding and
effective robot control.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Balanced Representation Learning for Long-tailed Skeleton-based Action
  Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.14024v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.14024v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hongda Liu, Yunlong Wang, Min Ren, Junxing Hu, Zhengquan Luo, Guangqi Hou, Zhenan Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Skeleton-based action recognition has recently made significant progress.
However, data imbalance is still a great challenge in real-world scenarios. The
performance of current action recognition algorithms declines sharply when
training data suffers from heavy class imbalance. The imbalanced data actually
degrades the representations learned by these methods and becomes the
bottleneck for action recognition. How to learn unbiased representations from
imbalanced action data is the key to long-tailed action recognition. In this
paper, we propose a novel balanced representation learning method to address
the long-tailed problem in action recognition. Firstly, a spatial-temporal
action exploration strategy is presented to expand the sample space
effectively, generating more valuable samples in a rebalanced manner. Secondly,
we design a detached action-aware learning schedule to further mitigate the
bias in the representation space. The schedule detaches the representation
learning of tail classes from training and proposes an action-aware loss to
impose more effective constraints. Additionally, a skip-modal representation is
proposed to provide complementary structural information. The proposed method
is validated on four skeleton datasets, NTU RGB+D 60, NTU RGB+D 120, NW-UCLA,
and Kinetics. It not only achieves consistently large improvement compared to
the state-of-the-art (SOTA) methods, but also demonstrates a superior
generalization capacity through extensive experiments. Our code is available at
https://github.com/firework8/BRL.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by Machine Intelligence Research
  https://link.springer.com/article/10.1007/s11633-023-1487-8</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ S-NeRF++: Autonomous Driving Simulation via Neural Reconstruction and
  Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.02112v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.02112v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yurui Chen, Junge Zhang, Ziyang Xie, Wenye Li, Feihu Zhang, Jiachen Lu, Li Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Autonomous driving simulation system plays a crucial role in enhancing
self-driving data and simulating complex and rare traffic scenarios, ensuring
navigation safety. However, traditional simulation systems, which often heavily
rely on manual modeling and 2D image editing, struggled with scaling to
extensive scenes and generating realistic simulation data. In this study, we
present S-NeRF++, an innovative autonomous driving simulation system based on
neural reconstruction. Trained on widely-used self-driving datasets such as
nuScenes and Waymo, S-NeRF++ can generate a large number of realistic street
scenes and foreground objects with high rendering quality as well as offering
considerable flexibility in manipulation and simulation. Specifically, S-NeRF++
is an enhanced neural radiance field for synthesizing large-scale scenes and
moving vehicles, with improved scene parameterization and camera pose learning.
The system effectively utilizes noisy and sparse LiDAR data to refine training
and address depth outliers, ensuring high-quality reconstruction and novel-view
rendering. It also provides a diverse foreground asset bank by reconstructing
and generating different foreground vehicles to support comprehensive scenario
creation.Moreover, we have developed an advanced foreground-background fusion
pipeline that skillfully integrates illumination and shadow effects, further
enhancing the realism of our simulations. With the high-quality simulated data
provided by our S-NeRF++, we found the perception methods enjoy performance
boosts on several autonomous driving downstream tasks, further demonstrating
our proposed simulator's effectiveness.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ UltraBones100k: A reliable automated labeling method and large-scale
  <span class="highlight-title">dataset</span> for ultrasound-based bone surface extraction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.03783v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.03783v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Luohong Wu, Nicola A. Cavalcanti, Matthias Seibold, Giuseppe Loggia, Lisa Reissner, Jonas Hein, Silvan Beeler, Arnd Viehöfer, Stephan Wirth, Lilian Calvet, Philipp Fürnstahl
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Ultrasound-based bone surface segmentation is crucial in computer-assisted
orthopedic surgery. However, ultrasound images have limitations, including a
low signal-to-noise ratio, and acoustic shadowing, which make interpretation
difficult. Existing deep learning models for bone segmentation rely primarily
on costly manual labeling by experts, limiting dataset size and model
generalizability. Additionally, the complexity of ultrasound physics and
acoustic shadow makes the images difficult for humans to interpret, leading to
incomplete labels in anechoic regions and limiting model performance. To
advance ultrasound bone segmentation and establish effective model benchmarks,
larger and higher-quality datasets are needed.
  We propose a methodology for collecting ex-vivo ultrasound datasets with
automatically generated bone labels, including anechoic regions. The proposed
labels are derived by accurately superimposing tracked bone CT models onto the
tracked ultrasound images. These initial labels are refined to account for
ultrasound physics. A clinical evaluation is conducted by an expert physician
specialized on orthopedic sonography to assess the quality of the generated
bone labels. A neural network for bone segmentation is trained on the collected
dataset and its predictions are compared to expert manual labels, evaluating
accuracy, completeness, and F1-score.
  We collected the largest known dataset of 100k ultrasound images of human
lower limbs with bone labels, called UltraBones100k. A Wilcoxon signed-rank
test with Bonferroni correction confirmed that the bone alignment after our
method significantly improved the quality of bone labeling (p < 0.001). The
model trained on UltraBones100k consistently outperforms manual labeling in all
metrics, particularly in low-intensity regions (320% improvement in
completeness at a distance threshold of 0.5 mm).
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ESIQA: Perceptual Quality Assessment of Vision-Pro-based Egocentric
  Spatial Images 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.21363v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.21363v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xilei Zhu, Liu Yang, Huiyu Duan, Xiongkuo Min, Guangtao Zhai, Patrick Le Callet
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the development of eXtended Reality (XR), photo capturing and display
technology based on head-mounted displays (HMDs) have experienced significant
advancements and gained considerable attention. Egocentric spatial images and
videos are emerging as a compelling form of stereoscopic XR content. The
assessment for the Quality of Experience (QoE) of XR content is important to
ensure a high-quality viewing experience. Different from traditional 2D images,
egocentric spatial images present challenges for perceptual quality assessment
due to their special shooting, processing methods, and stereoscopic
characteristics. However, the corresponding image quality assessment (IQA)
research for egocentric spatial images is still lacking. In this paper, we
establish the Egocentric Spatial Images Quality Assessment Database (ESIQAD),
the first IQA database dedicated for egocentric spatial images as far as we
know. Our ESIQAD includes 500 egocentric spatial images and the corresponding
mean opinion scores (MOSs) under three display modes, including 2D display,
3D-window display, and 3D-immersive display. Based on our ESIQAD, we propose a
novel mamba2-based multi-stage feature fusion model, termed ESIQAnet, which
predicts the perceptual quality of egocentric spatial images under the three
display modes. Specifically, we first extract features from multiple visual
state space duality (VSSD) blocks, then apply cross attention to fuse binocular
view information and use transposed attention to further refine the features.
The multi-stage features are finally concatenated and fed into a quality
regression network to predict the quality score. Extensive experimental results
demonstrate that the ESIQAnet outperforms 22 state-of-the-art IQA models on the
ESIQAD under all three display modes. The database and code are available at
https://github.com/IntMeGroup/ESIQA.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 12 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ZeroPS: High-quality Cross-modal Knowledge Transfer for Zero-Shot 3D
  Part Segmentation <span class="chip">3DV</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.14262v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.14262v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuheng Xue, Nenglun Chen, Jun Liu, Wenyun Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Zero-shot 3D part segmentation is a challenging and fundamental task. In this
work, we propose a novel pipeline, ZeroPS, which achieves high-quality
knowledge transfer from 2D pretrained foundation models (FMs), SAM and GLIP, to
3D object point clouds. We aim to explore the natural relationship between
multi-view correspondence and the FMs' prompt mechanism and build bridges on
it. In ZeroPS, the relationship manifests as follows: 1) lifting 2D to 3D by
leveraging co-viewed regions and SAM's prompt mechanism, 2) relating 1D classes
to 3D parts by leveraging 2D-3D view projection and GLIP's prompt mechanism,
and 3) enhancing prediction performance by leveraging multi-view observations.
Extensive evaluations on the PartNetE and AKBSeg benchmarks demonstrate that
ZeroPS significantly outperforms the SOTA method across zero-shot unlabeled and
instance segmentation tasks. ZeroPS does not require additional training or
fine-tuning for the FMs. ZeroPS applies to both simulated and real-world data.
It is hardly affected by domain shift. The project page is available at
https://luis2088.github.io/ZeroPS_page.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>2025 International Conference on 3D Vision (3DV)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Rethinking Open-Vocabulary Segmentation of Radiance Fields in 3D Space <span class="chip">AAAI 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.07416v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.07416v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hyunjee Lee, Youngsik Yun, Jeongmin Bae, Seoha Kim, Youngjung Uh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Understanding the 3D semantics of a scene is a fundamental problem for
various scenarios such as embodied agents. While NeRFs and 3DGS excel at
novel-view synthesis, previous methods for understanding their semantics have
been limited to incomplete 3D understanding: their segmentation results are
rendered as 2D masks that do not represent the entire 3D space. To address this
limitation, we redefine the problem to segment the 3D volume and propose the
following methods for better 3D understanding. We directly supervise the 3D
points to train the language embedding field, unlike previous methods that
anchor supervision at 2D pixels. We transfer the learned language field to
3DGS, achieving the first real-time rendering speed without sacrificing
training time or accuracy. Lastly, we introduce a 3D querying and evaluation
protocol for assessing the reconstructed geometry and semantics together. Code,
checkpoints, and annotations are available at the project page.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>AAAI 2025. Project page: https://hyunji12.github.io/Open3DRF</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SCALES: Boost Binary Neural Network for Image Super-Resolution with
  Efficient Scalings <span class="chip">DATE 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.12270v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.12270v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Renjie Wei, Zechun Liu, Yuchen Fan, Runsheng Wang, Ru Huang, Meng Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep neural networks for image super-resolution (SR) have demonstrated
superior performance. However, the large memory and computation consumption
hinders their deployment on resource-constrained devices. Binary neural
networks (BNNs), which quantize the floating point weights and activations to
1-bit can significantly reduce the cost. Although BNNs for image classification
have made great progress these days, existing BNNs for SR still suffer from a
large performance gap between the FP SR networks. To this end, we observe the
activation distribution in SR networks and find much larger pixel-to-pixel,
channel-to-channel, layer-to-layer, and image-to-image variation in the
activation distribution than image classification networks. However, existing
BNNs for SR fail to capture these variations that contain rich information for
image reconstruction, leading to inferior performance. To address this problem,
we propose SCALES, a binarization method for SR networks that consists of the
layer-wise scaling factor, the spatial re-scaling method, and the channel-wise
re-scaling method, capturing the layer-wise, pixel-wise, and channel-wise
variations efficiently in an input-dependent manner. We evaluate our method
across different network architectures and datasets. For CNN-based SR networks,
our binarization method SCALES outperforms the prior art method by 0.2dB with
fewer parameters and operations. With SCALES, we achieve the first accurate
binary Transformer-based SR network, improving PSNR by more than 1dB compared
to the baseline method.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accpeted by DATE 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ FUNCTO: Function-Centric One-Shot Imitation Learning for Tool
  Manipulation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11744v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11744v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chao Tang, Anxing Xiao, Yuhong Deng, Tianrun Hu, Wenlong Dong, Hanbo Zhang, David Hsu, Hong Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Learning tool use from a single human demonstration video offers a highly
intuitive and efficient approach to robot teaching. While humans can
effortlessly generalize a demonstrated tool manipulation skill to diverse tools
that support the same function (e.g., pouring with a mug versus a teapot),
current one-shot imitation learning (OSIL) methods struggle to achieve this. A
key challenge lies in establishing functional correspondences between
demonstration and test tools, considering significant geometric variations
among tools with the same function (i.e., intra-function variations). To
address this challenge, we propose FUNCTO (Function-Centric OSIL for Tool
Manipulation), an OSIL method that establishes function-centric correspondences
with a 3D functional keypoint representation, enabling robots to generalize
tool manipulation skills from a single human demonstration video to novel tools
with the same function despite significant intra-function variations. With this
formulation, we factorize FUNCTO into three stages: (1) functional keypoint
extraction, (2) function-centric correspondence establishment, and (3)
functional keypoint-based action planning. We evaluate FUNCTO against exiting
modular OSIL methods and end-to-end behavioral cloning methods through
real-robot experiments on diverse tool manipulation tasks. The results
demonstrate the superiority of FUNCTO when generalizing to novel tools with
intra-function geometric variations. More details are available at
https://sites.google.com/view/functo.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ AVD2: Accident Video Diffusion for Accident Video Description <span class="chip">ICRA 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.14801v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.14801v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Cheng Li, Keyuan Zhou, Tong Liu, Yu Wang, Mingqiao Zhuang, Huan-ang Gao, Bu Jin, Hao Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Traffic accidents present complex challenges for autonomous driving, often
featuring unpredictable scenarios that hinder accurate system interpretation
and responses. Nonetheless, prevailing methodologies fall short in elucidating
the causes of accidents and proposing preventive measures due to the paucity of
training data specific to accident scenarios. In this work, we introduce AVD2
(Accident Video Diffusion for Accident Video Description), a novel framework
that enhances accident scene understanding by generating accident videos that
aligned with detailed natural language descriptions and reasoning, resulting in
the contributed EMM-AU (Enhanced Multi-Modal Accident Video Understanding)
dataset. Empirical results reveal that the integration of the EMM-AU dataset
establishes state-of-the-art performance across both automated metrics and
human evaluations, markedly advancing the domains of accident analysis and
prevention. Project resources are available at https://an-answer-tree.github.io
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICRA 2025, Project Page: https://an-answer-tree.github.io/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Tracto<span class="highlight-title">GPT</span>: A <span class="highlight-title">GPT</span> architecture for White Matter Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.15464v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.15464v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anoushkrit Goel, Simroop Singh, Ankita Joshi, Ranjeet Ranjan Jha, Chirag Ahuja, Aditya Nigam, Arnav Bhavsar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  White matter bundle segmentation is crucial for studying brain structural
connectivity, neurosurgical planning, and neurological disorders. White Matter
Segmentation remains challenging due to structural similarity in streamlines,
subject variability, symmetry in 2 hemispheres, etc. To address these
challenges, we propose TractoGPT, a GPT-based architecture trained on
streamline, cluster, and fusion data representations separately. TractoGPT is a
fully-automatic method that generalizes across datasets and retains shape
information of the white matter bundles. Experiments also show that TractoGPT
outperforms state-of-the-art methods on average DICE, Overlap and Overreach
scores. We use TractoInferno and 105HCP datasets and validate generalization
across dataset.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted as a conference paper at 23rd IEEE International Symposium
  on Biomedical Imaging 2025. IEEE holds the copyright for this publication</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MMAD: A Comprehensive Benchmark for Multimodal Large Language Models in
  Industrial Anomaly Detection <span class="chip">ICLR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.09453v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.09453v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xi Jiang, Jian Li, Hanqiu Deng, Yong Liu, Bin-Bin Gao, Yifeng Zhou, Jialin Li, Chengjie Wang, Feng Zheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the field of industrial inspection, Multimodal Large Language Models
(MLLMs) have a high potential to renew the paradigms in practical applications
due to their robust language capabilities and generalization abilities.
However, despite their impressive problem-solving skills in many domains,
MLLMs' ability in industrial anomaly detection has not been systematically
studied. To bridge this gap, we present MMAD, the first-ever full-spectrum
MLLMs benchmark in industrial Anomaly Detection. We defined seven key subtasks
of MLLMs in industrial inspection and designed a novel pipeline to generate the
MMAD dataset with 39,672 questions for 8,366 industrial images. With MMAD, we
have conducted a comprehensive, quantitative evaluation of various
state-of-the-art MLLMs. The commercial models performed the best, with the
average accuracy of GPT-4o models reaching 74.9%. However, this result falls
far short of industrial requirements. Our analysis reveals that current MLLMs
still have significant room for improvement in answering questions related to
industrial anomalies and defects. We further explore two training-free
performance enhancement strategies to help models improve in industrial
scenarios, highlighting their promising potential for future research.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICLR 2025. The code and data are available at
  https://github.com/jam-cc/MMAD</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Poison as Cure: Visual Noise for Mitigating Object Hallucinations in
  LVMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.19164v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.19164v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kejia Zhang, Keda Tao, Jiasheng Tang, Huan Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large vision-language models (LVMs) extend large language models (LLMs) with
visual perception capabilities, enabling them to process and interpret visual
information. A major challenge compromising their reliability is object
hallucination that LVMs may generate plausible but factually inaccurate
information. We propose a novel visual adversarial perturbation (VAP) method to
mitigate this hallucination issue. VAP alleviates LVM hallucination by applying
strategically optimized visual noise without altering the base model. Our
approach formulates hallucination suppression as an optimization problem,
leveraging adversarial strategies to generate beneficial visual perturbations
that enhance the model's factual grounding and reduce parametric knowledge
bias. Extensive experimental results demonstrate that our method consistently
reduces object hallucinations across 8 state-of-the-art LVMs, validating its
efficacy across diverse evaluations.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Exploring Quasi-Global Solutions to Compound Lens Based Computational
  Imaging Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.19201v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.19201v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yao Gao, Qi Jiang, Shaohua Gao, Lei Sun, Kailun Yang, Kaiwei Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, joint design approaches that simultaneously optimize optical
systems and downstream algorithms through data-driven learning have
demonstrated superior performance over traditional separate design approaches.
However, current joint design approaches heavily rely on the manual
identification of initial lenses, posing challenges and limitations,
particularly for compound lens systems with multiple potential starting points.
In this work, we present Quasi-Global Search Optics (QGSO) to automatically
design compound lens based computational imaging systems through two parts: (i)
Fused Optimization Method for Automatic Optical Design (OptiFusion), which
searches for diverse initial optical systems under certain design
specifications; and (ii) Efficient Physic-aware Joint Optimization (EPJO),
which conducts parallel joint optimization of initial optical systems and image
reconstruction networks with the consideration of physical constraints,
culminating in the selection of the optimal solution in all search results.
Extensive experimental results illustrate that QGSO serves as a transformative
end-to-end lens design paradigm for superior global search ability, which
automatically provides compound lens based computational imaging systems with
higher imaging quality compared to existing paradigms. The source code will be
made publicly available at https://github.com/LiGpy/QGSO.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to IEEE Transactions on Computational Imaging (TCI). The
  source code will be made publicly available at https://github.com/LiGpy/QGSO</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MIA-Bench: Towards Better Instruction Following Evaluation of Multimodal
  LLMs <span class="chip">ICLR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.01509v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.01509v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yusu Qian, Hanrong Ye, Jean-Philippe Fauconnier, Peter Grasch, Yinfei Yang, Zhe Gan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce MIA-Bench, a new benchmark designed to evaluate multimodal large
language models (MLLMs) on their ability to strictly adhere to complex
instructions. Our benchmark comprises a diverse set of 400 image-prompt pairs,
each crafted to challenge the models' compliance with layered instructions in
generating accurate responses that satisfy specific requested patterns.
Evaluation results from a wide array of state-of-the-art MLLMs reveal
significant variations in performance, highlighting areas for improvement in
instruction fidelity. Additionally, we create extra training data and explore
supervised fine-tuning to enhance the models' ability to strictly follow
instructions without compromising performance on other tasks. We hope this
benchmark not only serves as a tool for measuring MLLM adherence to
instructions, but also guides future developments in MLLM training methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at ICLR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ YOLOv12 to Its Genesis: A Decadal and Comprehensive <span class="highlight-title">Review</span> of The You
  Only Look Once (YOLO) Series 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19407v6">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19407v6.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ranjan Sapkota, Rizwan Qureshi, Marco Flores Calero, Chetan Badjugar, Upesh Nepal, Alwin Poulose, Peter Zeno, Uday Bhanu Prakash Vaddevolu, Sheheryar Khan, Maged Shoman, Hong Yan, Manoj Karkee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This review systematically examines the progression of the You Only Look Once
(YOLO) object detection algorithms from YOLOv1 to the recently unveiled
YOLOv12. Employing a reverse chronological analysis, this study examines the
advancements introduced by YOLO algorithms, beginning with YOLOv12 and
progressing through YOLO11 (or YOLOv11), YOLOv10, YOLOv9, YOLOv8, and
subsequent versions to explore each version's contributions to enhancing speed,
detection accuracy, and computational efficiency in real-time object detection.
Additionally, this study reviews the alternative versions derived from YOLO
architectural advancements of YOLO-NAS, YOLO-X, YOLO-R, DAMO-YOLO, and
Gold-YOLO. By detailing the incremental technological advancements in
subsequent YOLO versions, this review chronicles the evolution of YOLO, and
discusses the challenges and limitations in each of the earlier versions. The
evolution signifies a path towards integrating YOLO with multimodal,
context-aware, and Artificial General Intelligence (AGI) systems for the next
YOLO decade, promising significant implications for future developments in
AI-driven applications. (Key terms: YOLOv12, YOLOv12 architecture, YOLOv11,
YOLO11, YOLO Review, YOLOv14, YOLOv15, YOLO architecture, YOLOv12 architecture)
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 Figures, 7 Tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Vision Foundation Models in Medical Image Analysis: Advances and
  Challenges 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.14584v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.14584v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pengchen Liang, Bin Pu, Haishan Huang, Yiwei Li, Hualiang Wang, Weibo Ma, Qing Chang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rapid development of Vision Foundation Models (VFMs), particularly Vision
Transformers (ViT) and Segment Anything Model (SAM), has sparked significant
advances in the field of medical image analysis. These models have demonstrated
exceptional capabilities in capturing long-range dependencies and achieving
high generalization in segmentation tasks. However, adapting these large models
to medical image analysis presents several challenges, including domain
differences between medical and natural images, the need for efficient model
adaptation strategies, and the limitations of small-scale medical datasets.
This paper reviews the state-of-the-art research on the adaptation of VFMs to
medical image segmentation, focusing on the challenges of domain adaptation,
model compression, and federated learning. We discuss the latest developments
in adapter-based improvements, knowledge distillation techniques, and
multi-scale contextual feature modeling, and propose future directions to
overcome these bottlenecks. Our analysis highlights the potential of VFMs,
along with emerging methodologies such as federated learning and model
compression, to revolutionize medical image analysis and enhance clinical
applications. The goal of this work is to provide a comprehensive overview of
current approaches and suggest key areas for future research that can drive the
next wave of innovation in medical image segmentation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages, 1 figure</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PhysAug: A Physical-guided and Frequency-based Data Augmentation for
  Single-Domain Generalized Object Detection <span class="chip">AAAI</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.11807v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.11807v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaoran Xu, Jiangang Yang, Wenhui Shi, Siyuan Ding, Luqing Luo, Jian Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Single-Domain Generalized Object Detection~(S-DGOD) aims to train on a single
source domain for robust performance across a variety of unseen target domains
by taking advantage of an object detector. Existing S-DGOD approaches often
rely on data augmentation strategies, including a composition of visual
transformations, to enhance the detector's generalization ability. However, the
absence of real-world prior knowledge hinders data augmentation from
contributing to the diversity of training data distributions. To address this
issue, we propose PhysAug, a novel physical model-based non-ideal imaging
condition data augmentation method, to enhance the adaptability of the S-DGOD
tasks. Drawing upon the principles of atmospheric optics, we develop a
universal perturbation model that serves as the foundation for our proposed
PhysAug. Given that visual perturbations typically arise from the interaction
of light with atmospheric particles, the image frequency spectrum is harnessed
to simulate real-world variations during training. This approach fosters the
detector to learn domain-invariant representations, thereby enhancing its
ability to generalize across various settings. Without altering the network
architecture or loss function, our approach significantly outperforms the
state-of-the-art across various S-DGOD datasets. In particular, it achieves a
substantial improvement of $7.3\%$ and $7.2\%$ over the baseline on DWD and
Cityscape-C, highlighting its enhanced generalizability in real-world settings.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to AAAI,2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PC-Agent: A Hierarchical Multi-Agent Collaboration Framework for Complex
  Task Automation on PC 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.14282v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.14282v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haowei Liu, Xi Zhang, Haiyang Xu, Yuyang Wanyan, Junyang Wang, Ming Yan, Ji Zhang, Chunfeng Yuan, Changsheng Xu, Weiming Hu, Fei Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the field of MLLM-based GUI agents, compared to smartphones, the PC
scenario not only features a more complex interactive environment, but also
involves more intricate intra- and inter-app workflows. To address these
issues, we propose a hierarchical agent framework named PC-Agent. Specifically,
from the perception perspective, we devise an Active Perception Module (APM) to
overcome the inadequate abilities of current MLLMs in perceiving screenshot
content. From the decision-making perspective, to handle complex user
instructions and interdependent subtasks more effectively, we propose a
hierarchical multi-agent collaboration architecture that decomposes
decision-making processes into Instruction-Subtask-Action levels. Within this
architecture, three agents (i.e., Manager, Progress and Decision) are set up
for instruction decomposition, progress tracking and step-by-step
decision-making respectively. Additionally, a Reflection agent is adopted to
enable timely bottom-up error feedback and adjustment. We also introduce a new
benchmark PC-Eval with 25 real-world complex instructions. Empirical results on
PC-Eval show that our PC-Agent achieves a 32% absolute improvement of task
success rate over previous state-of-the-art methods. The code is available at
https://github.com/X-PLUG/MobileAgent/tree/main/PC-Agent.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Neural Augmentation Based Panoramic High Dynamic Range Stitching 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.04679v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.04679v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chaobing Zheng, Yilun Xu, Weihai Chen, Shiqian Wu, Sen Zhang, Zhengguo Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Due to saturated regions of inputting low dynamic range (LDR) images and
large intensity changes among the LDR images caused by different exposures, it
is challenging to produce an information enriched panoramic LDR image without
visual artifacts for a high dynamic range (HDR) scene through stitching
multiple geometrically synchronized LDR images with different exposures and
pairwise overlapping fields of views (OFOVs). Fortunately, the stitching of
such images is innately a perfect scenario for the fusion of a physics-driven
approach and a data-driven approach due to their OFOVs. Based on this new
insight, a novel neural augmentation based panoramic HDR stitching algorithm is
proposed in this paper. The physics-driven approach is built up using the
OFOVs. Different exposed images of each view are initially generated by using
the physics-driven approach, are then refined by a data-driven approach, and
are finally used to produce panoramic LDR images with different exposures. All
the panoramic LDR images with different exposures are combined together via a
multi-scale exposure fusion algorithm to produce the final panoramic LDR image.
Experimental results demonstrate the proposed algorithm outperforms existing
panoramic stitching algorithms.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Chrono: A Simple Blueprint for Representing Time in MLLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.18113v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.18113v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Boris Meinardus, Hector Garcia Rodriguez, Anil Batra, Anna Rohrbach, Marcus Rohrbach
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The recent success of Large Language Models (LLMs) has prompted the extension
to the multimodal domain developing image-text Multimodal LLMs (MLLMs) and then
video-text models. In this work, we investigate the challenge of contextual and
temporal comprehension in video-language models by exploring the task of
temporal localization in videos. To address this problem, prior works have
developed complex task-specific architectures, novel modules to embed time into
MLLMs, or leveraged additional input signals such as video transcripts to best
encode contextual and temporal information. Interestingly, we find that most of
these efforts are surpassed by a much simpler design. We introduce Chrono, a
universal sequence blueprint that can be applied to an image-text pretrained
MLLM. Through extensive ablations across different MLLM architectures,
finetuning and zero-shot settings, and different datasets, we achieve a new
SOTA in moment retrieval on the most widely used benchmarks Charades-STA,
QVHighlights, ActivityNet Captions, and grounded video question answering on
NeXT-GQA.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code: https://github.com/sudo-Boris/mr-Blip</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Information Retrieval <span class="chip" style="font-size: 60%">17</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Dynamic Knowledge Selector and Evaluator for recommendation with
  Knowledge Graph 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.15623v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.15623v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Feng Xia, Zhifei Hu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years recommendation systems typically employ the edge information
provided by knowledge graphs combined with the advantages of high-order
connectivity of graph networks in the recommendation field. However, this
method is limited by the sparsity of labels, cannot learn the graph structure
well, and a large number of noisy entities in the knowledge graph will affect
the accuracy of the recommendation results. In order to alleviate the above
problems, we propose a dynamic knowledge-selecting and evaluating method guided
by collaborative signals to distill information in the knowledge graph.
Specifically, we use a Chain Route Evaluator to evaluate the contributions of
different neighborhoods for the recommendation task and employ a Knowledge
Selector strategy to filter the less informative knowledge before evaluating.
We conduct baseline model comparison and experimental ablation evaluations on
three public datasets. The experiments demonstrate that our proposed model
outperforms current state-of-the-art baseline models, and each modules
effectiveness in our model is demonstrated through ablation experiments.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Cross-Format Retrieval-Augmented Generation in XR with LLMs for
  Context-Aware Maintenance Assistance 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.15604v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.15604v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Akos Nagy, Yannis Spyridis, Vasileios Argyriou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents a detailed evaluation of a Retrieval-Augmented Generation
(RAG) system that integrates large language models (LLMs) to enhance
information retrieval and instruction generation for maintenance personnel
across diverse data formats. We assessed the performance of eight LLMs,
emphasizing key metrics such as response speed and accuracy, which were
quantified using BLEU and METEOR scores. Our findings reveal that advanced
models like GPT-4 and GPT-4o-mini significantly outperform their counterparts,
particularly when addressing complex queries requiring multi-format data
integration. The results validate the system's ability to deliver timely and
accurate responses, highlighting the potential of RAG frameworks to optimize
maintenance operations. Future research will focus on refining retrieval
techniques for these models and enhancing response generation, particularly for
intricate scenarios, ultimately improving the system's practical applicability
in dynamic real-world environments.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LightThinker: Thinking Step-by-Step Compression 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.15589v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.15589v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jintian Zhang, Yuqi Zhu, Mengshu Sun, Yujie Luo, Shuofei Qiao, Lun Du, Da Zheng, Huajun Chen, Ningyu Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have shown remarkable performance in complex
reasoning tasks, but their efficiency is hindered by the substantial memory and
computational costs associated with generating lengthy tokens. In this paper,
we propose LightThinker, a novel method that enables LLMs to dynamically
compress intermediate thoughts during reasoning. Inspired by human cognitive
processes, LightThinker compresses verbose thought steps into compact
representations and discards the original reasoning chains, thereby
significantly reducing the number of tokens stored in the context window. This
is achieved by training the model on when and how to perform compression
through data construction, mapping hidden states to condensed gist tokens, and
creating specialized attention masks. Additionally, we introduce the Dependency
(Dep) metric to quantify the degree of compression by measuring the reliance on
historical tokens during generation. Extensive experiments on four datasets and
two models show that LightThinker reduces peak memory usage and inference time,
while maintaining competitive accuracy. Our work provides a new direction for
improving the efficiency of LLMs in complex reasoning tasks without sacrificing
performance. Code will be released at https://github.com/zjunlp/LightThinker.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Bridging Domain Gaps between <span class="highlight-title">Pretrain</span>ed Multimodal Models and
  Recommendations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.15542v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.15542v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenyu Zhang, Jie Luo, Xinming Zhang, Yuan Fang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the explosive growth of multimodal content online, pre-trained
visual-language models have shown great potential for multimodal
recommendation. However, while these models achieve decent performance when
applied in a frozen manner, surprisingly, due to significant domain gaps (e.g.,
feature distribution discrepancy and task objective misalignment) between
pre-training and personalized recommendation, adopting a joint training
approach instead leads to performance worse than baseline. Existing approaches
either rely on simple feature extraction or require computationally expensive
full model fine-tuning, struggling to balance effectiveness and efficiency. To
tackle these challenges, we propose \textbf{P}arameter-efficient
\textbf{T}uning for \textbf{M}ultimodal \textbf{Rec}ommendation
(\textbf{PTMRec}), a novel framework that bridges the domain gap between
pre-trained models and recommendation systems through a knowledge-guided
dual-stage parameter-efficient training strategy. This framework not only
eliminates the need for costly additional pre-training but also flexibly
accommodates various parameter-efficient tuning methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Scaling Sparse and Dense Retrieval in Decoder-Only LLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.15526v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.15526v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hansi Zeng, Julian Killingback, Hamed Zamani
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Scaling large language models (LLMs) has shown great potential for improving
retrieval model performance; however, previous studies have mainly focused on
dense retrieval trained with contrastive loss (CL), neglecting the scaling
behavior of other retrieval paradigms and optimization techniques, such as
sparse retrieval and knowledge distillation (KD). In this work, we conduct a
systematic comparative study on how different retrieval paradigms (sparse vs.
dense) and fine-tuning objectives (CL vs. KD vs. their combination) affect
retrieval performance across different model scales. Using MSMARCO passages as
the training dataset, decoder-only LLMs (Llama-3 series: 1B, 3B, 8B), and a
fixed compute budget, we evaluate various training configurations on both
in-domain (MSMARCO, TREC DL) and out-of-domain (BEIR) benchmarks. Our key
findings reveal that: (1) Scaling behaviors emerge clearly only with CL, where
larger models achieve significant performance gains, whereas KD-trained models
show minimal improvement, performing similarly across the 1B, 3B, and 8B
scales. (2) Sparse retrieval models consistently outperform dense retrieval
across both in-domain (MSMARCO, TREC DL) and out-of-domain (BEIR) benchmarks,
and they demonstrate greater robustness to imperfect supervised signals. (3) We
successfully scale sparse retrieval models with the combination of CL and KD
losses at 8B scale, achieving state-of-the-art (SOTA) results in all evaluation
sets.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Universal Framework for Compressing Embeddings in CTR Prediction <span class="chip">DASFAA2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.15355v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.15355v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kefan Wang, Hao Wang, Kenan Song, Wei Guo, Kai Cheng, Zhi Li, Yong Liu, Defu Lian, Enhong Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurate click-through rate (CTR) prediction is vital for online advertising
and recommendation systems. Recent deep learning advancements have improved the
ability to capture feature interactions and understand user interests. However,
optimizing the embedding layer often remains overlooked. Embedding tables,
which represent categorical and sequential features, can become excessively
large, surpassing GPU memory limits and necessitating storage in CPU memory.
This results in high memory consumption and increased latency due to frequent
GPU-CPU data transfers. To tackle these challenges, we introduce a
Model-agnostic Embedding Compression (MEC) framework that compresses embedding
tables by quantizing pre-trained embeddings, without sacrificing recommendation
quality. Our approach consists of two stages: first, we apply
popularity-weighted regularization to balance code distribution between high-
and low-frequency features. Then, we integrate a contrastive learning mechanism
to ensure a uniform distribution of quantized codes, enhancing the
distinctiveness of embeddings. Experiments on three datasets reveal that our
method reduces memory usage by over 50x while maintaining or improving
recommendation performance compared to existing models. The implementation code
is accessible in our project repository https://github.com/USTC-StarTeam/MEC.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by DASFAA2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Detecting Future-related Contexts of Entity Mentions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.15332v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.15332v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Puneet Prashar, Krishna Mohan Shukla, Adam Jatowt
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The ability to automatically identify whether an entity is referenced in a
future context can have multiple applications including decision making,
planning and trend forecasting. This paper focuses on detecting implicit future
references in entity-centric texts, addressing the growing need for automated
temporal analysis in information processing. We first present a novel dataset
of 19,540 sentences built around popular entities sourced from Wikipedia, which
consists of future-related and non-future-related contexts in which those
entities appear. As a second contribution, we evaluate the performance of
several Language Models including also Large Language Models (LLMs) on the task
of distinguishing future-oriented content in the absence of explicit temporal
references.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Lightweight yet Efficient: An External Attentive Graph Convolutional
  Network with Positional <span class="highlight-title">Prompt</span>s for Sequential Recommendation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.15331v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.15331v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jinyu Zhang, Chao Li, Zhongying Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Graph-based Sequential Recommender systems (GSRs) have gained significant
research attention due to their ability to simultaneously handle user-item
interactions and sequential relationships between items. Current GSRs often
utilize composite or in-depth structures for graph encoding (e.g., the Graph
Transformer). Nevertheless, they have high computational complexity, hindering
the deployment on resource-constrained edge devices. Moreover, the relative
position encoding in Graph Transformer has difficulty in considering the
complicated positional dependencies within sequence. To this end, we propose an
External Attentive Graph convolutional network with Positional prompts for
Sequential recommendation, namely EA-GPS. Specifically, we first introduce an
external attentive graph convolutional network that linearly measures the
global associations among nodes via two external memory units. Then, we present
a positional prompt-based decoder that explicitly treats the absolute item
positions as external prompts. By introducing length-adaptive sequential
masking and a soft attention network, such a decoder facilitates the model to
capture the long-term positional dependencies and contextual relationships
within sequences. Extensive experimental results on five real-world datasets
demonstrate that the proposed EA-GPS outperforms the state-of-the-art methods.
Remarkably, it achieves the superior performance while maintaining a smaller
parameter size and lower training overhead. The implementation of this work is
publicly available at https://github.com/ZZY-GraphMiningLab/EA-GPS.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>26 pages, 8 figures, journal paper, accepted by TOIS at 20th
  February, 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ From Documents to Dialogue: Building KG-RAG Enhanced AI Assistants 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.15237v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.15237v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Manisha Mukherjee, Sungchul Kim, Xiang Chen, Dan Luo, Tong Yu, Tung Mai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The Adobe Experience Platform AI Assistant is a conversational tool that
enables organizations to interact seamlessly with proprietary enterprise data
through a chatbot. However, due to access restrictions, Large Language Models
(LLMs) cannot retrieve these internal documents, limiting their ability to
generate accurate zero-shot responses. To overcome this limitation, we use a
Retrieval-Augmented Generation (RAG) framework powered by a Knowledge Graph
(KG) to retrieve relevant information from external knowledge sources, enabling
LLMs to answer questions over private or previously unseen document
collections. In this paper, we propose a novel approach for building a
high-quality, low-noise KG. We apply several techniques, including incremental
entity resolution using seed concepts, similarity-based filtering to
deduplicate entries, assigning confidence scores to entity-relation pairs to
filter for high-confidence pairs, and linking facts to source documents for
provenance. Our KG-RAG system retrieves relevant tuples, which are added to the
user prompts context before being sent to the LLM generating the response. Our
evaluation demonstrates that this approach significantly enhances response
relevance, reducing irrelevant answers by over 50% and increasing fully
relevant answers by 88% compared to the existing production system.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A <span class="highlight-title">BERT</span> Based Hybrid Recommendation System For Academic Collaboration 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.15223v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.15223v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sangeetha N, Harish Thangaraj, Varun Vashisht, Eshaan Joshi, Kanishka Verma, Diya Katariya
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Universities serve as a hub for academic collaboration, promoting the
exchange of diverse ideas and perspectives among students and faculty through
interdisciplinary dialogue. However, as universities expand in size,
conventional networking approaches via student chapters, class groups, and
faculty committees become cumbersome. To address this challenge, an
academia-specific profile recommendation system is proposed to connect
like-minded stakeholders within any university community. This study evaluates
three techniques: Term Frequency-Inverse Document Frequency (TF-IDF),
Bidirectional Encoder Representations from Transformers (BERT), and a hybrid
approach to generate effective recommendations. Due to the unlabelled nature of
the dataset, Affinity Propagation cluster-based relabelling is performed to
understand the grouping of similar profiles. The hybrid model demonstrated
superior performance, evidenced by its similarity score, Silhouette score,
Davies-Bouldin index, and Normalized Discounted Cumulative Gain (NDCG),
achieving an optimal balance between diversity and relevance in
recommendations. Furthermore, the optimal model has been implemented as a
mobile application, which dynamically suggests relevant profiles based on
users' skills and collaboration interests, incorporating contextual
understanding. The potential impact of this application is significant, as it
promises to enhance networking opportunities within large academic institutions
through the deployment of intelligent recommendation systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>International Conference on Intelligent Systems and Security - 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GNN-Coder: Boosting Semantic Code Retrieval with Combined GNNs and
  <span class="highlight-title">Transformer</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.15202v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.15202v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yufan Ye, Pu Pang, Ting Zhang, Hua Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Code retrieval is a crucial component in modern software development,
particularly in large-scale projects. However, existing approaches relying on
sequence-based models often fail to fully exploit the structural dependencies
inherent in code, leading to suboptimal retrieval performance, particularly
with structurally complex code fragments. In this paper, we introduce
GNN-Coder, a novel framework based on Graph Neural Network (GNN) to utilize
Abstract Syntax Tree (AST). We make the first attempt to study how
GNN-integrated Transformer can promote the development of semantic retrieval
tasks by capturing the structural and semantic features of code. We further
propose an innovative graph pooling method tailored for AST, utilizing the
number of child nodes as a key feature to highlight the intrinsic topological
relationships within the AST. This design effectively integrates both
sequential and hierarchical representations, enhancing the model's ability to
capture code structure and semantics. Additionally, we introduce the Mean
Angular Margin (MAM), a novel metric for quantifying the uniformity of code
embedding distributions, providing a standardized measure of feature
separability. The proposed method achieves a lower MAM, indicating a more
discriminative feature representation. This underscores GNN-Coder's superior
ability to distinguish between code snippets, thereby enhancing retrieval
accuracy. Experimental results show that GNN-Coder significantly boosts
retrieval performance, with a 1\%-10\% improvement in MRR on the CSN dataset,
and a notable 20\% gain in zero-shot performance on the CosQA dataset.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Range Retrieval with Graph-Based Indices 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.13245v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.13245v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Magdalen Dobson Manohar, Taekseung Kim, Guy E. Blelloch
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Retrieving points based on proximity in a high-dimensional vector space is a
crucial step in information retrieval applications. The approximate nearest
neighbor search (ANNS) problem, which identifies the $k$ nearest neighbors for
a query (approximately, since exactly is hard), has been extensively studied in
recent years. However, comparatively little attention has been paid to the
related problem of finding all points within a given distance of a query, the
range retrieval problem, despite its applications in areas such as duplicate
detection, plagiarism checking, and facial recognition. In this paper, we
present a set of algorithms for range retrieval on graph-based vector indices,
which are known to achieve excellent performance on ANNS queries. Since a range
query may have anywhere from no matching results to thousands of matching
results in the database, we introduce a set of range retrieval algorithms based
on modifications of the standard graph search that adapt to terminate quickly
on queries in the former group, and to put more resources into finding results
for the latter group. Due to the lack of existing benchmarks for range
retrieval, we also undertake a comprehensive study of range characteristics of
existing embedding datasets, and select a suitable range retrieval radius for
eight existing datasets with up to 100 million points in addition to the one
existing benchmark. We test our algorithms on these datasets, and find up to
100x improvement in query throughput over a naive baseline approach, with 5-10x
improvement on average, and strong performance up to 100 million data points.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Vague Preference Policy Learning for Conversational Recommendation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.04487v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.04487v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gangyi Zhang, Chongming Gao, Wenqiang Lei, Xiaojie Guo, Shijun Li, Hongshen Chen, Zhuozhi Ding, Sulong Xu, Lingfei Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Conversational recommendation systems (CRS) commonly assume users have clear
preferences, leading to potential over-filtering of relevant alternatives.
However, users often exhibit vague, non-binary preferences. We introduce the
Vague Preference Multi-round Conversational Recommendation (VPMCR) scenario,
employing a soft estimation mechanism to accommodate users' vague and dynamic
preferences while mitigating over-filtering. In VPMCR, we propose Vague
Preference Policy Learning (VPPL), consisting of Ambiguity-aware Soft
Estimation (ASE) and Dynamism-aware Policy Learning (DPL). ASE captures
preference vagueness by estimating scores for clicked and non-clicked options,
using a choice-based approach and time-aware preference decay. DPL leverages
ASE's preference distribution to guide the conversation and adapt to preference
changes for recommendations or attribute queries. Extensive experiments
demonstrate VPPL's effectiveness within VPMCR, outperforming existing methods
and setting a new benchmark. Our work advances CRS by accommodating users'
inherent ambiguity and relative decision-making processes, improving real-world
applicability.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Reproducing NevIR: Negation in Neural Information Retrieval <span class="chip">SIGIR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.13506v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.13506v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Coen van den Elsen, Francien Barkhof, Thijmen Nijdam, Simon Lupart, Mohammad Alliannejadi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Negation is a fundamental aspect of human communication, yet it remains a
challenge for Language Models (LMs) in Information Retrieval (IR). Despite the
heavy reliance of modern neural IR systems on LMs, little attention has been
given to their handling of negation. In this study, we reproduce and extend the
findings of NevIR, a benchmark study that revealed most IR models perform at or
below the level of random ranking when dealing with negation. We replicate
NevIR's original experiments and evaluate newly developed state-of-the-art IR
models. Our findings show that a recently emerging category - listwise Large
Language Model (LLM) rerankers - outperforms other models but still
underperforms human performance. Additionally, we leverage ExcluIR, a benchmark
dataset designed for exclusionary queries with extensive negation, to assess
the generalizability of negation understanding. Our findings suggest that
fine-tuning on one dataset does not reliably improve performance on the other,
indicating notable differences in their data distributions. Furthermore, we
observe that only cross-encoders and listwise LLM rerankers achieve reasonable
performance across both negation tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 5 figures, under review at SIGIR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ GraphFM: Graph Factorization Machines for Feature Interaction Modeling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2105.11866v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2105.11866v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shu Wu, Zekun Li, Yunyue Su, Zeyu Cui, Xiaoyu Zhang, Liang Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Factorization machine (FM) is a prevalent approach to modeling pairwise
(second-order) feature interactions when dealing with high-dimensional sparse
data. However, on the one hand, FM fails to capture higher-order feature
interactions suffering from combinatorial expansion. On the other hand, taking
into account interactions between every pair of features may introduce noise
and degrade prediction accuracy. To solve the problems, we propose a novel
approach, Graph Factorization Machine (GraphFM), by naturally representing
features in the graph structure. In particular, we design a mechanism to select
the beneficial feature interactions and formulate them as edges between
features. Then the proposed model, which integrates the interaction function of
FM into the feature aggregation strategy of Graph Neural Network (GNN), can
model arbitrary-order feature interactions on the graph-structured features by
stacking layers. Experimental results on several real-world datasets have
demonstrated the rationality and effectiveness of our proposed approach. The
code and data are available at
https://github.com/CRIPAC-DIG/GraphCTR}{https://github.com/CRIPAC-DIG/GraphCTR
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The code and data are available at
  https://github.com/CRIPAC-DIG/GraphCTR</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Automatic <span class="highlight-title">Self-supervised</span> Learning for Social Recommendations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.18735v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.18735v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xin He, Wenqi Fan, Mingchen Sun, Ying Wang, Xin Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, researchers have attempted to exploit social relations to
improve the performance in recommendation systems. Generally, most existing
social recommendation methods heavily depends on substantial domain knowledge
and expertise in primary recommendation tasks for designing useful auxiliary
tasks. Meanwhile, Self-Supervised Learning (SSL) recently has received
considerable attention in the field of recommendation, since it can provide
self-supervision signals in assisting the improvement of target recommendation
systems by constructing self-supervised auxiliary tasks from raw data without
human-annotated labels. Despite the great success, these SSL-based social
recommendations are insufficient to adaptively balance various self-supervised
auxiliary tasks, since assigning equal weights on various auxiliary tasks can
result in sub-optimal recommendation performance, where different
self-supervised auxiliary tasks may contribute differently to improving the
primary social recommendation across different datasets. To address this issue,
in this work, we propose Adaptive Self-supervised Learning for Social
Recommendations (AdasRec) by taking advantage of various self-supervised
auxiliary tasks. More specifically, an adaptive weighting mechanism is proposed
to learn adaptive weights for various self-supervised auxiliary tasks, so as to
balance the contribution of such self-supervised auxiliary tasks for enhancing
representation learning in social recommendations. The adaptive weighting
mechanism is used to assign different weights on auxiliary tasks to achieve an
overall weighting of the entire auxiliary tasks and ultimately assist the
primary recommendation task, achieved by a meta learning optimization problem
with an adaptive weighting network. Comprehensive experiments on various
real-world datasets are constructed to verify the effectiveness of our proposed
method.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Lost in Sequence: Do Large Language Models Understand Sequential
  Recommendation? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.13909v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.13909v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sein Kim, Hongseok Kang, Kibum Kim, Jiwan Kim, Donghyun Kim, Minchul Yang, Kwangjin Oh, Julian McAuley, Chanyoung Park
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have recently emerged as promising tools for
recommendation thanks to their advanced textual understanding ability and
context-awareness. Despite the current practice of training and evaluating
LLM-based recommendation (LLM4Rec) models under a sequential recommendation
scenario, we found that whether these models understand the sequential
information inherent in users' item interaction sequences has been largely
overlooked. In this paper, we first demonstrate through a series of experiments
that existing LLM4Rec models do not fully capture sequential information both
during training and inference. Then, we propose a simple yet effective
LLM-based sequential recommender, called LLM-SRec, a method that enhances the
integration of sequential information into LLMs by distilling the user
representations extracted from a pre-trained CF-SRec model into LLMs. Our
extensive experiments show that LLM-SRec enhances LLMs' ability to understand
users' item interaction sequences, ultimately leading to improved
recommendation performance. Furthermore, unlike existing LLM4Rec models that
require fine-tuning of LLMs, LLM-SRec achieves state-of-the-art performance by
training only a few lightweight MLPs, highlighting its practicality in
real-world applications. Our code is available at
https://github.com/Sein-Kim/LLM-SRec.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Machine Learning <span class="chip" style="font-size: 60%">143</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ One-step Diffusion Models with $f$-Divergence Distribution Matching 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.15681v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.15681v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yilun Xu, Weili Nie, Arash Vahdat
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Sampling from diffusion models involves a slow iterative process that hinders
their practical deployment, especially for interactive applications. To
accelerate generation speed, recent approaches distill a multi-step diffusion
model into a single-step student generator via variational score distillation,
which matches the distribution of samples generated by the student to the
teacher's distribution. However, these approaches use the reverse
Kullback-Leibler (KL) divergence for distribution matching which is known to be
mode seeking. In this paper, we generalize the distribution matching approach
using a novel $f$-divergence minimization framework, termed $f$-distill, that
covers different divergences with different trade-offs in terms of mode
coverage and training variance. We derive the gradient of the $f$-divergence
between the teacher and student distributions and show that it is expressed as
the product of their score differences and a weighting function determined by
their density ratio. This weighting function naturally emphasizes samples with
higher density in the teacher distribution, when using a less mode-seeking
divergence. We observe that the popular variational score distillation approach
using the reverse-KL divergence is a special case within our framework.
Empirically, we demonstrate that alternative $f$-divergences, such as
forward-KL and Jensen-Shannon divergences, outperform the current best
variational score distillation methods across image generation tasks. In
particular, when using Jensen-Shannon divergence, $f$-distill achieves current
state-of-the-art one-step generation performance on ImageNet64 and zero-shot
text-to-image generation on MS-COCO. Project page:
https://research.nvidia.com/labs/genair/f-distill
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Testing the limits of fine-tuning to improve reasoning in vision
  language models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.15678v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.15678v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Luca M. Schulze Buschoff, Konstantinos Voudouris, Elif Akata, Matthias Bethge, Joshua B. Tenenbaum, Eric Schulz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Pre-trained vision language models still fall short of human visual
cognition. In an effort to improve visual cognition and align models with human
behavior, we introduce visual stimuli and human judgments on visual cognition
tasks, allowing us to systematically evaluate performance across cognitive
domains under a consistent environment. We fine-tune models on ground truth
data for intuitive physics and causal reasoning and find that this improves
model performance in the respective fine-tuning domain. Furthermore, it can
improve model alignment with human behavior. However, we find that fine-tuning
does not contribute to robust human-like generalization to data with other
visual characteristics or to tasks in other cognitive domains.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FLEKE: Federated Locate-then-Edit Knowledge Editing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.15677v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.15677v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zongkai Zhao, Guozeng Xu, Xiuhua Li, Kaiwen Wei, Jiang Zhong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Locate-then-Edit Knowledge Editing (LEKE) is a key technique for updating
large language models (LLMs) without full retraining. However, existing methods
assume a single-user setting and become inefficient in real-world multi-client
scenarios, where decentralized organizations (e.g., hospitals, financial
institutions) independently update overlapping knowledge, leading to redundant
mediator knowledge vector (MKV) computations and privacy concerns. To address
these challenges, we introduce Federated Locate-then-Edit Knowledge Editing
(FLEKE), a novel task that enables multiple clients to collaboratively perform
LEKE while preserving privacy and reducing computational overhead. To achieve
this, we propose FedEdit, a two-stage framework that optimizes MKV selection
and reuse. In the first stage, clients locally apply LEKE and upload the
computed MKVs. In the second stage, rather than relying solely on server-based
MKV sharing, FLEKE allows clients retrieve relevant MKVs based on cosine
similarity, enabling knowledge re-edit and minimizing redundant computations.
Experimental results on two benchmark datasets demonstrate that FedEdit retains
over 96% of the performance of non-federated LEKE while significantly
outperforming a FedAvg-based baseline by approximately twofold. Besides, we
find that MEMIT performs more consistently than PMET in the FLEKE task with our
FedEdit framework. Our code is available at https://github.com/zongkaiz/FLEKE.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Almost AI, Almost Human: The Challenge of Detecting AI-Polished Writing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.15666v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.15666v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shoumik Saha, Soheil Feizi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The growing use of large language models (LLMs) for text generation has led
to widespread concerns about AI-generated content detection. However, an
overlooked challenge is AI-polished text, where human-written content undergoes
subtle refinements using AI tools. This raises a critical question: should
minimally polished text be classified as AI-generated? Misclassification can
lead to false plagiarism accusations and misleading claims about AI prevalence
in online content. In this study, we systematically evaluate eleven
state-of-the-art AI-text detectors using our AI-Polished-Text Evaluation
(APT-Eval) dataset, which contains $11.7K$ samples refined at varying
AI-involvement levels. Our findings reveal that detectors frequently
misclassify even minimally polished text as AI-generated, struggle to
differentiate between degrees of AI involvement, and exhibit biases against
older and smaller models. These limitations highlight the urgent need for more
nuanced detection methodologies.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages, 17 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Automating Curriculum Learning for Reinforcement Learning using a
  Skill-Based Bayesian Network 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.15662v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.15662v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vincent Hsiao, Mark Roberts, Laura M. Hiatt, George Konidaris, Dana Nau
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A major challenge for reinforcement learning is automatically generating
curricula to reduce training time or improve performance in some target task.
We introduce SEBNs (Skill-Environment Bayesian Networks) which model a
probabilistic relationship between a set of skills, a set of goals that relate
to the reward structure, and a set of environment features to predict policy
performance on (possibly unseen) tasks. We develop an algorithm that uses the
inferred estimates of agent success from SEBN to weigh the possible next tasks
by expected improvement. We evaluate the benefit of the resulting curriculum on
three environments: a discrete gridworld, continuous control, and simulated
robotics. The results show that curricula constructed using SEBN frequently
outperform other baselines.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Machine-generated text detection prevents language model collapse 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.15654v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.15654v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        George Drayson, Vasileios Lampos
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As Large Language Models (LLMs) become increasingly prevalent, their
generated outputs are proliferating across the web, risking a future where
machine-generated content dilutes human-authored text. Since web data is the
primary resource for LLM pretraining, future models will be trained on an
unknown portion of synthetic data. This will lead to model collapse, a
degenerative process which causes models to reinforce their own errors and
experience a drop in model performance. In this study, we investigate the
impact of decoding strategy on model collapse, where we analyse the
characteristics of the generated data during recursive training, its similarity
to human references and the resulting model performance. Using the decoding
strategies that lead to the most significant model degradation, we tackle the
question: how to avoid model collapse when the origin (human or synthetic) of
the training data is unknown. We design a novel methodology based on resampling
the data distribution using importance weights from our machine-generated text
detector. Our method is validated on two LLM variants (GPT-2 and SmolLM2) on
the open-ended text generation task, demonstrating that we can successfully
prevent model collapse and when there is enough human-authored data in the
training dataset, our method improves model performance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Logit Disagreement: OoD Detection with Bayesian Neural Networks <span class="chip">ECCV 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.15648v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.15648v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kevin Raina
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Bayesian neural networks (BNNs), which estimate the full posterior
distribution over model parameters, are well-known for their role in
uncertainty quantification and its promising application in out-of-distribution
detection (OoD). Amongst other uncertainty measures, BNNs provide a
state-of-the art estimation of predictive entropy (total uncertainty) which can
be decomposed as the sum of mutual information and expected entropy. In the
context of OoD detection the estimation of predictive uncertainty in the form
of the predictive entropy score confounds aleatoric and epistemic uncertainty,
the latter being hypothesized to be high for OoD points. Despite these
justifications, the mutual information score has been shown to perform worse
than predictive entropy. Taking inspiration from Bayesian variational
autoencoder (BVAE) literature, this work proposes to measure the disagreement
between a corrected version of the pre-softmax quantities, otherwise known as
logits, as an estimate of epistemic uncertainty for Bayesian NNs under mean
field variational inference. The three proposed epistemic uncertainty scores
demonstrate marked improvements over mutual information on a range of OoD
experiments, with equal performance otherwise. Moreover, the epistemic
uncertainty scores perform on par with the Bayesian benchmark predictive
entropy on a range of MNIST and CIFAR10 experiments.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Presented at ECCV 2024 Workshop: 3rd Workshop on Uncertainty
  Quantification for Computer Vision</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Predicting gene essentiality and drug response from perturbation screens
  in preclinical cancer models with LEAP: Layered Ensemble of Autoencoders and
  Predictors 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.15646v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.15646v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Barbara Bodinier, Gaetan Dissez, Linus Bleistein, Antonin Dauvin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Preclinical perturbation screens, where the effects of genetic, chemical, or
environmental perturbations are systematically tested on disease models, hold
significant promise for machine learning-enhanced drug discovery due to their
scale and causal nature. Predictive models can infer perturbation responses for
previously untested disease models based on molecular profiles. These in silico
labels can expand databases and guide experimental prioritization.
  However, modelling perturbation-specific effects and generating robust
prediction performances across diverse biological contexts remain elusive. We
introduce LEAP (Layered Ensemble of Autoencoders and Predictors), a novel
ensemble framework to improve robustness and generalization. LEAP leverages
multiple DAMAE (Data Augmented Masked Autoencoder) representations and LASSO
regressors. By combining diverse gene expression representation models learned
from different random initializations, LEAP consistently outperforms
state-of-the-art approaches in predicting gene essentiality or drug responses
in unseen cell lines, tissues and disease models. Notably, our results show
that ensembling representation models, rather than prediction models alone,
yields superior predictive performance.
  Beyond its performance gains, LEAP is computationally efficient, requires
minimal hyperparameter tuning and can therefore be readily incorporated into
drug discovery pipelines to prioritize promising targets and support
biomarker-driven stratification. The code and datasets used in this work are
made publicly available.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AutoTandemML: Active Learning Enhanced Tandem Neural Networks for
  Inverse Design Problems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.15643v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.15643v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Luka Grbcic, Juliane Müller, Wibe Albert de Jong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Inverse design in science and engineering involves determining optimal design
parameters that achieve desired performance outcomes, a process often hindered
by the complexity and high dimensionality of design spaces, leading to
significant computational costs. To tackle this challenge, we propose a novel
hybrid approach that combines active learning with Tandem Neural Networks to
enhance the efficiency and effectiveness of solving inverse design problems.
Active learning allows to selectively sample the most informative data points,
reducing the required dataset size without compromising accuracy. We
investigate this approach using three benchmark problems: airfoil inverse
design, photonic surface inverse design, and scalar boundary condition
reconstruction in diffusion partial differential equations. We demonstrate that
integrating active learning with Tandem Neural Networks outperforms standard
approaches across the benchmark suite, achieving better accuracy with fewer
training samples.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Training Neural ODEs Using Fully Discretized Simultaneous Optimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.15642v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.15642v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mariia Shapovalova, Calvin Tsay
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neural Ordinary Differential Equations (Neural ODEs) represent
continuous-time dynamics with neural networks, offering advancements for
modeling and control tasks. However, training Neural ODEs requires solving
differential equations at each epoch, leading to high computational costs. This
work investigates simultaneous optimization methods as a faster training
alternative. In particular, we employ a collocation-based, fully discretized
formulation and use IPOPT--a solver for large-scale nonlinear optimization--to
simultaneously optimize collocation coefficients and neural network parameters.
Using the Van der Pol Oscillator as a case study, we demonstrate faster
convergence compared to traditional training methods. Furthermore, we introduce
a decomposition framework utilizing Alternating Direction Method of Multipliers
(ADMM) to effectively coordinate sub-models among data batches. Our results
show significant potential for (collocation-based) simultaneous Neural ODE
training pipelines.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to the 14th IFAC Symposium on Dynamics and Control of
  Process Systems, including Biosystems (DYCOPS 2025)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Steering into New Embedding Spaces: Analyzing Cross-Lingual Alignment
  Induced by Model Interventions in Multilingual Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.15639v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.15639v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anirudh Sundar, Sinead Williamson, Katherine Metcalf, Barry-John Theobald, Skyler Seto, Masha Fedzechkina
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Aligned representations across languages is a desired property in
multilingual large language models (mLLMs), as alignment can improve
performance in cross-lingual tasks. Typically alignment requires fine-tuning a
model, which is computationally expensive, and sizable language data, which
often may not be available. A data-efficient alternative to fine-tuning is
model interventions -- a method for manipulating model activations to steer
generation into the desired direction. We analyze the effect of a popular
intervention (finding experts) on the alignment of cross-lingual
representations in mLLMs. We identify the neurons to manipulate for a given
language and introspect the embedding space of mLLMs pre- and
post-manipulation. We show that modifying the mLLM's activations changes its
embedding space such that cross-lingual alignment is enhanced. Further, we show
that the changes to the embedding space translate into improved downstream
performance on retrieval tasks, with up to 2x improvements in top-1 accuracy on
cross-lingual retrieval.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>34 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Mantis: Lightweight Calibrated Foundation Model for User-Friendly Time
  Series Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.15637v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.15637v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vasilii Feofanov, Songkang Wen, Marius Alonso, Romain Ilbert, Hongbo Guo, Malik Tiomoko, Lujia Pan, Jianfeng Zhang, Ievgen Redko
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, there has been increasing interest in developing foundation
models for time series data that can generalize across diverse downstream
tasks. While numerous forecasting-oriented foundation models have been
introduced, there is a notable scarcity of models tailored for time series
classification. To address this gap, we present Mantis, a new open-source
foundation model for time series classification based on the Vision Transformer
(ViT) architecture that has been pre-trained using a contrastive learning
approach. Our experimental results show that Mantis outperforms existing
foundation models both when the backbone is frozen and when fine-tuned, while
achieving the lowest calibration error. In addition, we propose several
adapters to handle the multivariate setting, reducing memory requirements and
modeling channel interdependence.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Sparks of cognitive flexibility: self-guided context inference for
  flexible stimulus-response mapping by attentional routing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.15634v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.15634v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rowan Sommers, Sushrut Thorat, Daniel Anthes, Tim C. Kietzmann
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Flexible cognition demands discovering hidden rules to quickly adapt
stimulus-response mappings. Standard neural networks struggle in tasks
requiring rapid, context-driven remapping. Recently, Hummos (2023) introduced a
fast-and-slow learning algorithm to mitigate this shortfall, but its
scalability to complex, image-computable tasks was unclear. Here, we propose
the Wisconsin Neural Network (WiNN), which expands on fast-and-slow learning
for real-world tasks demanding flexible rule-based behavior. WiNN employs a
pretrained convolutional neural network for vision, coupled with an adjustable
"context state" that guides attention to relevant features. If WiNN produces an
incorrect response, it first iteratively updates its context state to refocus
attention on task-relevant cues, then performs minimal parameter updates to
attention and readout layers. This strategy preserves generalizable
representations in the sensory network, reducing catastrophic forgetting. We
evaluate WiNN on an image-based extension of the Wisconsin Card Sorting Task,
revealing several markers of cognitive flexibility: (i) WiNN autonomously
infers underlying rules, (ii) requires fewer examples to do so than control
models reliant on large-scale parameter updates, (iii) can perform
context-based rule inference solely via context-state adjustments-further
enhanced by slow updates of attention and readout parameters, and (iv)
generalizes to unseen compositional rules through context-state inference
alone. By blending fast context inference with targeted attentional guidance,
WiNN achieves "sparks" of flexibility. This approach offers a path toward
context-sensitive models that retain knowledge while rapidly adapting to
complex, rule-based tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The Relationship Between Reasoning and Performance in Large Language
  Models -- o3 (mini) Thinks Harder, Not Longer 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.15631v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.15631v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Marthe Ballon, Andres Algaba, Vincent Ginis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models have demonstrated remarkable progress in mathematical
reasoning, leveraging chain-of-thought and test-time compute scaling. However,
many open questions remain regarding the interplay between reasoning token
usage and accuracy gains. In particular, when comparing models across
generations, it is unclear whether improved performance results from longer
reasoning chains or more efficient reasoning. We systematically analyze
chain-of-thought length across o1-mini and o3-mini variants on the Omni-MATH
benchmark, finding that o3-mini (m) achieves superior accuracy without
requiring longer reasoning chains than o1-mini. Moreover, we show that accuracy
generally declines as reasoning chains grow across all models and compute
settings, even when controlling for difficulty of the questions. This accuracy
drop is significantly smaller in more proficient models, suggesting that new
generations of reasoning models use test-time compute more effectively.
Finally, we highlight that while o3-mini (h) achieves a marginal accuracy gain
over o3-mini (m), it does so by allocating substantially more reasoning tokens
across all problems, even the ones that o3-mini (m) can already solve. These
findings provide new insights into the relationship between model capability
and reasoning length, with implications for efficiency, scaling, and evaluation
methodologies.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>19 pages, 11 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Paradigms of AI Evaluation: Mapping Goals, Methodologies and Culture 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.15620v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.15620v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        John Burden, Marko Tešić, Lorenzo Pacchiardi, José Hernández-Orallo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Research in AI evaluation has grown increasingly complex and
multidisciplinary, attracting researchers with diverse backgrounds and
objectives. As a result, divergent evaluation paradigms have emerged, often
developing in isolation, adopting conflicting terminologies, and overlooking
each other's contributions. This fragmentation has led to insular research
trajectories and communication barriers both among different paradigms and with
the general public, contributing to unmet expectations for deployed AI systems.
To help bridge this insularity, in this paper we survey recent work in the AI
evaluation landscape and identify six main paradigms. We characterise major
recent contributions within each paradigm across key dimensions related to
their goals, methodologies and research cultures. By clarifying the unique
combination of questions and approaches associated with each paradigm, we aim
to increase awareness of the breadth of current evaluation approaches and
foster cross-pollination between different paradigms. We also identify
potential gaps in the field to inspire future research directions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Probe Pruning: Accelerating LLMs through Dynamic Pruning via
  Model-Probing <span class="chip">ICLR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.15618v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.15618v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qi Le, Enmao Diao, Ziyan Wang, Xinran Wang, Jie Ding, Li Yang, Ali Anwar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce Probe Pruning (PP), a novel framework for online, dynamic,
structured pruning of Large Language Models (LLMs) applied in a batch-wise
manner. PP leverages the insight that not all samples and tokens contribute
equally to the model's output, and probing a small portion of each batch
effectively identifies crucial weights, enabling tailored dynamic pruning for
different batches. It comprises three main stages: probing, history-informed
pruning, and full inference. In the probing stage, PP selects a small yet
crucial set of hidden states, based on residual importance, to run a few model
layers ahead. During the history-informed pruning stage, PP strategically
integrates the probing states with historical states. Subsequently, it
structurally prunes weights based on the integrated states and the PP
importance score, a metric developed specifically to assess the importance of
each weight channel in maintaining performance. In the final stage, full
inference is conducted on the remaining weights. A major advantage of PP is its
compatibility with existing models, as it operates without requiring additional
neural network modules or fine-tuning. Comprehensive evaluations of PP on
LLaMA-2/3 and OPT models reveal that even minimal probing-using just 1.5% of
FLOPs-can substantially enhance the efficiency of structured pruning of LLMs.
For instance, when evaluated on LLaMA-2-7B with WikiText2, PP achieves a 2.56
times lower ratio of performance degradation per unit of runtime reduction
compared to the state-of-the-art method at a 40% pruning ratio. Our code is
available at https://github.com/Qi-Le1/Probe_Pruning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PDeepPP:A Deep learning framework with <span class="highlight-title">Pretrain</span>ed Protein language for
  peptide classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.15610v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.15610v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jixiu Zhai, Tianchi Lu, Haitian Zhong, Ziyang Xu, Yuhuan Liu, Xueying Wang, Dan Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Protein post-translational modifications (PTMs) and bioactive peptides (BPs)
play critical roles in various biological processes and have significant
therapeutic potential. However, identifying PTM sites and bioactive peptides
through experimental methods is often labor-intensive, costly, and
time-consuming. As a result, computational tools, particularly those based on
deep learning, have become effective solutions for predicting PTM sites and
peptide bioactivity. Despite progress in this field, existing methods still
struggle with the complexity of protein sequences and the challenge of
requiring high-quality predictions across diverse datasets.
  To address these issues, we propose a deep learning framework that integrates
pretrained protein language models with a neural network combining transformer
and CNN for peptide classification. By leveraging the ability of pretrained
models to capture complex relationships within protein sequences, combined with
the predictive power of parallel networks, our approach improves feature
extraction while enhancing prediction accuracy.
  This framework was applied to multiple tasks involving PTM site and bioactive
peptide prediction, utilizing large-scale datasets to enhance the model's
robustness. In the comparison across 33 tasks, the model achieved
state-of-the-art (SOTA) performance in 25 of them, surpassing existing methods
and demonstrating its versatility across different datasets. Our results
suggest that this approach provides a scalable and effective solution for
large-scale peptide discovery and PTM analysis, paving the way for more
efficient peptide classification and functional annotation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 5 figures, submitted to arXiv</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ On the Robustness of <span class="highlight-title">Transformer</span>s against Context Hijacking for Linear
  Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.15609v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.15609v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tianle Li, Chenyang Zhang, Xingwu Chen, Yuan Cao, Difan Zou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Transformer-based Large Language Models (LLMs) have demonstrated powerful
in-context learning capabilities. However, their predictions can be disrupted
by factually correct context, a phenomenon known as context hijacking,
revealing a significant robustness issue. To understand this phenomenon
theoretically, we explore an in-context linear classification problem based on
recent advances in linear transformers. In our setup, context tokens are
designed as factually correct query-answer pairs, where the queries are similar
to the final query but have opposite labels. Then, we develop a general
theoretical analysis on the robustness of the linear transformers, which is
formulated as a function of the model depth, training context lengths, and
number of hijacking context tokens. A key finding is that a well-trained deeper
transformer can achieve higher robustness, which aligns with empirical
observations. We show that this improvement arises because deeper layers enable
more fine-grained optimization steps, effectively mitigating interference from
context hijacking. This is also well supported by our numerical experiments.
Our findings provide theoretical insights into the benefits of deeper
architectures and contribute to enhancing the understanding of transformer
architectures.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Do Multilingual LLMs Think In English? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.15603v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.15603v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lisa Schut, Yarin Gal, Sebastian Farquhar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have multilingual capabilities and can solve
tasks across various languages. However, we show that current LLMs make key
decisions in a representation space closest to English, regardless of their
input and output languages. Exploring the internal representations with a logit
lens for sentences in French, German, Dutch, and Mandarin, we show that the LLM
first emits representations close to English for semantically-loaded words
before translating them into the target language. We further show that
activation steering in these LLMs is more effective when the steering vectors
are computed in English rather than in the language of the inputs and outputs.
This suggests that multilingual LLMs perform key reasoning steps in a
representation that is heavily shaped by English in a way that is not
transparent to system users.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Main paper 9 pages; including appendix 48 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ KAD: No More FAD! An Effective and Efficient Evaluation Metric for Audio
  Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.15602v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.15602v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yoonjin Chung, Pilsun Eu, Junwon Lee, Keunwoo Choi, Juhan Nam, Ben Sangbae Chon
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Although being widely adopted for evaluating generated audio signals, the
Fr\'echet Audio Distance (FAD) suffers from significant limitations, including
reliance on Gaussian assumptions, sensitivity to sample size, and high
computational complexity. As an alternative, we introduce the Kernel Audio
Distance (KAD), a novel, distribution-free, unbiased, and computationally
efficient metric based on Maximum Mean Discrepancy (MMD). Through analysis and
empirical validation, we demonstrate KAD's advantages: (1) faster convergence
with smaller sample sizes, enabling reliable evaluation with limited data; (2)
lower computational cost, with scalable GPU acceleration; and (3) stronger
alignment with human perceptual judgments. By leveraging advanced embeddings
and characteristic kernels, KAD captures nuanced differences between real and
generated audio. Open-sourced in the kadtk toolkit, KAD provides an efficient,
reliable, and perceptually aligned benchmark for evaluating generative audio
models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LightThinker: Thinking Step-by-Step Compression 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.15589v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.15589v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jintian Zhang, Yuqi Zhu, Mengshu Sun, Yujie Luo, Shuofei Qiao, Lun Du, Da Zheng, Huajun Chen, Ningyu Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have shown remarkable performance in complex
reasoning tasks, but their efficiency is hindered by the substantial memory and
computational costs associated with generating lengthy tokens. In this paper,
we propose LightThinker, a novel method that enables LLMs to dynamically
compress intermediate thoughts during reasoning. Inspired by human cognitive
processes, LightThinker compresses verbose thought steps into compact
representations and discards the original reasoning chains, thereby
significantly reducing the number of tokens stored in the context window. This
is achieved by training the model on when and how to perform compression
through data construction, mapping hidden states to condensed gist tokens, and
creating specialized attention masks. Additionally, we introduce the Dependency
(Dep) metric to quantify the degree of compression by measuring the reliance on
historical tokens during generation. Extensive experiments on four datasets and
two models show that LightThinker reduces peak memory usage and inference time,
while maintaining competitive accuracy. Our work provides a new direction for
improving the efficiency of LLMs in complex reasoning tasks without sacrificing
performance. Code will be released at https://github.com/zjunlp/LightThinker.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Improving the Scaling Laws of Synthetic Data with Deliberate Practice 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.15588v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.15588v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Reyhane Askari-Hemmat, Mohammad Pezeshki, Elvis Dohmatob, Florian Bordes, Pietro Astolfi, Melissa Hall, Jakob Verbeek, Michal Drozdzal, Adriana Romero-Soriano
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Inspired by the principle of deliberate practice in human learning, we
propose Deliberate Practice for Synthetic Data Generation (DP), a novel
framework that improves sample efficiency through dynamic synthetic data
generation. Prior work has shown that scaling synthetic data is inherently
challenging, as naively adding new data leads to diminishing returns. To
address this, pruning has been identified as a key mechanism for improving
scaling, enabling models to focus on the most informative synthetic samples.
Rather than generating a large dataset and pruning it afterward, DP efficiently
approximates the direct generation of informative samples. We theoretically
show how training on challenging, informative examples improves scaling laws
and empirically validate that DP achieves better scaling performance with
significantly fewer training samples and iterations. On ImageNet-100, DP
generates 3.4x fewer samples and requires six times fewer iterations, while on
ImageNet-1k, it generates 8x fewer samples with a 30 percent reduction in
iterations, all while achieving superior performance compared to prior work.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Context-Aware Doubly-Robust Semi-Supervised Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.15577v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.15577v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Clement Ruah, Houssem Sifaou, Osvaldo Simeone, Bashir Al-Hashimi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The widespread adoption of artificial intelligence (AI) in next-generation
communication systems is challenged by the heterogeneity of traffic and network
conditions, which call for the use of highly contextual, site-specific, data. A
promising solution is to rely not only on real-world data, but also on
synthetic pseudo-data generated by a network digital twin (NDT). However, the
effectiveness of this approach hinges on the accuracy of the NDT, which can
vary widely across different contexts. To address this problem, this paper
introduces context-aware doubly-robust (CDR) learning, a novel semi-supervised
scheme that adapts its reliance on the pseudo-data to the different levels of
fidelity of the NDT across contexts. CDR is evaluated on the task of downlink
beamforming, showing superior performance compared to previous state-of-the-art
semi-supervised approaches.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This work has been submitted to the IEEE for possible publication</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Feature maps for the Laplacian kernel and its generalizations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.15575v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.15575v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sudhendu Ahir, Parthe Pandit
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent applications of kernel methods in machine learning have seen a renewed
interest in the Laplacian kernel, due to its stability to the bandwidth
hyperparameter in comparison to the Gaussian kernel, as well as its
expressivity being equivalent to that of the neural tangent kernel of deep
fully connected networks. However, unlike the Gaussian kernel, the Laplacian
kernel is not separable. This poses challenges for techniques to approximate
it, especially via the random Fourier features (RFF) methodology and its
variants. In this work, we provide random features for the Laplacian kernel and
its two generalizations: Mat\'{e}rn kernel and the Exponential power kernel. We
provide efficiently implementable schemes to sample weight matrices so that
random features approximate these kernels. These weight matrices have a weakly
coupled heavy-tailed randomness. Via numerical experiments on real datasets we
demonstrate the efficacy of these random feature maps.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Cautionary Tale About "Neutrally" Informative AI Tools Ahead of the
  2025 Federal Elections in Germany 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.15568v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.15568v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ina Dormuth, Sven Franke, Marlies Hafer, Tim Katzke, Alexander Marx, Emmanuel Müller, Daniel Neider, Markus Pauly, Jérôme Rutinowski
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this study, we examine the reliability of AI-based Voting Advice
Applications (VAAs) and large language models (LLMs) in providing objective
political information. Our analysis is based upon a comparison with party
responses to 38 statements of the Wahl-O-Mat, a well-established German online
tool that helps inform voters by comparing their views with political party
positions. For the LLMs, we identify significant biases. They exhibit a strong
alignment (over 75% on average) with left-wing parties and a substantially
lower alignment with center-right (smaller 50%) and right-wing parties (around
30%). Furthermore, for the VAAs, intended to objectively inform voters, we
found substantial deviations from the parties' stated positions in Wahl-O-Mat:
While one VAA deviated in 25% of cases, another VAA showed deviations in more
than 50% of cases. For the latter, we even observed that simple prompt
injections led to severe hallucinations, including false claims such as
non-existent connections between political parties and right-wing extremist
ties.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Model Privacy: A Unified Framework to Understand Model Stealing Attacks
  and Defenses 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.15567v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.15567v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ganghua Wang, Yuhong Yang, Jie Ding
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The use of machine learning (ML) has become increasingly prevalent in various
domains, highlighting the importance of understanding and ensuring its safety.
One pressing concern is the vulnerability of ML applications to model stealing
attacks. These attacks involve adversaries attempting to recover a learned
model through limited query-response interactions, such as those found in
cloud-based services or on-chip artificial intelligence interfaces. While
existing literature proposes various attack and defense strategies, these often
lack a theoretical foundation and standardized evaluation criteria. In
response, this work presents a framework called ``Model Privacy'', providing a
foundation for comprehensively analyzing model stealing attacks and defenses.
We establish a rigorous formulation for the threat model and objectives,
propose methods to quantify the goodness of attack and defense strategies, and
analyze the fundamental tradeoffs between utility and privacy in ML models. Our
developed theory offers valuable insights into enhancing the security of ML
models, especially highlighting the importance of the attack-specific structure
of perturbations for effective defenses. We demonstrate the application of
model privacy from the defender's perspective through various learning
scenarios. Extensive experiments corroborate the insights and the effectiveness
of defense mechanisms developed under the proposed framework.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Bridging vision language model (VLM) evaluation gaps with a framework
  for scalable and cost-effective benchmark generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.15563v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.15563v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tim Rädsch, Leon Mayer, Simon Pavicic, A. Emre Kavur, Marcel Knopp, Barış Öztürk, Klaus Maier-Hein, Paul F. Jaeger, Fabian Isensee, Annika Reinke, Lena Maier-Hein
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reliable evaluation of AI models is critical for scientific progress and
practical application. While existing VLM benchmarks provide general insights
into model capabilities, their heterogeneous designs and limited focus on a few
imaging domains pose significant challenges for both cross-domain performance
comparison and targeted domain-specific evaluation. To address this, we propose
three key contributions: (1) a framework for the resource-efficient creation of
domain-specific VLM benchmarks enabled by task augmentation for creating
multiple diverse tasks from a single existing task, (2) the release of new VLM
benchmarks for seven domains, created according to the same homogeneous
protocol and including 162,946 thoroughly human-validated answers, and (3) an
extensive benchmarking of 22 state-of-the-art VLMs on a total of 37,171 tasks,
revealing performance variances across domains and tasks, thereby supporting
the need for tailored VLM benchmarks. Adoption of our methodology will pave the
way for the resource-efficient domain-specific selection of models and guide
future research efforts toward addressing core open questions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Defensive Framework Against Adversarial Attacks on Machine
  Learning-Based Network Intrusion Detection Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.15561v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.15561v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Benyamin Tafreshian, Shengzhi Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As cyberattacks become increasingly sophisticated, advanced Network Intrusion
Detection Systems (NIDS) are critical for modern network security. Traditional
signature-based NIDS are inadequate against zero-day and evolving attacks. In
response, machine learning (ML)-based NIDS have emerged as promising solutions;
however, they are vulnerable to adversarial evasion attacks that subtly
manipulate network traffic to bypass detection. To address this vulnerability,
we propose a novel defensive framework that enhances the robustness of ML-based
NIDS by simultaneously integrating adversarial training, dataset balancing
techniques, advanced feature engineering, ensemble learning, and extensive
model fine-tuning. We validate our framework using the NSL-KDD and UNSW-NB15
datasets. Experimental results show, on average, a 35% increase in detection
accuracy and a 12.5% reduction in false positives compared to baseline models,
particularly under adversarial conditions. The proposed defense against
adversarial attacks significantly advances the practical deployment of robust
ML-based NIDS in real-world networks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to IEEE AI+ TrustCom 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Estimating Vehicle Speed on Roadways Using RNNs and <span class="highlight-title">Transformer</span>s: A
  Video-based Approach 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.15545v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.15545v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sai Krishna Reddy Mareddy, Dhanush Upplapati, Dhanush Kumar Antharam
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This project explores the application of advanced machine learning models,
specifically Long Short-Term Memory (LSTM), Gated Recurrent Units (GRU), and
Transformers, to the task of vehicle speed estimation using video data.
Traditional methods of speed estimation, such as radar and manual systems, are
often constrained by high costs, limited coverage, and potential disruptions.
In contrast, leveraging existing surveillance infrastructure and cutting-edge
neural network architectures presents a non-intrusive, scalable solution. Our
approach utilizes LSTM and GRU to effectively manage long-term dependencies
within the temporal sequence of video frames, while Transformers are employed
to harness their self-attention mechanisms, enabling the processing of entire
sequences in parallel and focusing on the most informative segments of the
data. This study demonstrates that both LSTM and GRU outperform basic Recurrent
Neural Networks (RNNs) due to their advanced gating mechanisms. Furthermore,
increasing the sequence length of input data consistently improves model
accuracy, highlighting the importance of contextual information in dynamic
environments. Transformers, in particular, show exceptional adaptability and
robustness across varied sequence lengths and complexities, making them highly
suitable for real-time applications in diverse traffic conditions. The findings
suggest that integrating these sophisticated neural network models can
significantly enhance the accuracy and reliability of automated speed detection
systems, thus promising to revolutionize traffic management and road safety.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Generalization Guarantees for Representation Learning via Data-Dependent
  Gaussian Mixture Priors <span class="chip">ICLR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.15540v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.15540v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Milad Sefidgaran, Abdellatif Zaidi, Piotr Krasnowski
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We establish in-expectation and tail bounds on the generalization error of
representation learning type algorithms. The bounds are in terms of the
relative entropy between the distribution of the representations extracted from
the training and "test'' datasets and a data-dependent symmetric prior, i.e.,
the Minimum Description Length (MDL) of the latent variables for the training
and test datasets. Our bounds are shown to reflect the "structure" and
"simplicity'' of the encoder and significantly improve upon the few existing
ones for the studied model. We then use our in-expectation bound to devise a
suitable data-dependent regularizer; and we investigate thoroughly the
important question of the selection of the prior. We propose a systematic
approach to simultaneously learning a data-dependent Gaussian mixture prior and
using it as a regularizer. Interestingly, we show that a weighted attention
mechanism emerges naturally in this procedure. Our experiments show that our
approach outperforms the now popular Variational Information Bottleneck (VIB)
method as well as the recent Category-Dependent VIB (CDVIB).
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted as a Spotlight Paper at ICLR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Solving Inverse Problems with Deep Linear Neural Networks: Global
  Convergence Guarantees for Gradient Descent with Weight Decay 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.15522v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.15522v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hannah Laus, Suzanna Parkinson, Vasileios Charisopoulos, Felix Krahmer, Rebecca Willett
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Machine learning methods are commonly used to solve inverse problems, wherein
an unknown signal must be estimated from few measurements generated via a known
acquisition procedure. In particular, neural networks perform well empirically
but have limited theoretical guarantees. In this work, we study an
underdetermined linear inverse problem that admits several possible solution
mappings. A standard remedy (e.g., in compressed sensing) establishing
uniqueness of the solution mapping is to assume knowledge of latent
low-dimensional structure in the source signal. We ask the following question:
do deep neural networks adapt to this low-dimensional structure when trained by
gradient descent with weight decay regularization? We prove that mildly
overparameterized deep linear networks trained in this manner converge to an
approximate solution that accurately solves the inverse problem while
implicitly encoding latent subspace structure. To our knowledge, this is the
first result to rigorously show that deep linear networks trained with weight
decay automatically adapt to latent subspace structure in the data under
practical stepsize and weight initialization schemes. Our work highlights that
regularization and overparameterization improve generalization, while
overparameterization also accelerates convergence during training.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SALSA-RL: Stability Analysis in the Latent Space of Actions for
  Reinforcement Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.15512v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.15512v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xuyang Li, Romit Maulik
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Modern deep reinforcement learning (DRL) methods have made significant
advances in handling continuous action spaces. However, real-world control
systems--especially those requiring precise and reliable performance--often
demand formal stability, and existing DRL approaches typically lack explicit
mechanisms to ensure or analyze stability. To address this limitation, we
propose SALSA-RL (Stability Analysis in the Latent Space of Actions), a novel
RL framework that models control actions as dynamic, time-dependent variables
evolving within a latent space. By employing a pre-trained encoder-decoder and
a state-dependent linear system, our approach enables both stability analysis
and interpretability. We demonstrated that SALSA-RL can be deployed in a
non-invasive manner for assessing the local stability of actions from
pretrained RL agents without compromising on performance across diverse
benchmark environments. By enabling a more interpretable analysis of action
generation, SALSA-RL provides a powerful tool for advancing the design,
analysis, and theoretical understanding of RL systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Activation Steering in Neural Theorem Provers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.15507v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.15507v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shashank Kirtania
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have shown promise in proving formal theorems
using proof assistants like Lean. However, current state of the art language
models struggles to predict next step in proofs leading practitioners to use
different sampling techniques to improve LLMs capabilities. We observe that the
LLM is capable of predicting the correct tactic; however, it faces challenges
in ranking it appropriately within the set of candidate tactics, affecting the
overall selection process. To overcome this hurdle, we use activation steering
to guide LLMs responses to improve the generations at the time of inference.
Our results suggest that activation steering offers a promising lightweight
alternative to specialized fine-tuning for enhancing theorem proving
capabilities in LLMs, particularly valuable in resource-constrained
environments.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Verification and Validation for Trustworthy Scientific Machine Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.15496v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.15496v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        John D. Jakeman, Lorena A. Barba, Joaquim R. R. A. Martins, Thomas O'Leary-Roseberry
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Scientific machine learning (SciML) models are transforming many scientific
disciplines. However, the development of good modeling practices to increase
the trustworthiness of SciML has lagged behind its application, limiting its
potential impact. The goal of this paper is to start a discussion on
establishing consensus-based good practices for predictive SciML. We identify
key challenges in applying existing computational science and engineering
guidelines, such as verification and validation protocols, and provide
recommendations to address these challenges. Our discussion focuses on
predictive SciML, which uses machine learning models to learn, improve, and
accelerate numerical simulations of physical systems. While centered on
predictive applications, our 16 recommendations aim to help researchers conduc
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Network Resource Optimization for ML-Based UAV Condition Monitoring with
  Vibration Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.15491v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.15491v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alexandre Gemayel, Dimitrios Michael Manias, Abdallah Shami
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As smart cities begin to materialize, the role of Unmanned Aerial Vehicles
(UAVs) and their reliability becomes increasingly important. One aspect of
reliability relates to Condition Monitoring (CM), where Machine Learning (ML)
models are leveraged to identify abnormal and adverse conditions. Given the
resource-constrained nature of next-generation edge networks, the utilization
of precious network resources must be minimized. This work explores the
optimization of network resources for ML-based UAV CM frameworks. The developed
framework uses experimental data and varies the feature extraction aggregation
interval to optimize ML model selection. Additionally, by leveraging
dimensionality reduction techniques, there is a 99.9% reduction in network
resource consumption.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for publication in IEEE Networking Letters</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MoMa: A Modular Deep Learning Framework for Material Property Prediction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.15483v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.15483v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Botian Wang, Yawen Ouyang, Yaohui Li, Yiqun Wang, Haorui Cui, Jianbing Zhang, Xiaonan Wang, Wei-Ying Ma, Hao Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep learning methods for material property prediction have been widely
explored to advance materials discovery. However, the prevailing pre-train then
fine-tune paradigm often fails to address the inherent diversity and disparity
of material tasks. To overcome these challenges, we introduce MoMa, a Modular
framework for Materials that first trains specialized modules across a wide
range of tasks and then adaptively composes synergistic modules tailored to
each downstream scenario. Evaluation across 17 datasets demonstrates the
superiority of MoMa, with a substantial 14% average improvement over the
strongest baseline. Few-shot and continual learning experiments further
highlight MoMa's potential for real-world applications. Pioneering a new
paradigm of modular material learning, MoMa will be open-sourced to foster
broader community collaboration.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Sheaf theory: from deep geometry to deep learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.15476v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.15476v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anton Ayzenberg, Thomas Gebhart, German Magai, Grigory Solomadin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper provides an overview of the applications of sheaf theory in deep
learning, data science, and computer science in general. The primary text of
this work serves as a friendly introduction to applied and computational sheaf
theory accessible to those with modest mathematical familiarity. We describe
intuitions and motivations underlying sheaf theory shared by both theoretical
researchers and practitioners, bridging classical mathematical theory and its
more recent implementations within signal processing and deep learning. We
observe that most notions commonly considered specific to cellular sheaves
translate to sheaves on arbitrary posets, providing an interesting avenue for
further generalization of these methods in applications, and we present a new
algorithm to compute sheaf cohomology on arbitrary finite posets in response.
By integrating classical theory with recent applications, this work reveals
certain blind spots in current machine learning practices. We conclude with a
list of problems related to sheaf-theoretic applications that we find
mathematically insightful and practically instructive to solve. To ensure the
exposition of sheaf theory is self-contained, a rigorous mathematical
introduction is provided in appendices which moves from an introduction of
diagrams and sheaves to the definition of derived functors, higher order
cohomology, sheaf Laplacians, sheaf diffusion, and interconnections of these
subjects therein.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>117 pages, 8 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Decoding for Punctured Convolutional and Turbo Codes: A Deep Learning
  Solution for Protocols Compliance 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.15475v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.15475v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yongli Yan, Linglong Dai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neural network-based decoding methods have shown promise in enhancing error
correction performance, but traditional approaches struggle with the challenges
posed by punctured codes. In particular, these methods fail to address the
complexities of variable code rates and the need for protocol compatibility.
This paper presents a unified Long Short-Term Memory (LSTM)-based decoding
architecture specifically designed to overcome these challenges. The proposed
method unifies punctured convolutional and Turbo codes. A puncture embedding
mechanism integrates puncturing patterns directly into the network, enabling
seamless adaptation to varying code rates, while balanced bit error rate
training ensures robustness across different code lengths, rates, and channels,
maintaining protocol flexibility. Extensive simulations in Additive White
Gaussian Noise and Rayleigh fading channels demonstrate that the proposed
approach outperforms conventional decoding techniques, providing significant
improvements in decoding accuracy and robustness. These results underscore the
potential of LSTM-based decoding as a promising solution for next-generation
artificial intelligence powered communication systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PAPI: Exploiting Dynamic Parallelism in Large Language Model Decoding
  with a Processing-In-Memory-Enabled Computing System <span class="chip">ASPLOS 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.15470v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.15470v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yintao He, Haiyu Mao, Christina Giannoula, Mohammad Sadrosadati, Juan Gómez-Luna, Huawei Li, Xiaowei Li, Ying Wang, Onur Mutlu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) are widely used for natural language
understanding and text generation. An LLM model relies on a time-consuming step
called LLM decoding to generate output tokens. Several prior works focus on
improving the performance of LLM decoding using parallelism techniques, such as
batching and speculative decoding. State-of-the-art LLM decoding has both
compute-bound and memory-bound kernels. Some prior works statically identify
and map these different kernels to a heterogeneous architecture consisting of
both processing-in-memory (PIM) units and computation-centric accelerators. We
observe that characteristics of LLM decoding kernels (e.g., whether or not a
kernel is memory-bound) can change dynamically due to parameter changes to meet
user and/or system demands, making (1) static kernel mapping to PIM units and
computation-centric accelerators suboptimal, and (2) one-size-fits-all approach
of designing PIM units inefficient due to a large degree of heterogeneity even
in memory-bound kernels.
  In this paper, we aim to accelerate LLM decoding while considering the
dynamically changing characteristics of the kernels involved. We propose PAPI
(PArallel Decoding with PIM), a PIM-enabled heterogeneous architecture that
exploits dynamic scheduling of compute-bound or memory-bound kernels to
suitable hardware units. PAPI has two key mechanisms: (1) online kernel
characterization to dynamically schedule kernels to the most suitable hardware
units at runtime and (2) a PIM-enabled heterogeneous computing system that
harmoniously orchestrates both computation-centric processing units and hybrid
PIM units with different computing capabilities. Our experimental results on
three broadly-used LLMs show that PAPI achieves 1.8$\times$ and 11.1$\times$
speedups over a state-of-the-art heterogeneous LLM accelerator and a
state-of-the-art PIM-only LLM accelerator, respectively.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To appear in ASPLOS 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Mitigating Data Scarcity in Time Series Analysis: A Foundation Model
  with Series-Symbol Data Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.15466v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.15466v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenxuan Wang, Kai Wu, Yujian Betterest Li, Dan Wang, Xiaoyu Zhang, Jing Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Foundation models for time series analysis (TSA) have attracted significant
attention. However, challenges such as data scarcity and data imbalance
continue to hinder their development. To address this, we consider modeling
complex systems through symbolic expressions that serve as semantic descriptors
of time series. Building on this concept, we introduce a series-symbol (S2)
dual-modulity data generation mechanism, enabling the unrestricted creation of
high-quality time series data paired with corresponding symbolic
representations. Leveraging the S2 dataset, we develop SymTime, a pre-trained
foundation model for TSA. SymTime demonstrates competitive performance across
five major TSA tasks when fine-tuned with downstream task, rivaling foundation
models pre-trained on real-world datasets. This approach underscores the
potential of dual-modality data generation and pretraining mechanisms in
overcoming data scarcity and enhancing task performance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ R-LoRA: Random Initialization of Multi-Head LoRA for Multi-Task Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.15455v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.15455v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jinda Liu, Yi Chang, Yuan Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Fine-tuning large language models (LLMs) is prohibitively expensive in terms
of computational and memory costs. Low-rank Adaptation (LoRA), as one of the
most popular parameter-efficient fine-tuning (PEFT) methods, offers a
cost-effective alternative by approximating the model changes $\Delta W \in
\mathbb{R}^{m \times n}$ through the product of down-projection matrix $A \in
\mathbb{R}^{m \times r}$ and head matrix $B \in \mathbb{R}^{r \times n}$, where
$r \ll \min(m, n)$. In real-world scenarios, LLMs are fine-tuned on data from
multiple domains to perform tasks across various fields, embodying multi-task
learning (MTL). LoRA often underperforms in such complex scenarios. To enhance
LoRA's capability in multi-task learning, we propose R-LoRA, which incorporates
Multi-Head Randomization. Multi-Head Randomization diversifies the head
matrices through Multi-Head Random Initialization and Multi-Head Dropout,
enabling more efficient learning of task-specific features while maintaining
shared knowledge representation. Extensive experiments demonstrate that R-LoRA
is better at capturing task-specific knowledge, thereby improving performance
in multi-task scenarios. The code is available at
https://github.com/jinda-liu/R-LoRA.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 10 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A fast convergence algorithm based on binary integer programming for
  expert load balancing in MoE LLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.15451v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.15451v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuan Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  MoE (Mixture-of-Expert) architectures appear frequently in large language
models, and the number of experts can be over one hundred recently. However,
the expert load imbalance problem always happens in MoE model pre-training,
which will cause routing collapse or increased computational overhead. In order
to balance loads on experts, we propose BIP-Based Balancing, an expert load
balancing algorithm based on binary integer programming (BIP). The algorithm
maintains an additional vector q that can help change the top-K order of s by
solving a binary integer programming with very small time costs. In simulation
experiments, we observe that BIP-Based Balancing make imbalance disappoint very
fast, while the final sum of routine scores decreases very little. Our
algorithm achieves nearly perfect trade-off between expert load balance and
pre-training efficiency under the simulation view.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Dimension-free bounds in high-dimensional linear regression via
  error-in-operator approach 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.15437v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.15437v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fedor Noskov, Nikita Puchkin, Vladimir Spokoiny
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We consider a problem of high-dimensional linear regression with random
design. We suggest a novel approach referred to as error-in-operator which does
not estimate the design covariance $\Sigma$ directly but incorporates it into
empirical risk minimization. We provide an expansion of the excess prediction
risk and derive non-asymptotic dimension-free bounds on the leading term and
the remainder. This helps us to show that auxiliary variables do not increase
the effective dimension of the problem, provided that parameters of the
procedure are tuned properly. We also discuss computational aspects of our
method and illustrate its performance with numerical experiments.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>100 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Fed-SB: A Silver Bullet for Extreme Communication Efficiency and
  Performance in (Private) Federated LoRA Fine-Tuning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.15436v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.15436v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Raghav Singhal, Kaustubh Ponkshe, Rohit Vartak, Lav R. Varshney, Praneeth Vepakomma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Low-Rank Adaptation (LoRA) has become ubiquitous for efficiently fine-tuning
foundation models. However, federated fine-tuning using LoRA is challenging due
to suboptimal updates arising from traditional federated averaging of
individual adapters. Existing solutions either incur prohibitively high
communication cost that scales linearly with the number of clients or suffer
from performance degradation due to limited expressivity. We introduce
Federated Silver Bullet (Fed-SB), a novel approach for federated fine-tuning of
LLMs using LoRA-SB, a recently proposed low-rank adaptation method. LoRA-SB
optimally aligns the optimization trajectory with the ideal low-rank full
fine-tuning projection by learning a small square matrix (R) between adapters B
and A, keeping other components fixed. Direct averaging of R guarantees exact
updates, substantially reducing communication cost, which remains independent
of the number of clients, and enables scalability. Fed-SB achieves
state-of-the-art performance across commonsense reasoning, arithmetic
reasoning, and language inference tasks while reducing communication costs by
up to 230x. In private settings, Fed-SB further improves performance by (1)
reducing trainable parameters, thereby lowering the noise required for
differential privacy and (2) avoiding noise amplification introduced by other
methods. Overall, Fed-SB establishes a new Pareto frontier in the tradeoff
between communication and performance, offering an efficient and scalable
solution for both private and non-private federated fine-tuning. Our code is
publicly available at https://github.com/CERT-Lab/fed-sb.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Raghav Singhal and Kaustubh Ponkshe contributed equally to this work</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Single-pass Detection of Jailbreaking Input in Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.15435v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.15435v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Leyla Naz Candogan, Yongtao Wu, Elias Abad Rocamora, Grigorios G. Chrysos, Volkan Cevher
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Defending aligned Large Language Models (LLMs) against jailbreaking attacks
is a challenging problem, with existing approaches requiring multiple requests
or even queries to auxiliary LLMs, making them computationally heavy. Instead,
we focus on detecting jailbreaking input in a single forward pass. Our method,
called Single Pass Detection SPD, leverages the information carried by the
logits to predict whether the output sentence will be harmful. This allows us
to defend in just one forward pass. SPD can not only detect attacks effectively
on open-source models, but also minimizes the misclassification of harmless
inputs. Furthermore, we show that SPD remains effective even without complete
logit access in GPT-3.5 and GPT-4. We believe that our proposed method offers a
promising approach to efficiently safeguard LLMs against adversarial attacks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted in TMLR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Adversarial <span class="highlight-title">Prompt</span> Evaluation: Systematic Benchmarking of Guardrails
  Against <span class="highlight-title">Prompt</span> Input Attacks on LLMs <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.15427v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.15427v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Giulio Zizzo, Giandomenico Cornacchia, Kieran Fraser, Muhammad Zaid Hameed, Ambrish Rawat, Beat Buesser, Mark Purcell, Pin-Yu Chen, Prasanna Sattigeri, Kush Varshney
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As large language models (LLMs) become integrated into everyday applications,
ensuring their robustness and security is increasingly critical. In particular,
LLMs can be manipulated into unsafe behaviour by prompts known as jailbreaks.
The variety of jailbreak styles is growing, necessitating the use of external
defences known as guardrails. While many jailbreak defences have been proposed,
not all defences are able to handle new out-of-distribution attacks due to the
narrow segment of jailbreaks used to align them. Moreover, the lack of
systematisation around defences has created significant gaps in their practical
application. In this work, we perform systematic benchmarking across 15
different defences, considering a broad swathe of malicious and benign
datasets. We find that there is significant performance variation depending on
the style of jailbreak a defence is subject to. Additionally, we show that
based on current datasets available for evaluation, simple baselines can
display competitive out-of-distribution performance compared to many
state-of-the-art defences. Code is available at
https://github.com/IBM/Adversarial-Prompt-Evaluation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2024, Safe Generative AI Workshop</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Evaluate with the Inverse: Efficient Approximation of Latent Explanation
  Quality Distribution <span class="chip">AAAI 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.15403v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.15403v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Carlos Eiras-Franco, Anna Hedström, Marina M. -C. Höhne
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Obtaining high-quality explanations of a model's output enables developers to
identify and correct biases, align the system's behavior with human values, and
ensure ethical compliance. Explainable Artificial Intelligence (XAI)
practitioners rely on specific measures to gauge the quality of such
explanations. These measures assess key attributes, such as how closely an
explanation aligns with a model's decision process (faithfulness), how
accurately it pinpoints the relevant input features (localization), and its
consistency across different cases (robustness). Despite providing valuable
information, these measures do not fully address a critical practitioner's
concern: how does the quality of a given explanation compare to other potential
explanations? Traditionally, the quality of an explanation has been assessed by
comparing it to a randomly generated counterpart. This paper introduces an
alternative: the Quality Gap Estimate (QGE). The QGE method offers a direct
comparison to what can be viewed as the `inverse' explanation, one that
conceptually represents the antithesis of the original explanation. Our
extensive testing across multiple model architectures, datasets, and
established quality metrics demonstrates that the QGE method is superior to the
traditional approach. Furthermore, we show that QGE enhances the statistical
reliability of these quality assessments. This advance represents a significant
step toward a more insightful evaluation of explanations that enables a more
effective inspection of a model's behavior.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to AAAI 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning Chern Numbers of Topological Insulators with Gauge Equivariant
  Neural Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.15376v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.15376v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Longde Huang, Oleksandr Balabanov, Hampus Linander, Mats Granath, Daniel Persson, Jan E. Gerken
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Equivariant network architectures are a well-established tool for predicting
invariant or equivariant quantities. However, almost all learning problems
considered in this context feature a global symmetry, i.e. each point of the
underlying space is transformed with the same group element, as opposed to a
local ``gauge'' symmetry, where each point is transformed with a different
group element, exponentially enlarging the size of the symmetry group. Gauge
equivariant networks have so far mainly been applied to problems in quantum
chromodynamics. Here, we introduce a novel application domain for
gauge-equivariant networks in the theory of topological condensed matter
physics. We use gauge equivariant networks to predict topological invariants
(Chern numbers) of multiband topological insulators. The gauge symmetry of the
network guarantees that the predicted quantity is a topological invariant. We
introduce a novel gauge equivariant normalization layer to stabilize the
training and prove a universal approximation theorem for our setup. We train on
samples with trivial Chern number only but show that our models generalize to
samples with non-trivial Chern number. We provide various ablations of our
setup. Our code is available at https://github.com/sitronsea/GENet/tree/main.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Fréchet Cumulative Covariance Net for Deep Nonlinear Sufficient
  Dimension Reduction with Random Objects 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.15374v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.15374v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hang Yuan, Christina Dan Wang, Zhou Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Nonlinear sufficient dimension reduction\citep{libing_generalSDR}, which
constructs nonlinear low-dimensional representations to summarize essential
features of high-dimensional data, is an important branch of representation
learning. However, most existing methods are not applicable when the response
variables are complex non-Euclidean random objects, which are frequently
encountered in many recent statistical applications. In this paper, we
introduce a new statistical dependence measure termed Fr\'echet Cumulative
Covariance (FCCov) and develop a novel nonlinear SDR framework based on FCCov.
Our approach is not only applicable to complex non-Euclidean data, but also
exhibits robustness against outliers. We further incorporate Feedforward Neural
Networks (FNNs) and Convolutional Neural Networks (CNNs) to estimate nonlinear
sufficient directions in the sample level. Theoretically, we prove that our
method with squared Frobenius norm regularization achieves unbiasedness at the
$\sigma$-field level. Furthermore, we establish non-asymptotic convergence
rates for our estimators based on FNNs and ResNet-type CNNs, which match the
minimax rate of nonparametric regression up to logarithmic factors. Intensive
simulation studies verify the performance of our methods in both Euclidean and
non-Euclidean settings. We apply our method to facial expression recognition
datasets and the results underscore more realistic and broader applicability of
our proposal.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Efficient and Provable Algorithms for Covariate Shift 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.15372v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.15372v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Deeksha Adil, Jarosław Błasiok
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Covariate shift, a widely used assumption in tackling {\it distributional
shift} (when training and test distributions differ), focuses on scenarios
where the distribution of the labels conditioned on the feature vector is the
same, but the distribution of features in the training and test data are
different. Despite the significance and extensive work on covariate shift,
theoretical guarantees for algorithms in this domain remain sparse. In this
paper, we distill the essence of the covariate shift problem and focus on
estimating the average $\mathbb{E}_{\tilde{\mathbf{x}}\sim
p_{\mathrm{test}}}\mathbf{f}(\tilde{\mathbf{x}})$, of any unknown and bounded
function $\mathbf{f}$, given labeled training samples $(\mathbf{x}_i,
\mathbf{f}(\mathbf{x}_i))$, and unlabeled test samples $\tilde{\mathbf{x}}_i$;
this is a core subroutine for several widely studied learning problems. We give
several efficient algorithms, with provable sample complexity and computational
guarantees. Moreover, we provide the first rigorous analysis of algorithms in
this space when $\mathbf{f}$ is unrestricted, laying the groundwork for
developing a solid theoretical foundation for covariate shift problems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AttentionEngine: A Versatile Framework for Efficient Attention
  Mechanisms on Diverse Hardware Platforms 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.15349v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.15349v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Feiyang Chen, Yu Cheng, Lei Wang, Yuqing Xia, Ziming Miao, Lingxiao Ma, Fan Yang, Jilong Xue, Zhi Yang, Mao Yang, Haibo Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Transformers and large language models (LLMs) have revolutionized machine
learning, with attention mechanisms at the core of their success. As the
landscape of attention variants expands, so too do the challenges of optimizing
their performance, particularly across different hardware platforms. Current
optimization strategies are often narrowly focused, requiring extensive manual
intervention to accommodate changes in model configurations or hardware
environments. In this paper, we introduce AttentionEngine, a comprehensive
framework designed to streamline the optimization of attention mechanisms
across heterogeneous hardware backends. By decomposing attention computation
into modular operations with customizable components, AttentionEngine enables
flexible adaptation to diverse algorithmic requirements. The framework further
automates kernel optimization through a combination of programmable templates
and a robust cross-platform scheduling strategy. Empirical results reveal
performance gains of up to 10x on configurations beyond the reach of existing
methods. AttentionEngine offers a scalable, efficient foundation for developing
and deploying attention mechanisms with minimal manual tuning. Our code has
been open-sourced and is available at
https://github.com/microsoft/AttentionEngine.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Drug-Target Interaction/Affinity Prediction: Deep Learning Models and
  Advances <span class="highlight-title">Review</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.15346v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.15346v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ali Vefghi, Zahed Rahmati, Mohammad Akbari
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Drug discovery remains a slow and expensive process that involves many steps,
from detecting the target structure to obtaining approval from the Food and
Drug Administration (FDA), and is often riddled with safety concerns. Accurate
prediction of how drugs interact with their targets and the development of new
drugs by using better methods and technologies have immense potential to speed
up this process, ultimately leading to faster delivery of life-saving
medications. Traditional methods used for drug-target interaction prediction
show limitations, particularly in capturing complex relationships between drugs
and their targets. As an outcome, deep learning models have been presented to
overcome the challenges of interaction prediction through their precise and
efficient end results. By outlining promising research avenues and models, each
with a different solution but similar to the problem, this paper aims to give
researchers a better idea of methods for even more accurate and efficient
prediction of drug-target interaction, ultimately accelerating the development
of more effective drugs. A total of 180 prediction methods for drug-target
interactions were analyzed throughout the period spanning 2016 to 2025 using
different frameworks based on machine learning, mainly deep learning and graph
neural networks. Additionally, this paper discusses the novelty, architecture,
and input representation of these models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>64 pages, 7 figures, 10 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Efficiently Solving Discounted MDPs with Predictions on Transition
  Matrices 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.15345v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.15345v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lixing Lyu, Jiashuo Jiang, Wang Chi Cheung
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study infinite-horizon Discounted Markov Decision Processes (DMDPs) under
a generative model. Motivated by the Algorithm with Advice framework
Mitzenmacher and Vassilvitskii 2022, we propose a novel framework to
investigate how a prediction on the transition matrix can enhance the sample
efficiency in solving DMDPs and improve sample complexity bounds. We focus on
the DMDPs with $N$ state-action pairs and discounted factor $\gamma$. Firstly,
we provide an impossibility result that, without prior knowledge of the
prediction accuracy, no sampling policy can compute an $\epsilon$-optimal
policy with a sample complexity bound better than $\tilde{O}((1-\gamma)^{-3}
N\epsilon^{-2})$, which matches the state-of-the-art minimax sample complexity
bound with no prediction. In complement, we propose an algorithm based on
minimax optimization techniques that leverages the prediction on the transition
matrix. Our algorithm achieves a sample complexity bound depending on the
prediction error, and the bound is uniformly better than
$\tilde{O}((1-\gamma)^{-4} N \epsilon^{-2})$, the previous best result derived
from convex optimization methods. These theoretical findings are further
supported by our numerical experiments.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning with Limited Shared Information in Multi-agent Multi-armed
  Bandit 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.15338v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.15338v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junning Shao, Siwei Wang, Zhixuan Fang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-agent multi-armed bandit (MAMAB) is a classic collaborative learning
model and has gained much attention in recent years. However, existing studies
do not consider the case where an agent may refuse to share all her information
with others, e.g., when some of the data contains personal privacy. In this
paper, we propose a novel limited shared information multi-agent multi-armed
bandit (LSI-MAMAB) model in which each agent only shares the information that
she is willing to share, and propose the Balanced-ETC algorithm to help
multiple agents collaborate efficiently with limited shared information. Our
analysis shows that Balanced-ETC is asymptotically optimal and its average
regret (on each agent) approaches a constant when there are sufficient agents
involved. Moreover, to encourage agents to participate in this collaborative
learning, an incentive mechanism is proposed to make sure each agent can
benefit from the collaboration system. Finally, we present experimental results
to validate our theoretical results.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Attention Eclipse: Manipulating Attention to Bypass LLM Safety-Alignment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.15334v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.15334v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pedram Zaree, Md Abdullah Al Mamun, Quazi Mishkatul Alam, Yue Dong, Ihsen Alouani, Nael Abu-Ghazaleh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent research has shown that carefully crafted jailbreak inputs can induce
large language models to produce harmful outputs, despite safety measures such
as alignment. It is important to anticipate the range of potential Jailbreak
attacks to guide effective defenses and accurate assessment of model safety. In
this paper, we present a new approach for generating highly effective Jailbreak
attacks that manipulate the attention of the model to selectively strengthen or
weaken attention among different parts of the prompt. By harnessing attention
loss, we develop more effective jailbreak attacks, that are also transferrable.
The attacks amplify the success rate of existing Jailbreak algorithms including
GCG, AutoDAN, and ReNeLLM, while lowering their generation cost (for example,
the amplified GCG attack achieves 91.2% ASR, vs. 67.9% for the original attack
on Llama2-7B/AdvBench, using less than a third of the generation time).
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Lightweight yet Efficient: An External Attentive Graph Convolutional
  Network with Positional <span class="highlight-title">Prompt</span>s for Sequential Recommendation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.15331v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.15331v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jinyu Zhang, Chao Li, Zhongying Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Graph-based Sequential Recommender systems (GSRs) have gained significant
research attention due to their ability to simultaneously handle user-item
interactions and sequential relationships between items. Current GSRs often
utilize composite or in-depth structures for graph encoding (e.g., the Graph
Transformer). Nevertheless, they have high computational complexity, hindering
the deployment on resource-constrained edge devices. Moreover, the relative
position encoding in Graph Transformer has difficulty in considering the
complicated positional dependencies within sequence. To this end, we propose an
External Attentive Graph convolutional network with Positional prompts for
Sequential recommendation, namely EA-GPS. Specifically, we first introduce an
external attentive graph convolutional network that linearly measures the
global associations among nodes via two external memory units. Then, we present
a positional prompt-based decoder that explicitly treats the absolute item
positions as external prompts. By introducing length-adaptive sequential
masking and a soft attention network, such a decoder facilitates the model to
capture the long-term positional dependencies and contextual relationships
within sequences. Extensive experimental results on five real-world datasets
demonstrate that the proposed EA-GPS outperforms the state-of-the-art methods.
Remarkably, it achieves the superior performance while maintaining a smaller
parameter size and lower training overhead. The implementation of this work is
publicly available at https://github.com/ZZY-GraphMiningLab/EA-GPS.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>26 pages, 8 figures, journal paper, accepted by TOIS at 20th
  February, 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Utilizing Sequential Information of General Lab-test Results and
  Diagnoses History for Differential Diagnosis of Dementia 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.15317v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.15317v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yizong Xing, Dhita Putri Pratama, Yuke Wang, Yufan Zhang, Brian E. Chapman
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Early diagnosis of Alzheimer's Disease (AD) faces multiple data-related
challenges, including high variability in patient data, limited access to
specialized diagnostic tests, and overreliance on single-type indicators. These
challenges are exacerbated by the progressive nature of AD, where subtle
pathophysiological changes often precede clinical symptoms by decades. To
address these limitations, this study proposes a novel approach that takes
advantage of routinely collected general laboratory test histories for the
early detection and differential diagnosis of AD. By modeling lab test
sequences as "sentences", we apply word embedding techniques to capture latent
relationships between tests and employ deep time series models, including
long-short-term memory (LSTM) and Transformer networks, to model temporal
patterns in patient records. Experimental results demonstrate that our approach
improves diagnostic accuracy and enables scalable and costeffective AD
screening in diverse clinical settings.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, 6 figures. This work has been submitted to the IEEE for
  possible publication</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Tight Clusters Make Specialized Experts 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.15315v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.15315v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Stefan K. Nielsen, Rachel S. Y. Teo, Laziz U. Abdullaev, Tan M. Nguyen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Sparse Mixture-of-Experts (MoE) architectures have emerged as a promising
approach to decoupling model capacity from computational cost. At the core of
the MoE model is the router, which learns the underlying clustering structure
of the input distribution in order to send input tokens to appropriate experts.
However, latent clusters may be unidentifiable in high dimension, which causes
slow convergence, susceptibility to data contamination, and overall degraded
representations as the router is unable to perform appropriate token-expert
matching. We examine the router through the lens of clustering optimization and
derive optimal feature weights that maximally identify the latent clusters. We
use these weights to compute the token-expert routing assignments in an
adaptively transformed space that promotes well-separated clusters, which helps
identify the best-matched expert for each token. In particular, for each expert
cluster, we compute a set of weights that scales features according to whether
that expert clusters tightly along that feature. We term this novel router the
Adaptive Clustering (AC) router. Our AC router enables the MoE model to obtain
three connected benefits: 1) faster convergence, 2) better robustness to data
corruption, and 3) overall performance improvement, as experts are specialized
in semantically distinct regions of the input space. We empirically demonstrate
the advantages of our AC router over baseline routing methods when applied on a
variety of MoE backbones for language modeling and image recognition tasks in
both clean and corrupted settings.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Data-Driven Real-Time Optimal Power Flow Algorithm Using Local
  Feedback 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.15306v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.15306v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Heng Liang, Yujin Huang, Changhong Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The increasing penetration of distributed energy resources (DERs) adds
variability as well as fast control capabilities to power networks. Dispatching
the DERs based on local information to provide real-time optimal network
operation is the desideratum. In this paper, we propose a data-driven real-time
algorithm that uses only the local measurements to solve time-varying AC
optimal power flow (OPF). Specifically, we design a learnable function that
takes the local feedback as input in the algorithm. The learnable function,
under certain conditions, will result in a unique stationary point of the
algorithm, which in turn transfers the OPF problems to be optimized over the
parameters of the function. We then develop a stochastic primal-dual update to
solve the variant of the OPF problems based on a deep neural network (DNN)
parametrization of the learnable function, which is referred to as the training
stage. We also design a gradient-free alternative to bypass the cumbersome
gradient calculation of the nonlinear power flow model. The OPF
solution-tracking error bound is established in the sense of universal
approximation of DNN. Numerical results on the IEEE 37-bus test feeder show
that the proposed method can track the time-varying OPF solutions with higher
accuracy and faster computation compared to benchmark methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SVDq: 1.25-bit and 410x Key Cache Compression for LLM Attention 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.15304v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.15304v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hong Yankun, Li Xing, Zhen Hui-Ling, Yu Xianzhi, Liu Wulong, Yuan Mingxuan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  For the efficient inference of Large Language Models (LLMs), the effective
compression of key-value (KV) cache is essential. Three main types of KV cache
compression techniques, namely sparsity, channel compression, and quantization,
have been identified. This study presents SVDq, a Singular Value Decomposition
(SVD) - based mixed precision quantization method for K cache. Initially, K
cache is transformed into latent channels using SVD basis representations.
Since the values in latent channels decay rapidly and become negligible after
only a few latent channels, our method then incorporates importance-aware
quantization and compression for latent channels. This enables the effective
allocation of higher precision to more significant channels. Theoretically, we
prove that SVDq results in quantization errors (x0.1 or even lower) that are
much lower than those of per-channel key quantization in the original space.
Our findings based on RULER and LongBench benchmarks demonstrate that SVDq can
achieve an equivalent key cache precision as low as 1.25-bit. When combined
with key sparsity, it can reach a key compression ratio of up to 410x for
attention computation, all while maintaining comparable model performance.
Notably, our method is nearly lossless for LongBench datasets. This indicates
that SVDq enables high-precision low-bit quantization, providing a more
efficient solution for KV cache compression in LLMs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Beyond Fixed Variables: Expanding-variate Time Series Forecasting via
  Flat Scheme and Spatio-temporal Focal Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.15296v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.15296v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Minbo Ma, Kai Tang, Huan Li, Fei Teng, Dalin Zhang, Tianrui Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multivariate Time Series Forecasting (MTSF) has long been a key research
focus. Traditionally, these studies assume a fixed number of variables, but in
real-world applications, Cyber-Physical Systems often expand as new sensors are
deployed, increasing variables in MTSF. In light of this, we introduce a novel
task, Expanding-variate Time Series Forecasting (EVTSF). This task presents
unique challenges, specifically (1) handling inconsistent data shapes caused by
adding new variables, and (2) addressing imbalanced spatio-temporal learning,
where expanding variables have limited observed data due to the necessity for
timely operation. To address these challenges, we propose STEV, a flexible
spatio-temporal forecasting framework. STEV includes a new Flat Scheme to
tackle the inconsistent data shape issue, which extends the graph-based
spatio-temporal modeling architecture into 1D space by flattening the 2D
samples along the variable dimension, making the model variable-scale-agnostic
while still preserving dynamic spatial correlations through a holistic graph.
We introduce a novel Spatio-temporal Focal Learning strategy that incorporates
a negative filter to resolve potential conflicts between contrastive learning
and graph representation, and a focal contrastive loss as its core to guide the
framework to focus on optimizing the expanding variables. We benchmark EVTSF
performance using three real-world datasets and compare it against three
potential solutions employing SOTA MTSF models tailored for EVSTF. Experimental
results show that STEV significantly outperforms its competitors, particularly
on expanding variables. Notably, STEV, with only 5% of observations from the
expanding period, is on par with SOTA MTSF models trained with complete
observations. Further exploration of various expanding strategies underscores
the generalizability of STEV in real-world applications.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Hyperspherical Normalization for Scalable Deep Reinforcement Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.15280v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.15280v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hojoon Lee, Youngdo Lee, Takuma Seno, Donghu Kim, Peter Stone, Jaegul Choo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Scaling up the model size and computation has brought consistent performance
improvements in supervised learning. However, this lesson often fails to apply
to reinforcement learning (RL) because training the model on non-stationary
data easily leads to overfitting and unstable optimization. In response, we
introduce SimbaV2, a novel RL architecture designed to stabilize optimization
by (i) constraining the growth of weight and feature norm by hyperspherical
normalization; and (ii) using a distributional value estimation with reward
scaling to maintain stable gradients under varying reward magnitudes. Using the
soft actor-critic as a base algorithm, SimbaV2 scales up effectively with
larger models and greater compute, achieving state-of-the-art performance on 57
continuous control tasks across 4 domains. The code is available at
https://dojeon-ai.github.io/SimbaV2.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>50 pages. Preprint</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards a Reward-Free Reinforcement Learning Framework for Vehicle
  Control 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.15262v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.15262v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jielong Yang, Daoyuan Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reinforcement learning plays a crucial role in vehicle control by guiding
agents to learn optimal control strategies through designing or learning
appropriate reward signals. However, in vehicle control applications, rewards
typically need to be manually designed while considering multiple implicit
factors, which easily introduces human biases. Although imitation learning
methods does not rely on explicit reward signals, they necessitate high-quality
expert actions, which are often challenging to acquire. To address these
issues, we propose a reward-free reinforcement learning framework (RFRLF). This
framework directly learns the target states to optimize agent behavior through
a target state prediction network (TSPN) and a reward-free state-guided policy
network (RFSGPN), avoiding the dependence on manually designed reward signals.
Specifically, the policy network is learned via minimizing the differences
between the predicted state and the expert state. Experimental results
demonstrate the effectiveness of the proposed RFRLF in controlling vehicle
driving, showing its advantages in improving learning efficiency and adapting
to reward-free environments.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Real-Time Moving Flock Detection in Pedestrian Trajectories Using
  Sequential Deep Learning Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.15252v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.15252v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Amartaivan Sanjjamts, Hiroshi Morita, Togootogtokh Enkhtogtokh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Understanding collective pedestrian movement is crucial for applications in
crowd management, autonomous navigation, and human-robot interaction. This
paper investigates the use of sequential deep learning models, including
Recurrent Neural Networks (RNNs), Long Short-Term Memory (LSTM) networks, and
Transformers, for real-time flock detection in multi-pedestrian trajectories.
Our proposed approach consists of a two-stage process: first, a pre-trained
binary classification model is used for pairwise trajectory classification, and
second, the learned representations are applied to identify multi-agent flocks
dynamically.
  We validate our method using real-world group movement datasets,
demonstrating its robustness across varying sequence lengths and diverse
movement patterns. Experimental results indicate that our model consistently
detects pedestrian flocks with high accuracy and stability, even in dynamic and
noisy environments. Furthermore, we extend our approach to identify other forms
of collective motion, such as convoys and swarms, paving the way for more
comprehensive multi-agent behavior analysis.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Steganographic Embeddings as an Effective Data Augmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.15245v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.15245v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nicholas DiSalvo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Image Steganography is a cryptographic technique that embeds secret
information into an image, ensuring the hidden data remains undetectable to the
human eye while preserving the image's original visual integrity. Least
Significant Bit (LSB) Steganography achieves this by replacing the k least
significant bits of an image with the k most significant bits of a secret
image, maintaining the appearance of the original image while simultaneously
encoding the essential elements of the hidden data. In this work, we shift away
from conventional applications of steganography in deep learning and explore
its potential from a new angle. We present experimental results on CIFAR-10
showing that LSB Steganography, when used as a data augmentation strategy for
downstream computer vision tasks such as image classification, can
significantly improve the training efficiency of deep neural networks. It can
also act as an implicit, uniformly discretized piecewise linear approximation
of color augmentations such as (brightness, contrast, hue, and saturation),
without introducing additional training overhead through a new joint image
training regime that disregards the need for tuning sensitive augmentation
hyperparameters.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 4 figures. For associated code and experiments, see this
  http URL https://github.com/nickd16/steganographic-augmentations</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multi-agent Multi-armed Bandits with Minimum Reward Guarantee Fairness 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.15240v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.15240v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Piyushi Manupriya,  Himanshu, SakethaNath Jagarlapudi, Ganesh Ghalme
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We investigate the problem of maximizing social welfare while ensuring
fairness in a multi-agent multi-armed bandit (MA-MAB) setting. In this problem,
a centralized decision-maker takes actions over time, generating random rewards
for various agents. Our goal is to maximize the sum of expected cumulative
rewards, a.k.a. social welfare, while ensuring that each agent receives an
expected reward that is at least a constant fraction of the maximum possible
expected reward.
  Our proposed algorithm, RewardFairUCB, leverages the Upper Confidence Bound
(UCB) technique to achieve sublinear regret bounds for both fairness and social
welfare. The fairness regret measures the positive difference between the
minimum reward guarantee and the expected reward of a given policy, whereas the
social welfare regret measures the difference between the social welfare of the
optimal fair policy and that of the given policy.
  We show that RewardFairUCB algorithm achieves instance-independent social
welfare regret guarantees of $\tilde{O}(T^{1/2})$ and a fairness regret upper
bound of $\tilde{O}(T^{3/4})$. We also give the lower bound of
$\Omega(\sqrt{T})$ for both social welfare and fairness regret. We evaluate
RewardFairUCB's performance against various baseline and heuristic algorithms
using simulated data and real world data, highlighting trade-offs between
fairness and social welfare regrets.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Auto-Bench: An Automated Benchmark for Scientific Discovery in LLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.15224v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.15224v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tingting Chen, Srinivas Anumasa, Beibei Lin, Vedant Shah, Anirudh Goyal, Dianbo Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Given the remarkable performance of Large Language Models (LLMs), an
important question arises: Can LLMs conduct human-like scientific research and
discover new knowledge, and act as an AI scientist? Scientific discovery is an
iterative process that demands efficient knowledge updating and encoding. It
involves understanding the environment, identifying new hypotheses, and
reasoning about actions; however, no standardized benchmark specifically
designed for scientific discovery exists for LLM agents. In response to these
limitations, we introduce a novel benchmark, \textit{Auto-Bench}, that
encompasses necessary aspects to evaluate LLMs for scientific discovery in both
natural and social sciences. Our benchmark is based on the principles of causal
graph discovery. It challenges models to uncover hidden structures and make
optimal decisions, which includes generating valid justifications. By engaging
interactively with an oracle, the models iteratively refine their understanding
of underlying interactions, the chemistry and social interactions, through
strategic interventions. We evaluate state-of-the-art LLMs, including GPT-4,
Gemini, Qwen, Claude, and Llama, and observe a significant performance drop as
the problem complexity increases, which suggests an important gap between
machine and human intelligence that future development of LLMs need to take
into consideration.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FormalSpecCpp: A <span class="highlight-title">Dataset</span> of C++ Formal Specifications created using LLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.15217v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.15217v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Madhurima Chakraborty, Peter Pirkelbauer, Qing Yi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  FormalSpecCpp is a dataset designed to fill the gap in standardized
benchmarks for verifying formal specifications in C++ programs. To the best of
our knowledge, this is the first comprehensive collection of C++ programs with
well-defined preconditions and postconditions. It provides a structured
benchmark for evaluating specification inference tools and testing theaccuracy
of generated specifications. Researchers and developers can use this dataset to
benchmark specification inference tools,fine-tune Large Language Models (LLMs)
for automated specification generation, and analyze the role of formal
specifications in improving program verification and automated testing. By
making this dataset publicly available, we aim to advance research in program
verification, specification inference, and AI-assisted software development.
The dataset and the code are available at
https://github.com/MadhuNimmo/FormalSpecCpp.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at the 2025 IEEE/ACM 22nd International Conference on Mining
  Software Repositories (MSR)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The Evolving Landscape of LLM- and VLM-Integrated Reinforcement Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.15214v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.15214v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sheila Schoepp, Masoud Jafaripour, Yingyue Cao, Tianpei Yang, Fatemeh Abdollahi, Shadan Golestan, Zahin Sufiyan, Osmar R. Zaiane, Matthew E. Taylor
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reinforcement learning (RL) has shown impressive results in sequential
decision-making tasks. Meanwhile, Large Language Models (LLMs) and
Vision-Language Models (VLMs) have emerged, exhibiting impressive capabilities
in multimodal understanding and reasoning. These advances have led to a surge
of research integrating LLMs and VLMs into RL. In this survey, we review
representative works in which LLMs and VLMs are used to overcome key challenges
in RL, such as lack of prior knowledge, long-horizon planning, and reward
design. We present a taxonomy that categorizes these LLM/VLM-assisted RL
approaches into three roles: agent, planner, and reward. We conclude by
exploring open problems, including grounding, bias mitigation, improved
representations, and action advice. By consolidating existing research and
identifying future directions, this survey establishes a framework for
integrating LLMs and VLMs into RL, advancing approaches that unify natural
language and visual understanding with sequential decision-making.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Graph-Based Deep Learning on Stereo EEG for Predicting Seizure Freedom
  in Epilepsy Patients 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.15198v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.15198v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Artur Agaronyan, Syeda Abeera Amir, Nunthasiri Wittayanakorn, John Schreiber, Marius G. Linguraru, William Gaillard, Chima Oluigbo, Syed Muhammad Anwar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Predicting seizure freedom is essential for tailoring epilepsy treatment. But
accurate prediction remains challenging with traditional methods, especially
with diverse patient populations. This study developed a deep learning-based
graph neural network (GNN) model to predict seizure freedom from stereo
electroencephalography (sEEG) data in patients with refractory epilepsy. We
utilized high-quality sEEG data from 15 pediatric patients to train a deep
learning model that can accurately predict seizure freedom outcomes and advance
understanding of brain connectivity at the seizure onset zone. Our model
integrates local and global connectivity using graph convolutions with
multi-scale attention mechanisms to capture connections between
difficult-to-study regions such as the thalamus and motor regions. The model
achieved an accuracy of 92.4% in binary class analysis, 86.6% in patient-wise
analysis, and 81.4% in multi-class analysis. Node and edge-level feature
analysis highlighted the anterior cingulate and frontal pole regions as key
contributors to seizure freedom outcomes. The nodes identified by our model
were also more likely to coincide with seizure onset zones. Our findings
underscore the potential of new connectivity-based deep learning models such as
GNNs for enhancing the prediction of seizure freedom, predicting seizure onset
zones, connectivity analysis of the brain during seizure, as well as informing
AI-assisted personalized epilepsy treatment planning.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Optimizing Product Provenance Verification using Data Valuation Methods 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.15177v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.15177v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Raquib Bin Yousuf, Hoang Anh Just, Shengzhe Xu, Brian Mayer, Victor Deklerck, Jakub Truszkowski, John C. Simeone, Jade Saunders, Chang-Tien Lu, Ruoxi Jia, Naren Ramakrishnan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Determining and verifying product provenance remains a critical challenge in
global supply chains, particularly as geopolitical conflicts and shifting
borders create new incentives for misrepresentation of commodities, such as
hiding the origin of illegally harvested timber or stolen agricultural
products. Stable Isotope Ratio Analysis (SIRA), combined with Gaussian process
regression-based isoscapes, has emerged as a powerful tool for geographic
origin verification. However, the effectiveness of these models is often
constrained by data scarcity and suboptimal dataset selection. In this work, we
introduce a novel data valuation framework designed to enhance the selection
and utilization of training data for machine learning models applied in SIRA.
By prioritizing high-informative samples, our approach improves model
robustness and predictive accuracy across diverse datasets and geographies. We
validate our methodology with extensive experiments, demonstrating its
potential to significantly enhance provenance verification, mitigate fraudulent
trade practices, and strengthen regulatory enforcement of global supply chains.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CoT-ICL Lab: A Petri Dish for Studying Chain-of-Thought Learning from
  In-Context Demonstrations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.15132v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.15132v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vignesh Kothapalli, Hamed Firooz, Maziar Sanjabi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce CoT-ICL Lab, a framework and methodology to generate synthetic
tokenized datasets and systematically study chain-of-thought (CoT) in-context
learning (ICL) in language models. CoT-ICL Lab allows fine grained control over
the complexity of in-context examples by decoupling (1) the causal structure
involved in chain token generation from (2) the underlying token processing
functions. We train decoder-only transformers (up to 700M parameters) on these
datasets and show that CoT accelerates the accuracy transition to higher values
across model sizes. In particular, we find that model depth is crucial for
leveraging CoT with limited in-context examples, while more examples help
shallow models match deeper model performance. Additionally, limiting the
diversity of token processing functions throughout training improves causal
structure learning via ICL. We also interpret these transitions by analyzing
transformer embeddings and attention maps. Overall, CoT-ICL Lab serves as a
simple yet powerful testbed for theoretical and empirical insights into ICL and
CoT in language models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>22 pages, 27 figures, 3 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Optimal and Provable Calibration in High-Dimensional Binary
  Classification: Angular Calibration and Platt Scaling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.15131v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.15131v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yufan Li, Pragya Sur
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study the fundamental problem of calibrating a linear binary classifier of
the form $\sigma(\hat{w}^\top x)$, where the feature vector $x$ is Gaussian,
$\sigma$ is a link function, and $\hat{w}$ is an estimator of the true linear
weight $w^\star$. By interpolating with a noninformative $\textit{chance
classifier}$, we construct a well-calibrated predictor whose interpolation
weight depends on the angle $\angle(\hat{w}, w_\star)$ between the estimator
$\hat{w}$ and the true linear weight $w_\star$. We establish that this angular
calibration approach is provably well-calibrated in a high-dimensional regime
where the number of samples and features both diverge, at a comparable rate.
The angle $\angle(\hat{w}, w_\star)$ can be consistently estimated.
Furthermore, the resulting predictor is uniquely $\textit{Bregman-optimal}$,
minimizing the Bregman divergence to the true label distribution within a
suitable class of calibrated predictors. Our work is the first to provide a
calibration strategy that satisfies both calibration and optimality properties
provably in high dimensions. Additionally, we identify conditions under which a
classical Platt-scaling predictor converges to our Bregman-optimal calibrated
solution. Thus, Platt-scaling also inherits these desirable properties provably
in high dimensions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SWAN: SGD with Normalization and Whitening Enables Stateless LLM
  Training 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.13148v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.13148v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chao Ma, Wenbo Gong, Meyer Scetbon, Edward Meeds
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Adaptive optimizers such as Adam (Kingma & Ba, 2015) have been central to the
success of large language models. However, they often require to maintain
optimizer states throughout training, which can result in memory requirements
several times greater than the model footprint. This overhead imposes
constraints on scalability and computational efficiency. Stochastic Gradient
Descent (SGD), in contrast, is a stateless optimizer, as it does not track
state variables during training. Consequently, it achieves optimal memory
efficiency. However, its capability in LLM training is limited (Zhao et al.,
2024b). In this work, we show that pre-processing SGD in a stateless manner can
achieve the same performance as the Adam optimizer for LLM training, while
drastically reducing the memory cost. Specifically, we propose to pre-process
the instantaneous stochastic gradients using normalization and whitening. We
show that normalization stabilizes gradient distributions, and whitening
counteracts the local curvature of the loss landscape. This results in SWAN
(SGD with Whitening And Normalization), a stochastic optimizer that eliminates
the need to store any optimizer states. Empirically, SWAN has the same memory
footprint as SGD, achieving $\approx 50\%$ reduction on total end-to-end memory
compared to Adam. In language modeling tasks, SWAN demonstrates comparable or
even better performance than Adam: when pre-training the LLaMA model with 350M
and 1.3B parameters, SWAN achieves a 2x speedup by reaching the same evaluation
perplexity using half as many tokens.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>In v2 we have revised the related work, added more comprehensive
  citations, and clarified our key contributions</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Packet Inspection <span class="highlight-title">Transformer</span>: A <span class="highlight-title">Self-Supervised</span> Journey to Unseen
  Malware Detection with Few Samples 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.18219v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.18219v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kyle Stein, Arash Mahyari, Guillermo Francia III, Eman El-Sheikh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As networks continue to expand and become more interconnected, the need for
novel malware detection methods becomes more pronounced. Traditional security
measures are increasingly inadequate against the sophistication of modern cyber
attacks. Deep Packet Inspection (DPI) has been pivotal in enhancing network
security, offering an in-depth analysis of network traffic that surpasses
conventional monitoring techniques. DPI not only examines the metadata of
network packets, but also dives into the actual content being carried within
the packet payloads, providing a comprehensive view of the data flowing through
networks. While the integration of advanced deep learning techniques with DPI
has introduced modern methodologies into malware detection and network traffic
classification, state-of-the-art supervised learning approaches are limited by
their reliance on large amounts of annotated data and their inability to
generalize to novel, unseen malware threats. To address these limitations, this
paper leverages the recent advancements in self-supervised learning (SSL) and
few-shot learning (FSL). Our proposed self-supervised approach trains a
transformer via SSL to learn the embedding of packet content, including
payload, from vast amounts of unlabeled data by masking portions of packets,
leading to a learned representation that generalizes to various downstream
tasks. Once the representation is extracted from the packets, they are used to
train a malware detection algorithm. The representation obtained from the
transformer is then used to adapt the malware detector to novel types of
attacks using few-shot learning approaches. Our experimental results
demonstrate that our method achieves classification accuracies of up to 94.76%
on the UNSW-NB15 dataset and 83.25% on the CIC-IoT23 dataset.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SWEPO: Simultaneous Weighted Preference Optimization for Group
  Contrastive Alignment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.04628v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.04628v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Taneesh Gupta, Rahul Madhavan, Xuchao Zhang, Chetan Bansal, Saravan Rajmohan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Direct Preference Optimization (DPO) has proven effective in aligning large
language models with human preferences but is often constrained to pairwise
comparisons -- overlooking additional positive and negative responses that are
commonly available in real-world settings. We propose Simultaneous Weighted
Preference Optimization (SWEPO), which incorporates multiple responses per
query and prioritizes those that deviate most from the average reward. This
deviation-based weighting focuses training on the most informative outliers,
akin to a built-in curriculum. Theoretically, we prove that such
multi-preference sampling lowers alignment bias, bounding the expected
deviation from the true acceptable-response distribution at a rate of
$\mathcal{O}(\tfrac{1}{\sqrt{k}})$. Empirically, SWEPO outperforms
state-of-the-art baselines on the Ultra-Feedback dataset and demonstrates
substantial improvements over DPO and InfoNCA, yielding boosts of up to $\sim
4$% on length-controlled win-rate on AlpacaEval.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SpinSVAR: Estimating Structural Vector Autoregression Assuming Sparse
  Input 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.03130v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.03130v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Panagiotis Misiakos, Markus Püschel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce SpinSVAR, a novel method for estimating a structural vector
autoregression (SVAR) from time-series data under sparse input assumption.
Unlike prior approaches using Gaussian noise, we model the input as independent
Laplacian variables, enforcing sparsity and yielding a maximum likelihood
estimator (MLE) based on least absolute error regression. We provide
theoretical consistency guarantees for the MLE under mild assumptions. SpinSVAR
is efficient: it can leverage GPU acceleration to scale to thousands of nodes.
On synthetic data with Laplacian or Bernoulli-uniform inputs, SpinSVAR
outperforms state-of-the-art methods in accuracy and runtime. When applied to
S&P 500 data, it clusters stocks by sectors and identifies significant
structural shocks linked to major price movements, demonstrating the viability
of our sparse input assumption.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>38 pages, 11 figures, conference preprint</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Towards Foundation Models for Mixed Integer Linear Programming 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.08288v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.08288v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sirui Li, Janardhan Kulkarni, Ishai Menache, Cathy Wu, Beibin Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Mixed Integer Linear Programming (MILP) is essential for modeling complex
decision-making problems but faces challenges in computational tractability and
requires expert formulation. Current deep learning approaches for MILP focus on
specific problem classes and do not generalize to unseen classes. To address
this shortcoming, we take a foundation model training approach, where we train
a single deep learning model on a diverse set of MILP problems to generalize
across problem classes. As existing datasets for MILP lack diversity and
volume, we introduce MILP-Evolve, a novel LLM-based evolutionary framework that
is capable of generating a large set of diverse MILP classes with an unlimited
amount of instances. We study our methodology on three key learning tasks that
capture diverse aspects of MILP: (1) integrality gap prediction, (2) learning
to branch, and (3) a new task of aligning MILP instances with natural language
descriptions. Our empirical results show that models trained on the data
generated by MILP-Evolve achieve significant improvements on unseen problems,
including MIPLIB benchmarks. Our work highlights the potential of moving
towards a foundation model approach for MILP that can generalize to a broad
range of MILP applications. Our code and data are publicly available at
https://github.com/microsoft/OptiGuide.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Vibravox: A <span class="highlight-title">Dataset</span> of French Speech Captured with Body-conduction Audio
  Sensors 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.11828v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.11828v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Julien Hauret, Malo Olivier, Thomas Joubaud, Christophe Langrenne, Sarah Poirée, Véronique Zimpfer, Éric Bavu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vibravox is a dataset compliant with the General Data Protection Regulation
(GDPR) containing audio recordings using five different body-conduction audio
sensors : two in-ear microphones, two bone conduction vibration pickups and a
laryngophone. The dataset also includes audio data from an airborne microphone
used as a reference. The Vibravox corpus contains 45 hours of speech samples
and physiological sounds recorded by 188 participants under different acoustic
conditions imposed by an high order ambisonics 3D spatializer. Annotations
about the recording conditions and linguistic transcriptions are also included
in the corpus. We conducted a series of experiments on various speech-related
tasks, including speech recognition, speech enhancement and speaker
verification. These experiments were carried out using state-of-the-art models
to evaluate and compare their performances on signals captured by the different
audio sensors offered by the Vibravox dataset, with the aim of gaining a better
grasp of their individual characteristics.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>23 pages, 42 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Securing Healthcare with Deep Learning: A CNN-Based Model for medical
  IoT Threat Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.23306v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.23306v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alireza Mohamadi, Hosna Ghahramani, Seyyed Amir Asghari, Mehdi Aminian
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The increasing integration of the Internet of Medical Things (IoMT) into
healthcare systems has significantly enhanced patient care but has also
introduced critical cybersecurity challenges. This paper presents a novel
approach based on Convolutional Neural Networks (CNNs) for detecting
cyberattacks within IoMT environments. Unlike previous studies that
predominantly utilized traditional machine learning (ML) models or simpler Deep
Neural Networks (DNNs), the proposed model leverages the capabilities of CNNs
to effectively analyze the temporal characteristics of network traffic data.
Trained and evaluated on the CICIoMT2024 dataset, which comprises 18 distinct
types of cyberattacks across a range of IoMT devices, the proposed CNN model
demonstrates superior performance compared to previous state-of-the-art
methods, achieving a perfect accuracy of 99% in binary, categorical, and
multiclass classification tasks. This performance surpasses that of
conventional ML models such as Logistic Regression, AdaBoost, DNNs, and Random
Forests. These findings highlight the potential of CNNs to substantially
improve IoMT cybersecurity, thereby ensuring the protection and integrity of
connected healthcare systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The final published version is available in IEEE Xplore:
  https://doi.org/10.1109/ICIS64839.2024.10887510</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Exact Risk Curves of signSGD in High-Dimensions: Quantifying
  Preconditioning and Noise-Compression Effects 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.12135v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.12135v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ke Liang Xiao, Noah Marshall, Atish Agarwala, Elliot Paquette
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, signSGD has garnered interest as both a practical optimizer
as well as a simple model to understand adaptive optimizers like Adam. Though
there is a general consensus that signSGD acts to precondition optimization and
reshapes noise, quantitatively understanding these effects in theoretically
solvable settings remains difficult. We present an analysis of signSGD in a
high dimensional limit, and derive a limiting SDE and ODE to describe the risk.
Using this framework we quantify four effects of signSGD: effective learning
rate, noise compression, diagonal preconditioning, and gradient noise
reshaping. Our analysis is consistent with experimental observations but moves
beyond that by quantifying the dependence of these effects on the data and
noise distributions. We conclude with a conjecture on how these results might
be extended to Adam.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Refined climatologies of future precipitation over High Mountain Asia
  using probabilistic ensemble learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.15690v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.15690v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kenza Tazi, Sun Woo P. Kim, Marc Girona-Mata, Richard E. Turner
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  High Mountain Asia holds the largest concentration of frozen water outside
the polar regions, serving as a crucial water source for more than 1.9 billion
people. In the face of climate change, precipitation represents the largest
source of uncertainty for hydrological modelling in this area. Future
precipitation predictions remain challenging due to complex orography, lack of
in situ hydrological observations, and limitations in climate model resolution
and parametrisation for this region. To address the uncertainty posed by these
challenges, climate models are often aggregated into multi-model ensembles.
While multi-model ensembles are known to improve the predictive accuracy and
analysis of future climate projections, consensus regarding how models are
aggregated is lacking. In this study, we propose a probabilistic machine
learning framework to combine 13 regional climate models from the Coordinated
Regional Downscaling Experiment (CORDEX) over High Mountain Asia. Our approach
accounts for seasonal and spatial biases within the models, enabling the
prediction of more faithful precipitation distributions. The framework is
validated against gridded historical precipitation data and is used to generate
projections for the near future (2036$\unicode{x2013}$2065) and far future
(2066$\unicode{x2013}$2095) under RCP4.5 and RCP8.5 scenarios.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages 8 figures (main text), 32 pages 14 figures (total)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Tuning the Frequencies: Robust Training for Sinusoidal Neural Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.21121v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.21121v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tiago Novello, Diana Aldana, Andre Araujo, Luiz Velho
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Sinusoidal neural networks have been shown effective as implicit neural
representations (INRs) of low-dimensional signals, due to their smoothness and
high representation capacity. However, initializing and training them remain
empirical tasks which lack on deeper understanding to guide the learning
process. To fill this gap, our work introduces a theoretical framework that
explains the capacity property of sinusoidal networks and offers robust control
mechanisms for initialization and training. Our analysis is based on a novel
amplitude-phase expansion of the sinusoidal multilayer perceptron, showing how
its layer compositions produce a large number of new frequencies expressed as
integer combinations of the input frequencies. This relationship can be
directly used to initialize the input neurons, as a form of spectral sampling,
and to bound the network's spectrum while training. Our method, referred to as
TUNER (TUNing sinusoidal nEtwoRks), greatly improves the stability and
convergence of sinusoidal INR training, leading to detailed reconstructions,
while preventing overfitting.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ From Priest to Doctor: Domain Adaptation for Low-Resource Neural Machine
  Translation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.00966v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.00966v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ali Marashian, Enora Rice, Luke Gessler, Alexis Palmer, Katharina von der Wense
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Many of the world's languages have insufficient data to train high-performing
general neural machine translation (NMT) models, let alone domain-specific
models, and often the only available parallel data are small amounts of
religious texts. Hence, domain adaptation (DA) is a crucial issue faced by
contemporary NMT and has, so far, been underexplored for low-resource
languages. In this paper, we evaluate a set of methods from both low-resource
NMT and DA in a realistic setting, in which we aim to translate between a
high-resource and a low-resource language with access to only: a) parallel
Bible data, b) a bilingual dictionary, and c) a monolingual target-domain
corpus in the high-resource language. Our results show that the effectiveness
of the tested methods varies, with the simplest one, DALI, being most
effective. We follow up with a small human evaluation of DALI, which shows that
there is still a need for more careful investigation of how to accomplish DA
for low-resource NMT.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A graph neural network-based model with Out-of-Distribution Robustness
  for enhancing Antiretroviral Therapy Outcome Prediction for HIV-1 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.17506v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.17506v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Giulia Di Teodoro, Federico Siciliano, Valerio Guarrasi, Anne-Mieke Vandamme, Valeria Ghisetti, Anders Sönnerborg, Maurizio Zazzi, Fabrizio Silvestri, Laura Palagi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Predicting the outcome of antiretroviral therapies (ART) for HIV-1 is a
pressing clinical challenge, especially when the ART includes drugs with
limited effectiveness data. This scarcity of data can arise either due to the
introduction of a new drug to the market or due to limited use in clinical
settings, resulting in clinical dataset with highly unbalanced therapy
representation. To tackle this issue, we introduce a novel joint fusion model,
which combines features from a Fully Connected (FC) Neural Network and a Graph
Neural Network (GNN) in a multi-modality fashion. Our model uses both tabular
data about genetic sequences and a knowledge base derived from Stanford
drug-resistance mutation tables, which serve as benchmark references for
deducing in-vivo treatment efficacy based on the viral genetic sequence. By
leveraging this knowledge base structured as a graph, the GNN component enables
our model to adapt to imbalanced data distributions and account for
Out-of-Distribution (OoD) drugs. We evaluated these models' robustness against
OoD drugs in the test set. Our comprehensive analysis demonstrates that the
proposed model consistently outperforms the FC model. These results underscore
the advantage of integrating Stanford scores in the model, thereby enhancing
its generalizability and robustness, but also extending its utility in
contributing in more informed clinical decisions with limited data
availability. The source code is available at
https://github.com/federicosiciliano/graph-ood-hiv
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>32 pages, 2 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DistRL: An Asynchronous Distributed Reinforcement Learning Framework for
  On-Device Control Agents 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14803v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14803v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Taiyi Wang, Zhihao Wu, Jianheng Liu, Jianye Hao, Jun Wang, Kun Shao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  On-device control agents, especially on mobile devices, are responsible for
operating mobile devices to fulfill users' requests, enabling seamless and
intuitive interactions. Integrating Multimodal Large Language Models (MLLMs)
into these agents enhances their ability to understand and execute complex
commands, thereby improving user experience. However, fine-tuning MLLMs for
on-device control presents significant challenges due to limited data
availability and inefficient online training processes. This paper introduces
DistRL, a novel framework designed to enhance the efficiency of online RL
fine-tuning for mobile device control agents. DistRL employs centralized
training and decentralized data acquisition to ensure efficient fine-tuning in
the context of dynamic online interactions. Additionally, the framework is
backed by our tailor-made RL algorithm, which effectively balances exploration
with the prioritized utilization of collected data to ensure stable and robust
training. Our experiments show that, on average, DistRL delivers a 3X
improvement in training efficiency and enables training data collection 2.4X
faster than the leading synchronous multi-machine methods. Notably, after
training, DistRL achieves a 20% relative improvement in success rate compared
to state-of-the-art methods on general Android tasks from an open benchmark,
significantly outperforming existing approaches while maintaining the same
training time. These results validate DistRL as a scalable and efficient
solution, offering substantial improvements in both training efficiency and
agent performance for real-world, in-the-wild device control tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Paper and Appendix, 26 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ NEAT: Nonlinear Parameter-efficient Adaptation of <span class="highlight-title">Pre-train</span>ed Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01870v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01870v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yibo Zhong, Haoxiang Jiang, Lincan Li, Ryumei Nakada, Tianci Liu, Linjun Zhang, Huaxiu Yao, Haoyu Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Fine-tuning pre-trained models often yields state-of-the-art performance but
is computationally expensive when updating all parameters. Parameter-efficient
fine-tuning (PEFT) methods, such as Low-Rank Adaptation (LoRA), address this by
freezing pre-trained weights and introducing low-rank matrices. However,
because LoRA relies on low-rank decomposition, it struggles to capture complex
nonlinear dynamics and optimal optimization trajectories, resulting in a
performance gap relative to full fine-tuning and inefficient parameter
utilization. To overcome these issues, we propose NEAT, a nonlinear PEFT
approach that employs a lightweight neural network to learn a nonlinear
transformation of the pre-trained weights, thereby better approximating
cumulative weight updates. Our theoretical analysis shows that NEAT achieves
greater efficiency than LoRA while maintaining equivalent expressivity.
Extensive experiments on four benchmarks and over twenty datasets demonstrate
that NEAT significantly outperforms state-of-the-art baselines in both vision
and text tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Are Neuromorphic Architectures Inherently Privacy-preserving? An
  Exploratory Study 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.06613v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.06613v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ayana Moshruba, Ihsen Alouani, Maryam Parsa
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While machine learning (ML) models are becoming mainstream, especially in
sensitive application areas, the risk of data leakage has become a growing
concern. Attacks like membership inference (MIA) have shown that trained models
can reveal sensitive data, jeopardizing confidentiality. While traditional
Artificial Neural Networks (ANNs) dominate ML applications, neuromorphic
architectures, specifically Spiking Neural Networks (SNNs), are emerging as
promising alternatives due to their low power consumption and event-driven
processing, akin to biological neurons. Privacy in ANNs is well-studied;
however, little work has explored the privacy-preserving properties of SNNs.
This paper examines whether SNNs inherently offer better privacy. Using MIAs,
we assess the privacy resilience of SNNs versus ANNs across diverse datasets.
We analyze the impact of learning algorithms (surrogate gradient and
evolutionary), frameworks (snnTorch, TENNLab, LAVA), and parameters on SNN
privacy. Our findings show that SNNs consistently outperform ANNs in privacy
preservation, with evolutionary algorithms offering additional resilience. For
instance, on CIFAR-10, SNNs achieve an AUC of 0.59, significantly lower than
ANNs' 0.82, and on CIFAR-100, SNNs maintain an AUC of 0.58 compared to ANNs'
0.88. Additionally, we explore the privacy-utility trade-off with
Differentially Private Stochastic Gradient Descent (DPSGD), finding that SNNs
sustain less accuracy loss than ANNs under similar privacy constraints.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Tensorization of neural networks for improved privacy and
  interpretability 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.06300v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.06300v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        José Ramón Pareja Monturiol, Alejandro Pozas-Kerstjens, David Pérez-García
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a tensorization algorithm for constructing tensor train
representations of functions, drawing on sketching and cross interpolation
ideas. The method only requires black-box access to the target function and a
small set of sample points defining the domain of interest. Thus, it is
particularly well-suited for machine learning models, where the domain of
interest is naturally defined by the training dataset. We show that this
approach can be used to enhance the privacy and interpretability of neural
network models. Specifically, we apply our decomposition to (i) obfuscate
neural networks whose parameters encode patterns tied to the training data
distribution, and (ii) estimate topological phases of matter that are easily
accessible from the tensor train representation. Additionally, we show that
this tensorization can serve as an efficient initialization method for
optimizing tensor trains in general settings, and that, for model compression,
our algorithm achieves a superior trade-off between memory and time complexity
compared to conventional tensorization methods of neural networks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>40 pages, 9 figures. The code for the experiments is publicly
  available at https://github.com/joserapa98/tensorization-nns. V2: Added code
  repository</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ The Observational Partial Order of Causal Structures with Latent
  Variables 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.07891v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.07891v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Marina Maciel Ansanelli, Elie Wolfe, Robert W. Spekkens
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  For two causal structures with the same set of visible variables, one is said
to observationally dominate the other if the set of distributions over the
visible variables realizable by the first contains the set of distributions
over the visible variables realizable by the second. Knowing such dominance
relations is useful for adjudicating between these structures given
observational data. We here consider the problem of determining the partial
order of equivalence classes of causal structures with latent variables
relative to observational dominance. We provide a complete characterization of
the dominance order in the case of three visible variables, and a partial
characterization in the case of four visible variables. Our techniques also
help to identify which observational equivalence classes have a set of
realizable distributions that is characterized by nontrivial inequality
constraints, analogous to Bell inequalities and instrumental inequalities. We
find evidence that as one increases the number of visible variables, the
equivalence classes satisfying nontrivial inequality constraints become
ubiquitous. (Because such classes are the ones for which there can be a
difference in the distributions that are quantumly and classically realizable,
this implies that the potential for quantum-classical gaps is also ubiquitous.)
Furthermore, we find evidence that constraint-based causal discovery algorithms
that rely solely on conditional independence constraints have a significantly
weaker distinguishing power among observational equivalence classes than
algorithms that go beyond these (i.e., algorithms that also leverage nested
Markov constraints and inequality constraints).
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>48 pages, 30 figures; acknowledgements added</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Fully automatic extraction of morphological traits from the Web: utopia
  or reality? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.17179v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.17179v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Diego Marcos, Robert van de Vlasakker, Ioannis N. Athanasiadis, Pierre Bonnet, Hervé Goeau, Alexis Joly, W. Daniel Kissling, César Leblanc, André S. J. van Proosdij, Konstantinos P. Panousis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Plant morphological traits, their observable characteristics, are fundamental
to understand the role played by each species within their ecosystem. However,
compiling trait information for even a moderate number of species is a
demanding task that may take experts years to accomplish. At the same time,
massive amounts of information about species descriptions is available online
in the form of text, although the lack of structure makes this source of data
impossible to use at scale. To overcome this, we propose to leverage recent
advances in large language models (LLMs) and devise a mechanism for gathering
and processing information on plant traits in the form of unstructured textual
descriptions, without manual curation. We evaluate our approach by
automatically replicating three manually created species-trait matrices. Our
method managed to find values for over half of all species-trait pairs, with an
F1-score of over 75%. Our results suggest that large-scale creation of
structured trait databases from unstructured online text is currently feasible
thanks to the information extraction capabilities of LLMs, being limited by the
availability of textual descriptions covering all the traits of interest.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Optimizing Estimators of Squared Calibration Errors in Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.07014v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.07014v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sebastian G. Gruber, Francis Bach
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work, we propose a mean-squared error-based risk that enables the
comparison and optimization of estimators of squared calibration errors in
practical settings. Improving the calibration of classifiers is crucial for
enhancing the trustworthiness and interpretability of machine learning models,
especially in sensitive decision-making scenarios. Although various calibration
(error) estimators exist in the current literature, there is a lack of guidance
on selecting the appropriate estimator and tuning its hyperparameters. By
leveraging the bilinear structure of squared calibration errors, we reformulate
calibration estimation as a regression problem with independent and identically
distributed (i.i.d.) input pairs. This reformulation allows us to quantify the
performance of different estimators even for the most challenging calibration
criterion, known as canonical calibration. Our approach advocates for a
training-validation-testing pipeline when estimating a calibration error on an
evaluation dataset. We demonstrate the effectiveness of our pipeline by
optimizing existing calibration estimators and comparing them with novel kernel
ridge regression-based estimators on standard image classification tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published at TMLR, see https://openreview.net/forum?id=BPDVZajOW5</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Quantifying the Capability Boundary of DeepSeek Models: An
  Application-Driven Performance Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11164v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11164v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kaikai Zhao, Zhaoxiang Liu, Xuejiao Lei, Ning Wang, Jiaojiao Zhao, Zipeng Wang, Zhenhong Long, Peijun Yang, Minjie Hua, Chaoyang Ma, Wen Liu, Kai Wang, Shiguo Lian
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  DeepSeek-R1, known for its low training cost and exceptional reasoning
capabilities, has achieved state-of-the-art performance on various benchmarks.
However, detailed evaluations from the perspective of real-world applications
are lacking, making it challenging for users to select the most suitable
DeepSeek models for their specific needs. To address this gap, we evaluate the
DeepSeek-V3, DeepSeek-R1, DeepSeek-R1-Distill-Qwen series, and
DeepSeek-R1-Distill-Llama series on the improved version A-Eval (A-Eval-2.0),
an application-driven benchmark. By comparing original instruction-tuned models
with their distilled counterparts, we analyze how reasoning enhancements impact
performance across diverse practical tasks. Our results show that
reasoning-enhanced models, while generally powerful, do not universally
outperform across all tasks, with performance gains varying significantly
across tasks and models. To further assist users in model selection, we
quantify the capability boundary of DeepSeek models through performance tier
classifications and intuitive line charts. Specific examples provide actionable
insights to help users select and deploy the most cost-effective DeepSeek
models, ensuring optimal performance and resource efficiency in real-world
applications. It should be noted that, despite our efforts to establish a
comprehensive, objective, and authoritative evaluation benchmark, the selection
of test samples, characteristics of data distribution, and the setting of
evaluation criteria may inevitably introduce certain biases into the evaluation
results. We will continuously optimize the evaluation benchmarks and
periodically update this paper to provide more comprehensive and accurate
evaluation results. Please refer to the latest version of the paper for the
most recent results and conclusions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Harnessing omnipresent oscillator networks as computational resource 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.04818v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.04818v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Thomas Geert de Jong, Hirofumi Notsu, Kohei Nakajima
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Nature is pervaded with oscillatory behavior. In networks of coupled
oscillators patterns can arise when the system synchronizes to an external
input. Hence, these networks provide processing and memory of input. We present
a universal framework for harnessing oscillator networks as computational
resource. This reservoir computing framework is introduced by the ubiquitous
model for phase-locking, the Kuramoto model. We force the Kuramoto model by a
nonlinear target-system, then after substituting the target-system with a
trained feedback-loop it emulates the target-system. Our results are two-fold.
Firstly, the trained network inherits performance properties of the Kuramoto
model, where all-to-all coupling is performed in linear time with respect to
the number of nodes and parameters for synchronization are abundant. Secondly,
the learning capabilities of the oscillator network can be explained using
Kuramoto model's order parameter. This work provides the foundation for
utilizing nature's oscillator networks as a new class of information processing
systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Is Free Self-Alignment Possible? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.03642v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.03642v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dyah Adila, Changho Shin, Yijing Zhang, Frederic Sala
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Aligning pretrained language models (LMs) often requires large-scale
preference data and substantial computational resources. These costs become
even more prohibitive for multi-objective or pluralistic alignment. Is this
truly necessary? Can we perform efficient alignment using only internal model
capabilities, and without additional training? To answer this question, we
propose AlignEZ, a novel approach that leverages (1) self-generated preference
data and (2) representation editing to achieve cost-effective, efficient
alignment. By operating directly on learned representations, AlignEZ
independently targets different behavioral aspects without the overhead of
traditional alignment methods. Our experiments reveal that this cost-efficient
procedure improves performance across diverse tasks: up to 19.9% on general
alignment and 1.9% on challenging mathematical reasoning tasks, even when
starting from a strong base model. AlignEZ can also align models to multiple
objectives simultaneously, granting fine-grained control over multiple
preference axes. Finally, we show that AlignEZ can accelerate more expensive
alignment procedures--such as DPO--even under limited availability of
ground-truth preference data.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DarwinLM: Evolutionary Structured Pruning of Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.07780v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.07780v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shengkun Tang, Oliver Sieberling, Eldar Kurtic, Zhiqiang Shen, Dan Alistarh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have achieved significant success across various
NLP tasks. However, their massive computational costs limit their widespread
use, particularly in real-time applications. Structured pruning offers an
effective solution by compressing models and directly providing end-to-end
speed improvements, regardless of the hardware environment. Meanwhile,
different components of the model exhibit varying sensitivities towards
pruning, calling for \emph{non-uniform} model compression. However, a pruning
method should not only identify a capable substructure, but also account for
post-compression training. To this end, we propose \sysname, a method for
\emph{training-aware} structured pruning. \sysname builds upon an evolutionary
search process, generating multiple offspring models in each generation through
mutation, and selecting the fittest for survival. To assess the effect of
post-training, we incorporate a lightweight, multistep training process within
the offspring population, progressively increasing the number of tokens and
eliminating poorly performing models in each selection stage. We validate our
method through extensive experiments on Llama-2-7B, Llama-3.1-8B and
Qwen-2.5-14B-Instruct, achieving state-of-the-art performance for structured
pruning. For instance, \sysname surpasses ShearedLlama while requiring
$5\times$ less training data during post-compression training. Code is at:
https://github.com/IST-DASLab/DarwinLM
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code: https://github.com/IST-DASLab/DarwinLM</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ An Adversarial Analysis of Thompson Sampling for Full-information Online
  Learning: from Finite to Infinite Action Spaces 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.14790v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.14790v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alexander Terenin, Jeffrey Negrea
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We develop an analysis of Thompson sampling for online learning under full
feedback - also known as prediction with expert advice - where the learner's
prior is defined over the space of an adversary's future actions, rather than
the space of experts. We show regret decomposes into regret the learner
expected a priori, plus a prior-robustness-type term we call excess regret. In
the classical finite-expert setting, this recovers optimal rates. As an initial
step towards practical online learning in settings with a
potentially-uncountably-infinite number of experts, we show that Thompson
sampling with a certain Gaussian process prior widely-used in the Bayesian
optimization literature has a $\mathcal{O}(\beta\sqrt{T\log(1+\lambda)})$ rate
against a $\beta$-bounded $\lambda$-Lipschitz adversary.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Stein Discrepancy for Unsupervised Domain Adaptation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.03587v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.03587v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anneke von Seeger, Dongmian Zou, Gilad Lerman
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Unsupervised domain adaptation (UDA) leverages information from a labeled
source dataset to improve accuracy on a related but unlabeled target dataset. A
common approach to UDA is aligning representations from the source and target
domains by minimizing the distance between their data distributions. Previous
methods have employed distances such as Wasserstein distance and maximum mean
discrepancy. However, these approaches are less effective when the target data
is significantly scarcer than the source data. Stein discrepancy is an
asymmetric distance between distributions that relies on one distribution only
through its score function. In this paper, we propose a novel UDA method that
uses Stein discrepancy to measure the distance between source and target
domains. We develop a learning framework using both non-kernelized and
kernelized Stein discrepancy. Theoretically, we derive an upper bound for the
generalization error. Numerical experiments show that our method outperforms
existing methods using other domain discrepancy measures when only small
amounts of target data are available.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>24 pages, 9 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A View of the Certainty-Equivalence Method for PAC RL as an Application
  of the Trajectory Tree Method 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.02652v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.02652v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shivaram Kalyanakrishnan, Sheel Shah, Santhosh Kumar Guguloth
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reinforcement learning (RL) enables an agent interacting with an unknown MDP
$M$ to optimise its behaviour by observing transitions sampled from $M$. A
natural entity that emerges in the agent's reasoning is $\widehat{M}$, the
maximum likelihood estimate of $M$ based on the observed transitions. The
well-known \textit{certainty-equivalence} method (CEM) dictates that the agent
update its behaviour to $\widehat{\pi}$, which is an optimal policy for
$\widehat{M}$. Not only is CEM intuitive, it has been shown to enjoy
minimax-optimal sample complexity in some regions of the parameter space for
PAC RL with a generative model~\citep{Agarwal2020GenModel}.
  A seemingly unrelated algorithm is the ``trajectory tree method''
(TTM)~\citep{Kearns+MN:1999}, originally developed for efficient decision-time
planning in large POMDPs. This paper presents a theoretical investigation that
stems from the surprising finding that CEM may indeed be viewed as an
application of TTM. The qualitative benefits of this view are (1) new and
simple proofs of sample complexity upper bounds for CEM, in fact under a (2)
weaker assumption on the rewards than is prevalent in the current literature.
Our analysis applies to both non-stationary and stationary MDPs.
Quantitatively, we obtain (3) improvements in the sample-complexity upper
bounds for CEM both for non-stationary and stationary MDPs, in the regime that
the ``mistake probability'' $\delta$ is small. Additionally, we show (4) a
lower bound on the sample complexity for finite-horizon MDPs, which establishes
the minimax-optimality of our upper bound for non-stationary MDPs in the
small-$\delta$ regime.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages, excluding references and appendices. Total of 29 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Generalization Bounds via Meta-Learned Model Representations: PAC-Bayes
  and Sample Compression Hypernetworks <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13577v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13577v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Benjamin Leblanc, Mathieu Bazinet, Nathaniel D'Amours, Alexandre Drouin, Pascal Germain
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  PAC-Bayesian and Sample Compress learning frameworks are instrumental for
deriving tight (non-vacuous) generalization bounds for neural networks. We
leverage these results in a meta-learning scheme, relying on a hypernetwork
that outputs the parameters of a downstream predictor from a dataset input. The
originality of our approach lies in the investigated hypernetwork architectures
that encode the dataset before decoding the parameters: (1) a PAC-Bayesian
encoder that expresses a posterior distribution over a latent space, (2) a
Sample Compress encoder that selects a small sample of the dataset input along
with a message from a discrete set, and (3) a hybrid between both approaches
motivated by a new Sample Compress theorem handling continuous messages. The
latter theorem exploits the pivotal information transiting at the
encoder-decoder junction to compute generalization guarantees for each
downstream predictor obtained by our meta-learning scheme.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at the NeurIPS 2024 workshop on Compression in Machine
  Learning</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Adversarial Alignment for LLMs Requires Simpler, Reproducible, and More
  Measurable Objectives 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11910v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11910v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Leo Schwinn, Yan Scholten, Tom Wollschläger, Sophie Xhonneux, Stephen Casper, Stephan Günnemann, Gauthier Gidel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Misaligned research objectives have considerably hindered progress in
adversarial robustness research over the past decade. For instance, an
extensive focus on optimizing target metrics, while neglecting rigorous
standardized evaluation, has led researchers to pursue ad-hoc heuristic
defenses that were seemingly effective. Yet, most of these were exposed as
flawed by subsequent evaluations, ultimately contributing little measurable
progress to the field. In this position paper, we illustrate that current
research on the robustness of large language models (LLMs) risks repeating past
patterns with potentially worsened real-world implications. To address this, we
argue that realigned objectives are necessary for meaningful progress in
adversarial alignment. To this end, we build on established cybersecurity
taxonomy to formally define differences between past and emerging threat models
that apply to LLMs. Using this framework, we illustrate that progress requires
disentangling adversarial alignment into addressable sub-problems and returning
to core academic principles, such as measureability, reproducibility, and
comparability. Although the field presents significant challenges, the fresh
start on adversarial robustness offers the unique opportunity to build on past
experience while avoiding previous mistakes.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Is Q-learning an Ill-posed Problem? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.14365v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.14365v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Philipp Wissmann, Daniel Hein, Steffen Udluft, Thomas Runkler
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper investigates the instability of Q-learning in continuous
environments, a challenge frequently encountered by practitioners.
Traditionally, this instability is attributed to bootstrapping and regression
model errors. Using a representative reinforcement learning benchmark, we
systematically examine the effects of bootstrapping and model inaccuracies by
incrementally eliminating these potential error sources. Our findings reveal
that even in relatively simple benchmarks, the fundamental task of Q-learning -
iteratively learning a Q-function from policy-specific target values - can be
inherently ill-posed and prone to failure. These insights cast doubt on the
reliability of Q-learning as a universal solution for reinforcement learning
problems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at ESANN 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Graph Neural Diffusion Networks for Semi-supervised Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2201.09698v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2201.09698v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wei Ye, Zexi Huang, Yunqi Hong, Ambuj Singh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Graph Convolutional Networks (GCN) is a pioneering model for graph-based
semi-supervised learning. However, GCN does not perform well on
sparsely-labeled graphs. Its two-layer version cannot effectively propagate the
label information to the whole graph structure (i.e., the under-smoothing
problem) while its deep version over-smoothens and is hard to train (i.e., the
over-smoothing problem). To solve these two issues, we propose a new graph
neural network called GND-Nets (for Graph Neural Diffusion Networks) that
exploits the local and global neighborhood information of a vertex in a single
layer. Exploiting the shallow network mitigates the over-smoothing problem
while exploiting the local and global neighborhood information mitigates the
under-smoothing problem. The utilization of the local and global neighborhood
information of a vertex is achieved by a new graph diffusion method called
neural diffusions, which integrate neural networks into the conventional linear
and nonlinear graph diffusions. The adoption of neural networks makes neural
diffusions adaptable to different datasets. Extensive experiments on various
sparsely-labeled graphs verify the effectiveness and efficiency of GND-Nets
compared to state-of-the-art approaches.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ R-MTLLMF: Resilient Multi-Task Large Language Model Fusion at the
  Wireless Edge 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.18220v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.18220v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aladin Djuhera, Vlad C. Andrei, Mohsen Pourghasemian, Haris Gacanin, Holger Boche, Walid Saad
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-task large language models (MTLLMs) are important for many applications
at the wireless edge, where users demand specialized models to handle multiple
tasks efficiently. However, training MTLLMs is complex and exhaustive,
particularly when tasks are subject to change. Recently, the concept of model
fusion via task vectors has emerged as an efficient approach for combining
fine-tuning parameters to produce an MTLLM. In this paper, the problem of
enabling edge users to collaboratively craft such MTLMs via tasks vectors is
studied, under the assumption of worst-case adversarial attacks. To this end,
first the influence of adversarial noise to multi-task model fusion is
investigated and a relationship between the so-called weight disentanglement
error and the mean squared error (MSE) is derived. Using hypothesis testing, it
is directly shown that the MSE increases interference between task vectors,
thereby rendering model fusion ineffective. Then, a novel resilient MTLLM
fusion (R-MTLLMF) is proposed, which leverages insights about the LLM
architecture and fine-tuning process to safeguard task vector aggregation under
adversarial noise by realigning the MTLLM. The proposed R-MTLLMF is then
compared for both worst-case and ideal transmission scenarios to study the
impact of the wireless channel. Extensive model fusion experiments with vision
LLMs demonstrate R-MTLLMF's effectiveness, achieving close-to-baseline
performance across eight different tasks in ideal noise scenarios and
significantly outperforming unprotected model fusion in worst-case scenarios.
The results further advocate for additional physical layer protection for a
holistic approach to resilience, from both a wireless and LLM perspective.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Adaptive Retrieval Without Self-Knowledge? Bringing Uncertainty Back
  Home 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.12835v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.12835v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Viktor Moskvoretskii, Maria Lysyuk, Mikhail Salnikov, Nikolay Ivanov, Sergey Pletenev, Daria Galimzianova, Nikita Krayko, Vasily Konovalov, Irina Nikishina, Alexander Panchenko
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Retrieval Augmented Generation (RAG) improves correctness of Question
Answering (QA) and addresses hallucinations in Large Language Models (LLMs),
yet greatly increase computational costs. Besides, RAG is not always needed as
may introduce irrelevant information. Recent adaptive retrieval methods
integrate LLMs' intrinsic knowledge with external information appealing to LLM
self-knowledge, but they often neglect efficiency evaluations and comparisons
with uncertainty estimation techniques. We bridge this gap by conducting a
comprehensive analysis of 35 adaptive retrieval methods, including 8 recent
approaches and 27 uncertainty estimation techniques, across 6 datasets using 10
metrics for QA performance, self-knowledge, and efficiency. Our findings show
that uncertainty estimation techniques often outperform complex pipelines in
terms of efficiency and self-knowledge, while maintaining comparable QA
performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The code and data are at https://github.com/s-nlp/AdaRAGUE</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Privacy-Enhanced Training-as-a-Service for On-Device Intelligence:
  Concept, Architectural Scheme, and Open Problems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.10255v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.10255v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhiyuan Wu, Sheng Sun, Yuwei Wang, Min Liu, Bo Gao, Tianliu He, Wen Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  On-device intelligence (ODI) enables artificial intelligence (AI)
applications to run on end devices, providing real-time and customized AI
inference without relying on remote servers. However, training models for
on-device deployment face significant challenges due to the decentralized and
privacy-sensitive nature of users' data, along with end-side constraints
related to network connectivity, computation efficiency, etc. Existing training
paradigms, such as cloud-based training, federated learning, and transfer
learning, fail to sufficiently address these practical constraints that are
prevalent for devices. To overcome these challenges, we propose
Privacy-Enhanced Training-as-a-Service (PTaaS), a novel service computing
paradigm that provides privacy-friendly, customized AI model training for end
devices. PTaaS outsources the core training process to remote and powerful
cloud or edge servers, efficiently developing customized on-device models based
on uploaded anonymous queries, enhancing data privacy while reducing the
computation load on individual devices. We explore the definition, goals, and
design principles of PTaaS, alongside emerging technologies that support the
PTaaS paradigm. An architectural scheme for PTaaS is also presented, followed
by a series of open problems that set the stage for future research directions
in the field of PTaaS.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 3 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Post-edits Are Preferences Too 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02320v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02320v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nathaniel Berger, Miriam Exel, Matthias Huck, Stefan Riezler
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Preference Optimization (PO) techniques are currently one of the state of the
art techniques for fine-tuning large language models (LLMs) on pairwise
preference feedback from human annotators. However, in machine translation,
this sort of feedback can be difficult to solicit. Additionally, Kreutzer et
al. (2018) have shown that, for machine translation, pairwise preferences are
less reliable than other forms of human feedback, such as 5-point ratings.
  We examine post-edits to see if they can be a source of reliable human
preferences by construction. In PO, a human annotator is shown sequences $s_1$
and $s_2$ and asked for a preference judgment, %$s_1 > s_2$; while for
post-editing, editors create $s_1$ and know that it should be better than
$s_2$. We attempt to use these implicit preferences for PO and show that it
helps the model move towards post-edit-like hypotheses and away from machine
translation-like hypotheses. Furthermore, we show that best results are
obtained by pre-training the model with supervised fine-tuning (SFT) on
post-edits in order to promote post-edit-like hypotheses to the top output
ranks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To appear at the Ninth Conference on Machine Translation (WMT24)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Three Mechanisms of Feature Learning in a Linear Network 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.07085v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.07085v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yizhou Xu, Liu Ziyin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Understanding the dynamics of neural networks in different width regimes is
crucial for improving their training and performance. We present an exact
solution for the learning dynamics of a one-hidden-layer linear network, with
one-dimensional data, across any finite width, uniquely exhibiting both kernel
and feature learning phases. This study marks a technical advancement by
enabling the analysis of the training trajectory from any initialization and a
detailed phase diagram under varying common hyperparameters such as width,
layer-wise learning rates, and scales of output and initialization. We identify
three novel prototype mechanisms specific to the feature learning regime: (1)
learning by alignment, (2) learning by disalignment, and (3) learning by
rescaling, which contrast starkly with the dynamics observed in the kernel
regime. Our theoretical findings are substantiated with empirical evidence
showing that these mechanisms also manifest in deep nonlinear networks handling
real-world tasks, enhancing our understanding of neural network training
dynamics and guiding the design of more effective learning strategies.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SAMG: Offline-to-Online Reinforcement Learning via
  State-Action-Conditional Offline Model Guidance 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.18626v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.18626v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Liyu Zhang, Haochi Wu, Xu Wan, Quan Kong, Ruilong Deng, Mingyang Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Offline-to-online (O2O) reinforcement learning (RL) pre-trains models on
offline data and refines policies through online fine-tuning. However, existing
O2O RL algorithms typically require maintaining the tedious offline datasets to
mitigate the effects of out-of-distribution (OOD) data, which significantly
limits their efficiency in exploiting online samples. To address this
deficiency, we introduce a new paradigm for O2O RL called
State-Action-Conditional Offline \Model Guidance (SAMG). It freezes the
pre-trained offline critic to provide compact offline understanding for each
state-action sample, thus eliminating the need for retraining on offline data.
The frozen offline critic is incorporated with the online target critic
weighted by a state-action-adaptive coefficient. This coefficient aims to
capture the offline degree of samples at the state-action level, and is updated
adaptively during training. In practice, SAMG could be easily integrated with
Q-function-based algorithms. Theoretical analysis shows good optimality and
lower estimation error. Empirically, SAMG outperforms state-of-the-art O2O RL
algorithms on the D4RL benchmark.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Towards Comparable Active Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.18356v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.18356v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Thorben Werner, Johannes Burchert, Lars Schmidt-Thieme
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Active Learning has received significant attention in the field of machine
learning for its potential in selecting the most informative samples for
labeling, thereby reducing data annotation costs. However, we show that the
reported lifts in recent literature generalize poorly to other domains leading
to an inconclusive landscape in Active Learning research. Furthermore, we
highlight overlooked problems for reproducing AL experiments that can lead to
unfair comparisons and increased variance in the results. This paper addresses
these issues by providing an Active Learning framework for a fair comparison of
algorithms across different tasks and domains, as well as a fast and performant
oracle algorithm for evaluation. To the best of our knowledge, we propose the
first AL benchmark that tests algorithms in 3 major domains: Tabular, Image,
and Text. We report empirical results for 6 widely used algorithms on 7
real-world and 2 synthetic datasets and aggregate them into a domain-specific
ranking of AL algorithms.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Deprecated version of the paper. Please use "A Cross-Domain Benchmark
  for Active Learning" (arXiv:2408.00426) instead.</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Feature Aggregation with Latent Generative Replay for Federated
  Continual Learning of Socially Appropriate Robot Behaviours 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.15773v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.15773v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nikhil Churamani, Saksham Checker, Fethiye Irmak Dogan, Hao-Tien Lewis Chiang, Hatice Gunes
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  It is critical for robots to explore Federated Learning (FL) settings where
several robots, deployed in parallel, can learn independently while also
sharing their learning with each other. This collaborative learning in
real-world environments requires social robots to adapt dynamically to changing
and unpredictable situations and varying task settings. Our work contributes to
addressing these challenges by exploring a simulated living room environment
where robots need to learn the social appropriateness of their actions. First,
we propose Federated Root (FedRoot) averaging, a novel weight aggregation
strategy which disentangles feature learning across clients from individual
task-based learning. Second, to adapt to challenging environments, we extend
FedRoot to Federated Latent Generative Replay (FedLGR), a novel Federated
Continual Learning (FCL) strategy that uses FedRoot-based weight aggregation
and embeds each client with a generator model for pseudo-rehearsal of learnt
feature embeddings to mitigate forgetting in a resource-efficient manner. Our
results show that FedRoot-based methods offer competitive performance while
also resulting in a sizeable reduction in resource consumption (up to 86% for
CPU usage and up to 72% for GPU usage). Additionally, our results demonstrate
that FedRoot-based FCL methods outperform other methods while also offering an
efficient solution (up to 84% CPU and 92% GPU usage reduction), with FedLGR
providing the best results across evaluations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 4 figures, IEEE RA-L submission</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ OptMATH: A Scalable Bidirectional Data Synthesis Framework for
  Optimization Modeling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11102v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11102v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hongliang Lu, Zhonglin Xie, Yaoyu Wu, Can Ren, Yuxuan Chen, Zaiwen Wen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite the rapid development of large language models (LLMs), a fundamental
challenge persists: the lack of high-quality optimization modeling datasets
hampers LLMs' robust modeling of practical optimization problems from natural
language descriptions (NL). This data scarcity also contributes to the
generalization difficulties experienced by learning-based methods. To address
these challenges, we propose a scalable framework for synthesizing a
high-quality dataset, named OptMATH. Starting from curated seed data with
mathematical formulations (MF), this framework automatically generates problem
data (PD) with controllable complexity. Then, a back-translation step is
employed to obtain NL. To verify the correspondence between the NL and the PD,
a forward modeling step followed by rejection sampling is used. The accepted
pairs constitute the training part of OptMATH. Then a collection of rejected
pairs is identified and further filtered. This collection serves as a new
benchmark for optimization modeling, containing difficult instances whose
lengths are much longer than these of NL4OPT and MAMO. Through extensive
experiments, we demonstrate that models of various sizes (0.5B-32B parameters)
trained on OptMATH achieve superior results on multiple modeling benchmarks,
thereby validating the effectiveness and scalability of our approach. Our
dataset is publicly available at https://github.com/AuroraLHL/OptMATH.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper has 36 pages, 18 figures, and two co-first authors:
  Hongliang Lu and Zhonglin Xie</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Integrating the Expected Future in Load Forecasts with Contextually
  Enhanced <span class="highlight-title">Transformer</span> Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.05884v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.05884v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Raffael Theiler, Leandro Von Krannichfeldt, Giovanni Sansavini, Michael F. Howland, Olga Fink
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurate and reliable energy forecasting is essential for power grid
operators who strive to minimize extreme forecasting errors that pose
significant operational challenges and incur high intra-day trading costs.
Incorporating planning information -- such as anticipated user behavior,
scheduled events or timetables -- provides substantial contextual information
to enhance forecast accuracy and reduce the occurrence of large forecasting
errors. Existing approaches, however, lack the flexibility to effectively
integrate both dynamic, forward-looking contextual inputs and historical data.
In this work, we conceptualize forecasting as a combined forecasting-regression
task, formulated as a sequence-to-sequence prediction problem, and introduce
contextually-enhanced transformer models designed to leverage all contextual
information effectively. We demonstrate the effectiveness of our approach
through a primary case study on nationwide railway energy consumption
forecasting, where integrating contextual information into transformer models,
particularly timetable data, resulted in a significant average mean absolute
error reduction of 26.6%. An auxiliary case study on building energy
forecasting, leveraging planned office occupancy data, further illustrates the
generalizability of our method, showing an average reduction of 56.3% in mean
absolute error. Compared to other state-of-the-art methods, our approach
consistently outperforms existing models, underscoring the value of
context-aware deep learning techniques in energy forecasting applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>39 pages, 8 figures and tables, journal paper</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Adaptive <span class="highlight-title">Prompt</span>: Unlocking the Power of Visual <span class="highlight-title">Prompt</span> Tuning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.18936v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.18936v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Minh Le, Anh Nguyen, Huy Nguyen, Chau Nguyen, Nhat Ho
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Visual Prompt Tuning (VPT) has recently emerged as a powerful method for
adapting pre-trained vision models to downstream tasks. By introducing
learnable prompt tokens as task-specific instructions, VPT effectively guides
pre-trained transformer models with minimal overhead. Despite its empirical
success, a comprehensive theoretical understanding of VPT remains an active
area of research. Building on recent insights into the connection between
mixture of experts and prompt-based approaches, we identify a key limitation in
VPT: the restricted functional expressiveness in prompt formulation. To address
this limitation, we propose Visual Adaptive Prompt Tuning (VAPT), a new
generation of prompts that redefines prompts as adaptive functions of the
input. Our theoretical analysis shows that this simple yet intuitive approach
achieves optimal sample efficiency. Empirical results on VTAB-1K and FGVC
further demonstrate VAPT's effectiveness, with performance gains of 7.34% and
1.04% over fully fine-tuning baselines, respectively. Notably, VAPT also
surpasses VPT by a substantial margin while using fewer parameters. These
results highlight both the effectiveness and efficiency of our method and pave
the way for future research to explore the potential of adaptive prompts. Our
code is publicly available at https://github.com/Minhchuyentoancbn/VAPT
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>57 pages, 10 figures, 18 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ IGN : Implicit Generative Networks <span class="chip">ICML</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2206.05860v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2206.05860v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haozheng Luo, Tianyi Wu, Colin Feiyu Han, Zhijun Yan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work, we build recent advances in distributional reinforcement
learning to give a state-of-art distributional variant of the model based on
the IQN. We achieve this by using the GAN model's generator and discriminator
function with the quantile regression to approximate the full quantile value
for the state-action return distribution. We demonstrate improved performance
on our baseline dataset - 57 Atari 2600 games in the ALE. Also, we use our
algorithm to show the state-of-art training performance of risk-sensitive
policies in Atari games with the policy optimization and evaluation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>2022 21st IEEE International Conference on Machine Learning and
  Applications (ICMLA)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ GraphFM: Graph Factorization Machines for Feature Interaction Modeling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2105.11866v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2105.11866v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shu Wu, Zekun Li, Yunyue Su, Zeyu Cui, Xiaoyu Zhang, Liang Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Factorization machine (FM) is a prevalent approach to modeling pairwise
(second-order) feature interactions when dealing with high-dimensional sparse
data. However, on the one hand, FM fails to capture higher-order feature
interactions suffering from combinatorial expansion. On the other hand, taking
into account interactions between every pair of features may introduce noise
and degrade prediction accuracy. To solve the problems, we propose a novel
approach, Graph Factorization Machine (GraphFM), by naturally representing
features in the graph structure. In particular, we design a mechanism to select
the beneficial feature interactions and formulate them as edges between
features. Then the proposed model, which integrates the interaction function of
FM into the feature aggregation strategy of Graph Neural Network (GNN), can
model arbitrary-order feature interactions on the graph-structured features by
stacking layers. Experimental results on several real-world datasets have
demonstrated the rationality and effectiveness of our proposed approach. The
code and data are available at
https://github.com/CRIPAC-DIG/GraphCTR}{https://github.com/CRIPAC-DIG/GraphCTR
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The code and data are available at
  https://github.com/CRIPAC-DIG/GraphCTR</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Temporal Misalignment in ANN-SNN Conversion and Its Mitigation via
  Probabilistic Spiking Neurons 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.14487v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.14487v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Velibor Bojković, Xiaofeng Wu, Bin Gu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Spiking Neural Networks (SNNs) offer a more energy-efficient alternative to
Artificial Neural Networks (ANNs) by mimicking biological neural principles,
establishing them as a promising approach to mitigate the increasing energy
demands of large-scale neural models. However, fully harnessing the
capabilities of SNNs remains challenging due to their discrete signal
processing and temporal dynamics. ANN-SNN conversion has emerged as a practical
approach, enabling SNNs to achieve competitive performance on complex machine
learning tasks. In this work, we identify a phenomenon in the ANN-SNN
conversion framework, termed temporal misalignment, in which random spike
rearrangement across SNN layers leads to performance improvements. Based on
this observation, we introduce biologically plausible two-phase probabilistic
(TPP) spiking neurons, further enhancing the conversion process. We demonstrate
the advantages of our proposed method both theoretically and empirically
through comprehensive experiments on CIFAR-10/100, CIFAR10-DVS, and ImageNet
across a variety of architectures, achieving state-of-the-art results.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Infinite-dimensional next-generation reservoir computing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.09800v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.09800v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lyudmila Grigoryeva, Hannah Lim Jing Ting, Juan-Pablo Ortega
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Next-generation reservoir computing (NG-RC) has attracted much attention due
to its excellent performance in spatio-temporal forecasting of complex systems
and its ease of implementation. This paper shows that NG-RC can be encoded as a
kernel ridge regression that makes training efficient and feasible even when
the space of chosen polynomial features is very large. Additionally, an
extension to an infinite number of covariates is possible, which makes the
methodology agnostic with respect to the lags into the past that are considered
as explanatory factors, as well as with respect to the number of polynomial
covariates, an important hyperparameter in traditional NG-RC. We show that this
approach has solid theoretical backing and good behavior based on kernel
universality properties previously established in the literature. Various
numerical illustrations show that these generalizations of NG-RC outperform the
traditional approach in several forecasting applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 2 figures, 3 tables; corrected typos, added GitHub link,
  added acknowledgments; editorial changes to improve clarity, corrected typos,
  replaced GitHub link with repository citation</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Interactive incremental learning of generalizable skills with local
  trajectory modulation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.05655v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.05655v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Markus Knauer, Alin Albu-Schäffer, Freek Stulp, João Silvério
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The problem of generalization in learning from demonstration (LfD) has
received considerable attention over the years, particularly within the context
of movement primitives, where a number of approaches have emerged. Recently,
two important approaches have gained recognition. While one leverages
via-points to adapt skills locally by modulating demonstrated trajectories,
another relies on so-called task-parameterized models that encode movements
with respect to different coordinate systems, using a product of probabilities
for generalization. While the former are well-suited to precise, local
modulations, the latter aim at generalizing over large regions of the workspace
and often involve multiple objects. Addressing the quality of generalization by
leveraging both approaches simultaneously has received little attention. In
this work, we propose an interactive imitation learning framework that
simultaneously leverages local and global modulations of trajectory
distributions. Building on the kernelized movement primitives (KMP) framework,
we introduce novel mechanisms for skill modulation from direct human corrective
feedback. Our approach particularly exploits the concept of via-points to
incrementally and interactively 1) improve the model accuracy locally, 2) add
new objects to the task during execution and 3) extend the skill into regions
where demonstrations were not provided. We evaluate our method on a bearing
ring-loading task using a torque-controlled, 7-DoF, DLR SARA robot.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at IEEE Robotics and Automation Letters (RA-L), 16 pages, 19
  figures, 6 tables. See
  https://github.com/DLR-RM/interactive-incremental-learning for further
  information and video</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Active Learning of Deep Neural Networks via Gradient-Free Cutting Planes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02145v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02145v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Erica Zhang, Fangzhao Zhang, Mert Pilanci
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Active learning methods aim to improve sample complexity in machine learning.
In this work, we investigate an active learning scheme via a novel
gradient-free cutting-plane training method for ReLU networks of arbitrary
depth and develop a convergence theory. We demonstrate, for the first time,
that cutting-plane algorithms, traditionally used in linear models, can be
extended to deep neural networks despite their nonconvexity and nonlinear
decision boundaries. Moreover, this training method induces the first deep
active learning scheme known to achieve convergence guarantees, revealing a
geometric contraction rate of the feasible set. We exemplify the effectiveness
of our proposed active learning method against popular deep active learning
baselines via both synthetic data experiments and sentimental classification
task on real datasets.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Graph Perceiver IO: A General Architecture for Graph Structured Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2209.06418v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2209.06418v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Seyun Bae, Hoyoon Byun, Changdae Oh, Yoon-Sik Cho, Kyungwoo Song
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multimodal machine learning has been widely studied for the development of
general intelligence. Recently, the Perceiver and Perceiver IO, show
competitive results for diverse dataset domains and tasks. However, recent
works, Perceiver and Perceiver IO, have focused on heterogeneous modalities,
including image, text, and there are few research works for graph structured
datasets. A graph has an adjacency matrix different from other datasets such as
text and image, and it is not trivial to handle the topological information. In
this study, we provide a Graph Perceiver IO (GPIO), the Perceiver IO for the
graph structured dataset. We keep the main structure of the GPIO as the
Perceiver IO because the Perceiver IO already handles the diverse dataset well,
except for the graph structured dataset. The GPIO is a general method that
handles diverse datasets, such as graph-structured data, text, and images, by
leveraging positional encoding and output query smoothing. Compared to graph
neural networks (GNNs), GPIO requires lower complexity and can efficiently
incorporate global and local information, which is also empirically validated
through experiments. Furthermore, we propose GPIO+ for the multimodal few-shot
classification that incorporates both images and graphs simultaneously. GPIO
achieves higher benchmark accuracy than GNNs across multiple tasks, including
graph classification, node classification, and multimodal text classification,
while also attaining superior AP and AUC in link prediction. Additionally,
GPIO+ outperforms GNNs in multimodal few-shot classification. Our GPIO(+) can
serve as a general architecture for handling various modalities and tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Mitigating Tail Narrowing in LLM Self-Improvement via Socratic-Guided
  Sampling <span class="chip">NAACL 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.00750v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.00750v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yiwen Ding, Zhiheng Xi, Wei He, Zhuoyuan Li, Yitao Zhai, Xiaowei Shi, Xunliang Cai, Tao Gui, Qi Zhang, Xuanjing Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Self-improvement methods enable large language models (LLMs) to generate
solutions themselves and iteratively train on filtered, high-quality
rationales. This process proves effective and reduces the reliance on human
supervision in LLMs' reasoning, but the performance soon plateaus. We delve
into the process and find that models tend to over-sample on easy queries and
under-sample on queries they have yet to master. As iterations proceed, this
imbalance in sampling is exacerbated, leading to a long-tail distribution where
solutions to difficult queries almost diminish. This phenomenon limits the
performance gain of self-improving models. A straightforward solution is
brute-force sampling to balance the distribution, which significantly raises
computational costs. In this paper, we introduce Guided Self-Improvement (GSI),
a strategy aimed at improving the efficiency of sampling challenging
heavy-tailed data. It leverages Socratic-style guidance signals to help LLM
reasoning with complex queries, reducing the exploration effort and minimizing
computational overhead. Experiments on four models across diverse mathematical
tasks show that GSI strikes a balance between performance and efficiency, while
also being effective on held-out tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to NAACL 2025 Main Conference. Codes are publicly available
  at https://github.com/Yiwen-Ding/Guided-Self-Improvement</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ZeroFlow: Overcoming Catastrophic Forgetting is Easier than You Think 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.01045v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.01045v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tao Feng, Wei Li, Didi Zhu, Hangjie Yuan, Wendi Zheng, Dan Zhang, Jie Tang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Backpropagation provides a generalized configuration for overcoming
catastrophic forgetting. Like, SGD and Adam are commonly used for weight
updates in continual learning and continual pre-training. In practice,
permission to access gradient information is not always granted (the gradient
ban), such as black-box APIs, hardware limitations, and non-differentiable
systems. To bridge this gap, we introduce the first benchmark ZeroFlow to
evaluate gradient-free optimization algorithms for overcoming forgetting. This
benchmark examines a suite of forward pass methods across multiple methods,
forgetting scenarios, and datasets. We find that forward passes alone are
enough to overcome forgetting. Our findings reveal new optimization principles
that highlight the potential of forward-pass in mitigating forgetting, managing
task conflicts, and reducing memory demands, alongside novel enhancements that
further mitigate forgetting with just one forward pass. This work provides
essential insights and tools for advancing forward pass methods to overcome
forgetting.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Optimizing DNN Inference on Multi-Accelerator SoCs at Training-time 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.18566v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.18566v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Matteo Risso, Alessio Burrello, Daniele Jahier Pagliari
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The demand for executing Deep Neural Networks (DNNs) with low latency and
minimal power consumption at the edge has led to the development of advanced
heterogeneous Systems-on-Chips (SoCs) that incorporate multiple specialized
computing units (CUs), such as accelerators. Offloading DNN computations to a
specific CU from the available set often exposes accuracy vs efficiency
trade-offs, due to differences in their supported operations (e.g., standard
vs. depthwise convolution) or data representations (e.g., more/less
aggressively quantized). A challenging yet unresolved issue is how to map a DNN
onto these multi-CU systems to maximally exploit the parallelization
possibilities while taking accuracy into account. To address this problem, we
present ODiMO, a hardware-aware tool that efficiently explores fine-grain
mapping of DNNs among various on-chip CUs, during the training phase. ODiMO
strategically splits individual layers of the neural network and executes them
in parallel on the multiple available CUs, aiming to balance the total
inference energy consumption or latency with the resulting accuracy, impacted
by the unique features of the different hardware units. We test our approach on
CIFAR-10, CIFAR-100, and ImageNet, targeting two open-source heterogeneous
SoCs, i.e., DIANA and Darkside. We obtain a rich collection of Pareto-optimal
networks in the accuracy vs. energy or latency space. We show that ODiMO
reduces the latency of a DNN executed on the Darkside SoC by up to 8x at
iso-accuracy, compared to manual heuristic mappings. When targeting energy, on
the same SoC, ODiMO produced up to 50.8x more efficient mappings, with minimal
accuracy drop (< 0.3%).
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for publication in the IEEE Transactions on Computer-Aided
  Design of Integrated Circuits and Systems</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ChatVLA: Unified Multimodal Understanding and Robot Control with
  Vision-Language-Action Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.14420v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.14420v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhongyi Zhou, Yichen Zhu, Minjie Zhu, Junjie Wen, Ning Liu, Zhiyuan Xu, Weibin Meng, Ran Cheng, Yaxin Peng, Chaomin Shen, Feifei Feng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Humans possess a unified cognitive ability to perceive, comprehend, and
interact with the physical world. Why can't large language models replicate
this holistic understanding? Through a systematic analysis of existing training
paradigms in vision-language-action models (VLA), we identify two key
challenges: spurious forgetting, where robot training overwrites crucial
visual-text alignments, and task interference, where competing control and
understanding tasks degrade performance when trained jointly. To overcome these
limitations, we propose ChatVLA, a novel framework featuring Phased Alignment
Training, which incrementally integrates multimodal data after initial control
mastery, and a Mixture-of-Experts architecture to minimize task interference.
ChatVLA demonstrates competitive performance on visual question-answering
datasets and significantly surpasses state-of-the-art vision-language-action
(VLA) methods on multimodal understanding benchmarks. Notably, it achieves a
six times higher performance on MMMU and scores 47.2% on MMStar with a more
parameter-efficient design than ECoT. Furthermore, ChatVLA demonstrates
superior performance on 25 real-world robot manipulation tasks compared to
existing VLA methods like OpenVLA. Our findings highlight the potential of our
unified framework for achieving both robust multimodal understanding and
effective robot control.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LaM-SLidE: Latent Space Modeling of Spatial Dynamical Systems via Linked
  Entities 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.12128v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.12128v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Florian Sestak, Artur Toshev, Andreas Fürst, Günter Klambauer, Andreas Mayr, Johannes Brandstetter
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generative models are spearheading recent progress in deep learning, showing
strong promise for trajectory sampling in dynamical systems as well. However,
while latent space modeling paradigms have transformed image and video
generation, similar approaches are more difficult for most dynamical systems.
Such systems -- from chemical molecule structures to collective human behavior
-- are described by interactions of entities, making them inherently linked to
connectivity patterns and the traceability of entities over time. Our approach,
LaM-SLidE (Latent Space Modeling of Spatial Dynamical Systems via Linked
Entities), combines the advantages of graph neural networks, i.e., the
traceability of entities across time-steps, with the efficiency and scalability
of recent advances in image and video generation, where pre-trained encoder and
decoder are frozen to enable generative modeling in the latent space. The core
idea of LaM-SLidE is to introduce identifier representations (IDs) to allow for
retrieval of entity properties, e.g., entity coordinates, from latent system
representations and thus enables traceability. Experimentally, across different
domains, we show that LaM-SLidE performs favorably in terms of speed, accuracy,
and generalizability. Code is available at https://github.com/ml-jku/LaM-SLidE .
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://ml-jku.github.io/LaM-SLidE/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Score-Based Diffusion Policy Compatible with Reinforcement Learning via
  Optimal Transport 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.12631v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.12631v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mingyang Sun, Pengxiang Ding, Weinan Zhang, Donglin Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diffusion policies have shown promise in learning complex behaviors from
demonstrations, particularly for tasks requiring precise control and long-term
planning. However, they face challenges in robustness when encountering
distribution shifts. This paper explores improving diffusion-based imitation
learning models through online interactions with the environment. We propose
OTPR (Optimal Transport-guided score-based diffusion Policy for Reinforcement
learning fine-tuning), a novel method that integrates diffusion policies with
RL using optimal transport theory. OTPR leverages the Q-function as a transport
cost and views the policy as an optimal transport map, enabling efficient and
stable fine-tuning. Moreover, we introduce masked optimal transport to guide
state-action matching using expert keypoints and a compatibility-based
resampling strategy to enhance training stability. Experiments on three
simulation tasks demonstrate OTPR's superior performance and robustness
compared to existing methods, especially in complex and sparse-reward
environments. In sum, OTPR provides an effective framework for combining IL and
RL, achieving versatile and reliable policy learning. The code will be released
at https://github.com/Sunmmyy/OTPR.git.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PiCO: Peer <span class="highlight-title">Review</span> in LLMs based on the Consistency Optimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.01830v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.01830v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kun-Peng Ning, Shuo Yang, Yu-Yang Liu, Jia-Yu Yao, Zhen-Hui Liu, Yong-Hong Tian, Yibing Song, Li Yuan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Existing large language models (LLMs) evaluation methods typically focus on
testing the performance on some closed-environment and domain-specific
benchmarks with human annotations. In this paper, we explore a novel
unsupervised evaluation direction, utilizing peer-review mechanisms to measure
LLMs automatically. In this setting, both open-source and closed-source LLMs
lie in the same environment, capable of answering unlabeled questions and
evaluating each other, where each LLM's response score is jointly determined by
other anonymous ones. To obtain the ability hierarchy among these models, we
assign each LLM a learnable capability parameter to adjust the final ranking.
We formalize it as a constrained optimization problem, intending to maximize
the consistency of each LLM's capabilities and scores. The key assumption
behind is that high-level LLM can evaluate others' answers more accurately than
low-level ones, while higher-level LLM can also achieve higher response scores.
Moreover, we propose three metrics called PEN, CIN, and LIS to evaluate the gap
in aligning human rankings. We perform experiments on multiple datasets with
these metrics, validating the effectiveness of the proposed approach.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CoverLib: Classifiers-equipped Experience Library by Iterative Problem
  Distribution Coverage Maximization for Domain-tuned Motion Planning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.02968v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.02968v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hirokazu Ishida, Naoki Hiraoka, Kei Okada, Masayuki Inaba
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Library-based methods are known to be very effective for fast motion planning
by adapting an experience retrieved from a precomputed library. This article
presents CoverLib, a principled approach for constructing and utilizing such a
library. CoverLib iteratively adds an experience-classifier-pair to the
library, where each classifier corresponds to an adaptable region of the
experience within the problem space. This iterative process is an active
procedure, as it selects the next experience based on its ability to
effectively cover the uncovered region. During the query phase, these
classifiers are utilized to select an experience that is expected to be
adaptable for a given problem. Experimental results demonstrate that CoverLib
effectively mitigates the trade-off between plannability and speed observed in
global (e.g. sampling-based) and local (e.g. optimization-based) methods. As a
result, it achieves both fast planning and high success rates over the problem
domain. Moreover, due to its adaptation-algorithm-agnostic nature, CoverLib
seamlessly integrates with various adaptation methods, including nonlinear
programming-based and sampling-based algorithms.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for publication in IEEE Transactions on Robotics</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning to Keep a Promise: Scaling Language Model Decoding Parallelism
  with Learned Asynchronous Decoding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11517v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11517v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tian Jin, Ellie Y. Cheng, Zack Ankner, Nikunj Saunshi, Blake M. Elias, Amir Yazdanbakhsh, Jonathan Ragan-Kelley, Suvinay Subramanian, Michael Carbin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Decoding with autoregressive large language models (LLMs) traditionally
occurs sequentially, generating one token after another. An emerging line of
work explored parallel decoding by identifying and simultaneously generating
semantically independent chunks of LLM responses. However, these techniques
rely on hand-crafted heuristics tied to syntactic structures like lists and
paragraphs, making them rigid and imprecise. We present PASTA, a learning-based
system that teaches LLMs to identify semantic independence and express parallel
decoding opportunities in their own responses. At its core are PASTA-LANG and
its interpreter: PASTA-LANG is an annotation language that enables LLMs to
express semantic independence in their own responses; the language interpreter
acts on these annotations to orchestrate parallel decoding on-the-fly at
inference time. Through a two-stage finetuning process, we train LLMs to
generate PASTA-LANG annotations that optimize both response quality and
decoding speed. Evaluation on AlpacaEval, an instruction following benchmark,
shows that our approach Pareto-dominates existing methods in terms of decoding
speed and response quality; our results demonstrate geometric mean speedups
ranging from 1.21x to 1.93x with corresponding quality changes of +2.2% to
-7.1%, measured by length-controlled win rates against sequential decoding
baseline.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Automatic <span class="highlight-title">Self-supervised</span> Learning for Social Recommendations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.18735v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.18735v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xin He, Wenqi Fan, Mingchen Sun, Ying Wang, Xin Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, researchers have attempted to exploit social relations to
improve the performance in recommendation systems. Generally, most existing
social recommendation methods heavily depends on substantial domain knowledge
and expertise in primary recommendation tasks for designing useful auxiliary
tasks. Meanwhile, Self-Supervised Learning (SSL) recently has received
considerable attention in the field of recommendation, since it can provide
self-supervision signals in assisting the improvement of target recommendation
systems by constructing self-supervised auxiliary tasks from raw data without
human-annotated labels. Despite the great success, these SSL-based social
recommendations are insufficient to adaptively balance various self-supervised
auxiliary tasks, since assigning equal weights on various auxiliary tasks can
result in sub-optimal recommendation performance, where different
self-supervised auxiliary tasks may contribute differently to improving the
primary social recommendation across different datasets. To address this issue,
in this work, we propose Adaptive Self-supervised Learning for Social
Recommendations (AdasRec) by taking advantage of various self-supervised
auxiliary tasks. More specifically, an adaptive weighting mechanism is proposed
to learn adaptive weights for various self-supervised auxiliary tasks, so as to
balance the contribution of such self-supervised auxiliary tasks for enhancing
representation learning in social recommendations. The adaptive weighting
mechanism is used to assign different weights on auxiliary tasks to achieve an
overall weighting of the entire auxiliary tasks and ultimately assist the
primary recommendation task, achieved by a meta learning optimization problem
with an adaptive weighting network. Comprehensive experiments on various
real-world datasets are constructed to verify the effectiveness of our proposed
method.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Deep Parallel Spectral Neural Operators for Solving Partial Differential
  Equations with Enhanced Low-Frequency Learning Capability 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.19976v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.19976v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qinglong Ma, Peizhi Zhao, Sen Wang, Tao Song
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Designing universal artificial intelligence (AI) solver for partial
differential equations (PDEs) is an open-ended problem and a significant
challenge in science and engineering. Currently, data-driven solvers have
achieved great success, such as neural operators. However, the ability of
various neural operator solvers to learn low-frequency information still needs
improvement. In this study, we propose a Deep Parallel Spectral Neural Operator
(DPNO) to enhance the ability to learn low-frequency information. Our method
enhances the neural operator's ability to learn low-frequency information
through parallel modules. In addition, due to the presence of truncation
coefficients, some high-frequency information is lost during the nonlinear
learning process. We smooth this information through convolutional mappings,
thereby reducing high-frequency errors. We selected several challenging partial
differential equation datasets for experimentation, and DPNO performed
exceptionally well. As a neural operator, DPNO also possesses the capability of
resolution invariance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Instance-Level Difficulty: A Missing Perspective in Machine Unlearning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.03043v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.03043v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hammad Rizwan, Mahtab Sarvmaili, Hassan Sajjad, Ga Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Current research on deep machine unlearning primarily focuses on improving or
evaluating the overall effectiveness of unlearning methods while overlooking
the varying difficulty of unlearning individual training samples. As a result,
the broader feasibility of machine unlearning remains under-explored. This
paper studies the cruxes that make machine unlearning difficult through a
thorough instance-level unlearning performance analysis over various unlearning
algorithms and datasets. In particular, we summarize four factors that make
unlearning a data point difficult, and we empirically show that these factors
are independent of a specific unlearning algorithm but only relevant to the
target model and its training data. Given these findings, we argue that machine
unlearning research should pay attention to the instance-level difficulty of
unlearning.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ BA-LoRA: Bias-Alleviating Low-Rank Adaptation to Mitigate Catastrophic
  Inheritance in Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.04556v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.04556v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yupeng Chang, Yi Chang, Yuan Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have demonstrated remarkable proficiency across
various natural language processing (NLP) tasks. However, adapting LLMs to
downstream applications requires computationally intensive and memory-demanding
fine-tuning procedures. To alleviate these burdens, parameter-efficient
fine-tuning (PEFT) techniques have emerged as a promising approach to tailor
LLMs with minimal computational overhead. While PEFT methods offer substantial
advantages, they do not fully address the pervasive issue of bias propagation
from pre-training data. This work introduces Bias-Alleviating Low-Rank
Adaptation (BA-LoRA), a novel PEFT method designed to counteract bias
inheritance. BA-LoRA incorporates three distinct regularization terms: (1) a
consistency regularizer, (2) a diversity regularizer, and (3) a singular value
decomposition regularizer. These regularizers aim to enhance the models'
consistency, diversity, and generalization capabilities during fine-tuning. We
conduct extensive experiments on natural language understanding (NLU) and
natural language generation (NLG) tasks using prominent LLMs such as LLaMA,
Mistral, and Gemma. The results demonstrate that BA-LoRA outperforms LoRA and
its state-of-the-art variants. Moreover, our method effectively mitigates the
adverse effects of pre-training bias, leading to more reliable and robust model
outputs. The code is available at https://github.com/cyp-jlu-ai/BA-LoRA.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>25 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Refined Risk Bounds for Unbounded Losses via Transductive Priors 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.21621v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.21621v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jian Qian, Alexander Rakhlin, Nikita Zhivotovskiy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We revisit the sequential variants of linear regression with the squared
loss, classification problems with hinge loss, and logistic regression, all
characterized by unbounded losses in the setup where no assumptions are made on
the magnitude of design vectors and the norm of the optimal vector of
parameters. The key distinction from existing results lies in our assumption
that the set of design vectors is known in advance (though their order is not),
a setup sometimes referred to as transductive online learning. While this
assumption seems similar to fixed design regression or denoising, we
demonstrate that the sequential nature of our algorithms allows us to convert
our bounds into statistical ones with random design without making any
additional assumptions about the distribution of the design vectors--an
impossibility for standard denoising results. Our key tools are based on the
exponential weights algorithm with carefully chosen transductive
(design-dependent) priors, which exploit the full horizon of the design
vectors.
  Our classification regret bounds have a feature that is only attributed to
bounded losses in the literature: they depend solely on the dimension of the
parameter space and on the number of rounds, independent of the design vectors
or the norm of the optimal solution. For linear regression with squared loss,
we further extend our analysis to the sparse case, providing sparsity regret
bounds that additionally depend on the magnitude of the response variables. We
argue that these improved bounds are specific to the transductive setting and
unattainable in the worst-case sequential setup. Our algorithms, in several
cases, have polynomial time approximations and reduce to sampling with respect
to log-concave measures instead of aggregating over hard-to-construct
$\varepsilon$-covers of classes.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Large Language Models for Mobility Analysis in Transportation Systems: A
  <span class="highlight-title">Survey</span> on Forecasting Tasks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.02357v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.02357v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zijian Zhang, Yujie Sun, Zepu Wang, Yuqi Nie, Xiaobo Ma, Ruolin Li, Peng Sun, Xuegang Ban
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Mobility analysis is a crucial element in the research area of transportation
systems. Forecasting traffic information offers a viable solution to address
the conflict between increasing transportation demands and the limitations of
transportation infrastructure. Predicting human travel is significant in aiding
various transportation and urban management tasks, such as taxi dispatch and
urban planning. Machine learning and deep learning methods are favored for
their flexibility and accuracy. Nowadays, with the advent of large language
models (LLMs), many researchers have combined these models with previous
techniques or applied LLMs to directly predict future traffic information and
human travel behaviors. However, there is a lack of comprehensive studies on
how LLMs can contribute to this field. This survey explores existing approaches
using LLMs for time series forecasting problems for mobility in transportation
systems. We provide a literature review concerning the forecasting applications
within transportation systems, elucidating how researchers utilize LLMs,
showcasing recent state-of-the-art advancements, and identifying the challenges
that must be overcome to fully leverage LLMs in this domain.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>27 pages, presented in 2025 TRB meeting</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Selective <span class="highlight-title">Prompt</span> Anchoring for Code Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.09121v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.09121v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuan Tian, Tianyi Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances in large language models (LLMs) have transformed software
development by automatically generating code from natural language. Yet
challenges remain in generating fully correct code that aligns with user
intent. Our study reveals that LLMs tend to pay less attention to user prompts
as more code tokens are generated. We hypothesize that this attention dilution
issue is an important reason for code generation errors. To mitigate this
issue, we propose Selective Prompt Anchoring (SPA) to guide code LLMs to pay
more attention to user intent when generating code. We evaluate SPA using six
base LLMs across six benchmarks. Our results demonstrate that SPA enhances
Pass@1 by up to 12.9%, consistently outperforming SOTA code generation methods
in all settings. Our code is available at
https://github.com/magic-YuanTian/Selective-Prompt-Anchoring.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ TimeDART: A Diffusion Autoregressive <span class="highlight-title">Transformer</span> for <span class="highlight-title">Self-Supervised</span>
  Time Series Representation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05711v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05711v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Daoyu Wang, Mingyue Cheng, Zhiding Liu, Qi Liu, Enhong Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Self-supervised learning has garnered increasing attention in time series
analysis for benefiting various downstream tasks and reducing reliance on
labeled data. Despite its effectiveness, existing methods often struggle to
comprehensively capture both long-term dynamic evolution and subtle local
patterns in a unified manner. In this work, we propose TimeDART, a novel
self-supervised time series pre-training framework that unifies two powerful
generative paradigms to learn more transferable representations. Specifically,
we first employ a causal Transformer encoder, accompanied by a patch-based
embedding strategy, to model the evolving trends from left to right. Building
on this global modeling, we further introduce a denoising diffusion process to
capture fine-grained local patterns through forward diffusion and reverse
denoising. Finally, we optimize the model in an autoregressive manner. As a
result, TimeDART effectively accounts for both global and local sequence
features in a coherent way. We conduct extensive experiments on public datasets
for time series forecasting and classification. The experimental results
demonstrate that TimeDART consistently outperforms previous compared methods,
validating the effectiveness of our approach. Our code is available at
https://github.com/Melmaphother/TimeDART.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>22 pages, 8 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ RLTHF: Targeted Human Feedback for LLM Alignment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.13417v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.13417v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yifei Xu, Tusher Chakraborty, Emre Kıcıman, Bibek Aryal, Eduardo Rodrigues, Srinagesh Sharma, Roberto Estevao, Maria Angels de Luis Balaguer, Jessica Wolk, Rafael Padilha, Leonardo Nunes, Shobana Balakrishnan, Songwu Lu, Ranveer Chandra
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Fine-tuning large language models (LLMs) to align with user preferences is
challenging due to the high cost of quality human annotations in Reinforcement
Learning from Human Feedback (RLHF) and the generalizability limitations of AI
Feedback. To address these challenges, we propose RLTHF, a human-AI hybrid
framework that combines LLM-based initial alignment with selective human
annotations to achieve full-human annotation alignment with minimal effort.
RLTHF identifies hard-to-annotate samples mislabeled by LLMs using a reward
model's reward distribution and iteratively enhances alignment by integrating
strategic human corrections while leveraging LLM's correctly labeled samples.
Evaluations on HH-RLHF and TL;DR datasets show that RLTHF reaches full-human
annotation-level alignment with only 6-7% of the human annotation effort.
Furthermore, models trained on RLTHF's curated datasets for downstream tasks
outperform those trained on fully human-annotated datasets, underscoring the
effectiveness of RLTHF's strategic data curation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Not All Data are Good Labels: On the <span class="highlight-title">Self-supervised</span> Labeling for Time
  Series Forecasting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.14704v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.14704v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuxuan Yang, Dalin Zhang, Yuxuan Liang, Hua Lu, Gang Chen, Huan Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Time Series Forecasting (TSF) is a crucial task in various domains, yet
existing TSF models rely heavily on high-quality data and insufficiently
exploit all available data. This paper explores a novel self-supervised
approach to re-label time series datasets by inherently constructing candidate
datasets. During the optimization of a simple reconstruction network,
intermediates are used as pseudo labels in a self-supervised paradigm,
improving generalization for any predictor. We introduce the Self-Correction
with Adaptive Mask (SCAM), which discards overfitted components and selectively
replaces them with pseudo labels generated from reconstructions. Additionally,
we incorporate Spectral Norm Regularization (SNR) to further suppress
overfitting from a loss landscape perspective. Our experiments on eleven
real-world datasets demonstrate that SCAM consistently improves the performance
of various backbone models. This work offers a new perspective on constructing
datasets and enhancing the generalization of TSF models through self-supervised
learning.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Identification and estimation for matrix time series CP-factor models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05634v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05634v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jinyuan Chang, Yue Du, Guanglin Huang, Qiwei Yao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose a new method for identifying and estimating the CP-factor models
for matrix time series. Unlike the generalized eigenanalysis-based method of
Chang et al.(2023) for which the convergence rates may suffer from small
eigengaps as the asymptotic theory is based on some matrix perturbation
analysis, the proposed new method enjoys faster convergence rates which are
free from any eigengaps. It achieves this by turning the problem into a joint
diagonalization of several matrices whose elements are determined by a basis of
a linear system, and by choosing the basis carefully to avoid near co-linearity
(see Proposition 5 and Section 4.3 below). Furthermore, unlike Chang et
al.(2023) which requires the two factor loading matrices to be full-ranked, the
new method can handle rank-deficient factor loading matrices. Illustration with
both simulated and real matrix time series data shows the advantages of the
proposed new method.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Aligning the Objective of LLM-based Program Repair <span class="chip">ICSE'25</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.08877v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.08877v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junjielong Xu, Ying Fu, Shin Hwei Tan, Pinjia He
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have achieved decent results on automated
program repair (APR). However, the next token prediction training objective of
decoder-only LLMs (e.g., GPT-4) is misaligned with the masked span prediction
objective of current infilling-style methods, which impedes LLMs from fully
leveraging pre-trained knowledge for program repair. In addition, while some
LLMs can locate and repair bugs in certain functions using the related
artifacts (e.g., test cases), existing methods still depend on statement-level
fault localization methods to provide a list of buggy hunks for repair. This
restriction hinders LLMs from exploring potential patches beyond the given
locations.
  In this paper, we investigate a new approach to adapt LLMs to program repair.
Our core insight is that LLM's APR capability can be greatly improved by simply
aligning the output to their training objective and allowing them to refine the
whole program without first identifying faulty statements. Based on this
insight, we designed D4C, a straightforward prompting framework for APR. D4C
can repair 180 bugs correctly in Defects4J, with each patch being sampled only
10 times. This surpasses the SOTA APR methods with perfect fault localization
by 10% and reduces the patch sampling number by 90%. Our findings reveal that
(1) objective alignment is crucial for fully exploiting LLM's pre-trained
capability, and (2) replacing the traditional localize-buggy-hunks-then-repair
workflow with direct debugging is more effective for LLM-based APR methods.
Thus, we believe this paper introduces a new mindset for harnessing LLMs in
APR.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICSE'25</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DualDynamics: Synergizing Implicit and Explicit Methods for Robust
  Irregular Time Series Analysis <span class="chip">AAAI</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.04979v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.04979v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        YongKyung Oh, Dong-Young Lim, Sungil Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Real-world time series analysis faces significant challenges when dealing
with irregular and incomplete data. While Neural Differential Equation (NDE)
based methods have shown promise, they struggle with limited expressiveness,
scalability issues, and stability concerns. Conversely, Neural Flows offer
stability but falter with irregular data. We introduce 'DualDynamics', a novel
framework that synergistically combines NDE-based method and Neural Flow-based
method. This approach enhances expressive power while balancing computational
demands, addressing critical limitations of existing techniques. We demonstrate
DualDynamics' effectiveness across diverse tasks: classification of robustness
to dataset shift, irregularly-sampled series analysis, interpolation of missing
data, and forecasting with partial observations. Our results show consistent
outperformance over state-of-the-art methods, indicating DualDynamics'
potential to advance irregular time series analysis significantly.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published at the 39th Annual AAAI Conference on Artificial
  Intelligence (AAAI 2025)</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Multimedia <span class="chip" style="font-size: 60%">6</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LightThinker: Thinking Step-by-Step Compression 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.15589v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.15589v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jintian Zhang, Yuqi Zhu, Mengshu Sun, Yujie Luo, Shuofei Qiao, Lun Du, Da Zheng, Huajun Chen, Ningyu Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have shown remarkable performance in complex
reasoning tasks, but their efficiency is hindered by the substantial memory and
computational costs associated with generating lengthy tokens. In this paper,
we propose LightThinker, a novel method that enables LLMs to dynamically
compress intermediate thoughts during reasoning. Inspired by human cognitive
processes, LightThinker compresses verbose thought steps into compact
representations and discards the original reasoning chains, thereby
significantly reducing the number of tokens stored in the context window. This
is achieved by training the model on when and how to perform compression
through data construction, mapping hidden states to condensed gist tokens, and
creating specialized attention masks. Additionally, we introduce the Dependency
(Dep) metric to quantify the degree of compression by measuring the reliance on
historical tokens during generation. Extensive experiments on four datasets and
two models show that LightThinker reduces peak memory usage and inference time,
while maintaining competitive accuracy. Our work provides a new direction for
improving the efficiency of LLMs in complex reasoning tasks without sacrificing
performance. Code will be released at https://github.com/zjunlp/LightThinker.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multimodal Graph-Based Variational Mixture of Experts Network for
  Zero-Shot Multimodal Information Extraction <span class="chip">WWW 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.15290v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.15290v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Baohang Zhou, Ying Zhang, Yu Zhao, Xuhui Sui, Xiaojie Yuan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multimodal information extraction on social media is a series of fundamental
tasks to construct the multimodal knowledge graph. The tasks aim to extract the
structural information in free texts with the incorporate images, including:
multimodal named entity typing and multimodal relation extraction. However, the
growing number of multimodal data implies a growing category set and the newly
emerged entity types or relations should be recognized without additional
training. To address the aforementioned challenges, we focus on the zero-shot
multimodal information extraction tasks which require using textual and visual
modalities for recognizing unseen categories. Compared with text-based
zero-shot information extraction models, the existing multimodal ones make the
textual and visual modalities aligned directly and exploit various fusion
strategies to improve their performances. But the existing methods ignore the
fine-grained semantic correlation of text-image pairs and samples. Therefore,
we propose the multimodal graph-based variational mixture of experts network
(MG-VMoE) which takes the MoE network as the backbone and exploits it for
aligning multimodal representations in a fine-grained way. Considering to learn
informative representations of multimodal data, we design each expert network
as a variational information bottleneck to process two modalities in a
uni-backbone. Moreover, we also propose the multimodal graph-based virtual
adversarial training to learn the semantic correlation between the samples. The
experimental results on the two benchmark datasets demonstrate the superiority
of MG-VMoE over the baselines.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by WWW 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Steganographic Embeddings as an Effective Data Augmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.15245v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.15245v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nicholas DiSalvo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Image Steganography is a cryptographic technique that embeds secret
information into an image, ensuring the hidden data remains undetectable to the
human eye while preserving the image's original visual integrity. Least
Significant Bit (LSB) Steganography achieves this by replacing the k least
significant bits of an image with the k most significant bits of a secret
image, maintaining the appearance of the original image while simultaneously
encoding the essential elements of the hidden data. In this work, we shift away
from conventional applications of steganography in deep learning and explore
its potential from a new angle. We present experimental results on CIFAR-10
showing that LSB Steganography, when used as a data augmentation strategy for
downstream computer vision tasks such as image classification, can
significantly improve the training efficiency of deep neural networks. It can
also act as an implicit, uniformly discretized piecewise linear approximation
of color augmentations such as (brightness, contrast, hue, and saturation),
without introducing additional training overhead through a new joint image
training regime that disregards the need for tuning sensitive augmentation
hyperparameters.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 4 figures. For associated code and experiments, see this
  http URL https://github.com/nickd16/steganographic-augmentations</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Multi-view Hypergraph-based Contrastive Learning Model for Cold-Start
  Micro-video Recommendation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.09638v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.09638v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sisuo Lyu, Xiuze Zhou, Xuming Hu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the widespread use of mobile devices and the rapid growth of micro-video
platforms such as TikTok and Kwai, the demand for personalized micro-video
recommendation systems has significantly increased. Micro-videos typically
contain diverse information, such as textual metadata, visual cues (e.g., cover
images), and dynamic video content, significantly affecting user interaction
and engagement patterns. However, most existing approaches often suffer from
the problem of over-smoothing, which limits their ability to capture
comprehensive interaction information effectively. Additionally, cold-start
scenarios present ongoing challenges due to sparse interaction data and the
underutilization of available interaction signals. To address these issues, we
propose a Multi-view Hypergraph-based Contrastive learning model for cold-start
micro-video Recommendation (MHCR). MHCR introduces a multi-view multimodal
feature extraction layer to capture interaction signals from various
perspectives and incorporates multi-view self-supervised learning tasks to
provide additional supervisory signals. Through extensive experiments on two
real-world datasets, we show that MHCR significantly outperforms existing video
recommendation models and effectively mitigates cold-start challenges. Our code
is available at https://github.com/sisuolv/MHCR.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Magnifier <span class="highlight-title">Prompt</span>: Tackling Multimodal Hallucination via Extremely Simple
  Instructions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.11701v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.11701v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuhan Fu, Ruobing Xie, Jiazhen Liu, Bangxiang Lan, Xingwu Sun, Zhanhui Kang, Xirong Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Hallucinations in multimodal large language models (MLLMs) hinder their
practical applications. To address this, we propose a Magnifier Prompt
(MagPrompt), a simple yet effective method to tackle hallucinations in MLLMs
via extremely simple instructions. MagPrompt is based on the following two key
principles, which guide the design of various effective prompts, demonstrating
robustness: (1) MLLMs should focus more on the image. (2) When there are
conflicts between the image and the model's inner knowledge, MLLMs should
prioritize the image. MagPrompt is training-free and can be applied to
open-source and closed-source models, such as GPT-4o and Gemini-pro. It
performs well across many datasets and its effectiveness is comparable or even
better than more complex methods like VCD. Furthermore, our prompt design
principles and experimental analyses provide valuable insights into multimodal
hallucination.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The proposed method does not work for up-to-date MLLMs.</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ESIQA: Perceptual Quality Assessment of Vision-Pro-based Egocentric
  Spatial Images 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.21363v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.21363v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xilei Zhu, Liu Yang, Huiyu Duan, Xiongkuo Min, Guangtao Zhai, Patrick Le Callet
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the development of eXtended Reality (XR), photo capturing and display
technology based on head-mounted displays (HMDs) have experienced significant
advancements and gained considerable attention. Egocentric spatial images and
videos are emerging as a compelling form of stereoscopic XR content. The
assessment for the Quality of Experience (QoE) of XR content is important to
ensure a high-quality viewing experience. Different from traditional 2D images,
egocentric spatial images present challenges for perceptual quality assessment
due to their special shooting, processing methods, and stereoscopic
characteristics. However, the corresponding image quality assessment (IQA)
research for egocentric spatial images is still lacking. In this paper, we
establish the Egocentric Spatial Images Quality Assessment Database (ESIQAD),
the first IQA database dedicated for egocentric spatial images as far as we
know. Our ESIQAD includes 500 egocentric spatial images and the corresponding
mean opinion scores (MOSs) under three display modes, including 2D display,
3D-window display, and 3D-immersive display. Based on our ESIQAD, we propose a
novel mamba2-based multi-stage feature fusion model, termed ESIQAnet, which
predicts the perceptual quality of egocentric spatial images under the three
display modes. Specifically, we first extract features from multiple visual
state space duality (VSSD) blocks, then apply cross attention to fuse binocular
view information and use transposed attention to further refine the features.
The multi-stage features are finally concatenated and fed into a quality
regression network to predict the quality score. Extensive experimental results
demonstrate that the ESIQAnet outperforms 22 state-of-the-art IQA models on the
ESIQAD under all three display modes. The database and code are available at
https://github.com/IntMeGroup/ESIQA.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 12 figures</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2025-02-20T00:00:00Z">2025-02-20</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Computation and Language <span class="chip" style="font-size: 60%">10</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LUME: LLM Unlearning with Multitask Evaluations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.15097v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.15097v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anil Ramakrishna, Yixin Wan, Xiaomeng Jin, Kai-Wei Chang, Zhiqi Bu, Bhanukiran Vinzamuri, Volkan Cevher, Mingyi Hong, Rahul Gupta
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Unlearning aims to remove copyrighted, sensitive, or private content from
large language models (LLMs) without a full retraining. In this work, we
develop a multi-task unlearning benchmark (LUME) which features three tasks:
(1) unlearn synthetically generated creative short novels, (2) unlearn
synthetic biographies with sensitive information, and (3) unlearn a collection
of public biographies. We further release two fine-tuned LLMs of 1B and 7B
parameter sizes as the target models. We conduct detailed evaluations of
several recently proposed unlearning algorithms and present results on
carefully crafted metrics to understand their behavior and limitations.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Judging It, Washing It: Scoring and Greenwashing Corporate Climate
  Disclosures using Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.15094v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.15094v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Marianne Chuang, Gabriel Chuang, Cheryl Chuang, John Chuang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study the use of large language models (LLMs) to both evaluate and
greenwash corporate climate disclosures. First, we investigate the use of the
LLM-as-a-Judge (LLMJ) methodology for scoring company-submitted reports on
emissions reduction targets and progress. Second, we probe the behavior of an
LLM when it is prompted to greenwash a response subject to accuracy and length
constraints. Finally, we test the robustness of the LLMJ methodology against
responses that may be greenwashed using an LLM. We find that two LLMJ scoring
systems, numerical rating and pairwise comparison, are effective in
distinguishing high-performing companies from others, with the pairwise
comparison system showing greater robustness against LLM-greenwashed responses.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages, 12 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Optimizing Singular Spectrum for Large Language Model Compression 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.15092v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.15092v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dengjie Li, Tiancheng Shen, Yao Zhou, Baisong Yang, Zhongying Liu, Masheng Yang, Bernard Ghanem, Yibo Yang, Yujie Zhong, Ming-Hsuan Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have demonstrated remarkable capabilities, yet
prohibitive parameter complexity often hinders their deployment. Existing
singular value decomposition (SVD) based compression methods simply deem
singular values as importance scores of decomposed components. However, this
importance ordered by singular values does not necessarily correlate with the
performance of a downstream task. In this work, we introduce SoCo (Singular
spectrum optimization for large language model Compression), a novel
compression framework that learns to rescale the decomposed components of SVD
in a data-driven manner. Concretely, we employ a learnable diagonal matrix to
assign importance scores for singular spectrum and develop a three-stage
training process that progressively refines these scores from initial coarse
compression to fine-grained sparsification-thereby striking an effective
balance between aggressive model compression and performance preservation.
Thanks to the learnable singular spectrum, SoCo adaptively prunes components
according to the sparsified importance scores, rather than relying on the fixed
order of singular values. More importantly, the remaining components with
amplified importance scores can compensate for the loss of the pruned ones.
Experimental evaluations across multiple LLMs and benchmarks demonstrate that
SoCo surpasses the state-of-the-art methods in model compression.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Analyze the Neurons, not the Embeddings: Understanding When and Where
  LLM Representations Align with Humans 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.15090v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.15090v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Masha Fedzechkina, Eleonora Gualdoni, Sinead Williamson, Katherine Metcalf, Skyler Seto, Barry-John Theobald
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Modern large language models (LLMs) achieve impressive performance on some
tasks, while exhibiting distinctly non-human-like behaviors on others. This
raises the question of how well the LLM's learned representations align with
human representations. In this work, we introduce a novel approach to the study
of representation alignment: we adopt a method from research on activation
steering to identify neurons responsible for specific concepts (e.g., 'cat')
and then analyze the corresponding activation patterns. Our findings reveal
that LLM representations closely align with human representations inferred from
behavioral data. Notably, this alignment surpasses that of word embeddings,
which have been center stage in prior work on human and model alignment.
Additionally, our approach enables a more granular view of how LLMs represent
concepts. Specifically, we show that LLMs organize concepts in a way that
reflects hierarchical relationships interpretable to humans (e.g.,
'animal'-'dog').
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Is Safety Standard Same for Everyone? User-Specific Safety Evaluation of
  Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.15086v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.15086v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yeonjun In, Wonjoong Kim, Kanghoon Yoon, Sungchul Kim, Mehrab Tanjim, Kibum Kim, Chanyoung Park
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As the use of large language model (LLM) agents continues to grow, their
safety vulnerabilities have become increasingly evident. Extensive benchmarks
evaluate various aspects of LLM safety by defining the safety relying heavily
on general standards, overlooking user-specific standards. However, safety
standards for LLM may vary based on a user-specific profiles rather than being
universally consistent across all users. This raises a critical research
question: Do LLM agents act safely when considering user-specific safety
standards? Despite its importance for safe LLM use, no benchmark datasets
currently exist to evaluate the user-specific safety of LLMs. To address this
gap, we introduce U-SAFEBENCH, the first benchmark designed to assess
user-specific aspect of LLM safety. Our evaluation of 18 widely used LLMs
reveals current LLMs fail to act safely when considering user-specific safety
standards, marking a new discovery in this field. To address this
vulnerability, we propose a simple remedy based on chain-of-thought,
demonstrating its effectiveness in improving user-specific safety. Our
benchmark and code are available at https://github.com/yeonjun-in/U-SafeBench.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ UPCORE: Utility-Preserving Coreset Selection for Balanced Unlearning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.15082v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.15082v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vaidehi Patil, Elias Stengel-Eskin, Mohit Bansal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  User specifications or legal frameworks often require information to be
removed from pretrained models, including large language models (LLMs). This
requires deleting or "forgetting" a set of data points from an already-trained
model, which typically degrades its performance on other data points. Thus, a
balance must be struck between removing information and keeping the model's
other abilities intact, with a failure to balance this trade-off leading to
poor deletion or an unusable model. To this end, we propose UPCORE
(Utility-Preserving Coreset Selection), a method-agnostic data selection
framework for mitigating collateral damage during unlearning. Finding that the
model damage is correlated with the variance of the model's representations on
the forget set, we selectively prune the forget set to remove outliers, thereby
minimizing model degradation after unlearning. We evaluate UPCORE across three
standard unlearning methods consistently achieving a superior balance between
the competing objectives of deletion efficacy and model preservation. To better
evaluate this trade-off, we introduce a new metric, measuring the
area-under-the-curve (AUC) across standard metrics. We find that UPCORE
improves both standard metrics and AUC, benefitting from positive transfer
between the coreset and pruned points while reducing negative transfer from the
forget set to points outside of it.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code: https://github.com/Vaidehi99/UPCORE</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Can Hallucination Correction Improve Video-Language Alignment? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.15079v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.15079v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lingjun Zhao, Mingyang Xie, Paola Cascante-Bonilla, Hal Daumé III, Kwonjoon Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Vision-Language Models often generate hallucinated content that is not
grounded in its visual inputs. While prior work focuses on mitigating
hallucinations, we instead explore leveraging hallucination correction as a
training objective to improve video-language alignment. We introduce HACA, a
self-training framework learning to correct hallucinations in descriptions that
do not align with the video content. By identifying and correcting
inconsistencies, HACA enhances the model's ability to align video and textual
representations for spatio-temporal reasoning. Our experimental results show
consistent gains in video-caption binding and text-to-video retrieval tasks,
demonstrating that hallucination correction-inspired tasks serve as an
effective strategy for improving vision and language alignment.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Rare Disease Differential Diagnosis with Large Language Models at Scale:
  From Abdominal Actinomycosis to Wilson's Disease 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.15069v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.15069v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Elliot Schumacher, Dhruv Naik, Anitha Kannan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have demonstrated impressive capabilities in
disease diagnosis. However, their effectiveness in identifying rarer diseases,
which are inherently more challenging to diagnose, remains an open question.
Rare disease performance is critical with the increasing use of LLMs in
healthcare settings. This is especially true if a primary care physician needs
to make a rarer prognosis from only a patient conversation so that they can
take the appropriate next step. To that end, several clinical decision support
systems are designed to support providers in rare disease identification. Yet
their utility is limited due to their lack of knowledge of common disorders and
difficulty of use.
  In this paper, we propose RareScale to combine the knowledge LLMs with expert
systems. We use jointly use an expert system and LLM to simulate rare disease
chats. This data is used to train a rare disease candidate predictor model.
Candidates from this smaller model are then used as additional inputs to
black-box LLM to make the final differential diagnosis. Thus, RareScale allows
for a balance between rare and common diagnoses. We present results on over 575
rare diseases, beginning with Abdominal Actinomycosis and ending with Wilson's
Disease. Our approach significantly improves the baseline performance of
black-box LLMs by over 17% in Top-5 accuracy. We also find that our candidate
generation performance is high (e.g. 88.8% on gpt-4o generated chats).
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Scaling Laws for Downstream Task Performance in Machine Translation <span class="chip">ICLR</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.04177v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.04177v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Berivan Isik, Natalia Ponomareva, Hussein Hazimeh, Dimitris Paparas, Sergei Vassilvitskii, Sanmi Koyejo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Scaling laws provide important insights that can guide the design of large
language models (LLMs). Existing work has primarily focused on studying scaling
laws for pretraining (upstream) loss. However, in transfer learning settings,
in which LLMs are pretrained on an unsupervised dataset and then finetuned on a
downstream task, we often also care about the downstream performance. In this
work, we study the scaling behavior in a transfer learning setting, where LLMs
are finetuned for machine translation tasks. Specifically, we investigate how
the choice of the pretraining data and its size affect downstream performance
(translation quality) as judged by: downstream cross-entropy and translation
quality metrics such as BLEU and COMET scores. Our experiments indicate that
the size of the finetuning dataset and the distribution alignment between the
pretraining and downstream data significantly influence the scaling behavior.
With sufficient alignment, both downstream cross-entropy and translation
quality scores improve monotonically with more pretraining data. In such cases,
we show that it is possible to predict the downstream translation quality
metrics with good accuracy using a log-law. However, there are cases where
moderate misalignment causes the downstream translation scores to fluctuate or
get worse with more pretraining, whereas downstream cross-entropy monotonically
improves. By analyzing these, we provide new practical insights for choosing
appropriate pretraining data.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published at the International Conference on Learning Representations
  (ICLR) 2025. Previous title: "Scaling Laws for Downstream Task Performance of
  Large Language Models"</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Use of a Structured Knowledge Base Enhances Metadata Curation by Large
  Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.05893v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.05893v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sowmya S. Sundaram, Benjamin Solomon, Avani Khatri, Anisha Laumas, Purvesh Khatri, Mark A. Musen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Metadata play a crucial role in ensuring the findability, accessibility,
interoperability, and reusability of datasets. This paper investigates the
potential of large language models (LLMs), specifically GPT-4, to improve
adherence to metadata standards. We conducted experiments on 200 random data
records describing human samples relating to lung cancer from the NCBI
BioSample repository, evaluating GPT-4's ability to suggest edits for adherence
to metadata standards. We computed the adherence accuracy of field name-field
value pairs through a peer review process, and we observed a marginal average
improvement in adherence to the standard data dictionary from 79% to 80%
(p<0.5). We then prompted GPT-4 with domain information in the form of the
textual descriptions of CEDAR templates and recorded a significant improvement
to 97% from 79% (p<0.01). These results indicate that, while LLMs may not be
able to correct legacy metadata to ensure satisfactory adherence to standards
when unaided, they do show promise for use in automated metadata curation when
integrated with a structured knowledge base
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Computer Vision and Pattern Recognition <span class="chip" style="font-size: 60%">42</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Can Hallucination Correction Improve Video-Language Alignment? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.15079v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.15079v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lingjun Zhao, Mingyang Xie, Paola Cascante-Bonilla, Hal Daumé III, Kwonjoon Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Vision-Language Models often generate hallucinated content that is not
grounded in its visual inputs. While prior work focuses on mitigating
hallucinations, we instead explore leveraging hallucination correction as a
training objective to improve video-language alignment. We introduce HACA, a
self-training framework learning to correct hallucinations in descriptions that
do not align with the video content. By identifying and correcting
inconsistencies, HACA enhances the model's ability to align video and textual
representations for spatio-temporal reasoning. Our experimental results show
consistent gains in video-caption binding and text-to-video retrieval tasks,
demonstrating that hallucination correction-inspired tasks serve as an
effective strategy for improving vision and language alignment.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Hardware-Friendly Static Quantization Method for Video Diffusion
  <span class="highlight-title">Transformer</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.15077v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.15077v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sanghyun Yi, Qingfeng Liu, Mostafa El-Khamy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diffusion Transformers for video generation have gained significant research
interest since the impressive performance of SORA. Efficient deployment of such
generative-AI models on GPUs has been demonstrated with dynamic quantization.
However, resource-constrained devices cannot support dynamic quantization, and
need static quantization of the models for their efficient deployment on AI
processors. In this paper, we propose a novel method for the post-training
quantization of OpenSora\cite{opensora}, a Video Diffusion Transformer, without
relying on dynamic quantization techniques. Our approach employs static
quantization, achieving video quality comparable to FP16 and dynamically
quantized ViDiT-Q methods, as measured by CLIP, and VQA metrics. In particular,
we utilize per-step calibration data to adequately provide a post-training
statically quantized model for each time step, incorporating channel-wise
quantization for weights and tensor-wise quantization for activations. By
further applying the smooth-quantization technique, we can obtain high-quality
video outputs with the statically quantized models. Extensive experimental
results demonstrate that static quantization can be a viable alternative to
dynamic quantization for video diffusion transformers, offering a more
efficient approach without sacrificing performance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Synth It Like KITTI: Synthetic Data Generation for Object Detection in
  Driving Scenarios 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.15076v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.15076v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Richard Marcus, Christian Vogel, Inga Jatzkowski, Niklas Knoop, Marc Stamminger
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  An important factor in advancing autonomous driving systems is simulation.
Yet, there is rather small progress for transferability between the virtual and
real world. We revisit this problem for 3D object detection on LiDAR point
clouds and propose a dataset generation pipeline based on the CARLA simulator.
Utilizing domain randomization strategies and careful modeling, we are able to
train an object detector on the synthetic data and demonstrate strong
generalization capabilities to the KITTI dataset. Furthermore, we compare
different virtual sensor variants to gather insights, which sensor attributes
can be responsible for the prevalent domain gap. Finally, fine-tuning with a
small portion of real data almost matches the baseline and with the full
training set slightly surpasses it.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint, to appear in ROBOVIS 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Fostering Inclusion: A Virtual Reality Experience to Raise Awareness of
  Dyslexia-Related Barriers in University Settings 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.15039v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.15039v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        José Manuel Alcalde-Llergo, Pilar Aparicio-Martínez, Andrea Zingoni, Sara Pinzi, Enrique Yeguas-Bolívar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This work introduces the design, implementation, and validation of a virtual
reality (VR) experience aimed at promoting the inclusion of individuals with
dyslexia in university settings. Unlike traditional awareness methods, this
immersive approach offers a novel way to foster empathy by allowing
participants to experience firsthand the challenges faced by students with
dyslexia. Specifically, the experience raises awareness by exposing
non-dyslexic individuals to the difficulties commonly encountered by dyslexic
students. In the virtual environment, participants explore a virtual campus
with multiple buildings, navigating between them while completing tasks and
simultaneously encountering barriers that simulate some of the challenges faced
by individuals with dyslexia. These barriers include reading signs with
shifting letters, following directional arrows that may point incorrectly, and
dealing with a lack of assistance. The campus is a comprehensive model
featuring both indoor and outdoor spaces and supporting various modes of
locomotion. To validate the experience, more than 30 non-dyslexic participants
from the university environment, mainly professors and students, evaluated it
through ad hoc satisfaction surveys. The results indicated heightened awareness
of the barriers encountered by students with dyslexia, with participants
deeming the experience a valuable tool for increasing visibility and fostering
understanding of dyslexic students.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>20 pages, 9 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ InterFeedback: Unveiling Interactive Intelligence of Large Multimodal
  Models via Human Feedback 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.15027v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.15027v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Henry Hengyuan Zhao, Wenqi Pei, Yifei Tao, Haiyang Mei, Mike Zheng Shou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Existing benchmarks do not test Large Multimodal Models (LMMs) on their
interactive intelligence with human users which is vital for developing
general-purpose AI assistants. We design InterFeedback, an interactive
framework, which can be applied to any LMM and dataset to assess this ability
autonomously. On top of this, we introduce InterFeedback-Bench which evaluates
interactive intelligence using two representative datasets, MMMU-Pro and
MathVerse, to test 10 different open-source LMMs. Additionally, we present
InterFeedback-Human, a newly collected dataset of 120 cases designed for
manually testing interactive performance in leading models such as OpenAI-o1
and Claude-3.5-Sonnet. Our evaluation results show that even state-of-the-art
LMM (like OpenAI-o1) can correct their results through human feedback less than
50%. Our findings point to the need for methods that can enhance the LMMs'
capability to interpret and benefit from feedback.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>18 pages, 10 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Simpler Fast Vision <span class="highlight-title">Transformer</span>s with a Jumbo CLS Token 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.15021v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.15021v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anthony Fuller, Yousef Yassin, Daniel G. Kyrollos, Evan Shelhamer, James R. Green
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce a simple enhancement to the global processing of vision
transformers (ViTs) to improve accuracy while maintaining throughput. Our
approach, Jumbo, creates a wider CLS token, which is split to match the patch
token width before attention, processed with self-attention, and reassembled.
After attention, Jumbo applies a dedicated, wider FFN to this token. Jumbo
significantly improves over ViT+Registers on ImageNet-1K at high speeds (by
3.2% for ViT-tiny and 13.5% for ViT-nano); these Jumbo models even outperform
specialized compute-efficient models while preserving the architectural
advantages of plain ViTs. Although Jumbo sees no gains for ViT-small on
ImageNet-1K, it gains 3.4% on ImageNet-21K over ViT+Registers. Both findings
indicate that Jumbo is most helpful when the ViT is otherwise too narrow for
the task. Finally, we show that Jumbo can be easily adapted to excel on data
beyond images, e.g., time series.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CrossOver: 3D Scene Cross-Modal Alignment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.15011v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.15011v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sayan Deb Sarkar, Ondrej Miksik, Marc Pollefeys, Daniel Barath, Iro Armeni
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-modal 3D object understanding has gained significant attention, yet
current approaches often assume complete data availability and rigid alignment
across all modalities. We present CrossOver, a novel framework for cross-modal
3D scene understanding via flexible, scene-level modality alignment. Unlike
traditional methods that require aligned modality data for every object
instance, CrossOver learns a unified, modality-agnostic embedding space for
scenes by aligning modalities - RGB images, point clouds, CAD models,
floorplans, and text descriptions - with relaxed constraints and without
explicit object semantics. Leveraging dimensionality-specific encoders, a
multi-stage training pipeline, and emergent cross-modal behaviors, CrossOver
supports robust scene retrieval and object localization, even with missing
modalities. Evaluations on ScanNet and 3RScan datasets show its superior
performance across diverse metrics, highlighting adaptability for real-world
applications in 3D scene understanding.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project Page: sayands.github.io/crossover/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Digital implementations of deep feature extractors are intrinsically
  informative 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.15004v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.15004v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Max Getter
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Rapid information (energy) propagation in deep feature extractors is crucial
to balance computational complexity versus expressiveness as a representation
of the input. We prove an upper bound for the speed of energy propagation in a
unified framework that covers different neural network models, both over
Euclidean and non-Euclidean domains. Additional structural information about
the signal domain can be used to explicitly determine or improve the rate of
decay. To illustrate this, we show global exponential energy decay for a range
of 1) feature extractors with discrete-domain input signals, and 2)
convolutional neural networks (CNNs) via scattering over locally compact
abelian (LCA) groups.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Rapid Test for Accuracy and Bias of Face Recognition Technology <span class="chip">WACV 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.14996v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.14996v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Manuel Knott, Ignacio Serna, Ethan Mann, Pietro Perona
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Measuring the accuracy of face recognition (FR) systems is essential for
improving performance and ensuring responsible use. Accuracy is typically
estimated using large annotated datasets, which are costly and difficult to
obtain. We propose a novel method for 1:1 face verification that benchmarks FR
systems quickly and without manual annotation, starting from approximate labels
(e.g., from web search results). Unlike previous methods for training set label
cleaning, ours leverages the embedding representation of the models being
evaluated, achieving high accuracy in smaller-sized test datasets. Our approach
reliably estimates FR accuracy and ranking, significantly reducing the time and
cost of manual labeling. We also introduce the first public benchmark of five
FR cloud services, revealing demographic biases, particularly lower accuracy
for Asian women. Our rapid test method can democratize FR testing, promoting
scrutiny and responsible use of the technology. Our method is provided as a
publicly accessible tool at https://github.com/caltechvisionlab/frt-rapid-test
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted as a conference paper for WACV 2025. Manuel Knott, Ignacio
  Serna, and Ethan Mann contributed equally</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LAVID: An Agentic LVLM Framework for Diffusion-Generated Video Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.14994v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.14994v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qingyuan Liu, Yun-Yun Tsai, Ruijian Zha, Victoria Li, Pengyuan Shi, Chengzhi Mao, Junfeng Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The impressive achievements of generative models in creating high-quality
videos have raised concerns about digital integrity and privacy
vulnerabilities. Recent works of AI-generated content detection have been
widely studied in the image field (e.g., deepfake), yet the video field has
been unexplored. Large Vision Language Model (LVLM) has become an emerging tool
for AI-generated content detection for its strong reasoning and multimodal
capabilities. It breaks the limitations of traditional deep learning based
methods faced with like lack of transparency and inability to recognize new
artifacts. Motivated by this, we propose LAVID, a novel LVLMs-based
ai-generated video detection with explicit knowledge enhancement. Our insight
list as follows: (1) The leading LVLMs can call external tools to extract
useful information to facilitate its own video detection task; (2) Structuring
the prompt can affect LVLM's reasoning ability to interpret information in
video content. Our proposed pipeline automatically selects a set of explicit
knowledge tools for detection, and then adaptively adjusts the structure prompt
by self-rewriting. Different from prior SOTA that trains additional detectors,
our method is fully training-free and only requires inference of the LVLM for
detection. To facilitate our research, we also create a new benchmark \vidfor
with high-quality videos generated from multiple sources of video generation
tools. Evaluation results show that LAVID improves F1 scores by 6.2 to 30.2%
over the top baselines on our datasets across four SOTA LVLMs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Ultra-High-Frequency Harmony: mmWave Radar and Event Camera Orchestrate
  Accurate Drone Landing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.14992v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.14992v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haoyang Wang, Jingao Xu, Xinyu Luo, Xuecheng Chen, Ting Zhang, Ruiyang Duan, Yunhao Liu, Xinlei Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  For precise, efficient, and safe drone landings, ground platforms should
real-time, accurately locate descending drones and guide them to designated
spots. While mmWave sensing combined with cameras improves localization
accuracy, the lower sampling frequency of traditional frame cameras compared to
mmWave radar creates bottlenecks in system throughput. In this work, we replace
the traditional frame camera with event camera, a novel sensor that harmonizes
in sampling frequency with mmWave radar within the ground platform setup, and
introduce mmE-Loc, a high-precision, low-latency ground localization system
designed for drone landings. To fully leverage the \textit{temporal
consistency} and \textit{spatial complementarity} between these modalities, we
propose two innovative modules, \textit{consistency-instructed collaborative
tracking} and \textit{graph-informed adaptive joint optimization}, for accurate
drone measurement extraction and efficient sensor fusion. Extensive real-world
experiments in landing scenarios from a leading drone delivery company
demonstrate that mmE-Loc outperforms state-of-the-art methods in both
localization accuracy and latency.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper is accepted by ACM SenSys 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Few-shot Species Range Estimation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.14977v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.14977v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Christian Lange, Max Hamilton, Elijah Cole, Alexander Shepard, Samuel Heinrich, Angela Zhu, Subhransu Maji, Grant Van Horn, Oisin Mac Aodha
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Knowing where a particular species can or cannot be found on Earth is crucial
for ecological research and conservation efforts. By mapping the spatial ranges
of all species, we would obtain deeper insights into how global biodiversity is
affected by climate change and habitat loss. However, accurate range estimates
are only available for a relatively small proportion of all known species. For
the majority of the remaining species, we often only have a small number of
records denoting the spatial locations where they have previously been
observed. We outline a new approach for few-shot species range estimation to
address the challenge of accurately estimating the range of a species from
limited data. During inference, our model takes a set of spatial locations as
input, along with optional metadata such as text or an image, and outputs a
species encoding that can be used to predict the range of a previously unseen
species in feed-forward manner. We validate our method on two challenging
benchmarks, where we obtain state-of-the-art range estimation performance, in a
fraction of the compute time, compared to recent alternative approaches.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ EigenShield: Causal Subspace Filtering via Random Matrix Theory for
  Adversarially Robust Vision-Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.14976v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.14976v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nastaran Darabi, Devashri Naik, Sina Tayebati, Dinithi Jayasuriya, Ranganath Krishnan, Amit Ranjan Trivedi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vision-Language Models (VLMs) inherit adversarial vulnerabilities of Large
Language Models (LLMs), which are further exacerbated by their multimodal
nature. Existing defenses, including adversarial training, input
transformations, and heuristic detection, are computationally expensive,
architecture-dependent, and fragile against adaptive attacks. We introduce
EigenShield, an inference-time defense leveraging Random Matrix Theory to
quantify adversarial disruptions in high-dimensional VLM representations.
Unlike prior methods that rely on empirical heuristics, EigenShield employs the
spiked covariance model to detect structured spectral deviations. Using a
Robustness-based Nonconformity Score (RbNS) and quantile-based thresholding, it
separates causal eigenvectors, which encode semantic information, from
correlational eigenvectors that are susceptible to adversarial artifacts. By
projecting embeddings onto the causal subspace, EigenShield filters adversarial
noise without modifying model parameters or requiring adversarial training.
This architecture-independent, attack-agnostic approach significantly reduces
the attack success rate, establishing spectral analysis as a principled
alternative to conventional defenses. Our results demonstrate that EigenShield
consistently outperforms all existing defenses, including adversarial training,
UNIGUARD, and CIDER.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Time Travel: A Comprehensive Benchmark to Evaluate LMMs on Historical
  and Cultural Artifacts 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.14865v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.14865v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sara Ghaboura, Ketan More, Ritesh Thawkar, Wafa Alghallabi, Omkar Thawakar, Fahad Shahbaz Khan, Hisham Cholakkal, Salman Khan, Rao Muhammad Anwer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Understanding historical and cultural artifacts demands human expertise and
advanced computational techniques, yet the process remains complex and
time-intensive. While large multimodal models offer promising support, their
evaluation and improvement require a standardized benchmark. To address this,
we introduce TimeTravel, a benchmark of 10,250 expert-verified samples spanning
266 distinct cultures across 10 major historical regions. Designed for
AI-driven analysis of manuscripts, artworks, inscriptions, and archaeological
discoveries, TimeTravel provides a structured dataset and robust evaluation
framework to assess AI models' capabilities in classification, interpretation,
and historical comprehension. By integrating AI with historical research,
TimeTravel fosters AI-powered tools for historians, archaeologists,
researchers, and cultural tourists to extract valuable insights while ensuring
technology contributes meaningfully to historical discovery and cultural
heritage preservation. We evaluate contemporary AI models on TimeTravel,
highlighting their strengths and identifying areas for improvement. Our goal is
to establish AI as a reliable partner in preserving cultural heritage, ensuring
that technological advancements contribute meaningfully to historical
discovery. Our code is available at:
\url{https://github.com/mbzuai-oryx/TimeTravel}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>4 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Benchmarking Multimodal RAG through a Chart-based Document
  Question-Answering Generation Framework 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.14864v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.14864v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuming Yang, Jiang Zhong, Li Jin, Jingwang Huang, Jingpeng Gao, Qing Liu, Yang Bai, Jingyuan Zhang, Rui Jiang, Kaiwen Wei
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multimodal Retrieval-Augmented Generation (MRAG) enhances reasoning
capabilities by integrating external knowledge. However, existing benchmarks
primarily focus on simple image-text interactions, overlooking complex visual
formats like charts that are prevalent in real-world applications. In this
work, we introduce a novel task, Chart-based MRAG, to address this limitation.
To semi-automatically generate high-quality evaluation samples, we propose
CHARt-based document question-answering GEneration (CHARGE), a framework that
produces evaluation data through structured keypoint extraction, crossmodal
verification, and keypoint-based generation. By combining CHARGE with expert
validation, we construct Chart-MRAG Bench, a comprehensive benchmark for
chart-based MRAG evaluation, featuring 4,738 question-answering pairs across 8
domains from real-world documents. Our evaluation reveals three critical
limitations in current approaches: (1) unified multimodal embedding retrieval
methods struggles in chart-based scenarios, (2) even with ground-truth
retrieval, state-of-the-art MLLMs achieve only 58.19% Correctness and 73.87%
Coverage scores, and (3) MLLMs demonstrate consistent text-over-visual modality
bias during Chart-based MRAG reasoning. The CHARGE and Chart-MRAG Bench are
released at https://github.com/Nomothings/CHARGE.git.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Scaling Text-Rich Image Understanding via Code-Guided Synthetic
  Multimodal Data Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.14846v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.14846v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yue Yang, Ajay Patel, Matt Deitke, Tanmay Gupta, Luca Weihs, Andrew Head, Mark Yatskar, Chris Callison-Burch, Ranjay Krishna, Aniruddha Kembhavi, Christopher Clark
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reasoning about images with rich text, such as charts and documents, is a
critical application of vision-language models (VLMs). However, VLMs often
struggle in these domains due to the scarcity of diverse text-rich
vision-language data. To address this challenge, we present CoSyn, a framework
that leverages the coding capabilities of text-only large language models
(LLMs) to automatically create synthetic text-rich multimodal data. Given input
text describing a target domain (e.g., "nutrition fact labels"), CoSyn prompts
an LLM to generate code (Python, HTML, LaTeX, etc.) for rendering synthetic
images. With the underlying code as textual representations of the synthetic
images, CoSyn can generate high-quality instruction-tuning data, again relying
on a text-only LLM. Using CoSyn, we constructed a dataset comprising 400K
images and 2.7M rows of vision-language instruction-tuning data. Comprehensive
experiments on seven benchmarks demonstrate that models trained on our
synthetic data achieve state-of-the-art performance among competitive
open-source models, including Llama 3.2, and surpass proprietary models such as
GPT-4V and Gemini 1.5 Flash. Furthermore, CoSyn can produce synthetic pointing
data, enabling VLMs to ground information within input images, showcasing its
potential for developing multimodal agents capable of acting in real-world
environments.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>20 pages, 19 figures, 9 tables, website:
  https://yueyang1996.github.io/cosyn/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Dynamic Concepts Personalization from Single Videos 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.14844v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.14844v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rameen Abdal, Or Patashnik, Ivan Skorokhodov, Willi Menapace, Aliaksandr Siarohin, Sergey Tulyakov, Daniel Cohen-Or, Kfir Aberman
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Personalizing generative text-to-image models has seen remarkable progress,
but extending this personalization to text-to-video models presents unique
challenges. Unlike static concepts, personalizing text-to-video models has the
potential to capture dynamic concepts, i.e., entities defined not only by their
appearance but also by their motion. In this paper, we introduce
Set-and-Sequence, a novel framework for personalizing Diffusion Transformers
(DiTs)-based generative video models with dynamic concepts. Our approach
imposes a spatio-temporal weight space within an architecture that does not
explicitly separate spatial and temporal features. This is achieved in two key
stages. First, we fine-tune Low-Rank Adaptation (LoRA) layers using an
unordered set of frames from the video to learn an identity LoRA basis that
represents the appearance, free from temporal interference. In the second
stage, with the identity LoRAs frozen, we augment their coefficients with
Motion Residuals and fine-tune them on the full video sequence, capturing
motion dynamics. Our Set-and-Sequence framework results in a spatio-temporal
weight space that effectively embeds dynamic concepts into the video model's
output domain, enabling unprecedented editability and compositionality while
setting a new benchmark for personalizing dynamic concepts.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Webpage: https://snap-research.github.io/dynamic_concepts/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LongWriter-V: Enabling Ultra-Long and High-Fidelity Generation in
  Vision-Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.14834v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.14834v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shangqing Tu, Yucheng Wang, Daniel Zhang-Li, Yushi Bai, Jifan Yu, Yuhao Wu, Lei Hou, Huiqin Liu, Zhiyuan Liu, Bin Xu, Juanzi Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Existing Large Vision-Language Models (LVLMs) can process inputs with context
lengths up to 128k visual and text tokens, yet they struggle to generate
coherent outputs beyond 1,000 words. We find that the primary limitation is the
absence of long output examples during supervised fine-tuning (SFT). To tackle
this issue, we introduce LongWriter-V-22k, a SFT dataset comprising 22,158
examples, each with multiple input images, an instruction, and corresponding
outputs ranging from 0 to 10,000 words. Moreover, to achieve long outputs that
maintain high-fidelity to the input images, we employ Direct Preference
Optimization (DPO) to the SFT model. Given the high cost of collecting human
feedback for lengthy outputs (e.g., 3,000 words), we propose IterDPO, which
breaks long outputs into segments and uses iterative corrections to form
preference pairs with the original outputs. Additionally, we develop
MMLongBench-Write, a benchmark featuring six tasks to evaluate the
long-generation capabilities of VLMs. Our 7B parameter model, trained with
LongWriter-V-22k and IterDPO, achieves impressive performance on this
benchmark, outperforming larger proprietary models like GPT-4o. Code and data:
https://github.com/THU-KEG/LongWriter-V
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Improving the Diffusability of Autoencoders 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.14831v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.14831v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ivan Skorokhodov, Sharath Girish, Benran Hu, Willi Menapace, Yanyu Li, Rameen Abdal, Sergey Tulyakov, Aliaksandr Siarohin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Latent diffusion models have emerged as the leading approach for generating
high-quality images and videos, utilizing compressed latent representations to
reduce the computational burden of the diffusion process. While recent
advancements have primarily focused on scaling diffusion backbones and
improving autoencoder reconstruction quality, the interaction between these
components has received comparatively less attention. In this work, we perform
a spectral analysis of modern autoencoders and identify inordinate
high-frequency components in their latent spaces, which are especially
pronounced in the autoencoders with a large bottleneck channel size. We
hypothesize that this high-frequency component interferes with the
coarse-to-fine nature of the diffusion synthesis process and hinders the
generation quality. To mitigate the issue, we propose scale equivariance: a
simple regularization strategy that aligns latent and RGB spaces across
frequencies by enforcing scale equivariance in the decoder. It requires minimal
code changes and only up to 20K autoencoder fine-tuning steps, yet
significantly improves generation quality, reducing FID by 19% for image
generation on ImageNet-1K 256x256 and FVD by at least 44% for video generation
on Kinetics-700 17x256x256.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>26 pages, 22 figures, 9 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Exploring Advanced Techniques for Visual Question Answering: A
  Comprehensive Comparison 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.14827v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.14827v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aiswarya Baby, Tintu Thankom Koshy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Visual Question Answering (VQA) has emerged as a pivotal task in the
intersection of computer vision and natural language processing, requiring
models to understand and reason about visual content in response to natural
language questions. Analyzing VQA datasets is essential for developing robust
models that can handle the complexities of multimodal reasoning. Several
approaches have been developed to examine these datasets, each offering
distinct perspectives on question diversity, answer distribution, and
visual-textual correlations. Despite significant progress, existing VQA models
face challenges related to dataset bias, limited model complexity, commonsense
reasoning gaps, rigid evaluation methods, and generalization to real world
scenarios. This paper presents a comprehensive comparative study of five
advanced VQA models: ABC-CNN, KICNLE, Masked Vision and Language Modeling,
BLIP-2, and OFA, each employing distinct methodologies to address these
challenges.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, No figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ KITAB-Bench: A Comprehensive Multi-Domain Benchmark for Arabic OCR and
  Document Understanding <span class="chip">ACL 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.14949v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.14949v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ahmed Heakl, Abdullah Sohail, Mukul Ranjan, Rania Hossam, Ghazi Ahmed, Mohamed El-Geish, Omar Maher, Zhiqiang Shen, Fahad Khan, Salman Khan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the growing adoption of Retrieval-Augmented Generation (RAG) in document
processing, robust text recognition has become increasingly critical for
knowledge extraction. While OCR (Optical Character Recognition) for English and
other languages benefits from large datasets and well-established benchmarks,
Arabic OCR faces unique challenges due to its cursive script, right-to-left
text flow, and complex typographic and calligraphic features. We present
KITAB-Bench, a comprehensive Arabic OCR benchmark that fills the gaps in
current evaluation systems. Our benchmark comprises 8,809 samples across 9
major domains and 36 sub-domains, encompassing diverse document types including
handwritten text, structured tables, and specialized coverage of 21 chart types
for business intelligence. Our findings show that modern vision-language models
(such as GPT-4, Gemini, and Qwen) outperform traditional OCR approaches (like
EasyOCR, PaddleOCR, and Surya) by an average of 60% in Character Error Rate
(CER). Furthermore, we highlight significant limitations of current Arabic OCR
models, particularly in PDF-to-Markdown conversion, where the best model
Gemini-2.0-Flash achieves only 65% accuracy. This underscores the challenges in
accurately recognizing Arabic text, including issues with complex fonts,
numeral recognition errors, word elongation, and table structure detection.
This work establishes a rigorous evaluation framework that can drive
improvements in Arabic document analysis methods and bridge the performance gap
with English OCR technologies.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages, 5 figures, ACL 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FetalCLIP: A Visual-Language Foundation Model for Fetal Ultrasound Image
  Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.14807v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.14807v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fadillah Maani, Numan Saeed, Tausifa Saleem, Zaid Farooq, Hussain Alasmawi, Werner Diehl, Ameera Mohammad, Gareth Waring, Saudabi Valappi, Leanne Bricker, Mohammad Yaqub
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Foundation models are becoming increasingly effective in the medical domain,
offering pre-trained models on large datasets that can be readily adapted for
downstream tasks. Despite progress, fetal ultrasound images remain a
challenging domain for foundation models due to their inherent complexity,
often requiring substantial additional training and facing limitations due to
the scarcity of paired multimodal data. To overcome these challenges, here we
introduce FetalCLIP, a vision-language foundation model capable of generating
universal representation of fetal ultrasound images. FetalCLIP was pre-trained
using a multimodal learning approach on a diverse dataset of 210,035 fetal
ultrasound images paired with text. This represents the largest paired dataset
of its kind used for foundation model development to date. This unique training
approach allows FetalCLIP to effectively learn the intricate anatomical
features present in fetal ultrasound images, resulting in robust
representations that can be used for a variety of downstream applications. In
extensive benchmarking across a range of key fetal ultrasound applications,
including classification, gestational age estimation, congenital heart defect
(CHD) detection, and fetal structure segmentation, FetalCLIP outperformed all
baselines while demonstrating remarkable generalizability and strong
performance even with limited labeled data. We plan to release the FetalCLIP
model publicly for the benefit of the broader scientific community.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A <span class="highlight-title">Survey</span> on Text-Driven 360-Degree Panorama Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.14799v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.14799v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hai Wang, Xiaoyu Xiang, Weihao Xia, Jing-Hao Xue
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The advent of text-driven 360-degree panorama generation, enabling the
synthesis of 360-degree panoramic images directly from textual descriptions,
marks a transformative advancement in immersive visual content creation. This
innovation significantly simplifies the traditionally complex process of
producing such content. Recent progress in text-to-image diffusion models has
accelerated the rapid development in this emerging field. This survey presents
a comprehensive review of text-driven 360-degree panorama generation, offering
an in-depth analysis of state-of-the-art algorithms and their expanding
applications in 360-degree 3D scene generation. Furthermore, we critically
examine current limitations and propose promising directions for future
research. A curated project page with relevant resources and research papers is
available at https://littlewhitesea.github.io/Text-Driven-Pano-Gen/.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ RendBEV: Semantic Novel View Synthesis for <span class="highlight-title">Self-Supervised</span> Bird's Eye
  View Segmentation <span class="chip">WACV 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.14792v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.14792v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Henrique Piñeiro Monteagudo, Leonardo Taccari, Aurel Pjetri, Francesco Sambo, Samuele Salti
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Bird's Eye View (BEV) semantic maps have recently garnered a lot of attention
as a useful representation of the environment to tackle assisted and autonomous
driving tasks. However, most of the existing work focuses on the fully
supervised setting, training networks on large annotated datasets. In this
work, we present RendBEV, a new method for the self-supervised training of BEV
semantic segmentation networks, leveraging differentiable volumetric rendering
to receive supervision from semantic perspective views computed by a 2D
semantic segmentation model. Our method enables zero-shot BEV semantic
segmentation, and already delivers competitive results in this challenging
setting. When used as pretraining to then fine-tune on labeled BEV
ground-truth, our method significantly boosts performance in low-annotation
regimes, and sets a new state of the art when fine-tuning on all available
labels.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at WACV 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Structurally Disentangled Feature Fields Distillation for 3D
  Understanding and Editing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.14789v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.14789v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yoel Levy, David Shavin, Itai Lang, Sagie Benaim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent work has demonstrated the ability to leverage or distill pre-trained
2D features obtained using large pre-trained 2D models into 3D features,
enabling impressive 3D editing and understanding capabilities using only 2D
supervision. Although impressive, models assume that 3D features are captured
using a single feature field and often make a simplifying assumption that
features are view-independent. In this work, we propose instead to capture 3D
features using multiple disentangled feature fields that capture different
structural components of 3D features involving view-dependent and
view-independent components, which can be learned from 2D feature supervision
only. Subsequently, each element can be controlled in isolation, enabling
semantic and structural understanding and editing capabilities. For instance,
using a user click, one can segment 3D features corresponding to a given object
and then segment, edit, or remove their view-dependent (reflective) properties.
We evaluate our approach on the task of 3D segmentation and demonstrate a set
of novel understanding and editing tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SigLIP 2: Multilingual Vision-Language Encoders with Improved Semantic
  Understanding, Localization, and Dense Features 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.14786v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.14786v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Michael Tschannen, Alexey Gritsenko, Xiao Wang, Muhammad Ferjad Naeem, Ibrahim Alabdulmohsin, Nikhil Parthasarathy, Talfan Evans, Lucas Beyer, Ye Xia, Basil Mustafa, Olivier Hénaff, Jeremiah Harmsen, Andreas Steiner, Xiaohua Zhai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce SigLIP 2, a family of new multilingual vision-language encoders
that build on the success of the original SigLIP. In this second iteration, we
extend the original image-text training objective with several prior,
independently developed techniques into a unified recipe -- this includes
captioning-based pretraining, self-supervised losses (self-distillation, masked
prediction) and online data curation. With these changes, SigLIP 2 models
outperform their SigLIP counterparts at all model scales in core capabilities,
including zero-shot classification, image-text retrieval, and transfer
performance when extracting visual representations for Vision-Language Models
(VLMs). Furthermore, the new training recipe leads to significant improvements
on localization and dense prediction tasks. We also train variants which
support multiple resolutions and preserve the input's native aspect ratio.
Finally, we train on a more diverse data-mixture that includes de-biasing
techniques, leading to much better multilingual understanding and improved
fairness. To allow users to trade off inference cost with performance, we
release model checkpoints at four sizes: ViT-B (86M), L (303M), So400m (400M),
and g (1B).
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Model checkpoints are available at
  https://github.com/google-research/big_vision/tree/main/big_vision/configs/proj/image_text/README_siglip2.md</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ReVision: A <span class="highlight-title">Dataset</span> and Baseline VLM for Privacy-Preserving
  Task-Oriented Visual Instruction Rewriting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.14780v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.14780v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Abhijit Mishra, Richard Noh, Hsiang Fu, Mingda Li, Minji Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Efficient and privacy-preserving multimodal interaction is essential as AR,
VR, and modern smartphones with powerful cameras become primary interfaces for
human-computer communication. Existing powerful large vision-language models
(VLMs) enabling multimodal interaction often rely on cloud-based processing,
raising significant concerns about (1) visual privacy by transmitting sensitive
vision data to servers, and (2) their limited real-time, on-device usability.
This paper explores Visual Instruction Rewriting, a novel approach that
transforms multimodal instructions into text-only commands, allowing seamless
integration of lightweight on-device instruction rewriter VLMs (250M
parameters) with existing conversational AI systems, enhancing vision data
privacy. To achieve this, we present a dataset of over 39,000 examples across
14 domains and develop a compact VLM, pretrained on image captioning datasets
and fine-tuned for instruction rewriting. Experimental results, evaluated
through NLG metrics such as BLEU, METEOR, and ROUGE, along with semantic
parsing analysis, demonstrate that even a quantized version of the model
(<500MB storage footprint) can achieve effective instruction rewriting, thus
enabling privacy-focused, multimodal AI applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 7 figures, 3 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DC-ControlNet: Decoupling Inter- and Intra-Element Conditions in Image
  Generation with Diffusion Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.14779v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.14779v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hongji Yang, Wencheng Han, Yucheng Zhou, Jianbing Shen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we introduce DC (Decouple)-ControlNet, a highly flexible and
precisely controllable framework for multi-condition image generation. The core
idea behind DC-ControlNet is to decouple control conditions, transforming
global control into a hierarchical system that integrates distinct elements,
contents, and layouts. This enables users to mix these individual conditions
with greater flexibility, leading to more efficient and accurate image
generation control. Previous ControlNet-based models rely solely on global
conditions, which affect the entire image and lack the ability of element- or
region-specific control. This limitation reduces flexibility and can cause
condition misunderstandings in multi-conditional image generation. To address
these challenges, we propose both intra-element and Inter-element Controllers
in DC-ControlNet. The Intra-Element Controller handles different types of
control signals within individual elements, accurately describing the content
and layout characteristics of the object. For interactions between elements, we
introduce the Inter-Element Controller, which accurately handles multi-element
interactions and occlusion based on user-defined relationships. Extensive
evaluations show that DC-ControlNet significantly outperforms existing
ControlNet models and Layout-to-Image generative models in terms of control
flexibility and precision in multi-condition control.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Harnessing PDF Data for Improving Japanese Large Multimodal Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.14778v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.14778v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jeonghun Baek, Akiko Aizawa, Kiyoharu Aizawa
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Multimodal Models (LMMs) have demonstrated strong performance in
English, but their effectiveness in Japanese remains limited due to the lack of
high-quality training data. Current Japanese LMMs often rely on translated
English datasets, restricting their ability to capture Japan-specific cultural
knowledge. To address this, we explore the potential of Japanese PDF data as a
training resource, an area that remains largely underutilized. We introduce a
fully automated pipeline that leverages pretrained models to extract image-text
pairs from PDFs through layout analysis, OCR, and vision-language pairing,
removing the need for manual annotation. Additionally, we construct instruction
data from extracted image-text pairs to enrich the training data. To evaluate
the effectiveness of PDF-derived data, we train Japanese LMMs and assess their
performance on the Japanese LMM Benchmark. Our results demonstrate substantial
improvements, with performance gains ranging from 3.9% to 13.8% on Heron-Bench.
Further analysis highlights the impact of PDF-derived data on various factors,
such as model size and language models, reinforcing its value as a multimodal
resource for Japanese LMMs. We plan to make the source code and data publicly
available upon acceptance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages, 8 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Design of a Visual Pose Estimation Algorithm for Moon Landing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.14942v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.14942v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Atakan Süslü, Betül Rana Kuran, Halil Ersin Söken
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In order to make a pinpoint landing on the Moon, the spacecraft's navigation
system must be accurate. To achieve the desired accuracy, navigational drift
caused by the inertial sensors must be corrected. One way to correct this drift
is to use absolute navigation solutions. In this study, a terrain absolute
navigation method to estimate the spacecraft's position and attitude is
proposed. This algorithm uses the position of the craters below the spacecraft
for estimation. Craters seen by the camera onboard the spacecraft are detected
and identified using a crater database known beforehand. In order to focus on
estimation algorithms, image processing and crater matching steps are skipped.
The accuracy of the algorithm and the effect of the crater number used for
estimation are inspected by performing simulations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, 8 figures, Presented in 11th Nano-Satellite Symposium</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Sculpting [CLS] Features for <span class="highlight-title">Pre-Train</span>ed Model-Based Class-Incremental
  Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.14762v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.14762v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Murat Onur Yildirim, Elif Ceren Gok Yildirim, Joaquin Vanschoren
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Class-incremental learning requires models to continually acquire knowledge
of new classes without forgetting old ones. Although pre-trained models have
demonstrated strong performance in class-incremental learning, they remain
susceptible to catastrophic forgetting when learning new concepts. Excessive
plasticity in the models breaks generalizability and causes forgetting, while
strong stability results in insufficient adaptation to new classes. This
necessitates effective adaptation with minimal modifications to preserve the
general knowledge of pre-trained models. To address this challenge, we first
introduce a new parameter-efficient fine-tuning module 'Learn and Calibrate',
or LuCA, designed to acquire knowledge through an adapter-calibrator couple,
enabling effective adaptation with well-refined feature representations.
Second, for each learning session, we deploy a sparse LuCA module on top of the
last token just before the classifier, which we refer to as 'Token-level Sparse
Calibration and Adaptation', or TOSCA. This strategic design improves the
orthogonality between the modules and significantly reduces both training and
inference complexity. By leaving the generalization capabilities of the
pre-trained models intact and adapting exclusively via the last token, our
approach achieves a harmonious balance between stability and plasticity.
Extensive experiments demonstrate TOSCA's state-of-the-art performance while
introducing ~8 times fewer parameters compared to prior methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FacaDiffy: Inpainting Unseen Facade Parts Using Diffusion Models <span class="chip">SP</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.14940v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.14940v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Thomas Froech, Olaf Wysocki, Yan Xia, Junyu Xie, Benedikt Schwab, Daniel Cremers, Thomas H. Kolbe
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  High-detail semantic 3D building models are frequently utilized in robotics,
geoinformatics, and computer vision. One key aspect of creating such models is
employing 2D conflict maps that detect openings' locations in building facades.
Yet, in reality, these maps are often incomplete due to obstacles encountered
during laser scanning. To address this challenge, we introduce FacaDiffy, a
novel method for inpainting unseen facade parts by completing conflict maps
with a personalized Stable Diffusion model. Specifically, we first propose a
deterministic ray analysis approach to derive 2D conflict maps from existing 3D
building models and corresponding laser scanning point clouds. Furthermore, we
facilitate the inpainting of unseen facade objects into these 2D conflict maps
by leveraging the potential of personalizing a Stable Diffusion model. To
complement the scarcity of real-world training data, we also develop a scalable
pipeline to produce synthetic conflict maps using random city model generators
and annotated facade images. Extensive experiments demonstrate that FacaDiffy
achieves state-of-the-art performance in conflict map completion compared to
various inpainting baselines and increases the detection rate by $22\%$ when
applying the completed conflict maps for high-definition 3D semantic building
reconstruction. The code is be publicly available in the corresponding GitHub
repository: https://github.com/ThomasFroech/InpaintingofUnseenFacadeObjects
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for GeoSpatial Week 2025, ISPRS Annals</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Online hand gesture recognition using Continual Graph <span class="highlight-title">Transformer</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.14939v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.14939v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rim Slama, Wael Rabah, Hazem Wannous
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Online continuous action recognition has emerged as a critical research area
due to its practical implications in real-world applications, such as
human-computer interaction, healthcare, and robotics. Among various modalities,
skeleton-based approaches have gained significant popularity, demonstrating
their effectiveness in capturing 3D temporal data while ensuring robustness to
environmental variations. However, most existing works focus on segment-based
recognition, making them unsuitable for real-time, continuous recognition
scenarios. In this paper, we propose a novel online recognition system designed
for real-time skeleton sequence streaming. Our approach leverages a hybrid
architecture combining Spatial Graph Convolutional Networks (S-GCN) for spatial
feature extraction and a Transformer-based Graph Encoder (TGE) for capturing
temporal dependencies across frames. Additionally, we introduce a continual
learning mechanism to enhance model adaptability to evolving data
distributions, ensuring robust recognition in dynamic environments. We evaluate
our method on the SHREC'21 benchmark dataset, demonstrating its superior
performance in online hand gesture recognition. Our approach not only achieves
state-of-the-art accuracy but also significantly reduces false positive rates,
making it a compelling solution for real-time applications. The proposed system
can be seamlessly integrated into various domains, including human-robot
collaboration and assistive technologies, where natural and intuitive
interaction is crucial.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MIM-Refiner: A Contrastive Learning Boost from Intermediate <span class="highlight-title">Pre-Train</span>ed
  Representations <span class="chip">ICLR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.10093v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.10093v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Benedikt Alkin, Lukas Miklautz, Sepp Hochreiter, Johannes Brandstetter
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce MIM (Masked Image Modeling)-Refiner, a contrastive learning
boost for pre-trained MIM models. MIM-Refiner is motivated by the insight that
strong representations within MIM models generally reside in intermediate
layers. Accordingly, MIM-Refiner leverages multiple contrastive heads that are
connected to different intermediate layers. In each head, a modified nearest
neighbor objective constructs semantic clusters that capture semantic
information which improves performance on downstream tasks, including
off-the-shelf and fine-tuning settings.
  The refinement process is short and simple - yet highly effective. Within a
few epochs, we refine the features of MIM models from subpar to
state-of-the-art, off-the-shelf features. Refining a ViT-H, pre-trained with
data2vec 2.0 on ImageNet-1K, sets a new state-of-the-art in linear probing
(84.7%) and low-shot classification among models that are pre-trained on
ImageNet-1K. MIM-Refiner efficiently combines the advantages of MIM and ID
objectives and compares favorably against previous state-of-the-art SSL models
on a variety of benchmarks such as low-shot classification, long-tailed
classification, clustering and semantic segmentation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published as a conference paper at ICLR 2025. Github:
  https://github.com/ml-jku/MIM-Refiner</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Vision-LSTM: xLSTM as Generic Vision Backbone <span class="chip">ICLR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.04303v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.04303v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Benedikt Alkin, Maximilian Beck, Korbinian Pöppel, Sepp Hochreiter, Johannes Brandstetter
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Transformers are widely used as generic backbones in computer vision, despite
initially introduced for natural language processing. Recently, the Long
Short-Term Memory (LSTM) has been extended to a scalable and performant
architecture - the xLSTM - which overcomes long-standing LSTM limitations via
exponential gating and parallelizable matrix memory structure. In this report,
we introduce Vision-LSTM (ViL), an adaption of the xLSTM building blocks to
computer vision. ViL comprises a stack of xLSTM blocks where odd blocks process
the sequence of patch tokens from top to bottom while even blocks go from
bottom to top. Experiments show that ViL holds promise to be further deployed
as new generic backbone for computer vision architectures.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published as a conference paper at ICLR 2025, Github:
  https://github.com/NX-AI/vision-lstm</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PuzzleFusion++: Auto-agglomerative 3D Fracture Assembly by Denoise and
  Verify 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.00259v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.00259v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhengqing Wang, Jiacheng Chen, Yasutaka Furukawa
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper proposes a novel "auto-agglomerative" 3D fracture assembly method,
PuzzleFusion++, resembling how humans solve challenging spatial puzzles.
Starting from individual fragments, the approach 1) aligns and merges fragments
into larger groups akin to agglomerative clustering and 2) repeats the process
iteratively in completing the assembly akin to auto-regressive methods.
Concretely, a diffusion model denoises the 6-DoF alignment parameters of the
fragments simultaneously, and a transformer model verifies and merges pairwise
alignments into larger ones, whose process repeats iteratively. Extensive
experiments on the Breaking Bad dataset show that PuzzleFusion++ outperforms
all other state-of-the-art techniques by significant margins across all
metrics, in particular by over 10% in part accuracy and 50% in Chamfer
distance. The code will be available on our project page:
https://puzzlefusion-plusplus.github.io.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://puzzlefusion-plusplus.github.io</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ FLINT: Learning-based Flow Estimation and Temporal Interpolation for
  Scientific Ensemble Visualization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.19178v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.19178v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hamid Gadirov, Jos B. T. M. Roerdink, Steffen Frey
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present FLINT (learning-based FLow estimation and temporal INTerpolation),
a novel deep learning-based approach to estimate flow fields for 2D+time and
3D+time scientific ensemble data. FLINT can flexibly handle different types of
scenarios with (1) a flow field being partially available for some members
(e.g., omitted due to space constraints) or (2) no flow field being available
at all (e.g., because it could not be acquired during an experiment). The
design of our architecture allows to flexibly cater to both cases simply by
adapting our modular loss functions, effectively treating the different
scenarios as flow-supervised and flow-unsupervised problems, respectively (with
respect to the presence or absence of ground-truth flow). To the best of our
knowledge, FLINT is the first approach to perform flow estimation from
scientific ensembles, generating a corresponding flow field for each discrete
timestep, even in the absence of original flow information. Additionally, FLINT
produces high-quality temporal interpolants between scalar fields. FLINT
employs several neural blocks, each featuring several convolutional and
deconvolutional layers. We demonstrate performance and accuracy for different
usage scenarios with scientific ensembles from both simulations and
experiments.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ RUN: Reversible Unfolding Network for Concealed Object Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.18783v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.18783v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chunming He, Rihan Zhang, Fengyang Xiao, Chengyu Fang, Longxiang Tang, Yulun Zhang, Linghe Kong, Deng-Ping Fan, Kai Li, Sina Farsiu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Existing concealed object segmentation (COS) methods frequently utilize
reversible strategies to address uncertain regions. However, these approaches
are typically restricted to the mask domain, leaving the potential of the RGB
domain underexplored. To address this, we propose the Reversible Unfolding
Network (RUN), which applies reversible strategies across both mask and RGB
domains through a theoretically grounded framework, enabling accurate
segmentation. RUN first formulates a novel COS model by incorporating an extra
residual sparsity constraint to minimize segmentation uncertainties. The
iterative optimization steps of the proposed model are then unfolded into a
multistage network, with each step corresponding to a stage. Each stage of RUN
consists of two reversible modules: the Segmentation-Oriented Foreground
Separation (SOFS) module and the Reconstruction-Oriented Background Extraction
(ROBE) module. SOFS applies the reversible strategy at the mask level and
introduces Reversible State Space to capture non-local information. ROBE
extends this to the RGB domain, employing a reconstruction network to address
conflicting foreground and background regions identified as distortion-prone
areas, which arise from their separate estimation by independent modules. As
the stages progress, RUN gradually facilitates reversible modeling of
foreground and background in both the mask and RGB domains, directing the
network's attention to uncertain regions and mitigating false-positive and
false-negative results. Extensive experiments demonstrate the superior
performance of RUN and highlight the potential of unfolding-based frameworks
for COS and other high-level vision tasks. We will release the code and models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 tables, 8 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DGNN-YOLO: Interpretable Dynamic Graph Neural Networks with YOLO11 for
  Small Occluded Object Detection and Tracking 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.17251v6">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.17251v6.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shahriar Soudeep, M. F. Mridha, Md Abrar Jahin, Nilanjan Dey
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The detection and tracking of small, occluded objects such as pedestrians,
cyclists, and motorbikes pose significant challenges for traffic surveillance
systems because of their erratic movement, frequent occlusion, and poor
visibility in dynamic urban environments. Traditional methods like YOLO11,
while proficient in spatial feature extraction for precise detection, often
struggle with these small and dynamically moving objects, particularly in
handling real-time data updates and resource efficiency. This paper introduces
DGNN-YOLO, a novel framework that integrates dynamic graph neural networks
(DGNNs) with YOLO11 to address these limitations. Unlike standard GNNs, DGNNs
are chosen for their superior ability to dynamically update graph structures in
real-time, which enables adaptive and robust tracking of objects in highly
variable urban traffic scenarios. This framework constructs and regularly
updates its graph representations, capturing objects as nodes and their
interactions as edges, thus effectively responding to rapidly changing
conditions. Additionally, DGNN-YOLO incorporates Grad-CAM, Grad-CAM++, and
Eigen-CAM visualization techniques to enhance interpretability and foster
trust, offering insights into the model's decision-making process. Extensive
experiments validate the framework's performance, achieving a precision of
0.8382, recall of 0.6875, and mAP@0.5:0.95 of 0.6476, significantly
outperforming existing methods. This study offers a scalable and interpretable
solution for real-time traffic surveillance and significantly advances
intelligent transportation systems' capabilities by addressing the critical
challenge of detecting and tracking small, occluded objects.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MUSE: Mamba is Efficient Multi-scale Learner for Text-video Retrieval <span class="chip">AAAI 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.10575v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.10575v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haoran Tang, Meng Cao, Jinfa Huang, Ruyang Liu, Peng Jin, Ge Li, Xiaodan Liang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Text-Video Retrieval (TVR) aims to align and associate relevant video content
with corresponding natural language queries. Most existing TVR methods are
based on large-scale pre-trained vision-language models (e.g., CLIP). However,
due to the inherent plain structure of CLIP, few TVR methods explore the
multi-scale representations which offer richer contextual information for a
more thorough understanding. To this end, we propose MUSE, a multi-scale mamba
with linear computational complexity for efficient cross-resolution modeling.
Specifically, the multi-scale representations are generated by applying a
feature pyramid on the last single-scale feature map. Then, we employ the Mamba
structure as an efficient multi-scale learner to jointly learn scale-wise
representations. Furthermore, we conduct comprehensive studies to investigate
different model structures and designs. Extensive results on three popular
benchmarks have validated the superiority of MUSE.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by AAAI 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ VidStyleODE: Disentangled Video Editing via StyleGAN and NeuralODEs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2304.06020v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2304.06020v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Moayed Haji Ali, Andrew Bond, Tolga Birdal, Duygu Ceylan, Levent Karacan, Erkut Erdem, Aykut Erdem
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose $\textbf{VidStyleODE}$, a spatiotemporally continuous disentangled
$\textbf{Vid}$eo representation based upon $\textbf{Style}$GAN and
Neural-$\textbf{ODE}$s. Effective traversal of the latent space learned by
Generative Adversarial Networks (GANs) has been the basis for recent
breakthroughs in image editing. However, the applicability of such advancements
to the video domain has been hindered by the difficulty of representing and
controlling videos in the latent space of GANs. In particular, videos are
composed of content (i.e., appearance) and complex motion components that
require a special mechanism to disentangle and control. To achieve this,
VidStyleODE encodes the video content in a pre-trained StyleGAN $\mathcal{W}_+$
space and benefits from a latent ODE component to summarize the spatiotemporal
dynamics of the input video. Our novel continuous video generation process then
combines the two to generate high-quality and temporally consistent videos with
varying frame rates. We show that our proposed method enables a variety of
applications on real videos: text-guided appearance manipulation, motion
manipulation, image animation, and video interpolation and extrapolation.
Project website: https://cyberiada.github.io/VidStyleODE
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project website: https://cyberiada.github.io/VidStyleODE</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Robin3D: Improving 3D Large Language Model via Robust Instruction Tuning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.00255v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.00255v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Weitai Kang, Haifeng Huang, Yuzhang Shang, Mubarak Shah, Yan Yan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in 3D Large Language Models (3DLLMs) have highlighted
their potential in building general-purpose agents in the 3D real world, yet
challenges remain due to the lack of high-quality robust instruction-following
data, leading to limited discriminative power and generalization of 3DLLMs. In
this paper, we introduce Robin3D, a powerful 3DLLM trained on large-scale
instruction-following data generated by our novel data engine, Robust
Instruction Generation (RIG) engine. RIG generates two key instruction data: 1)
the Adversarial Instruction-following data, which features mixed negative and
positive samples to enhance the model's discriminative understanding. 2) the
Diverse Instruction-following data, which contains various instruction styles
to enhance model's generalization. As a result, we construct 1 million
instruction-following data, consisting of 344K Adversarial samples, 508K
Diverse samples, and 165K benchmark training set samples. To better handle
these complex instructions, Robin3D first incorporates Relation-Augmented
Projector to enhance spatial understanding, and then strengthens the object
referring and grounding ability through ID-Feature Bonding. Robin3D
consistently outperforms previous methods across five widely-used 3D multimodal
learning benchmarks, without the need for task-specific fine-tuning. Notably,
we achieve a 7.8\% improvement in the grounding task (Multi3DRefer) and a 6.9\%
improvement in the captioning task (Scan2Cap).
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Information Retrieval <span class="chip" style="font-size: 60%">20</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Is Relevance Propagated from Retriever to Generator in RAG? <span class="chip">ECIR'25</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.15025v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.15025v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fangzheng Tian, Debasis Ganguly, Craig Macdonald
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Retrieval Augmented Generation (RAG) is a framework for incorporating
external knowledge, usually in the form of a set of documents retrieved from a
collection, as a part of a prompt to a large language model (LLM) to
potentially improve the performance of a downstream task, such as question
answering. Different from a standard retrieval task's objective of maximising
the relevance of a set of top-ranked documents, a RAG system's objective is
rather to maximise their total utility, where the utility of a document
indicates whether including it as a part of the additional contextual
information in an LLM prompt improves a downstream task. Existing studies
investigate the role of the relevance of a RAG context for knowledge-intensive
language tasks (KILT), where relevance essentially takes the form of answer
containment. In contrast, in our work, relevance corresponds to that of topical
overlap between a query and a document for an information seeking task.
Specifically, we make use of an IR test collection to empirically investigate
whether a RAG context comprised of topically relevant documents leads to
improved downstream performance. Our experiments lead to the following
findings: (a) there is a small positive correlation between relevance and
utility; (b) this correlation decreases with increasing context sizes (higher
values of k in k-shot); and (c) a more effective retrieval model generally
leads to better downstream RAG performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>18 pages (including reference), 5 figures, 1 table, 48 references;
  this paper has been accepted by ECIR'25 as a full paper</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Interpretable Text Embeddings and Text Similarity Explanation: A Primer 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.14862v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.14862v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Juri Opitz, Lucas Möller, Andrianos Michail, Simon Clematide
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Text embeddings and text embedding models are a backbone of many AI and NLP
systems, particularly those involving search. However, interpretability
challenges persist, especially in explaining obtained similarity scores, which
is crucial for applications requiring transparency. In this paper, we give a
structured overview of interpretability methods specializing in explaining
those similarity scores, an emerging research area. We study the methods'
individual ideas and techniques, evaluating their potential for improving
interpretability of text embeddings and explaining predicted similarities.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A <span class="highlight-title">Survey</span> of Model Architectures in Information Retrieval 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.14822v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.14822v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhichao Xu, Fengran Mo, Zhiqi Huang, Crystina Zhang, Puxuan Yu, Bei Wang, Jimmy Lin, Vivek Srikumar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This survey examines the evolution of model architectures in information
retrieval (IR), focusing on two key aspects: backbone models for feature
extraction and end-to-end system architectures for relevance estimation. The
review intentionally separates architectural considerations from training
methodologies to provide a focused analysis of structural innovations in IR
systems.We trace the development from traditional term-based methods to modern
neural approaches, particularly highlighting the impact of transformer-based
models and subsequent large language models (LLMs). We conclude by discussing
emerging challenges and future directions, including architectural
optimizations for performance and scalability, handling of multimodal,
multilingual data, and adaptation to novel application domains beyond
traditional search paradigms.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Multi-Agent Perspective on Modern Information Retrieval 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.14796v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.14796v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haya Nachimovsky, Moshe Tennenholtz, Oren Kurland
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rise of large language models (LLMs) has introduced a new era in
information retrieval (IR), where queries and documents that were once assumed
to be generated exclusively by humans can now also be created by automated
agents. These agents can formulate queries, generate documents, and perform
ranking. This shift challenges some long-standing IR paradigms and calls for a
reassessment of both theoretical frameworks and practical methodologies. We
advocate for a multi-agent perspective to better capture the complex
interactions between query agents, document agents, and ranker agents. Through
empirical exploration of various multi-agent retrieval settings, we reveal the
significant impact of these interactions on system performance. Our findings
underscore the need to revisit classical IR paradigms and develop new
frameworks for more effective modeling and evaluation of modern retrieval
systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ EAGER-LLM: Enhancing Large Language Models as Recommenders through
  Exogenous Behavior-Semantic Integration <span class="chip">WWW 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.14735v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.14735v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Minjie Hong, Yan Xia, Zehan Wang, Jieming Zhu, Ye Wang, Sihang Cai, Xiaoda Yang, Quanyu Dai, Zhenhua Dong, Zhimeng Zhang, Zhou Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) are increasingly leveraged as foundational
backbones in the development of advanced recommender systems, offering enhanced
capabilities through their extensive knowledge and reasoning. Existing
llm-based recommender systems (RSs) often face challenges due to the
significant differences between the linguistic semantics of pre-trained LLMs
and the collaborative semantics essential for RSs. These systems use
pre-trained linguistic semantics but learn collaborative semantics from scratch
via the llm-Backbone. However, LLMs are not designed for recommendations,
leading to inefficient collaborative learning, weak result correlations, and
poor integration of traditional RS features. To address these challenges, we
propose EAGER-LLM, a decoder-only llm-based generative recommendation framework
that integrates endogenous and exogenous behavioral and semantic information in
a non-intrusive manner. Specifically, we propose 1)dual-source knowledge-rich
item indices that integrates indexing sequences for exogenous signals, enabling
efficient link-wide processing; 2)non-invasive multiscale alignment
reconstruction tasks guide the model toward a deeper understanding of both
collaborative and semantic signals; 3)an annealing adapter designed to finely
balance the model's recommendation performance with its comprehension
capabilities. We demonstrate EAGER-LLM's effectiveness through rigorous testing
on three public benchmarks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 6 figures, accpeted by WWW 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ From Knowledge Generation to Knowledge Verification: Examining the
  BioMedical Generative Capabilities of Chat<span class="highlight-title">GPT</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.14714v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.14714v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ahmed Abdeen Hamed, Byung Suk Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The generative capabilities of LLM models present opportunities in
accelerating tasks and concerns with the authenticity of the knowledge it
produces. To address the concerns, we present a computational approach that
systematically evaluates the factual accuracy of biomedical knowledge that an
LLM model has been prompted to generate. Our approach encompasses two
processes: the generation of disease-centric associations and the verification
of them using the semantic knowledge of the biomedical ontologies. Using
ChatGPT as the select LLM model, we designed a set of prompt-engineering
processes to generate linkages between diseases, drugs, symptoms, and genes to
establish grounds for assessments. Experimental results demonstrate high
accuracy in identifying disease terms (88%-97%), drug names (90%-91%), and
genetic information (88%-98%). The symptom term identification accuracy was
notably lower (49%-61%), as verified against the DOID, ChEBI, SYMPTOM, and GO
ontologies accordingly. The verification of associations reveals literature
coverage rates of (89%-91%) among disease-drug and disease-gene associations.
The low identification accuracy for symptom terms also contributed to the
verification of symptom-related associations (49%-62%).
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>26 pages, 6 figures, In Review with a Cell Press Journal</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ InstructAgent: Building User Controllable Recommender via LLM Agent <span class="chip">WWW2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.14662v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.14662v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wujiang Xu, Yunxiao Shi, Zujie Liang, Xuying Ning, Kai Mei, Kun Wang, Xi Zhu, Min Xu, Yongfeng Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Traditional recommender systems usually take the user-platform paradigm,
where users are directly exposed under the control of the platform's
recommendation algorithms. However, the defect of recommendation algorithms may
put users in very vulnerable positions under this paradigm. First, many
sophisticated models are often designed with commercial objectives in mind,
focusing on the platform's benefits, which may hinder their ability to protect
and capture users' true interests. Second, these models are typically optimized
using data from all users, which may overlook individual user's preferences.
Due to these shortcomings, users may experience several disadvantages under the
traditional user-platform direct exposure paradigm, such as lack of control
over the recommender system, potential manipulation by the platform, echo
chamber effects, or lack of personalization for less active users due to the
dominance of active users during collaborative learning. Therefore, there is an
urgent need to develop a new paradigm to protect user interests and alleviate
these issues. Recently, some researchers have introduced LLM agents to simulate
user behaviors, these approaches primarily aim to optimize platform-side
performance, leaving core issues in recommender systems unresolved. To address
these limitations, we propose a new user-agent-platform paradigm, where agent
serves as the protective shield between user and recommender system that
enables indirect exposure. To this end, we first construct four recommendation
datasets, denoted as $\dataset$, along with user instructions for each record.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>WWW2025@HCRS</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multi-Record Web Page Information Extraction From News Websites 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.14625v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.14625v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alexander Kustenkov, Maksim Varlamov, Alexander Yatskov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we focused on the problem of extracting information from web
pages containing many records, a task of growing importance in the era of
massive web data. Recently, the development of neural network methods has
improved the quality of information extraction from web pages. Nevertheless,
most of the research and datasets are aimed at studying detailed pages. This
has left multi-record "list pages" relatively understudied, despite their
widespread presence and practical significance.
  To address this gap, we created a large-scale, open-access dataset
specifically designed for list pages. This is the first dataset for this task
in the Russian language. Our dataset contains 13,120 web pages with news lists,
significantly exceeding existing datasets in both scale and complexity. Our
dataset contains attributes of various types, including optional and
multi-valued, providing a realistic representation of real-world list pages.
These features make our dataset a valuable resource for studying information
extraction from pages containing many records.
  Furthermore, we proposed our own multi-stage information extraction methods.
In this work, we explore and demonstrate several strategies for applying
MarkupLM to the specific challenges of multi-record web pages. Our experiments
validate the advantages of our methods.
  By releasing our dataset to the public, we aim to advance the field of
information extraction from multi-record pages.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Unstructured Evidence Attribution for Long Context Query Focused
  Summarization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.14409v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.14409v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dustin Wright, Zain Muhammad Mujahid, Lu Wang, Isabelle Augenstein, David Jurgens
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) are capable of generating coherent summaries
from very long contexts given a user query. Extracting and properly citing
evidence spans could help improve the transparency and reliability of these
summaries. At the same time, LLMs suffer from positional biases in terms of
which information they understand and attend to, which could affect evidence
citation. Whereas previous work has focused on evidence citation with
predefined levels of granularity (e.g. sentence, paragraph, document, etc.), we
propose the task of long-context query focused summarization with unstructured
evidence citation. We show how existing systems struggle to generate and
properly cite unstructured evidence from their context, and that evidence tends
to be "lost-in-the-middle". To help mitigate this, we create the Summaries with
Unstructured Evidence Text dataset (SUnsET), a synthetic dataset generated
using a novel domain-agnostic pipeline which can be used as supervision to
adapt LLMs to this task. We demonstrate across 5 LLMs of different sizes and 4
datasets with varying document types and lengths that LLMs adapted with SUnsET
data generate more relevant and factually consistent evidence than their base
models, extract evidence from more diverse locations in their context, and can
generate more relevant and consistent summaries.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>24 pages; 21 figures; 5 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Retrieval-Augmented Process Reward Model for Generalizable Mathematical
  Reasoning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.14361v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.14361v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiachen Zhu, Congmin Zheng, Jianghao Lin, Kounianhua Du, Ying Wen, Yong Yu, Jun Wang, Weinan Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While large language models (LLMs) have significantly advanced mathematical
reasoning, Process Reward Models (PRMs) have been developed to evaluate the
logical validity of reasoning steps. However, PRMs still struggle with
out-of-distribution (OOD) challenges. This paper identifies key OOD issues,
including step OOD, caused by differences in reasoning patterns across model
types and sizes, and question OOD, which arises from dataset shifts between
training data and real-world problems. To address these issues, we introduce
Retrieval-Augmented Process Reward Model (RetrievalPRM), a novel framework
designed to tackle these OOD issues. By utilizing a two-stage
retrieval-enhanced mechanism, RetrievalPRM retrieves semantically similar
questions and steps as a warmup, enhancing PRM's ability to evaluate target
steps and improving generalization and reasoning consistency across different
models and problem types. Our extensive experiments demonstrate that
RetrievalPRM outperforms existing baselines across multiple real-world
datasets. Our open-source contributions include a retrieval-enhanced dataset, a
tuning framework for PRM training, and the RetrievalPRM model, establishing a
new standard for PRM performance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Collaborative Jade Recognition System for Mobile Devices Based on
  Lightweight and Large Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.14332v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.14332v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhenyu Wang, Wenjia Li, Pengyu Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the widespread adoption and development of mobile devices, vision-based
recognition applications have become a hot topic in research. Jade, as an
important cultural heritage and artistic item, has significant applications in
fields such as jewelry identification and cultural relic preservation. However,
existing jade recognition systems still face challenges in mobile
implementation, such as limited computing resources, real-time requirements,
and accuracy issues. To address these challenges, this paper proposes a jade
recognition system based on size model collaboration, aiming to achieve
efficient and accurate jade identification using mobile devices such as
smartphones.First, we design a size model based on multi-scale image
processing, extracting key visual information by analyzing jade's dimensions,
shapes, and surface textures. Then, a collaborative multi-model classification
framework is built by combining deep learning and traditional computer vision
algorithms. This framework can effectively select and adjust models based on
different jade characteristics, providing high accuracy results across various
environments and devices.Experimental results show that the proposed system can
provide high recognition accuracy and fast processing time on mobile devices,
while consuming relatively low computational resources. The system not only
holds great application potential but also provides new ideas and technical
support for the intelligent development of jade identification.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Efficient AI in Practice: Training and Deployment of Efficient LLMs for
  Industry Applications 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.14305v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.14305v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kayhan Behdin, Yun Dai, Ata Fatahibaarzi, Aman Gupta, Qingquan Song, Shao Tang, Hejian Sang, Gregory Dexter, Sirou Zhu, Siyu Zhu, Tejas Dharamsi, Maziar Sanjabi, Vignesh Kothapalli, Hamed Firooz, Zhoutong Fu, Yihan Cao, Pin-Lun Hsu, Fedor Borisyuk, Zhipeng Wang, Rahul Mazumder, Natesh Pillai, Luke Simon
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have demonstrated remarkable performance across
a wide range of industrial applications, from search and recommendations to
generative tasks. Although scaling laws indicate that larger models generally
yield better generalization and performance, their substantial computational
requirements often render them impractical for many real-world scenarios at
scale. In this paper, we present methods and insights for training small
language models (SLMs) that deliver high performance and efficiency in
deployment. We focus on two key techniques: (1) knowledge distillation and (2)
model compression via quantization and pruning. These approaches enable SLMs to
retain much of the quality of their larger counterparts while significantly
reducing training, serving costs, and latency. We detail the impact of these
techniques on a variety of use cases at a large professional social network
platform and share deployment lessons - including hardware optimization
strategies that enhance speed and throughput for both predictive and
reasoning-based applications.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Less is More: On the Importance of Data Quality for Unit Test Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.14212v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.14212v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junwei Zhang, Xing Hu, Shan Gao, Xin Xia, David Lo, Shanping Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Unit testing is crucial for software development and maintenance. Effective
unit testing ensures and improves software quality, but writing unit tests is
time-consuming and labor-intensive. Recent studies have proposed deep learning
(DL) techniques or large language models (LLMs) to automate unit test
generation. These models are usually trained or fine-tuned on large-scale
datasets. Despite growing awareness of the importance of data quality, there
has been limited research on the quality of datasets used for test generation.
To bridge this gap, we systematically examine the impact of noise on the
performance of learning-based test generation models. We first apply the open
card sorting method to analyze the most popular and largest test generation
dataset, Methods2Test, to categorize eight distinct types of noise. Further, we
conduct detailed interviews with 17 domain experts to validate and assess the
importance, reasonableness, and correctness of the noise taxonomy. Then, we
propose CleanTest, an automated noise-cleaning framework designed to improve
the quality of test generation datasets. CleanTest comprises three filters: a
rule-based syntax filter, a rule-based relevance filter, and a model-based
coverage filter. To evaluate its effectiveness, we apply CleanTest on two
widely-used test generation datasets, i.e., Methods2Test and Atlas. Our
findings indicate that 43.52% and 29.65% of datasets contain noise,
highlighting its prevalence. Finally, we conduct comparative experiments using
four LLMs (i.e., CodeBERT, AthenaTest, StarCoder, and CodeLlama7B) to assess
the impact of noise on test generation performance. The results show that
filtering noise positively influences the test generation ability of the
models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Use of a Structured Knowledge Base Enhances Metadata Curation by Large
  Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.05893v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.05893v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sowmya S. Sundaram, Benjamin Solomon, Avani Khatri, Anisha Laumas, Purvesh Khatri, Mark A. Musen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Metadata play a crucial role in ensuring the findability, accessibility,
interoperability, and reusability of datasets. This paper investigates the
potential of large language models (LLMs), specifically GPT-4, to improve
adherence to metadata standards. We conducted experiments on 200 random data
records describing human samples relating to lung cancer from the NCBI
BioSample repository, evaluating GPT-4's ability to suggest edits for adherence
to metadata standards. We computed the adherence accuracy of field name-field
value pairs through a peer review process, and we observed a marginal average
improvement in adherence to the standard data dictionary from 79% to 80%
(p<0.5). We then prompted GPT-4 with domain information in the form of the
textual descriptions of CEDAR templates and recorded a significant improvement
to 97% from 79% (p<0.01). These results indicate that, while LLMs may not be
able to correct legacy metadata to ensure satisfactory adherence to standards
when unaided, they do show promise for use in automated metadata curation when
integrated with a structured knowledge base
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CFaiRLLM: Consumer Fairness Evaluation in Large-Language Model
  Recommender System 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.05668v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.05668v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yashar Deldjoo, Tommaso di Noia
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This work takes a critical stance on previous studies concerning fairness
evaluation in Large Language Model (LLM)-based recommender systems, which have
primarily assessed consumer fairness by comparing recommendation lists
generated with and without sensitive user attributes. Such approaches
implicitly treat discrepancies in recommended items as biases, overlooking
whether these changes might stem from genuine personalization aligned with the
true preferences of users. Moreover, these earlier studies typically address
single sensitive attributes in isolation, neglecting the complex interplay of
intersectional identities. In response to these shortcomings, we introduce
CFaiRLLM, an enhanced evaluation framework that not only incorporates true
preference alignment but also rigorously examines intersectional fairness by
considering overlapping sensitive attributes. Additionally, CFaiRLLM introduces
diverse user profile sampling strategies-random, top-rated, and
recency-focused-to better understand the impact of profile generation fed to
LLMs in light of inherent token limitations in these systems. Given that
fairness depends on accurately understanding users' tastes and preferences,
these strategies provide a more realistic assessment of fairness within
RecLLMs.
  To validate the efficacy of CFaiRLLM, we conducted extensive experiments
using MovieLens and LastFM datasets, applying various sampling strategies and
sensitive attribute configurations. The evaluation metrics include both item
similarity measures and true preference alignment considering both hit and
ranking (Jaccard Similarity and PRAG), thereby conducting a multifaceted
analysis of recommendation fairness.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ OmniThink: Expanding Knowledge Boundaries in Machine Writing through
  Thinking 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09751v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09751v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zekun Xi, Wenbiao Yin, Jizhan Fang, Jialong Wu, Runnan Fang, Ningyu Zhang, Jiang Yong, Pengjun Xie, Fei Huang, Huajun Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Machine writing with large language models often relies on
retrieval-augmented generation. However, these approaches remain confined
within the boundaries of the model's predefined scope, limiting the generation
of content with rich information. Specifically, vanilla-retrieved information
tends to lack depth, novelty, and suffers from redundancy, which negatively
impacts the quality of generated articles, leading to shallow, unoriginal, and
repetitive outputs. To address these issues, we propose OmniThink, a
slow-thinking machine writing framework that emulates the human-like process of
iterative expansion and reflection. The core idea behind OmniThink is to
simulate the cognitive behavior of learners as they slowly deepen their
knowledge of the topics. Experimental results demonstrate that OmniThink
improves the knowledge density of generated articles without compromising
metrics such as coherence and depth. Human evaluations and expert feedback
further highlight the potential of OmniThink to address real-world challenges
in the generation of long-form articles.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code is available at https://github.com/zjunlp/OmniThink</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Extracting Sentence Embeddings from <span class="highlight-title">Pretrain</span>ed <span class="highlight-title">Transformer</span> Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.08073v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.08073v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lukas Stankevičius, Mantas Lukoševičius
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Pre-trained transformer models shine in many natural language processing
tasks and therefore are expected to bear the representation of the input
sentence or text meaning. These sentence-level embeddings are also important in
retrieval-augmented generation. But do commonly used plain averaging or prompt
templates sufficiently capture and represent the underlying meaning? After
providing a comprehensive review of existing sentence embedding extraction and
refinement methods, we thoroughly test different combinations and our original
extensions of the most promising ones on pretrained models. Namely, given 110 M
parameters, BERT's hidden representations from multiple layers, and many
tokens, we try diverse ways to extract optimal sentence embeddings. We test
various token aggregation and representation post-processing techniques. We
also test multiple ways of using a general Wikitext dataset to complement
BERT's sentence embeddings. All methods are tested on eight Semantic Textual
Similarity (STS), six short text clustering, and twelve classification tasks.
We also evaluate our representation-shaping techniques on other static models,
including random token representations. Proposed representation extraction
methods improve the performance on STS and clustering tasks for all models
considered. Very high improvements for static token-based models, especially
random embeddings for STS tasks, almost reach the performance of BERT-derived
representations. Our work shows that the representation-shaping techniques
significantly improve sentence embeddings extracted from BERT-based and simple
baseline models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Postprint update</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Confidence-aware Fine-tuning of Sequential Recommendation Systems via
  Conformal Prediction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.08976v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.08976v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chen Wang, Fangxin Wang, Ruocheng Guo, Yueqing Liang, Philip S. Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In Sequential Recommendation Systems (SRecsys), traditional training
approaches that rely on Cross-Entropy (CE) loss often prioritize accuracy but
fail to align well with user satisfaction metrics. CE loss focuses on
maximizing the confidence of the ground truth item, which is challenging to
achieve universally across all users and sessions. It also overlooks the
practical acceptability of ranking the ground truth item within the top-$K$
positions, a common metric in SRecsys. To address this limitation, we propose
\textbf{CPFT}, a novel fine-tuning framework that integrates Conformal
Prediction (CP)-based losses with CE loss to optimize accuracy alongside
confidence that better aligns with widely used top-$K$ metrics. CPFT embeds CP
principles into the training loop using differentiable proxy losses and
computationally efficient calibration strategies, enabling the generation of
high-confidence prediction sets. These sets focus on items with high relevance
while maintaining robust coverage guarantees. Extensive experiments on five
real-world datasets and four distinct sequential models demonstrate that CPFT
improves precision metrics and confidence calibration. Our results highlight
the importance of confidence-aware fine-tuning in delivering accurate,
trustworthy recommendations that enhance user satisfaction.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CoSQA+: Pioneering the Multi-Choice Code Search Benchmark with
  Test-Driven Agents 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.11589v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.11589v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jing Gong, Yanghui Wu, Linxi Liang, Yanlin Wang, Jiachi Chen, Mingwei Liu, Zibin Zheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Semantic code search, retrieving code that matches a given natural language
query, is an important task to improve productivity in software engineering.
Existing code search datasets face limitations: they rely on human annotators
who assess code primarily through semantic understanding rather than functional
verification, leading to potential inaccuracies and scalability issues.
Additionally, current evaluation metrics often overlook the multi-choice nature
of code search. This paper introduces CoSQA+, pairing high-quality queries from
CoSQA with multiple suitable codes. We develop an automated pipeline featuring
multiple model-based candidate selections and the novel test-driven agent
annotation system. Among a single Large Language Model (LLM) annotator and
Python expert annotators (without test-based verification), agents leverage
test-based verification and achieve the highest accuracy of 96.4%. Through
extensive experiments, CoSQA+ has demonstrated superior quality over CoSQA.
Models trained on CoSQA+ exhibit improved performance. We provide the code and
data at https://github.com/DeepSoftwareAnalytics/CoSQA_Plus.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages, 4 figures, conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ TALKPLAY: Multimodal Music Recommendation with Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.13713v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.13713v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Seungheon Doh, Keunwoo Choi, Juhan Nam
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present TalkPlay, a multimodal music recommendation system that
reformulates the recommendation task as large language model token generation.
TalkPlay represents music through an expanded token vocabulary that encodes
multiple modalities - audio, lyrics, metadata, semantic tags, and playlist
co-occurrence. Using these rich representations, the model learns to generate
recommendations through next-token prediction on music recommendation
conversations, that requires learning the associations natural language query
and response, as well as music items. In other words, the formulation
transforms music recommendation into a natural language understanding task,
where the model's ability to predict conversation tokens directly optimizes
query-item relevance. Our approach eliminates traditional
recommendation-dialogue pipeline complexity, enabling end-to-end learning of
query-aware music recommendations. In the experiment, TalkPlay is successfully
trained and outperforms baseline methods in various aspects, demonstrating
strong context understanding as a conversational music recommender.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Multimedia <span class="chip" style="font-size: 60%">6</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Visual and Auditory Aesthetic Preferences Across Cultures 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.14439v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.14439v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Harin Lee, Eline Van Geert, Elif Celen, Raja Marjieh, Pol van Rijn, Minsu Park, Nori Jacoby
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Research on how humans perceive aesthetics in shapes, colours, and music has
predominantly focused on Western populations, limiting our understanding of how
cultural environments shape aesthetic preferences. We present a large-scale
cross-cultural study examining aesthetic preferences across five distinct
modalities extensively explored in the literature: shape, curvature, colour,
musical harmony and melody. Our investigation gathers 401,403 preference
judgements from 4,835 participants across 10 countries, systematically sampling
two-dimensional parameter spaces for each modality. The findings reveal both
universal patterns and cultural variations. Preferences for shape and curvature
cross-culturally demonstrate a consistent preference for symmetrical forms.
While colour preferences are categorically consistent, relational preferences
vary across cultures. Musical harmony shows strong agreement in interval
relationships despite differing regions of preference within the broad
frequency spectrum, while melody shows the highest cross-cultural variation.
These results suggest that aesthetic preferences emerge from an interplay
between shared perceptual mechanisms and cultural learning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submission to CogSci 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LLM-EvRep: Learning an LLM-Compatible Event Representation Using a
  <span class="highlight-title">Self-Supervised</span> Framework <span class="chip">WWW</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.14273v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.14273v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zongyou Yu, Qiang Qu, Qian Zhang, Nan Zhang, Xiaoming Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in event-based recognition have demonstrated significant
promise, yet most existing approaches rely on extensive training, limiting
their adaptability for efficient processing of event-driven visual content.
Meanwhile, large language models (LLMs) have exhibited remarkable zero-shot
capabilities across diverse domains, but their application to event-based
visual recognition remains largely unexplored. To bridge this gap, we propose
\textbf{LLM-EvGen}, an event representation generator that produces
LLM-compatible event representations \textbf{LLM-EvRep}, thereby enhancing the
performance of LLMs on event recognition tasks. The generator is trained using
a self-supervised framework, aligning the generated representations with
semantic consistency and structural fidelity. Comprehensive experiments were
conducted on three datasets: N-ImageNet, N-Caltech101, and N-MNIST. The results
demonstrate that our method, \textbf{LLM-EvRep}, outperforms the event-to-video
method, E2VID, by 15.93\%, 0.82\%, and 50.21\%, respectively, in recognition
tasks when evaluated using GPT-4o.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, 2 figures,Companion Proceedings of the ACM Web Conference
  2025 (WWW Companion '25)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ NeRF-3DTalker: Neural Radiance Field with 3D Prior Aided Audio
  Disentanglement for Talking Head Synthesis <span class="chip">ICASSP 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.14178v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.14178v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaoxing Liu, Zhilei Liu, Chongke Bi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Talking head synthesis is to synthesize a lip-synchronized talking head video
using audio. Recently, the capability of NeRF to enhance the realism and
texture details of synthesized talking heads has attracted the attention of
researchers. However, most current NeRF methods based on audio are exclusively
concerned with the rendering of frontal faces. These methods are unable to
generate clear talking heads in novel views. Another prevalent challenge in
current 3D talking head synthesis is the difficulty in aligning acoustic and
visual spaces, which often results in suboptimal lip-syncing of the generated
talking heads. To address these issues, we propose Neural Radiance Field with
3D Prior Aided Audio Disentanglement for Talking Head Synthesis
(NeRF-3DTalker). Specifically, the proposed method employs 3D prior information
to synthesize clear talking heads with free views. Additionally, we propose a
3D Prior Aided Audio Disentanglement module, which is designed to disentangle
the audio into two distinct categories: features related to 3D awarded speech
movements and features related to speaking style. Moreover, to reposition the
generated frames that are distant from the speaker's motion space in the real
space, we have devised a local-global Standardized Space. This method
normalizes the irregular positions in the generated frames from both global and
local semantic perspectives. Through comprehensive qualitative and quantitative
experiments, it has been demonstrated that our NeRF-3DTalker outperforms
state-of-the-art in synthesizing realistic talking head videos, exhibiting
superior image quality and lip synchronization. Project page:
https://nerf-3dtalker.github.io/NeRF-3Dtalker.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICASSP 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Sandwiched Compression: Repurposing Standard Codecs with Neural Network
  Wrappers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.05887v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.05887v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Onur G. Guleryuz, Philip A. Chou, Berivan Isik, Hugues Hoppe, Danhang Tang, Ruofei Du, Jonathan Taylor, Philip Davidson, Sean Fanello
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose sandwiching standard image and video codecs between pre- and
post-processing neural networks. The networks are jointly trained through a
differentiable codec proxy to minimize a given rate-distortion loss. This
sandwich architecture not only improves the standard codec's performance on its
intended content, but more importantly, adapts the codec to other types of
image/video content and to other distortion measures. The sandwich learns to
transmit ``neural code images'' that optimize and improve overall
rate-distortion performance, with the improvements becoming significant
especially when the overall problem is well outside of the scope of the codec's
design. We apply the sandwich architecture to standard codecs with mismatched
sources transporting different numbers of channels, higher resolution, higher
dynamic range, computer graphics, and with perceptual distortion measures. The
results demonstrate substantial improvements (up to 9 dB gains or up to 30\%
bitrate reductions) compared to alternative adaptations. We establish
optimality properties for sandwiched compression and design differentiable
codec proxies approximating current standard codecs. We further analyze model
complexity, visual quality under perceptual metrics, as well as sandwich
configurations that offer interesting potentials in video compression and
streaming.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Do Audio-Visual Segmentation Models Truly Segment Sounding Objects? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.00358v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.00358v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jia Li, Wenjie Zhao, Ziru Huang, Yunhui Guo, Yapeng Tian
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Unlike traditional visual segmentation, audio-visual segmentation (AVS)
requires the model not only to identify and segment objects but also to
determine whether they are sound sources. Recent AVS approaches, leveraging
transformer architectures and powerful foundation models like SAM, have
achieved impressive performance on standard benchmarks. Yet, an important
question remains: Do these models genuinely integrate audio-visual cues to
segment sounding objects? In this paper, we systematically investigate this
issue in the context of robust AVS. Our study reveals a fundamental bias in
current methods: they tend to generate segmentation masks based predominantly
on visual salience, irrespective of the audio context. This bias results in
unreliable predictions when sounds are absent or irrelevant. To address this
challenge, we introduce AVSBench-Robust, a comprehensive benchmark
incorporating diverse negative audio scenarios including silence, ambient
noise, and off-screen sounds. We also propose a simple yet effective approach
combining balanced training with negative samples and classifier-guided
similarity learning. Our extensive experiments show that state-of-theart AVS
methods consistently fail under negative audio conditions, demonstrating the
prevalence of visual bias. In contrast, our approach achieves remarkable
improvements in both standard metrics and robustness measures, maintaining
near-perfect false positive rates while preserving highquality segmentation
performance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ From Code to Canvas 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.06616v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.06616v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bernhard O. Werner
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The web-based dynamic geometry software CindyJS is a versatile tool to create
interactive applications for mathematics and other topics. In this workshop, we
will look at a code package that makes the creation of animations in CindyJS
easier and more streamlined. Animations, which can then be embedded into
presentations or be used in (lecture) videos. The focus lies on the creation of
the animations themselves and some of the technical and artistic fundamentals
to do so.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>A workshop paper for the Bridges 2025 conference</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2025-02-19T00:00:00Z">2025-02-19</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Information Retrieval <span class="chip" style="font-size: 60%">31</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Collaborative Retrieval for Large Language Model-based Conversational
  Recommender Systems <span class="chip">WWW'2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.14137v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.14137v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yaochen Zhu, Chao Wan, Harald Steck, Dawen Liang, Yesu Feng, Nathan Kallus, Jundong Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Conversational recommender systems (CRS) aim to provide personalized
recommendations via interactive dialogues with users. While large language
models (LLMs) enhance CRS with their superior understanding of context-aware
user preferences, they typically struggle to leverage behavioral data, which
have proven to be important for classical collaborative filtering (CF)-based
approaches. For this reason, we propose CRAG, Collaborative Retrieval Augmented
Generation for LLM-based CRS. To the best of our knowledge, CRAG is the first
approach that combines state-of-the-art LLMs with CF for conversational
recommendations. Our experiments on two publicly available movie conversational
recommendation datasets, i.e., a refined Reddit dataset (which we name
Reddit-v2) as well as the Redial dataset, demonstrate the superior item
coverage and recommendation performance of CRAG, compared to several CRS
baselines. Moreover, we observe that the improvements are mainly due to better
recommendation accuracy on recently released movies. The code and data are
available at https://github.com/yaochenzhu/CRAG.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by WWW'2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Optimizing Research Portfolio For Semantic Impact 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.13912v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.13912v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alexander V. Belikov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Citation metrics are widely used to assess academic impact but suffer from
social biases, including institutional prestige and journal visibility. Here we
introduce rXiv Semantic Impact (XSI), a novel framework that predicts research
impact by analyzing how scientific semantic graphs evolve in underlying fabric
of science. Rather than counting citations, XSI tracks the evolution of
research concepts in the academic knowledge graph (KG). Starting with a
construction of a comprehensive KG from 324K biomedical publications
(2003-2025), we demonstrate that XSI can predict a paper's future semantic
impact (SI) with remarkable accuracy ($R^2$ = 0.69) three years in advance. We
leverage these predictions to develop an optimization framework for research
portfolio selection that systematically outperforms random allocation. We
propose SI as a complementary metric to citations and present XSI as a tool to
guide funding and publishing decisions, enhancing research impact while
mitigating risk.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>24 pages; 13 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Judging the Judges: A Collection of LLM-Generated Relevance Judgements 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.13908v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.13908v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hossein A. Rahmani, Clemencia Siro, Mohammad Aliannejadi, Nick Craswell, Charles L. A. Clarke, Guglielmo Faggioli, Bhaskar Mitra, Paul Thomas, Emine Yilmaz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Using Large Language Models (LLMs) for relevance assessments offers promising
opportunities to improve Information Retrieval (IR), Natural Language
Processing (NLP), and related fields. Indeed, LLMs hold the promise of allowing
IR experimenters to build evaluation collections with a fraction of the manual
human labor currently required. This could help with fresh topics on which
there is still limited knowledge and could mitigate the challenges of
evaluating ranking systems in low-resource scenarios, where it is challenging
to find human annotators. Given the fast-paced recent developments in the
domain, many questions concerning LLMs as assessors are yet to be answered.
Among the aspects that require further investigation, we can list the impact of
various components in a relevance judgment generation pipeline, such as the
prompt used or the LLM chosen.
  This paper benchmarks and reports on the results of a large-scale automatic
relevance judgment evaluation, the LLMJudge challenge at SIGIR 2024, where
different relevance assessment approaches were proposed. In detail, we release
and benchmark 42 LLM-generated labels of the TREC 2023 Deep Learning track
relevance judgments produced by eight international teams who participated in
the challenge. Given their diverse nature, these automatically generated
relevance judgments can help the community not only investigate systematic
biases caused by LLMs but also explore the effectiveness of ensemble models,
analyze the trade-offs between different models and human assessors, and
advance methodologies for improving automated evaluation techniques. The
released resource is available at the following link:
https://llm4eval.github.io/LLMJudge-benchmark/
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PSCon: Toward Conversational Product Search 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.13881v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.13881v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jie Zou, Mohammad Aliannejadi, Evangelos Kanoulas, Shuxi Han, Heli Ma, Zheng Wang, Yang Yang, Heng Tao Shen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Conversational Product Search (CPS) is confined to simulated conversations
due to the lack of real-world CPS datasets that reflect human-like language.
Additionally, current conversational datasets are limited to support
cross-market and multi-lingual usage. In this paper, we introduce a new CPS
data collection protocol and present PSCon, a novel CPS dataset designed to
assist product search via human-like conversations. The dataset is constructed
using a coached human-to-human data collection protocol and supports two
languages and dual markets. Also, the dataset enables thorough exploration of
six subtasks of CPS: user intent detection, keyword extraction, system action
prediction, question selection, item ranking, and response generation.
Furthermore, we also offer an analysis of the dataset and propose a benchmark
model on the proposed CPS dataset.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Enhancing LLM-Based Recommendations Through Personalized Reasoning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.13845v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.13845v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiahao Liu, Xueshuo Yan, Dongsheng Li, Guangping Zhang, Hansu Gu, Peng Zhang, Tun Lu, Li Shang, Ning Gu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Current recommendation systems powered by large language models (LLMs) often
underutilize their reasoning capabilities due to a lack of explicit logical
structuring. To address this limitation, we introduce CoT-Rec, a framework that
integrates Chain-of-Thought (CoT) reasoning into LLM-driven recommendations by
incorporating two crucial processes: user preference analysis and item
perception evaluation. CoT-Rec operates in two key phases: (1) personalized
data extraction, where user preferences and item perceptions are identified,
and (2) personalized data application, where this information is leveraged to
refine recommendations. Our experimental analysis demonstrates that CoT-Rec
improves recommendation accuracy by making better use of LLMs' reasoning
potential. The implementation is publicly available at
https://anonymous.4open.science/r/CoT-Rec.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, under review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Enhancing Cross-Domain Recommendations with Memory-Optimized LLM-Based
  User Agents 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.13843v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.13843v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiahao Liu, Shengkang Gu, Dongsheng Li, Guangping Zhang, Mingzhe Han, Hansu Gu, Peng Zhang, Tun Lu, Li Shang, Ning Gu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Model (LLM)-based user agents have emerged as a powerful tool
for improving recommender systems by simulating user interactions. However,
existing methods struggle with cross-domain scenarios due to inefficient memory
structures, leading to irrelevant information retention and failure to account
for social influence factors such as popularity. To address these limitations,
we introduce AgentCF++, a novel framework featuring a dual-layer memory
architecture and a two-step fusion mechanism to filter domain-specific
preferences effectively. Additionally, we propose interest groups with shared
memory, allowing the model to capture the impact of popularity trends on users
with similar interests. Through extensive experiments on multiple cross-domain
datasets, AgentCF++ demonstrates superior performance over baseline models,
highlighting its effectiveness in refining user behavior simulation for
recommender systems. Our code is available at
https://anonymous.4open.science/r/AgentCF-plus.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, under review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Mitigating Popularity Bias in Collaborative Filtering through Fair
  Sampling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.13840v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.13840v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiahao Liu, Dongsheng Li, Hansu Gu, Peng Zhang, Tun Lu, Li Shang, Ning Gu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recommender systems often suffer from popularity bias, where frequently
interacted items are overrepresented in recommendations. This bias stems from
propensity factors influencing training data, leading to imbalanced exposure.
In this paper, we introduce a Fair Sampling (FS) approach to address this issue
by ensuring that both users and items are selected with equal probability as
positive and negative instances. Unlike traditional inverse propensity score
(IPS) methods, FS does not require propensity estimation, eliminating errors
associated with inaccurate calculations. Our theoretical analysis demonstrates
that FS effectively neutralizes the influence of propensity factors, achieving
unbiased learning. Experimental results validate that FS outperforms
state-of-the-art methods in both point-wise and pair-wise recommendation tasks,
enhancing recommendation fairness without sacrificing accuracy. The
implementation is available at https://anonymous.4open.science/r/Fair-Sampling.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, under review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ In-Place Updates of a Graph Index for Streaming Approximate Nearest
  Neighbor Search 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.13826v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.13826v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haike Xu, Magdalen Dobson Manohar, Philip A. Bernstein, Badrish Chandramouli, Richard Wen, Harsha Vardhan Simhadri
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Indices for approximate nearest neighbor search (ANNS) are a basic component
for information retrieval and widely used in database, search, recommendation
and RAG systems. In these scenarios, documents or other objects are inserted
into and deleted from the working set at a high rate, requiring a stream of
updates to the vector index. Algorithms based on proximity graph indices are
the most efficient indices for ANNS, winning many benchmark competitions.
However, it is challenging to update such graph index at a high rate, while
supporting stable recall after many updates. Since the graph is singly-linked,
deletions are hard because there is no fast way to find in-neighbors of a
deleted vertex. Therefore, to update the graph, state-of-the-art algorithms
such as FreshDiskANN accumulate deletions in a batch and periodically
consolidate, removing edges to deleted vertices and modifying the graph to
ensure recall stability. In this paper, we present IP-DiskANN
(InPlaceUpdate-DiskANN), the first algorithm to avoid batch consolidation by
efficiently processing each insertion and deletion in-place. Our experiments
using standard benchmarks show that IP-DiskANN has stable recall over various
lengthy update patterns in both high-recall and low-recall regimes. Further,
its query throughput and update speed are better than using the batch
consolidation algorithm and HNSW.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Generative Large Recommendation Models: Emerging Trends in LLMs for
  Recommendation <span class="chip">WWW 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.13783v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.13783v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hao Wang, Wei Guo, Luankang Zhang, Jin Yao Chin, Yufei Ye, Huifeng Guo, Yong Liu, Defu Lian, Ruiming Tang, Enhong Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the era of information overload, recommendation systems play a pivotal
role in filtering data and delivering personalized content. Recent advancements
in feature interaction and user behavior modeling have significantly enhanced
the recall and ranking processes of these systems. With the rise of large
language models (LLMs), new opportunities have emerged to further improve
recommendation systems. This tutorial explores two primary approaches for
integrating LLMs: LLMs-enhanced recommendations, which leverage the reasoning
capabilities of general LLMs, and generative large recommendation models, which
focus on scaling and sophistication. While the former has been extensively
covered in existing literature, the latter remains underexplored. This tutorial
aims to fill this gap by providing a comprehensive overview of generative large
recommendation models, including their recent advancements, challenges, and
potential research directions. Key topics include data quality, scaling laws,
user behavior mining, and efficiency in training and inference. By engaging
with this tutorial, participants will gain insights into the latest
developments and future opportunities in the field, aiding both academic
research and practical applications. The timely nature of this exploration
supports the rapid evolution of recommendation systems, offering valuable
guidance for researchers and practitioners alike.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper has been accepted for the tutorial track at WWW 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Unsupervised Graph Embeddings for Session-based Recommendation with Item
  Features 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.13763v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.13763v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Andreas Peintner, Marta Moscati, Emilia Parada-Cabaleiro, Markus Schedl, Eva Zangerle
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In session-based recommender systems, predictions are based on the user's
preceding behavior in the session. State-of-the-art sequential recommendation
algorithms either use graph neural networks to model sessions in a graph or
leverage the similarity of sessions by exploiting item features. In this paper,
we combine these two approaches and propose a novel method, Graph Convolutional
Network Extension (GCNext), which incorporates item features directly into the
graph representation via graph convolutional networks. GCNext creates a
feature-rich item co-occurrence graph and learns the corresponding item
embeddings in an unsupervised manner. We show on three datasets that
integrating GCNext into sequential recommendation algorithms significantly
boosts the performance of nearest-neighbor methods as well as neural network
models. Our flexible extension is easy to incorporate in state-of-the-art
methods and increases the MRR@20 by up to 12.79%.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ TrustRAG: An Information Assistant with Retrieval Augmented Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.13719v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.13719v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yixing Fan, Qiang Yan, Wenshan Wang, Jiafeng Guo, Ruqing Zhang, Xueqi Cheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  \Ac{RAG} has emerged as a crucial technique for enhancing large models with
real-time and domain-specific knowledge. While numerous improvements and
open-source tools have been proposed to refine the \ac{RAG} framework for
accuracy, relatively little attention has been given to improving the
trustworthiness of generated results. To address this gap, we introduce
TrustRAG, a novel framework that enhances \ac{RAG} from three perspectives:
indexing, retrieval, and generation. Specifically, in the indexing stage, we
propose a semantic-enhanced chunking strategy that incorporates hierarchical
indexing to supplement each chunk with contextual information, ensuring
semantic completeness. In the retrieval stage, we introduce a utility-based
filtering mechanism to identify high-quality information, supporting answer
generation while reducing input length. In the generation stage, we propose
fine-grained citation enhancement, which detects opinion-bearing sentences in
responses and infers citation relationships at the sentence-level, thereby
improving citation accuracy. We open-source the TrustRAG framework and provide
a demonstration studio designed for excerpt-based question answering tasks
\footnote{https://huggingface.co/spaces/golaxy/TrustRAG}. Based on these, we
aim to help researchers: 1) systematically enhancing the trustworthiness of
\ac{RAG} systems and (2) developing their own \ac{RAG} systems with more
reliable outputs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PeerQA: A Scientific Question Answering <span class="highlight-title">Dataset</span> from Peer <span class="highlight-title">Review</span>s <span class="chip">NAACL 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.13668v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.13668v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tim Baumgärtner, Ted Briscoe, Iryna Gurevych
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present PeerQA, a real-world, scientific, document-level Question
Answering (QA) dataset. PeerQA questions have been sourced from peer reviews,
which contain questions that reviewers raised while thoroughly examining the
scientific article. Answers have been annotated by the original authors of each
paper. The dataset contains 579 QA pairs from 208 academic articles, with a
majority from ML and NLP, as well as a subset of other scientific communities
like Geoscience and Public Health. PeerQA supports three critical tasks for
developing practical QA systems: Evidence retrieval, unanswerable question
classification, and answer generation. We provide a detailed analysis of the
collected dataset and conduct experiments establishing baseline systems for all
three tasks. Our experiments and analyses reveal the need for
decontextualization in document-level retrieval, where we find that even simple
decontextualization approaches consistently improve retrieval performance
across architectures. On answer generation, PeerQA serves as a challenging
benchmark for long-context modeling, as the papers have an average size of 12k
tokens. Our code and data is available at https://github.com/UKPLab/peerqa.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at NAACL 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MMTEB: Massive Multilingual Text Embedding Benchmark <span class="chip">ICLR</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.13595v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.13595v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kenneth Enevoldsen, Isaac Chung, Imene Kerboua, Márton Kardos, Ashwin Mathur, David Stap, Jay Gala, Wissam Siblini, Dominik Krzemiński, Genta Indra Winata, Saba Sturua, Saiteja Utpala, Mathieu Ciancone, Marion Schaeffer, Gabriel Sequeira, Diganta Misra, Shreeya Dhakal, Jonathan Rystrøm, Roman Solomatin, Ömer Çağatan, Akash Kundu, Martin Bernstorff, Shitao Xiao, Akshita Sukhlecha, Bhavish Pahwa, Rafał Poświata, Kranthi Kiran GV, Shawon Ashraf, Daniel Auras, Björn Plüster, Jan Philipp Harries, Loïc Magne, Isabelle Mohr, Mariya Hendriksen, Dawei Zhu, Hippolyte Gisserot-Boukhlef, Tom Aarsen, Jan Kostkan, Konrad Wojtasik, Taemin Lee, Marek Šuppa, Crystina Zhang, Roberta Rocca, Mohammed Hamdy, Andrianos Michail, John Yang, Manuel Faysse, Aleksei Vatolin, Nandan Thakur, Manan Dey, Dipam Vasani, Pranjal Chitale, Simone Tedeschi, Nguyen Tai, Artem Snegirev, Michael Günther, Mengzhou Xia, Weijia Shi, Xing Han Lù, Jordan Clive, Gayatri Krishnakumar, Anna Maksimova, Silvan Wehrli, Maria Tikhonova, Henil Panchal, Aleksandr Abramov, Malte Ostendorff, Zheng Liu, Simon Clematide, Lester James Miranda, Alena Fenogenova, Guangyu Song, Ruqiya Bin Safi, Wen-Ding Li, Alessia Borghini, Federico Cassano, Hongjin Su, Jimmy Lin, Howard Yen, Lasse Hansen, Sara Hooker, Chenghao Xiao, Vaibhav Adlakha, Orion Weller, Siva Reddy, Niklas Muennighoff
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Text embeddings are typically evaluated on a limited set of tasks, which are
constrained by language, domain, and task diversity. To address these
limitations and provide a more comprehensive evaluation, we introduce the
Massive Multilingual Text Embedding Benchmark (MMTEB) - a large-scale,
community-driven expansion of MTEB, covering over 500 quality-controlled
evaluation tasks across 250+ languages. MMTEB includes a diverse set of
challenging, novel tasks such as instruction following, long-document
retrieval, and code retrieval, representing the largest multilingual collection
of evaluation tasks for embedding models to date. Using this collection, we
develop several highly multilingual benchmarks, which we use to evaluate a
representative set of models. We find that while large language models (LLMs)
with billions of parameters can achieve state-of-the-art performance on certain
language subsets and task categories, the best-performing publicly available
model is multilingual-e5-large-instruct with only 560 million parameters. To
facilitate accessibility and reduce computational cost, we introduce a novel
downsampling method based on inter-task correlation, ensuring a diverse
selection while preserving relative model rankings. Furthermore, we optimize
tasks such as retrieval by sampling hard negatives, creating smaller but
effective splits. These optimizations allow us to introduce benchmarks that
drastically reduce computational demands. For instance, our newly introduced
zero-shot English benchmark maintains a ranking order similar to the full-scale
version but at a fraction of the computational cost.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for ICLR: https://openreview.net/forum?id=zl3pfz4VCV</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ActionPiece: Contextually Tokenizing Action Sequences for Generative
  Recommendation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.13581v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.13581v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yupeng Hou, Jianmo Ni, Zhankui He, Noveen Sachdeva, Wang-Cheng Kang, Ed H. Chi, Julian McAuley, Derek Zhiyuan Cheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generative recommendation (GR) is an emerging paradigm where user actions are
tokenized into discrete token patterns and autoregressively generated as
predictions. However, existing GR models tokenize each action independently,
assigning the same fixed tokens to identical actions across all sequences
without considering contextual relationships. This lack of context-awareness
can lead to suboptimal performance, as the same action may hold different
meanings depending on its surrounding context. To address this issue, we
propose ActionPiece to explicitly incorporate context when tokenizing action
sequences. In ActionPiece, each action is represented as a set of item
features, which serve as the initial tokens. Given the action sequence corpora,
we construct the vocabulary by merging feature patterns as new tokens, based on
their co-occurrence frequency both within individual sets and across adjacent
sets. Considering the unordered nature of feature sets, we further introduce
set permutation regularization, which produces multiple segmentations of action
sequences with the same semantics. Experiments on public datasets demonstrate
that ActionPiece consistently outperforms existing action tokenization methods,
improving NDCG@$10$ by $6.00\%$ to $12.82\%$.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Bursting Filter Bubble: Enhancing Serendipity Recommendations with
  Aligned Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.13539v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.13539v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yunjia Xi, Muyan Weng, Wen Chen, Chao Yi, Dian Chen, Gaoyang Guo, Mao Zhang, Jian Wu, Yuning Jiang, Qingwen Liu, Yong Yu, Weinan Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recommender systems (RSs) often suffer from the feedback loop phenomenon,
e.g., RSs are trained on data biased by their recommendations. This leads to
the filter bubble effect that reinforces homogeneous content and reduces user
satisfaction. To this end, serendipity recommendations, which offer unexpected
yet relevant items, are proposed. Recently, large language models (LLMs) have
shown potential in serendipity prediction due to their extensive world
knowledge and reasoning capabilities. However, they still face challenges in
aligning serendipity judgments with human assessments, handling long user
behavior sequences, and meeting the latency requirements of industrial RSs. To
address these issues, we propose SERAL (Serendipity Recommendations with
Aligned Large Language Models), a framework comprising three stages: (1)
Cognition Profile Generation to compress user behavior into multi-level
profiles; (2) SerenGPT Alignment to align serendipity judgments with human
preferences using enriched training data; and (3) Nearline Adaptation to
integrate SerenGPT into industrial RSs pipelines efficiently. Online
experiments demonstrate that SERAL improves exposure ratio (PVR), clicks, and
transactions of serendipitous items by 5.7%, 29.56%, and 27.6%, enhancing user
experience without much impact on overall revenue. Now, it has been fully
deployed in the "Guess What You Like" of the Taobao App homepage.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Breaking the Clusters: Uniformity-Optimization for Text-Based Sequential
  Recommendation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.13530v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.13530v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wuhan Chen, Zongwei Wang, Min Gao, Xin Xia, Feng Jiang, Junhao Wen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Traditional sequential recommendation (SR) methods heavily rely on explicit
item IDs to capture user preferences over time. This reliance introduces
critical limitations in cold-start scenarios and domain transfer tasks, where
unseen items and new contexts often lack established ID mappings. To overcome
these limitations, recent studies have shifted towards leveraging text-only
information for recommendation, thereby improving model generalization and
adaptability across domains. Although promising, text-based SR faces unique
difficulties: items' text descriptions often share semantic similarities that
lead to clustered item representations, compromising their uniformity, a
property essential for promoting diversity and enhancing generalization in
recommendation systems. In this paper, we explore a novel framework to improve
the uniformity of item representations in text-based SR. Our analysis reveals
that items within a sequence exhibit marked semantic similarity, meaning they
are closer in representation than items overall, and that this effect is more
pronounced for less popular items, which form tighter clusters compared to
their more popular counterparts. Based on these findings, we propose UniT, a
framework that employs three pairwise item sampling strategies: Unified General
Sampling Strategy, Sequence-Driven Sampling Strategy, and Popularity-Driven
Sampling Strategy. Each strategy applies varying degrees of repulsion to
selectively adjust the distances between item pairs, thereby refining
representation uniformity while considering both sequence context and item
popularity. Extensive experiments on multiple real-world datasets demonstrate
that our proposed approach outperforms state-of-the-art models, validating the
effectiveness of UniT in enhancing both representation uniformity and
recommendation accuracy.The source code is available at
https://github.com/ccwwhhh/Model-Rec.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ OpenSearch-SQL: Enhancing Text-to-SQL with Dynamic Few-shot and
  Consistency Alignment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.14913v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.14913v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiangjin Xie, Guangwei Xu, Lingyan Zhao, Ruijie Guo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Although multi-agent collaborative Large Language Models (LLMs) have achieved
significant breakthroughs in the Text-to-SQL task, their performance is still
constrained by various factors. These factors include the incompleteness of the
framework, failure to follow instructions, and model hallucination problems. To
address these problems, we propose OpenSearch-SQL, which divides the
Text-to-SQL task into four main modules: Preprocessing, Extraction, Generation,
and Refinement, along with an Alignment module based on a consistency alignment
mechanism. This architecture aligns the inputs and outputs of agents through
the Alignment module, reducing failures in instruction following and
hallucination. Additionally, we designed an intermediate language called
SQL-Like and optimized the structured CoT based on SQL-Like. Meanwhile, we
developed a dynamic few-shot strategy in the form of self-taught Query-CoT-SQL.
These methods have significantly improved the performance of LLMs in the
Text-to-SQL task.
  In terms of model selection, we directly applied the base LLMs without any
post-training, thereby simplifying the task chain and enhancing the framework's
portability. Experimental results show that OpenSearch-SQL achieves an
execution accuracy(EX) of 69.3% on the BIRD development set, 72.28% on the test
set, and a reward-based validity efficiency score (R-VES) of 69.36%, with all
three metrics ranking first at the time of submission. These results
demonstrate the comprehensive advantages of the proposed method in both
effectiveness and efficiency.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LLM4Tag: Automatic Tagging System for Information Retrieval via Large
  Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.13481v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.13481v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ruiming Tang, Chenxu Zhu, Bo Chen, Weipeng Zhang, Menghui Zhu, Xinyi Dai, Huifeng Guo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Tagging systems play an essential role in various information retrieval
applications such as search engines and recommender systems. Recently, Large
Language Models (LLMs) have been applied in tagging systems due to their
extensive world knowledge, semantic understanding, and reasoning capabilities.
Despite achieving remarkable performance, existing methods still have
limitations, including difficulties in retrieving relevant candidate tags
comprehensively, challenges in adapting to emerging domain-specific knowledge,
and the lack of reliable tag confidence quantification. To address these three
limitations above, we propose an automatic tagging system LLM4Tag. First, a
graph-based tag recall module is designed to effectively and comprehensively
construct a small-scale highly relevant candidate tag set. Subsequently, a
knowledge-enhanced tag generation module is employed to generate accurate tags
with long-term and short-term knowledge injection. Finally, a tag confidence
calibration module is introduced to generate reliable tag confidence scores.
Extensive experiments over three large-scale industrial datasets show that
LLM4Tag significantly outperforms the state-of-the-art baselines and LLM4Tag
has been deployed online for content tagging to serve hundreds of millions of
users.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ HawkBench: Investigating Resilience of RAG Methods on Stratified
  Information-Seeking Tasks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.13465v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.13465v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hongjin Qian, Zheng Liu, Chao Gao, Yankai Wang, Defu Lian, Zhicheng Dou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In real-world information-seeking scenarios, users have dynamic and diverse
needs, requiring RAG systems to demonstrate adaptable resilience. To
comprehensively evaluate the resilience of current RAG methods, we introduce
HawkBench, a human-labeled, multi-domain benchmark designed to rigorously
assess RAG performance across categorized task types. By stratifying tasks
based on information-seeking behaviors, HawkBench provides a systematic
evaluation of how well RAG systems adapt to diverse user needs.
  Unlike existing benchmarks, which focus primarily on specific task types
(mostly factoid queries) and rely on varying knowledge bases, HawkBench offers:
(1) systematic task stratification to cover a broad range of query types,
including both factoid and rationale queries, (2) integration of multi-domain
corpora across all task types to mitigate corpus bias, and (3) rigorous
annotation for high-quality evaluation.
  HawkBench includes 1,600 high-quality test samples, evenly distributed across
domains and task types. Using this benchmark, we evaluate representative RAG
methods, analyzing their performance in terms of answer quality and response
latency. Our findings highlight the need for dynamic task strategies that
integrate decision-making, query interpretation, and global knowledge
understanding to improve RAG generalizability. We believe HawkBench serves as a
pivotal benchmark for advancing the resilience of RAG methods and their ability
to achieve general-purpose information seeking.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ STAR: A Simple Training-free Approach for Recommendations using Large
  Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.16458v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.16458v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dong-Ho Lee, Adam Kraft, Long Jin, Nikhil Mehta, Taibai Xu, Lichan Hong, Ed H. Chi, Xinyang Yi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent progress in large language models (LLMs) offers promising new
approaches for recommendation system tasks. While the current state-of-the-art
methods rely on fine-tuning LLMs to achieve optimal results, this process is
costly and introduces significant engineering complexities. Conversely, methods
that directly use LLMs without additional fine-tuning result in a large drop in
recommendation quality, often due to the inability to capture collaborative
information. In this paper, we propose a Simple Training-free Approach for
Recommendation (STAR), a framework that utilizes LLMs and can be applied to
various recommendation tasks without the need for fine-tuning, while
maintaining high quality recommendation performance. Our approach involves a
retrieval stage that uses semantic embeddings from LLMs combined with
collaborative user information to retrieve candidate items. We then apply an
LLM for pairwise ranking to enhance next-item prediction. Experimental results
on the Amazon Review dataset show competitive performance for next item
prediction, even with our retrieval stage alone. Our full method achieves
Hits@10 performance of +23.8% on Beauty, +37.5% on Toys & Games, and -1.8% on
Sports & Outdoors relative to the best supervised models. This framework offers
an effective alternative to traditional supervised models, highlighting the
potential of LLMs in recommendation systems without extensive training or
custom architectures.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Rankify: A Comprehensive Python Toolkit for Retrieval, Re-Ranking, and
  Retrieval-Augmented Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.02464v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.02464v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Abdelrahman Abdallah, Bhawna Piryani, Jamshid Mozafari, Mohammed Ali, Adam Jatowt
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Retrieval, re-ranking, and retrieval-augmented generation (RAG) are critical
components of modern applications in information retrieval, question answering,
or knowledge-based text generation. However, existing solutions are often
fragmented, lacking a unified framework that easily integrates these essential
processes. The absence of a standardized implementation, coupled with the
complexity of retrieval and re-ranking workflows, makes it challenging for
researchers to compare and evaluate different approaches in a consistent
environment. While existing toolkits such as Rerankers and RankLLM provide
general-purpose reranking pipelines, they often lack the flexibility required
for fine-grained experimentation and benchmarking. In response to these
challenges, we introduce Rankify, a powerful and modular open-source toolkit
designed to unify retrieval, re-ranking, and RAG within a cohesive framework.
Rankify supports a wide range of retrieval techniques, including dense and
sparse retrievers, while incorporating state-of-the-art re-ranking models to
enhance retrieval quality. Additionally, Rankify includes a collection of
pre-retrieved datasets to facilitate benchmarking, available at Huggingface
(https://huggingface.co/datasets/abdoelsayed/reranking-datasets-light). To
encourage adoption and ease of integration, we provide comprehensive
documentation (http://rankify.readthedocs.io/), an open-source implementation
on GitHub (https://github.com/DataScienceUIBK/rankify), and a PyPI package for
easy installation (https://pypi.org/project/rankify/). As a unified and
lightweight framework, Rankify allows researchers and practitioners to advance
retrieval and re-ranking methodologies while ensuring consistency, scalability,
and ease of use.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Work in Progress</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Taxonomy-Guided Zero-Shot Recommendations with LLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.14043v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.14043v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yueqing Liang, Liangwei Yang, Chen Wang, Xiongxiao Xu, Philip S. Yu, Kai Shu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the emergence of large language models (LLMs) and their ability to
perform a variety of tasks, their application in recommender systems (RecSys)
has shown promise. However, we are facing significant challenges when deploying
LLMs into RecSys, such as limited prompt length, unstructured item information,
and un-constrained generation of recommendations, leading to sub-optimal
performance. To address these issues, we propose a novel method using a
taxonomy dictionary. This method provides a systematic framework for
categorizing and organizing items, improving the clarity and structure of item
information. By incorporating the taxonomy dictionary into LLM prompts, we
achieve efficient token utilization and controlled feature generation, leading
to more accurate and contextually relevant recommendations. Our Taxonomy-guided
Recommendation (TaxRec) approach features a two-step process: one-time taxonomy
categorization and LLM-based recommendation, enabling zero-shot recommendations
without the need for domain-specific fine-tuning. Experimental results
demonstrate TaxRec significantly enhances recommendation quality compared to
traditional zero-shot approaches, showcasing its efficacy as personal
recommender with LLMs. Code is available at
https://github.com/yueqingliang1/TaxRec.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ An Open-Source Web-Based Tool for Evaluating Open-Source Large Language
  Models Leveraging Information Retrieval from Custom Documents 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.10916v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.10916v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Godfrey I
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In our work, we present the first-of-its-kind open-source web-based tool
which is able to demonstrate the impacts of a user's speech act during
discourse with conversational agents, which leverages open-source large
language models. With this software resource, it is possible for researchers
and experts to evaluate the performance of various dialogues, visualize the
user's communicative intents, and utilise uploaded specific documents for the
chat agent to use for its information retrieval to respond to the user query.
The context gathered by these models is obtained from a set of linguistic
features extracted, which forms the context embeddings of the models.
Regardless of these models showing good context understanding based on these
features, there still remains a gap in including deeper pragmatic features to
improve the model's comprehension of the query, hence the efforts to develop
this web resource, which is able to extract and then inject this overlooked
feature in the encoder-decoder pipeline of the conversational agent. To
demonstrate the effect and impact of the resource, we carried out an experiment
which evaluated the system using 2 knowledge files for information retrieval,
with two user queries each, across 5 open-source large language models using 10
standard metrics. Our results showed that larger open-source models,
demonstrated an improved alignment when the user speech act was included with
their query. The smaller models in contrast showed an increased perplexity and
mixed performance, which explicitly indicated struggles in processing queries
that explicitly included speech acts. The results from the analysis using the
developed web resource highlight the potential of speech acts towards enhancing
conversational depths while underscoring the need for model-specific
optimizations to address increased computational costs and response times.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>19 pages, 1 figure, 6 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Multilingual Non-Factoid Question Answering with Answer Paragraph
  Selection <span class="chip">PAKDD 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.10604v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.10604v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ritwik Mishra, Sreeram Vennam, Rajiv Ratn Shah, Ponnurangam Kumaraguru
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Most existing Question Answering Datasets (QuADs) primarily focus on
factoid-based short-context Question Answering (QA) in high-resource languages.
However, the scope of such datasets for low-resource languages remains limited,
with only a few works centered on factoid-based QuADs and none on non-factoid
QuADs. Therefore, this work presents MuNfQuAD, a multilingual QuAD with
non-factoid questions. It utilizes interrogative sub-headings from BBC news
articles as questions and the corresponding paragraphs as silver answers. The
dataset comprises over 578K QA pairs across 38 languages, encompassing several
low-resource languages, and stands as the largest multilingual QA dataset to
date. Based on the manual annotations of 790 QA-pairs from MuNfQuAD (golden
set), we observe that 98\% of questions can be answered using their
corresponding silver answer. Our fine-tuned Answer Paragraph Selection (APS)
model outperforms the baselines. The APS model attained an accuracy of 80\% and
72\%, as well as a macro F1 of 72\% and 66\%, on the MuNfQuAD testset and the
golden set, respectively. Furthermore, the APS model effectively generalizes a
certain language within the golden set, even after being fine-tuned on silver
labels. We also observe that the fine-tuned APS model is beneficial for
reducing the context of a question. These findings suggest that this resource
would be a valuable contribution to the QA research community.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Shorter version accepted into DSFA, a special session in PAKDD 2025,
  Sydney</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Emancipatory Information Retrieval 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.19241v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.19241v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bhaskar Mitra
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Our world today is facing a confluence of several mutually reinforcing crises
each of which intersects with concerns of social justice and emancipation. This
paper is a provocation for the role of computer-mediated information access in
our emancipatory struggles. We define emancipatory information retrieval as the
study and development of information access methods that challenge various
forms of human oppression, and situates its activities within broader
collective emancipatory praxis. The term "emancipatory" here signifies the
moral concerns of universal humanization of all peoples and the elimination of
oppression to create the conditions under which we can collectively flourish.
To develop an emancipatory research agenda for information retrieval (IR), in
this paper we speculate about the practices that the community can adopt,
enumerate some of the projects that the field should undertake, and discuss
provocations to spark new ideas and directions for research. We challenge the
field of IR research to embrace humanistic values and commit to universal
emancipation and social justice. We also invite scholars from fields such as
human-computer interaction, information sciences, media studies, design, social
sciences, humanities, democratic theory, and critical theory, as well as legal
and policy experts, civil rights and social justice activists, and artists to
join us in realizing this transformation. In this process, we must both imagine
post-oppressive worlds, and reimagine the role of IR in that world and in the
journey that leads us there.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Heterophily-Aware Fair Recommendation using Graph Convolutional Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.03365v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.03365v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nemat Gholinejad, Mostafa Haghir Chehreghani
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, graph neural networks (GNNs) have become a popular tool to
improve the accuracy and performance of recommender systems. Modern recommender
systems are not only designed to serve end users, but also to benefit other
participants, such as items and item providers. These participants may have
different or conflicting goals and interests, which raises the need for
fairness and popularity bias considerations. GNN-based recommendation methods
also face the challenges of unfairness and popularity bias, and their
normalization and aggregation processes suffer from these challenges. In this
paper, we propose a fair GNN-based recommender system, called HetroFair, to
improve item-side fairness. HetroFair uses two separate components to generate
fairness-aware embeddings: i) Fairness-aware attention, which incorporates the
dot product in the normalization process of GNNs to decrease the effect of
nodes' degrees. ii) Heterophily feature weighting, to assign distinct weights
to different features during the aggregation process. To evaluate the
effectiveness of HetroFair, we conduct extensive experiments over six
real-world datasets. Our experimental results reveal that HetroFair not only
alleviates unfairness and popularity bias on the item side but also achieves
superior accuracy on the user side. Our implementation is publicly available at
https://github.com/NematGH/HetroFair.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>24 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ From Local to Global: A Graph RAG Approach to Query-Focused
  Summarization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.16130v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.16130v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Darren Edge, Ha Trinh, Newman Cheng, Joshua Bradley, Alex Chao, Apurva Mody, Steven Truitt, Dasha Metropolitansky, Robert Osazuwa Ness, Jonathan Larson
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The use of retrieval-augmented generation (RAG) to retrieve relevant
information from an external knowledge source enables large language models
(LLMs) to answer questions over private and/or previously unseen document
collections. However, RAG fails on global questions directed at an entire text
corpus, such as "What are the main themes in the dataset?", since this is
inherently a query-focused summarization (QFS) task, rather than an explicit
retrieval task. Prior QFS methods, meanwhile, do not scale to the quantities of
text indexed by typical RAG systems. To combine the strengths of these
contrasting methods, we propose GraphRAG, a graph-based approach to question
answering over private text corpora that scales with both the generality of
user questions and the quantity of source text. Our approach uses an LLM to
build a graph index in two stages: first, to derive an entity knowledge graph
from the source documents, then to pregenerate community summaries for all
groups of closely related entities. Given a question, each community summary is
used to generate a partial response, before all partial responses are again
summarized in a final response to the user. For a class of global sensemaking
questions over datasets in the 1 million token range, we show that GraphRAG
leads to substantial improvements over a conventional RAG baseline for both the
comprehensiveness and diversity of generated answers.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Optimizing Sequential Recommendation Models with Scaling Laws and
  Approximate Entropy 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.00430v6">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.00430v6.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tingjia Shen, Hao Wang, Chuhan Wu, Jin Yao Chin, Wei Guo, Yong Liu, Huifeng Guo, Defu Lian, Ruiming Tang, Enhong Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Scaling Laws have emerged as a powerful framework for understanding how model
performance evolves as they increase in size, providing valuable insights for
optimizing computational resources. In the realm of Sequential Recommendation
(SR), which is pivotal for predicting users' sequential preferences, these laws
offer a lens through which to address the challenges posed by the scalability
of SR models. However, the presence of structural and collaborative issues in
recommender systems prevents the direct application of the Scaling Law (SL) in
these systems. In response, we introduce the Performance Law for SR models,
which aims to theoretically investigate and model the relationship between
model performance and data quality. Specifically, we first fit the HR and NDCG
metrics to transformer-based SR models. Subsequently, we propose Approximate
Entropy (ApEn) to assess data quality, presenting a more nuanced approach
compared to traditional data quantity metrics. Our method enables accurate
predictions across various dataset scales and model sizes, demonstrating a
strong correlation in large SR models and offering insights into achieving
optimal performance for any given model configuration.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Planted vertex cover problem on regular random graphs and nonmonotonic
  temperature-dependence in the supercooled region 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.06610v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.06610v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xin-Yi Fan, Hai-Jun Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce a planted vertex cover problem on regular random graphs and
study it by the cavity method of statistical mechanics. Different from
conventional Ising models, the equilibrium ferromagnetic phase transition of
this binary-spin two-body interaction system is discontinuous, as the
paramagnetic phase is separated from the ferromagnetic phase by an extensive
free energy barrier. The free energy landscape can be distinguished into three
different types depending on the two degree parameters of the planted graph.
The critical inverse temperatures at which the paramagnetic phase becomes
locally unstable towards the ferromagnetic phase ($\beta_{\textrm{pf}}$) and
towards spin glass phases ($\beta_{\textrm{pg}}$) satisfy $\beta_{\textrm{pf}}
> \beta_{\textrm{pg}}$, $\beta_{\textrm{pf}} < \beta_{\textrm{pg}}$ and
$\beta_{\textrm{pf}} = \beta_{\textrm{pg}}$, respectively, in these three
landscapes. A locally stable anti-ferromagnetic phase emerges in the free
energy landscape if $\beta_{\textrm{pf}} < \beta_{\textrm{pg}}$. When exploring
the free energy landscape by stochastic local search dynamics, we find that in
agreement with our theoretical prediction, the first-passage time from the
paramagnetic phase to the ferromagnetic phase is nonmonotonic with the inverse
temperature. The potential relevance of the planted vertex cover model to
supercooled glass-forming liquids is briefly discussed.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Extensively revised and expanded. Changed title. A mistake in
  numerical simulation corrected. Accepted for publication in PRE as a regular
  article</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Diffusion Models in Recommendation Systems: A <span class="highlight-title">Survey</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.10548v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.10548v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ting-Ruen Wei, Yi Fang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recommender systems remain an essential topic due to its wide application in
various domains and the business potential behind them. With the rise of deep
learning, common solutions have leveraged neural networks to facilitate
collaborative filtering, and some have turned to generative adversarial
networks to augment the dataset and tackle the data sparsity issue. However,
they are limited in learning the complex user and item distribution and still
suffer from model collapse. Given the great generation capability exhibited by
diffusion models in computer vision recently, many recommender systems have
adopted diffusion models and found improvements in performance for various
tasks. Diffusion models in recommender systems excel in managing complex user
and item distributions and do not suffer from mode collapse. With these
advantages, the amount of research in this domain have been growing rapidly and
calling for a systematic survey. In this survey paper, we present and propose a
taxonomy on past research papers in recommender systems that utilize diffusion
models. Distinct from a prior survey paper that categorizes based on the role
of the diffusion model, we categorize based on the recommendation task at hand.
The decision originates from the rationale that after all, the adoption of
diffusion models is to enhance the recommendation performance, not vice versa:
adapting the recommendation task to enable diffusion models. Nonetheless, we
offer a unique perspective for diffusion models in recommender systems
complementary to existing surveys. We present the foundation algorithms in
diffusion models and their applications in recommender systems to summarize the
rapid development in this field. Finally, we discuss open research directions
to prepare and encourage further efforts to advance the field. We compile the
relevant papers in a public GitHub repository.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ One Model for All: Large Language Models are Domain-Agnostic
  Recommendation Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.14304v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.14304v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zuoli Tang, Zhaoxin Huan, Zihao Li, Xiaolu Zhang, Jun Hu, Chilin Fu, Jun Zhou, Lixin Zou, Chenliang Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Sequential recommendation systems aim to predict users' next likely
interaction based on their history. However, these systems face data sparsity
and cold-start problems. Utilizing data from other domains, known as
multi-domain methods, is useful for alleviating these problems. However,
traditional multi-domain methods rely on meaningless ID-based item
representation, which makes it difficult to align items with similar meanings
from different domains, yielding sup-optimal knowledge transfer. This paper
introduces LLM-Rec, a framework that utilizes pre-trained large language models
(LLMs) for domain-agnostic recommendation. Specifically, we mix user's
behaviors from multiple domains and concatenate item titles into a sentence,
then use LLMs for generating user and item representations. By mixing behaviors
across different domains, we can exploit the knowledge encoded in LLMs to
bridge the semantic across over multi-domain behaviors, thus obtaining
semantically rich representations and improving performance in all domains.
Furthermore, we explore the underlying reasons why LLMs are effective and
investigate whether LLMs can understand the semantic correlations as the
recommendation model, and if advanced techniques like scaling laws in NLP also
work in recommendations. We conduct extensive experiments with LLMs ranging
from 40M to 6.7B to answer the above questions and to verify the effectiveness
of LLM-Rec in multi-domain recommendation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>27 pages, 15 figures, 7 tables, Accepted by TOIS</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Multimedia <span class="chip" style="font-size: 60%">7</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ d-Sketch: Improving Visual Fidelity of Sketch-to-Image Translation with
  <span class="highlight-title">Pretrain</span>ed Latent Diffusion Models without Retraining <span class="chip">ICPR</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.14007v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.14007v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Prasun Roy, Saumik Bhattacharya, Subhankar Ghosh, Umapada Pal, Michael Blumenstein
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Structural guidance in an image-to-image translation allows intricate control
over the shapes of synthesized images. Generating high-quality realistic images
from user-specified rough hand-drawn sketches is one such task that aims to
impose a structural constraint on the conditional generation process. While the
premise is intriguing for numerous use cases of content creation and academic
research, the problem becomes fundamentally challenging due to substantial
ambiguities in freehand sketches. Furthermore, balancing the trade-off between
shape consistency and realistic generation contributes to additional complexity
in the process. Existing approaches based on Generative Adversarial Networks
(GANs) generally utilize conditional GANs or GAN inversions, often requiring
application-specific data and optimization objectives. The recent introduction
of Denoising Diffusion Probabilistic Models (DDPMs) achieves a generational
leap for low-level visual attributes in general image synthesis. However,
directly retraining a large-scale diffusion model on a domain-specific subtask
is often extremely difficult due to demanding computation costs and
insufficient data. In this paper, we introduce a technique for sketch-to-image
translation by exploiting the feature generalization capabilities of a
large-scale diffusion model without retraining. In particular, we use a
learnable lightweight mapping network to achieve latent feature translation
from source to target domain. Experimental results demonstrate that the
proposed method outperforms the existing techniques in qualitative and
quantitative benchmarks, allowing high-resolution realistic image synthesis
from rough hand-drawn sketches.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted in The International Conference on Pattern Recognition
  (ICPR) 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Exploring Mutual Cross-Modal Attention for Context-Aware Human
  Affordance Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.13637v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.13637v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Prasun Roy, Saumik Bhattacharya, Subhankar Ghosh, Umapada Pal, Michael Blumenstein
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Human affordance learning investigates contextually relevant novel pose
prediction such that the estimated pose represents a valid human action within
the scene. While the task is fundamental to machine perception and automated
interactive navigation agents, the exponentially large number of probable pose
and action variations make the problem challenging and non-trivial. However,
the existing datasets and methods for human affordance prediction in 2D scenes
are significantly limited in the literature. In this paper, we propose a novel
cross-attention mechanism to encode the scene context for affordance prediction
by mutually attending spatial feature maps from two different modalities. The
proposed method is disentangled among individual subtasks to efficiently reduce
the problem complexity. First, we sample a probable location for a person
within the scene using a variational autoencoder (VAE) conditioned on the
global scene context encoding. Next, we predict a potential pose template from
a set of existing human pose candidates using a classifier on the local context
encoding around the predicted location. In the subsequent steps, we use two
VAEs to sample the scale and deformation parameters for the predicted pose
template by conditioning on the local context and template class. Our
experiments show significant improvements over the previous baseline of human
affordance injection into complex 2D scenes.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Integrated Sensing and Communication for 6G Holographic Digital Twins 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.13352v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.13352v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haijun Zhang, Ziyang Zhang, Xiangnan Liu, Wei Li, Haojin Li, Chen Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the advent of 6G networks, offering ultra-high bandwidth and ultra-low
latency, coupled with the enhancement of terminal device resolutions,
holographic communication is gradually becoming a reality. Holographic digital
twin (HDT) is considered one of key applications of holographic communication,
capable of creating virtual replicas for real-time mapping and prediction of
physical entity states, and performing three-dimensional reproduction of
spatial information. In this context, integrated sensing and communication
(ISAC) is expected to be a crucial pathway for providing data sources to HDT.
This paper proposes a four-layer architecture assisted by ISAC for HDT,
integrating emerging paradigms and key technologies to achieve low-cost,
high-precision environmental data collection for constructing HDT.
Specifically, to enhance sensing resolution, we explore super-resolution
techniques from the perspectives of parameter estimation and point cloud
construction. Additionally, we focus on multi-point collaborative sensing for
constructing HDT, and provide a comprehensive review of four key techniques:
node selection, multi-band collaboration, cooperative beamforming, and data
fusion. Finally, we highlight several interesting research directions to guide
and inspire future work.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Multimodal Emotion Recognition using Audio-Video <span class="highlight-title">Transformer</span> Fusion with
  Cross Attention 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.18552v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.18552v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Joe Dhanith P R, Shravan Venkatraman, Vigya Sharma, Santhosh Malarvannan, Modigari Narendra
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Understanding emotions is a fundamental aspect of human communication.
Integrating audio and video signals offers a more comprehensive understanding
of emotional states compared to traditional methods that rely on a single data
source, such as speech or facial expressions. Despite its potential, multimodal
emotion recognition faces significant challenges, particularly in
synchronization, feature extraction, and fusion of diverse data sources. To
address these issues, this paper introduces a novel transformer-based model
named Audio-Video Transformer Fusion with Cross Attention (AVT-CA). The AVT-CA
model employs a transformer fusion approach to effectively capture and
synchronize interlinked features from both audio and video inputs, thereby
resolving synchronization problems. Additionally, the Cross Attention mechanism
within AVT-CA selectively extracts and emphasizes critical features while
discarding irrelevant ones from both modalities, addressing feature extraction
and fusion challenges. Extensive experimental analysis conducted on the
CMU-MOSEI, RAVDESS and CREMA-D datasets demonstrates the efficacy of the
proposed model. The results underscore the importance of AVT-CA in developing
precise and reliable multimodal emotion recognition systems for practical
applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>38 Pages, 9 Tables, 12 Figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Multimodal Fake News Video Explanation Generation: <span class="highlight-title">Dataset</span>, Model, and
  Evaluation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08514v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08514v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lizhi Chen, Zhong Qian, Peifeng Li, Qiaoming Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Although existing methods have addressed fake news video detection as a
classification problem, it is not clear why certain news content is identified
as fake. Without proper explanation, end users may not be able to understand
the potential meaning of fake news. Therefore, we propose a novel task, Fake
News Video Explanation (FNVE), to generate natural language explanations that
reveal the falseness of news videos. To this end, we first developed ONVE and
VTSE, two new datasets to explain fake news video posts. Then, we propose a
Multimodal Relation Graph Transformer (MRGT) model to benchmark ONVE and VTSE.
MRGT introduces a multimodal relation graph to comprehensively represent
multimodal relations and then introduces a BART-based decoder to explain
generations. The experimental results show that the proposed MRGT outperforms
the strong baselines. In addition, the human evaluation on the annotated ONVE
and VTSE also achieves high scores in terms of adequacy rating.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ HDCompression: Hybrid-Diffusion Image Compression for Ultra-Low Bitrates 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.07160v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.07160v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lei Lu, Yize Li, Yanzhi Wang, Wei Wang, Wei Jiang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Image compression under ultra-low bitrates remains challenging for both
conventional learned image compression (LIC) and generative vector-quantized
(VQ) modeling. Conventional LIC suffers from severe artifacts due to heavy
quantization, while generative VQ modeling gives poor fidelity due to the
mismatch between learned generative priors and specific inputs. In this work,
we propose Hybrid-Diffusion Image Compression (HDCompression), a dual-stream
framework that utilizes both generative VQ-modeling and diffusion models, as
well as conventional LIC, to achieve both high fidelity and high perceptual
quality. Different from previous hybrid methods that directly use pre-trained
LIC models to generate low-quality fidelity-preserving information from heavily
quantized latent, we use diffusion models to extract high-quality complimentary
fidelity information from the ground-truth input, which can enhance the system
performance in several aspects: improving indices map prediction, enhancing the
fidelity-preserving output of the LIC stream, and refining conditioned image
reconstruction with VQ-latent correction. In addition, our diffusion model is
based on a dense representative vector (DRV), which is lightweight with very
simple sampling schedulers. Extensive experiments demonstrate that our
HDCompression outperforms the previous conventional LIC, generative
VQ-modeling, and hybrid frameworks in both quantitative metrics and qualitative
visualization, providing balanced robust compression performance at ultra-low
bitrates.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under Review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Bridging the Data Provenance Gap Across Text, Speech and Video <span class="chip">ICLR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.17847v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.17847v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shayne Longpre, Nikhil Singh, Manuel Cherep, Kushagra Tiwary, Joanna Materzynska, William Brannon, Robert Mahari, Naana Obeng-Marnu, Manan Dey, Mohammed Hamdy, Nayan Saxena, Ahmad Mustafa Anis, Emad A. Alghamdi, Vu Minh Chien, Da Yin, Kun Qian, Yizhi Li, Minnie Liang, An Dinh, Shrestha Mohanty, Deividas Mataciunas, Tobin South, Jianguo Zhang, Ariel N. Lee, Campbell S. Lund, Christopher Klamm, Damien Sileo, Diganta Misra, Enrico Shippole, Kevin Klyman, Lester JV Miranda, Niklas Muennighoff, Seonghyeon Ye, Seungone Kim, Vipul Gupta, Vivek Sharma, Xuhui Zhou, Caiming Xiong, Luis Villa, Stella Biderman, Alex Pentland, Sara Hooker, Jad Kabbara
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Progress in AI is driven largely by the scale and quality of training data.
Despite this, there is a deficit of empirical analysis examining the attributes
of well-established datasets beyond text. In this work we conduct the largest
and first-of-its-kind longitudinal audit across modalities--popular text,
speech, and video datasets--from their detailed sourcing trends and use
restrictions to their geographical and linguistic representation. Our manual
analysis covers nearly 4000 public datasets between 1990-2024, spanning 608
languages, 798 sources, 659 organizations, and 67 countries. We find that
multimodal machine learning applications have overwhelmingly turned to
web-crawled, synthetic, and social media platforms, such as YouTube, for their
training sets, eclipsing all other sources since 2019. Secondly, tracing the
chain of dataset derivations we find that while less than 33% of datasets are
restrictively licensed, over 80% of the source content in widely-used text,
speech, and video datasets, carry non-commercial restrictions. Finally, counter
to the rising number of languages and geographies represented in public AI
training datasets, our audit demonstrates measures of relative geographical and
multilingual representation have failed to significantly improve their coverage
since 2013. We believe the breadth of our audit enables us to empirically
examine trends in data sourcing, restrictions, and Western-centricity at an
ecosystem-level, and that visibility into these questions are essential to
progress in responsible AI. As a contribution to ongoing improvements in
dataset transparency and responsible use, we release our entire multimodal
audit, allowing practitioners to trace data provenance across text, speech, and
video.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR 2025. 10 pages, 5 figures (main paper)</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2025-02-18T00:00:00Z">2025-02-18</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Information Retrieval <span class="chip" style="font-size: 60%">28</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SearchRAG: Can Search Engines Be Helpful for LLM-based Medical Question
  Answering? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.13233v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.13233v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yucheng Shi, Tianze Yang, Canyu Chen, Quanzheng Li, Tianming Liu, Xiang Li, Ninghao Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have shown remarkable capabilities in general
domains but often struggle with tasks requiring specialized knowledge.
Conventional Retrieval-Augmented Generation (RAG) techniques typically retrieve
external information from static knowledge bases, which can be outdated or
incomplete, missing fine-grained clinical details essential for accurate
medical question answering. In this work, we propose SearchRAG, a novel
framework that overcomes these limitations by leveraging real-time search
engines. Our method employs synthetic query generation to convert complex
medical questions into search-engine-friendly queries and utilizes
uncertainty-based knowledge selection to filter and incorporate the most
relevant and informative medical knowledge into the LLM's input. Experimental
results demonstrate that our method significantly improves response accuracy in
medical question answering tasks, particularly for complex questions requiring
detailed and up-to-date knowledge.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, three figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning More Effective Representations for Dense Retrieval through
  Deliberate Thinking Before Search 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.12974v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.12974v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yifan Ji, Zhipeng Xu, Zhenghao Liu, Yukun Yan, Shi Yu, Yishan Li, Zhiyuan Liu, Yu Gu, Ge Yu, Maosong Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent dense retrievers usually thrive on the emergency capabilities of Large
Language Models (LLMs), using them to encode queries and documents into an
embedding space for retrieval. These LLM-based dense retrievers have shown
promising performance across various retrieval scenarios. However, relying on a
single embedding to represent documents proves less effective in capturing
different perspectives of documents for matching. In this paper, we propose
Deliberate Thinking based Dense Retriever (DEBATER), which enhances these
LLM-based retrievers by enabling them to learn more effective document
representations through a step-by-step thinking process. DEBATER introduces the
Chain-of-Deliberation mechanism to iteratively optimize document
representations using a continuous chain of thought. To consolidate information
from various thinking steps, DEBATER also incorporates the Self Distillation
mechanism, which identifies the most informative thinking steps and integrates
them into a unified text embedding. Experimental results show that DEBATER
significantly outperforms existing methods across several retrieval benchmarks,
demonstrating superior accuracy and robustness. All codes are available at
https://github.com/OpenBMB/DEBATER.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards Text-Image Interleaved Retrieval 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.12799v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.12799v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xin Zhang, Ziqi Dai, Yongqi Li, Yanzhao Zhang, Dingkun Long, Pengjun Xie, Meishan Zhang, Jun Yu, Wenjie Li, Min Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Current multimodal information retrieval studies mainly focus on single-image
inputs, which limits real-world applications involving multiple images and
text-image interleaved content. In this work, we introduce the text-image
interleaved retrieval (TIIR) task, where the query and document are interleaved
text-image sequences, and the model is required to understand the semantics
from the interleaved context for effective retrieval. We construct a TIIR
benchmark based on naturally interleaved wikiHow tutorials, where a specific
pipeline is designed to generate interleaved queries. To explore the task, we
adapt several off-the-shelf retrievers and build a dense baseline by
interleaved multimodal large language model (MLLM). We then propose a novel
Matryoshka Multimodal Embedder (MME), which compresses the number of visual
tokens at different granularity, to address the challenge of excessive visual
tokens in MLLM-based TIIR models. Experiments demonstrate that simple adaption
of existing models does not consistently yield effective results. Our MME
achieves significant improvements over the baseline by substantially fewer
visual tokens. We provide extensive analysis and will release the dataset and
code to facilitate future research.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages, 14 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PathRAG: Pruning Graph-based Retrieval Augmented Generation with
  Relational Paths 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.14902v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.14902v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Boyu Chen, Zirui Guo, Zidan Yang, Yuluo Chen, Junze Chen, Zhenghao Liu, Chuan Shi, Cheng Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Retrieval-augmented generation (RAG) improves the response quality of large
language models (LLMs) by retrieving knowledge from external databases. Typical
RAG approaches split the text database into chunks, organizing them in a flat
structure for efficient searches. To better capture the inherent dependencies
and structured relationships across the text database, researchers propose to
organize textual information into an indexing graph, known asgraph-based RAG.
However, we argue that the limitation of current graph-based RAG methods lies
in the redundancy of the retrieved information, rather than its insufficiency.
Moreover, previous methods use a flat structure to organize retrieved
information within the prompts, leading to suboptimal performance. To overcome
these limitations, we propose PathRAG, which retrieves key relational paths
from the indexing graph, and converts these paths into textual form for
prompting LLMs. Specifically, PathRAG effectively reduces redundant information
with flow-based pruning, while guiding LLMs to generate more logical and
coherent responses with path-based prompting. Experimental results show that
PathRAG consistently outperforms state-of-the-art baselines across six datasets
and five evaluation dimensions. The code is available at the following link:
https://github.com/BUPT-GAMMA/PathRAG
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Introducing Context Information in Lifelong Sequential Modeling using
  Temporal Convolutional Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.12634v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.12634v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ting Guo, Zhaoyang Yang, Qinsong Zeng, Ming Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The importance of lifelong sequential modeling (LSM) is growing in the realm
of social media recommendation systems. A key component in this process is the
attention module, which derives interest representations with respect to
candidate items from the sequence. Typically, attention modules function in a
point-wise fashion, concentrating only on the relevance of individual items in
the sequence to the candidate item. However, the context information in the
neighboring items that is useful for more accurately evaluating the
significance of each item has not been taken into account. In this study, we
introduce a novel network which employs the Temporal Convolutional Network
(TCN) to generate context-aware representations for each item throughout the
lifelong sequence. These improved representations are then utilized in the
attention module to produce context-aware interest representations. Expanding
on this TCN framework, we present a enhancement module which includes multiple
TCN layers and their respective attention modules to capture interest
representations across different context scopes. Additionally, we also
incorporate a lightweight sub-network to create convolution filters based on
users' basic profile features. These personalized filters are then applied in
the TCN layers instead of the original global filters to produce more
user-specific representations. We performed experiments on both a public
dataset and a proprietary dataset. The findings indicate that the proposed
network surpasses existing methods in terms of prediction accuracy and online
performance metrics.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, including 1 page of reference, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ G-Refer: Graph Retrieval-Augmented Large Language Model for Explainable
  Recommendation <span class="chip">WWW 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.12586v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.12586v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuhan Li, Xinni Zhang, Linhao Luo, Heng Chang, Yuxiang Ren, Irwin King, Jia Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Explainable recommendation has demonstrated significant advantages in
informing users about the logic behind recommendations, thereby increasing
system transparency, effectiveness, and trustworthiness. To provide
personalized and interpretable explanations, existing works often combine the
generation capabilities of large language models (LLMs) with collaborative
filtering (CF) information. CF information extracted from the user-item
interaction graph captures the user behaviors and preferences, which is crucial
for providing informative explanations. However, due to the complexity of graph
structure, effectively extracting the CF information from graphs still remains
a challenge. Moreover, existing methods often struggle with the integration of
extracted CF information with LLMs due to its implicit representation and the
modality gap between graph structures and natural language explanations. To
address these challenges, we propose G-Refer, a framework using graph
retrieval-augmented large language models (LLMs) for explainable
recommendation. Specifically, we first employ a hybrid graph retrieval
mechanism to retrieve explicit CF signals from both structural and semantic
perspectives. The retrieved CF information is explicitly formulated as
human-understandable text by the proposed graph translation and accounts for
the explanations generated by LLMs. To bridge the modality gap, we introduce
knowledge pruning and retrieval-augmented fine-tuning to enhance the ability of
LLMs to process and utilize the retrieved CF information to generate
explanations. Extensive experiments show that G-Refer achieves superior
performance compared with existing methods in both explainability and
stability. Codes and data are available at https://github.com/Yuhan1i/G-Refer.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by WWW 2025, research track</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ From Principles to Applications: A Comprehensive <span class="highlight-title">Survey</span> of Discrete
  Tokenizers in Generation, Comprehension, Recommendation, and Information
  Retrieval 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.12448v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.12448v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jian Jia, Jingtong Gao, Ben Xue, Junhao Wang, Qingpeng Cai, Quan Chen, Xiangyu Zhao, Peng Jiang, Kun Gai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Discrete tokenizers have emerged as indispensable components in modern
machine learning systems, particularly within the context of autoregressive
modeling and large language models (LLMs). These tokenizers serve as the
critical interface that transforms raw, unstructured data from diverse
modalities into discrete tokens, enabling LLMs to operate effectively across a
wide range of tasks. Despite their central role in generation, comprehension,
and recommendation systems, a comprehensive survey dedicated to discrete
tokenizers remains conspicuously absent in the literature. This paper addresses
this gap by providing a systematic review of the design principles,
applications, and challenges of discrete tokenizers. We begin by dissecting the
sub-modules of tokenizers and systematically demonstrate their internal
mechanisms to provide a comprehensive understanding of their functionality and
design. Building on this foundation, we synthesize state-of-the-art methods,
categorizing them into multimodal generation and comprehension tasks, and
semantic tokens for personalized recommendations. Furthermore, we critically
analyze the limitations of existing tokenizers and outline promising directions
for future research. By presenting a unified framework for understanding
discrete tokenizers, this survey aims to guide researchers and practitioners in
addressing open challenges and advancing the field, ultimately contributing to
the development of more robust and versatile AI systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ HopRAG: Multi-Hop Reasoning for Logic-Aware Retrieval-Augmented
  Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.12442v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.12442v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hao Liu, Zhengren Wang, Xi Chen, Zhiyu Li, Feiyu Xiong, Qinhan Yu, Wentao Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Retrieval-Augmented Generation (RAG) systems often struggle with imperfect
retrieval, as traditional retrievers focus on lexical or semantic similarity
rather than logical relevance. To address this, we propose HopRAG, a novel RAG
framework that augments retrieval with logical reasoning through
graph-structured knowledge exploration. During indexing, HopRAG constructs a
passage graph, with text chunks as vertices and logical connections established
via LLM-generated pseudo-queries as edges. During retrieval, it employs a
retrieve-reason-prune mechanism: starting with lexically or semantically
similar passages, the system explores multi-hop neighbors guided by
pseudo-queries and LLM reasoning to identify truly relevant ones. Extensive
experiments demonstrate HopRAG's superiority, achieving 76.78\% higher answer
accuracy and 65.07\% improved retrieval F1 score compared to conventional
methods. The repository is available at https://github.com/LIU-Hao-2002/HopRAG.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Retrieval-augmented systems can be dangerous medical communicators 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.14898v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.14898v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lionel Wong, Ayman Ali, Raymond Xiong, Shannon Zeijang Shen, Yoon Kim, Monica Agrawal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Patients have long sought health information online, and increasingly, they
are turning to generative AI to answer their health-related queries. Given the
high stakes of the medical domain, techniques like retrieval-augmented
generation and citation grounding have been widely promoted as methods to
reduce hallucinations and improve the accuracy of AI-generated responses and
have been widely adopted into search engines. This paper argues that even when
these methods produce literally accurate content drawn from source documents
sans hallucinations, they can still be highly misleading. Patients may derive
significantly different interpretations from AI-generated outputs than they
would from reading the original source material, let alone consulting a
knowledgeable clinician. Through a large-scale query analysis on topics
including disputed diagnoses and procedure safety, we support our argument with
quantitative and qualitative evidence of the suboptimal answers resulting from
current systems. In particular, we highlight how these models tend to
decontextualize facts, omit critical relevant sources, and reinforce patient
misconceptions or biases. We propose a series of recommendations -- such as the
incorporation of communication pragmatics and enhanced comprehension of source
documents -- that could help mitigate these issues and extend beyond the
medical domain.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Solving the Cold Start Problem on One's Own as an End User via
  Preference Transfer 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.12398v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.12398v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ryoma Sato
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose a new approach that enables end users to directly solve the cold
start problem by themselves. The cold start problem is a common issue in
recommender systems, and many methods have been proposed to address the problem
on the service provider's side. However, when the service provider does not
take action, users are left with poor recommendations and no means to improve
their experience. We propose an algorithm, Pretender, that allows end users to
proactively solve the cold start problem on their own. Pretender does not
require any special support from the service provider and can be deployed
independently by users. We formulate the problem as minimizing the distance
between the source and target distributions and optimize item selection from
the target service accordingly. Furthermore, we establish theoretical
guarantees for Pretender based on a discrete quadrature problem. We conduct
experiments on real-world datasets to demonstrate the effectiveness of
Pretender.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>25 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Open-Ended and Knowledge-Intensive Video Question Answering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11747v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11747v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Md Zarif Ul Alam, Hamed Zamani
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Video question answering that requires external knowledge beyond the visual
content remains a significant challenge in AI systems. While models can
effectively answer questions based on direct visual observations, they often
falter when faced with questions requiring broader contextual knowledge. To
address this limitation, we investigate knowledge-intensive video question
answering (KI-VideoQA) through the lens of multi-modal retrieval-augmented
generation, with a particular focus on handling open-ended questions rather
than just multiple-choice formats. Our comprehensive analysis examines various
retrieval augmentation approaches using cutting-edge retrieval and vision
language models, testing both zero-shot and fine-tuned configurations. We
investigate several critical dimensions: the interplay between different
information sources and modalities, strategies for integrating diverse
multi-modal contexts, and the dynamics between query formulation and retrieval
result utilization. Our findings reveal that while retrieval augmentation shows
promise in improving model performance, its success is heavily dependent on the
chosen modality and retrieval methodology. The study also highlights the
critical role of query construction and retrieval depth optimization in
effective knowledge integration. Through our proposed approach, we achieve a
substantial 17.5% improvement in accuracy on multiple choice questions in the
KnowIT VQA dataset, establishing new state-of-the-art performance levels.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CODE-ACCORD: A Corpus of building regulatory data for rule generation
  towards automatic compliance checking 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.02231v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.02231v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hansi Hettiarachchi, Amna Dridi, Mohamed Medhat Gaber, Pouyan Parsafard, Nicoleta Bocaneala, Katja Breitenfelder, Gonçal Costa, Maria Hedblom, Mihaela Juganaru-Mathieu, Thamer Mecharnia, Sumee Park, He Tan, Abdel-Rahman H. Tawil, Edlira Vakaj
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Automatic Compliance Checking (ACC) within the Architecture, Engineering, and
Construction (AEC) sector necessitates automating the interpretation of
building regulations to achieve its full potential. Converting textual rules
into machine-readable formats is challenging due to the complexities of natural
language and the scarcity of resources for advanced Machine Learning (ML).
Addressing these challenges, we introduce CODE-ACCORD, a dataset of 862
sentences from the building regulations of England and Finland. Only the
self-contained sentences, which express complete rules without needing
additional context, were considered as they are essential for ACC. Each
sentence was manually annotated with entities and relations by a team of 12
annotators to facilitate machine-readable rule generation, followed by careful
curation to ensure accuracy. The final dataset comprises 4,297 entities and
4,329 relations across various categories, serving as a robust ground truth.
CODE-ACCORD supports a range of ML and Natural Language Processing (NLP) tasks,
including text classification, entity recognition, and relation extraction. It
enables applying recent trends, such as deep neural networks and large language
models, to ACC.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This is a preprint of an article published in the Scientific Data
  Journal</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Large Language Models as Evaluators for Conversational Recommender
  Systems: Benchmarking System Performance from a User-Centric Perspective 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09493v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09493v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nuo Chen, Quanyu Dai, Xiaoyu Dong, Xiao-Ming Wu, Zhenhua Dong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Conversational recommender systems (CRS) involve both recommendation and
dialogue tasks, which makes their evaluation a unique challenge. Although past
research has analyzed various factors that may affect user satisfaction with
CRS interactions from the perspective of user studies, few evaluation metrics
for CRS have been proposed. Recent studies have shown that LLMs can align with
human preferences, and several LLM-based text quality evaluation measures have
been introduced. However, the application of LLMs in CRS evaluation remains
relatively limited. To address this research gap and advance the development of
user-centric conversational recommender systems, this study proposes an
automated LLM-based CRS evaluation framework, building upon existing research
in human-computer interaction and psychology. The framework evaluates CRS from
four dimensions: dialogue behavior, language expression, recommendation items,
and response content. We use this framework to evaluate four different
conversational recommender systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ WASHtsApp -- A RAG-powered WhatsApp Chatbot for supporting rural African
  clean water access, sanitation and hygiene 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02850v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02850v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Simon Kloker, Alex Cedric Luyima, Matthew Bazanya
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces WASHtsApp, a WhatsApp-based chatbot designed to educate
rural African communities on clean water access, sanitation, and hygiene (WASH)
principles. WASHtsApp leverages a Retrieval-Augmented Generation (RAG) approach
to address the limitations of previous approaches with limited reach or missing
contextualization. The paper details the development process, employing Design
Science Research Methodology. The evaluation consisted of two phases: content
validation by four WASH experts and community validation by potential users.
Content validation confirmed WASHtsApp's ability to provide accurate and
relevant WASH-related information. Community validation indicated high user
acceptance and perceived usefulness of the chatbot. The paper concludes by
discussing the potential for further development, including incorporating local
languages and user data analysis for targeted interventions. It also proposes
future research cycles focused on wider deployment and leveraging user data for
educational purposes.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Working Paper. Accepted at IST-Africa Conference 2025, Nairobi</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Creating a Taxonomy for Retrieval Augmented Generation Applications 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.02854v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.02854v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Irina Nikishina, Özge Sevgili, Mahei Manhai Li, Chris Biemann, Martin Semmann
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this research, we develop a taxonomy to conceptualize a comprehensive
overview of the constituting characteristics that define retrieval augmented
generation (RAG) applications, facilitating the adoption of this technology for
different application domains. To the best of our knowledge, no holistic RAG
application taxonomies have been developed so far. We employ the method foreign
to ACL and thus contribute to the set of methods in the taxonomy creation. It
comprises four iterative phases designed to refine and enhance our
understanding and presentation of RAG's core dimensions. We have developed a
total of five meta-dimensions and sixteen dimensions to comprehensively capture
the concept of RAG applications. Thus, the taxonomy can be used to better
understand RAG applications and to derive design knowledge for future solutions
in specific application domains.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Taxonomy and Analysis of Sensitive User Queries in Generative AI Search <span class="chip">NAACL2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.08672v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.08672v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hwiyeol Jo, Taiwoo Park, Hyunwoo Lee, Nayoung Choi, Changbong Kim, Ohjoon Kwon, Donghyeon Jeon, Eui-Hyeon Lee, Kyoungho Shin, Sun Suk Lim, Kyungmi Kim, Jihye Lee, Sun Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Although there has been a growing interest among industries in integrating
generative LLMs into their services, limited experience and scarcity of
resources act as a barrier in launching and servicing large-scale LLM-based
services. In this paper, we share our experiences in developing and operating
generative AI models within a national-scale search engine, with a specific
focus on the sensitiveness of user queries. We propose a taxonomy for sensitive
search queries, outline our approaches, and present a comprehensive analysis
report on sensitive queries from actual users. We believe that our experiences
in launching generative AI search systems can contribute to reducing the
barrier in building generative LLM-based services.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NAACL2025(Findings)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Causal Learning for Trustworthy Recommender Systems: A <span class="highlight-title">Survey</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.08241v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.08241v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jin Li, Shoujin Wang, Qi Zhang, Longbing Cao, Fang Chen, Xiuzhen Zhang, Dietmar Jannach, Charu C. Aggarwal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recommender Systems (RS) have significantly advanced online content filtering
and personalized decision-making. However, emerging vulnerabilities in RS have
catalyzed a paradigm shift towards Trustworthy RS (TRS). Despite substantial
progress on TRS, most efforts focus on data correlations while overlooking the
fundamental causal nature of recommendations. This drawback hinders TRS from
identifying the root cause of trustworthiness issues, leading to limited
fairness, robustness, and explainability. To bridge this gap, causal learning
emerges as a class of promising methods to augment TRS. These methods, grounded
in reliable causality, excel in mitigating various biases and noise while
offering insightful explanations for TRS. However, there is a lack of timely
and dedicated surveys in this vibrant area. This paper creates an overview of
TRS from the perspective of causal learning. We begin by presenting the
advantages and common procedures of Causality-oriented TRS (CTRS). Then, we
identify potential trustworthiness challenges at each stage and link them to
viable causal solutions, followed by a classification of CTRS methods. Finally,
we discuss several future directions for advancing this field.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CPRM: A LLM-based Continual <span class="highlight-title">Pre-train</span>ing Framework for Relevance
  Modeling in Commercial Search <span class="chip">NAACL 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.01269v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.01269v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kaixin Wu, Yixin Ji, Zeyuan Chen, Qiang Wang, Cunxiang Wang, Hong Liu, Baijun Ji, Jia Xu, Zhongyi Liu, Jinjie Gu, Yuan Zhou, Linjian Mo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Relevance modeling between queries and items stands as a pivotal component in
commercial search engines, directly affecting the user experience. Given the
remarkable achievements of large language models (LLMs) in various natural
language processing (NLP) tasks, LLM-based relevance modeling is gradually
being adopted within industrial search systems. Nevertheless, foundational LLMs
lack domain-specific knowledge and do not fully exploit the potential of
in-context learning. Furthermore, structured item text remains underutilized,
and there is a shortage in the supply of corresponding queries and background
knowledge. We thereby propose CPRM (Continual Pre-training for Relevance
Modeling), a framework designed for the continual pre-training of LLMs to
address these issues. Our CPRM framework includes three modules: 1) employing
both queries and multi-field item to jointly pre-train for enhancing domain
knowledge, 2) applying in-context pre-training, a novel approach where LLMs are
pre-trained on a sequence of related queries or items, and 3) conducting
reading comprehension on items to produce associated domain knowledge and
background information (e.g., generating summaries and corresponding queries)
to further strengthen LLMs. Results on offline experiments and online A/B
testing demonstrate that our model achieves convincing performance compared to
strong baselines.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NAACL 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Unbiased Learning to Rank with Query-Level Click Propensity Estimation:
  Beyond Pointwise Observation and Relevance <span class="chip">WWW</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11414v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11414v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lulu Yu, Keping Bi, Jiafeng Guo, Shihao Liu, Dawei Yin, Xueqi Cheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Most existing unbiased learning-to-rank (ULTR) approaches are based on the
user examination hypothesis, which assumes that users will click a result only
if it is both relevant and observed (typically modeled by position). However,
in real-world scenarios, users often click only one or two results after
examining multiple relevant options, due to limited patience or because their
information needs have already been satisfied. Motivated by this, we propose a
query-level click propensity model to capture the probability that users will
click on different result lists, allowing for non-zero probabilities that users
may not click on an observed relevant result. We hypothesize that this
propensity increases when more potentially relevant results are present, and
refer to this user behavior as relevance saturation bias. Our method introduces
a Dual Inverse Propensity Weighting (DualIPW) mechanism -- combining
query-level and position-level IPW -- to address both relevance saturation and
position bias. Through theoretical derivation, we prove that DualIPW can learn
an unbiased ranking model. Experiments on the real-world Baidu-ULTR dataset
demonstrate that our approach significantly outperforms state-of-the-art ULTR
baselines. The code and dataset information can be found at
https://github.com/Trustworthy-Information-Access/DualIPW.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages, 3 figures, accepted by The ACM Web Conference (WWW) 2025
  Short Paper Track</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CuriousLLM: Elevating Multi-Document Question Answering with
  LLM-Enhanced Knowledge Graph Reasoning <span class="chip">NAACL 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.09077v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.09077v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zukang Yang, Zixuan Zhu, Xuan Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have achieved significant success in open-domain
question answering. However, they continue to face challenges such as
hallucinations and knowledge cutoffs. These issues can be mitigated through
in-context learning by providing LLMs with relevant context before generating
answers. Recent literature proposes Knowledge Graph Prompting (KGP) which
integrates knowledge graphs with an LLM-based traversal agent to substantially
enhance document retrieval quality. However, KGP requires costly fine-tuning
with large datasets and remains prone to hallucination. In this paper, we
propose CuriousLLM, an enhancement that integrates a curiosity-driven reasoning
mechanism into an LLM agent. This mechanism enables the agent to generate
relevant follow-up questions, thereby guiding the information retrieval process
more efficiently. Central to our approach is the development of the new
Follow-upQA dataset, which includes questions and supporting evidence as input,
with follow-up questions serving as ground truths. These follow-up questions
either inquire about what is still missing to fully answer the user's query or
use special tokens to signify that the retrieved evidence is sufficient. Our
experiments show that CuriousLLM significantly boosts LLM performance in
multi-document question answering (MD-QA), circumventing the substantial
computational costs and latency from the original KGP framework.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for publication in NAACL 2025. The official version will be
  available in the ACL Anthology</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Aspect-Aware Decomposition for Opinion Summarization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.17191v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.17191v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Miao Li, Jey Han Lau, Eduard Hovy, Mirella Lapata
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Opinion summarization plays a key role in deriving meaningful insights from
large-scale online reviews. To make this process more explainable and grounded,
we propose a modular approach guided by review aspects which separates the
tasks of aspect identification, opinion consolidation, and meta-review
synthesis, enabling greater transparency and ease of inspection. We conduct
extensive experiments across datasets representing scientific research,
business, and product domains. Results show that our method generates more
grounded summaries compared to strong baseline models, as verified through
automated and human evaluations. Additionally, our modular approach, which
incorporates reasoning based on review aspects, produces more informative
intermediate outputs than knowledge-agnostic decomposed prompting. These
intermediate outputs can also effectively support humans in summarizing
opinions from large volumes of reviews.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>35 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Adaptive In-Context Learning with Large Language Models for Bundle
  Generation <span class="chip">SIGIR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.16262v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.16262v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhu Sun, Kaidong Feng, Jie Yang, Xinghua Qu, Hui Fang, Yew-Soon Ong, Wenyuan Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Most existing bundle generation approaches fall short in generating
fixed-size bundles. Furthermore, they often neglect the underlying user intents
reflected by the bundles in the generation process, resulting in less
intelligible bundles. This paper addresses these limitations through the
exploration of two interrelated tasks, i.e., personalized bundle generation and
the underlying intent inference, based on different user sessions. Inspired by
the reasoning capabilities of large language models (LLMs), we propose an
adaptive in-context learning paradigm, which allows LLMs to draw tailored
lessons from related sessions as demonstrations, enhancing the performance on
target sessions. Specifically, we first employ retrieval augmented generation
to identify nearest neighbor sessions, and then carefully design prompts to
guide LLMs in executing both tasks on these neighbor sessions. To tackle
reliability and hallucination challenges, we further introduce (1) a
self-correction strategy promoting mutual improvements of the two tasks without
supervision signals and (2) an auto-feedback mechanism for adaptive supervision
based on the distinct mistakes made by LLMs on different neighbor sessions.
Thereby, the target session can gain customized lessons for improved
performance by observing the demonstrations of its neighbor sessions.
Experiments on three real-world datasets demonstrate the effectiveness of our
proposed method.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by SIGIR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Differentially Private Graph Diffusion with Applications in Personalized
  PageRanks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.00077v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.00077v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rongzhe Wei, Eli Chien, Pan Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Graph diffusion, which iteratively propagates real-valued substances among
the graph, is used in numerous graph/network-involved applications. However,
releasing diffusion vectors may reveal sensitive linking information in the
data such as transaction information in financial network data. However,
protecting the privacy of graph data is challenging due to its interconnected
nature. This work proposes a novel graph diffusion framework with edge-level
differential privacy guarantees by using noisy diffusion iterates. The
algorithm injects Laplace noise per diffusion iteration and adopts a
degree-based thresholding function to mitigate the high sensitivity induced by
low-degree nodes. Our privacy loss analysis is based on Privacy Amplification
by Iteration (PABI), which to our best knowledge, is the first effort that
analyzes PABI with Laplace noise and provides relevant applications. We also
introduce a novel Infinity-Wasserstein distance tracking method, which tightens
the analysis of privacy leakage and makes PABI more applicable in practice. We
evaluate this framework by applying it to Personalized Pagerank computation for
ranking tasks. Experiments on real-world network data demonstrate the
superiority of our method under stringent privacy conditions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Github Code Available</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CSA: Data-efficient Mapping of Unimodal Features to Multimodal Features 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.07610v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.07610v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Po-han Li, Sandeep P. Chinchali, Ufuk Topcu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multimodal encoders like CLIP excel in tasks such as zero-shot image
classification and cross-modal retrieval. However, they require excessive
training data. We propose canonical similarity analysis (CSA), which uses two
unimodal encoders to replicate multimodal encoders using limited data. CSA maps
unimodal features into a multimodal space, using a new similarity score to
retain only the multimodal information. CSA only involves the inference of
unimodal encoders and a cubic-complexity matrix decomposition, eliminating the
need for extensive GPU-based model training. Experiments show that CSA
outperforms CLIP while requiring $50,000\times$ fewer multimodal data pairs to
bridge the modalities given pre-trained unimodal encoders on ImageNet
classification and misinformative news caption detection. CSA surpasses the
state-of-the-art method to map unimodal features to multimodal features. We
also demonstrate the ability of CSA with modalities beyond image and text,
paving the way for future modality pairs with limited paired multimodal data
but abundant unpaired unimodal data, such as lidar and text.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Bridging AI and Science: Implications from a Large-Scale Literature
  Analysis of AI4Science 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.09628v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.09628v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yutong Xie, Yijun Pan, Hua Xu, Qiaozhu Mei
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Artificial Intelligence has proven to be a transformative tool for advancing
scientific research across a wide range of disciplines. However, a significant
gap still exists between AI and scientific communities, limiting the full
potential of AI methods in driving broad scientific discovery. Existing efforts
in identifying and bridging this gap have often relied on qualitative
examination of small samples of literature, offering a limited perspective on
the broader AI4Science landscape. In this work, we present a large-scale
analysis of the AI4Science literature, starting by using large language models
to identify scientific problems and AI methods in publications from top science
and AI venues. Leveraging this new dataset, we quantitatively highlight key
disparities between AI methods and scientific problems, revealing substantial
opportunities for deeper AI integration across scientific disciplines.
Furthermore, we explore the potential and challenges of facilitating
collaboration between AI and scientific communities through the lens of link
prediction. Our findings and tools aim to promote more impactful
interdisciplinary collaborations and accelerate scientific discovery through
deeper and broader AI integration. Our code and dataset are available at:
https://github.com/charles-pyj/Bridging-AI-and-Science.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>22 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SessionRec: Next Session Prediction Paradigm For Generative Sequential
  Recommendation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.10157v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.10157v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lei Huang, Hao Guo, Linzhi Peng, Long Zhang, Xiaoteng Wang, Daoyuan Wang, Shichao Wang, Jinpeng Wang, Lei Wang, Sheng Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce SessionRec, a novel next-session prediction paradigm (NSPP) for
generative sequential recommendation, addressing the fundamental misalignment
between conventional next-item prediction paradigm (NIPP) and real-world
recommendation scenarios. Unlike NIPP's item-level autoregressive generation
that contradicts actual session-based user interactions, our framework
introduces a session-aware representation learning through hierarchical
sequence aggregation (intra/inter-session), reducing attention computation
complexity while enabling implicit modeling of massive negative interactions,
and a session-based prediction objective that better captures users' diverse
interests through multi-item recommendation in next sessions. Moreover, we
found that incorporating a rank loss for items within the session under the
next session prediction paradigm can significantly improve the ranking
effectiveness of generative sequence recommendation models. We also verified
that SessionRec exhibits clear power-law scaling laws similar to those observed
in LLMs. Extensive experiments conducted on public datasets and online A/B test
in Meituan App demonstrate the effectiveness of SessionRec. The proposed
paradigm establishes new foundations for developing industrial-scale generative
recommendation systems through its model-agnostic architecture and
computational efficiency.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Do We Need Domain-Specific Embedding Models? An Empirical Investigation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.18511v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.18511v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yixuan Tang, Yi Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Embedding models play a crucial role in representing and retrieving
information across various NLP applications. Recent advancements in Large
Language Models (LLMs) have further enhanced the performance of embedding
models, which are trained on massive amounts of text covering almost every
domain. These models are often benchmarked on general-purpose datasets like
Massive Text Embedding Benchmark (MTEB), where they demonstrate superior
performance. However, a critical question arises: Is the development of
domain-specific embedding models necessary when general-purpose models are
trained on vast corpora that already include specialized domain texts? In this
paper, we empirically investigate this question, choosing the finance domain as
an example. We introduce the Finance Massive Text Embedding Benchmark
(FinMTEB), a counterpart to MTEB that consists of financial domain-specific
text datasets. We evaluate the performance of seven state-of-the-art embedding
models on FinMTEB and observe a significant performance drop compared to their
performance on MTEB. To account for the possibility that this drop is driven by
FinMTEB's higher complexity, we propose four measures to quantify dataset
complexity and control for this factor in our analysis. Our analysis provides
compelling evidence that state-of-the-art embedding models struggle to capture
domain-specific linguistic and semantic patterns. Moreover, we find that the
performance of general-purpose embedding models on MTEB is not correlated with
their performance on FinMTEB, indicating the need for domain-specific embedding
benchmarks for domain-specific embedding models. This study sheds light on
developing domain-specific embedding models in the LLM era. FinMTEB comes with
open-source code at https://github.com/yixuantt/FinMTEB
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>https://github.com/yixuantt/FinMTEB, The newer version:
  arXiv:2502.10990</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Seed-Guided Topic Discovery with Out-of-Vocabulary Seeds <span class="chip">NAACL 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2205.01845v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2205.01845v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yu Zhang, Yu Meng, Xuan Wang, Sheng Wang, Jiawei Han
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Discovering latent topics from text corpora has been studied for decades.
Many existing topic models adopt a fully unsupervised setting, and their
discovered topics may not cater to users' particular interests due to their
inability of leveraging user guidance. Although there exist seed-guided topic
discovery approaches that leverage user-provided seeds to discover
topic-representative terms, they are less concerned with two factors: (1) the
existence of out-of-vocabulary seeds and (2) the power of pre-trained language
models (PLMs). In this paper, we generalize the task of seed-guided topic
discovery to allow out-of-vocabulary seeds. We propose a novel framework, named
SeeTopic, wherein the general knowledge of PLMs and the local semantics learned
from the input corpus can mutually benefit each other. Experiments on three
real datasets from different domains demonstrate the effectiveness of SeeTopic
in terms of topic coherence, accuracy, and diversity.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages; Accepted to NAACL 2022</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Multimedia <span class="chip" style="font-size: 60%">11</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GS-QA: Comprehensive Quality Assessment Benchmark for Gaussian Splatting
  View Synthesis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.13196v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.13196v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pedro Martin, António Rodrigues, João Ascenso, Maria Paula Queluz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Gaussian Splatting (GS) offers a promising alternative to Neural Radiance
Fields (NeRF) for real-time 3D scene rendering. Using a set of 3D Gaussians to
represent complex geometry and appearance, GS achieves faster rendering times
and reduced memory consumption compared to the neural network approach used in
NeRF. However, quality assessment of GS-generated static content is not yet
explored in-depth. This paper describes a subjective quality assessment study
that aims to evaluate synthesized videos obtained with several static GS
state-of-the-art methods. The methods were applied to diverse visual scenes,
covering both 360-degree and forward-facing (FF) camera trajectories. Moreover,
the performance of 18 objective quality metrics was analyzed using the scores
resulting from the subjective study, providing insights into their strengths,
limitations, and alignment with human perception. All videos and scores are
made available providing a comprehensive database that can be used as benchmark
on GS view synthesis and objective quality metrics.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DeepResonance: Enhancing Multimodal Music Understanding via
  Music-centric Multi-way Instruction Tuning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.12623v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.12623v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhuoyuan Mao, Mengjie Zhao, Qiyu Wu, Hiromi Wakaki, Yuki Mitsufuji
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in music large language models (LLMs) have significantly
improved music understanding tasks, which involve the model's ability to
analyze and interpret various musical elements. These improvements primarily
focused on integrating both music and text inputs. However, the potential of
incorporating additional modalities such as images, videos and textual music
features to enhance music understanding remains unexplored. To bridge this gap,
we propose DeepResonance, a multimodal music understanding LLM fine-tuned via
multi-way instruction tuning with multi-way aligned music, text, image, and
video data. To this end, we construct Music4way-MI2T, Music4way-MV2T, and
Music4way-Any2T, three 4-way training and evaluation datasets designed to
enable DeepResonance to integrate both visual and textual music feature
content. We also introduce multi-sampled ImageBind embeddings and a
pre-alignment Transformer to enhance modality fusion prior to input into text
LLMs, tailoring DeepResonance for multi-way instruction tuning. Our model
achieves state-of-the-art performances across six music understanding tasks,
highlighting the benefits of the auxiliary modalities and the structural
superiority of DeepResonance. We plan to open-source the models and the newly
constructed datasets.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SEA: Low-Resource Safety Alignment for Multimodal Large Language Models
  via Synthetic Embeddings 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.12562v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.12562v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Weikai Lu, Hao Peng, Huiping Zhuang, Cen Chen, Ziqian Zeng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multimodal Large Language Models (MLLMs) have serious security
vulnerabilities.While safety alignment using multimodal datasets consisting of
text and data of additional modalities can effectively enhance MLLM's security,
it is costly to construct these datasets. Existing low-resource security
alignment methods, including textual alignment, have been found to struggle
with the security risks posed by additional modalities. To address this, we
propose Synthetic Embedding augmented safety Alignment (SEA), which optimizes
embeddings of additional modality through gradient updates to expand textual
datasets. This enables multimodal safety alignment training even when only
textual data is available. Extensive experiments on image, video, and
audio-based MLLMs demonstrate that SEA can synthesize a high-quality embedding
on a single RTX3090 GPU within 24 seconds. SEA significantly improves the
security of MLLMs when faced with threats from additional modalities. To assess
the security risks introduced by video and audio, we also introduced a new
benchmark called VA-SafetyBench. High attack success rates across multiple
MLLMs validate its challenge. Our code and data will be available at
https://github.com/ZeroNLP/SEA.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Comprehensive <span class="highlight-title">Survey</span> on Generative AI for Video-to-Music Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.12489v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.12489v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shulei Ji, Songruoyao Wu, Zihao Wang, Shuyu Li, Kejun Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The burgeoning growth of video-to-music generation can be attributed to the
ascendancy of multimodal generative models. However, there is a lack of
literature that comprehensively combs through the work in this field. To fill
this gap, this paper presents a comprehensive review of video-to-music
generation using deep generative AI techniques, focusing on three key
components: visual feature extraction, music generation frameworks, and
conditioning mechanisms. We categorize existing approaches based on their
designs for each component, clarifying the roles of different strategies.
Preceding this, we provide a fine-grained classification of video and music
modalities, illustrating how different categories influence the design of
components within the generation pipelines. Furthermore, we summarize available
multimodal datasets and evaluation metrics while highlighting ongoing
challenges in the field.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ UMETTS: A Unified Framework for Emotional Text-to-Speech Synthesis with
  Multimodal <span class="highlight-title">Prompt</span>s <span class="chip">ICASSP 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.18398v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.18398v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhi-Qi Cheng, Xiang Li, Jun-Yan He, Junyao Chen, Xiaomao Fan, Xiaojiang Peng, Alexander G. Hauptmann
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Emotional Text-to-Speech (E-TTS) synthesis has garnered significant attention
in recent years due to its potential to revolutionize human-computer
interaction. However, current E-TTS approaches often struggle to capture the
intricacies of human emotions, primarily relying on oversimplified emotional
labels or single-modality input. In this paper, we introduce the Unified
Multimodal Prompt-Induced Emotional Text-to-Speech System (UMETTS), a novel
framework that leverages emotional cues from multiple modalities to generate
highly expressive and emotionally resonant speech. The core of UMETTS consists
of two key components: the Emotion Prompt Alignment Module (EP-Align) and the
Emotion Embedding-Induced TTS Module (EMI-TTS). (1) EP-Align employs
contrastive learning to align emotional features across text, audio, and visual
modalities, ensuring a coherent fusion of multimodal information. (2)
Subsequently, EMI-TTS integrates the aligned emotional embeddings with
state-of-the-art TTS models to synthesize speech that accurately reflects the
intended emotions. Extensive evaluations show that UMETTS achieves significant
improvements in emotion accuracy and speech naturalness, outperforming
traditional E-TTS methods on both objective and subjective metrics.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ICASSP 2025, Code available at
  https://github.com/KTTRCDL/UMETTS</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MetaDesigner: Advancing Artistic Typography Through AI-Driven,
  User-Centric, and Multilingual WordArt Synthesis <span class="chip">ICLR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19859v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19859v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jun-Yan He, Zhi-Qi Cheng, Chenyang Li, Jingdong Sun, Qi He, Wangmeng Xiang, Hanyuan Chen, Jin-Peng Lan, Xianhui Lin, Kang Zhu, Bin Luo, Yifeng Geng, Xuansong Xie, Alexander G. Hauptmann
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  MetaDesigner introduces a transformative framework for artistic typography
synthesis, powered by Large Language Models (LLMs) and grounded in a
user-centric design paradigm. Its foundation is a multi-agent system comprising
the Pipeline, Glyph, and Texture agents, which collectively orchestrate the
creation of customizable WordArt, ranging from semantic enhancements to
intricate textural elements. A central feedback mechanism leverages insights
from both multimodal models and user evaluations, enabling iterative refinement
of design parameters. Through this iterative process, MetaDesigner dynamically
adjusts hyperparameters to align with user-defined stylistic and thematic
preferences, consistently delivering WordArt that excels in visual quality and
contextual resonance. Empirical evaluations underscore the system's versatility
and effectiveness across diverse WordArt applications, yielding outputs that
are both aesthetically compelling and context-sensitive.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICLR 2025, Project:
  https://modelscope.cn/studios/WordArt/WordArt</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Semantically Consistent Person Image Generation <span class="chip">ICPR</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.14728v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.14728v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Prasun Roy, Saumik Bhattacharya, Subhankar Ghosh, Umapada Pal, Michael Blumenstein
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose a data-driven approach for context-aware person image generation.
Specifically, we attempt to generate a person image such that the synthesized
instance can blend into a complex scene. In our method, the position, scale,
and appearance of the generated person are semantically conditioned on the
existing persons in the scene. The proposed technique is divided into three
sequential steps. At first, we employ a Pix2PixHD model to infer a coarse
semantic mask that represents the new person's spatial location, scale, and
potential pose. Next, we use a data-centric approach to select the closest
representation from a precomputed cluster of fine semantic masks. Finally, we
adopt a multi-scale, attention-guided architecture to transfer the appearance
attributes from an exemplar image. The proposed strategy enables us to
synthesize semantically coherent realistic persons that can blend into an
existing scene without altering the global context. We conclude our findings
with relevant qualitative and quantitative evaluations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted in The International Conference on Pattern Recognition
  (ICPR) 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Scene Aware Person Image Generation through Global Contextual
  Conditioning <span class="chip">ICPR</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2206.02717v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2206.02717v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Prasun Roy, Subhankar Ghosh, Saumik Bhattacharya, Umapada Pal, Michael Blumenstein
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Person image generation is an intriguing yet challenging problem. However,
this task becomes even more difficult under constrained situations. In this
work, we propose a novel pipeline to generate and insert contextually relevant
person images into an existing scene while preserving the global semantics.
More specifically, we aim to insert a person such that the location, pose, and
scale of the person being inserted blends in with the existing persons in the
scene. Our method uses three individual networks in a sequential pipeline. At
first, we predict the potential location and the skeletal structure of the new
person by conditioning a Wasserstein Generative Adversarial Network (WGAN) on
the existing human skeletons present in the scene. Next, the predicted skeleton
is refined through a shallow linear network to achieve higher structural
accuracy in the generated image. Finally, the target image is generated from
the refined skeleton using another generative network conditioned on a given
image of the target person. In our experiments, we achieve high-resolution
photo-realistic generation results while preserving the general context of the
scene. We conclude our paper with multiple qualitative and quantitative
benchmarks on the results.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted in The International Conference on Pattern Recognition
  (ICPR) 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ TIPS: Text-Induced Pose Synthesis <span class="chip">ECCV</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.11718v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.11718v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Prasun Roy, Subhankar Ghosh, Saumik Bhattacharya, Umapada Pal, Michael Blumenstein
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In computer vision, human pose synthesis and transfer deal with probabilistic
image generation of a person in a previously unseen pose from an already
available observation of that person. Though researchers have recently proposed
several methods to achieve this task, most of these techniques derive the
target pose directly from the desired target image on a specific dataset,
making the underlying process challenging to apply in real-world scenarios as
the generation of the target image is the actual aim. In this paper, we first
present the shortcomings of current pose transfer algorithms and then propose a
novel text-based pose transfer technique to address those issues. We divide the
problem into three independent stages: (a) text to pose representation, (b)
pose refinement, and (c) pose rendering. To the best of our knowledge, this is
one of the first attempts to develop a text-based pose transfer framework where
we also introduce a new dataset DF-PASS, by adding descriptive pose annotations
for the images of the DeepFashion dataset. The proposed method generates
promising results with significant qualitative and quantitative scores in our
experiments.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted in The European Conference on Computer Vision (ECCV) 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Multi-scale Attention Guided Pose Transfer 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2202.06777v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2202.06777v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Prasun Roy, Saumik Bhattacharya, Subhankar Ghosh, Umapada Pal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Pose transfer refers to the probabilistic image generation of a person with a
previously unseen novel pose from another image of that person having a
different pose. Due to potential academic and commercial applications, this
problem is extensively studied in recent years. Among the various approaches to
the problem, attention guided progressive generation is shown to produce
state-of-the-art results in most cases. In this paper, we present an improved
network architecture for pose transfer by introducing attention links at every
resolution level of the encoder and decoder. By utilizing such dense
multi-scale attention guided approach, we are able to achieve significant
improvement over the existing methods both visually and analytically. We
conclude our findings with extensive qualitative and quantitative comparisons
against several existing methods on the DeepFashion dataset.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted in Pattern Recognition (PR) 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ STEFANN: Scene Text Editor using Font Adaptive Neural Network <span class="chip">CVPR</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/1903.01192v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/1903.01192v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Prasun Roy, Saumik Bhattacharya, Subhankar Ghosh, Umapada Pal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Textual information in a captured scene plays an important role in scene
interpretation and decision making. Though there exist methods that can
successfully detect and interpret complex text regions present in a scene, to
the best of our knowledge, there is no significant prior work that aims to
modify the textual information in an image. The ability to edit text directly
on images has several advantages including error correction, text restoration
and image reusability. In this paper, we propose a method to modify text in an
image at character-level. We approach the problem in two stages. At first, the
unobserved character (target) is generated from an observed character (source)
being modified. We propose two different neural network architectures - (a)
FANnet to achieve structural consistency with source font and (b) Colornet to
preserve source color. Next, we replace the source character with the generated
character maintaining both geometric and visual consistency with neighboring
characters. Our method works as a unified platform for modifying text in
images. We present the effectiveness of our method on COCO-Text and ICDAR
datasets both qualitatively and quantitatively.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted in The IEEE/CVF Conference on Computer Vision and Pattern
  Recognition (CVPR) 2020</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2025-02-17T00:00:00Z">2025-02-17</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Information Retrieval <span class="chip" style="font-size: 60%">29</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ REAL-MM-RAG: A Real-World Multi-Modal Retrieval Benchmark 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.12342v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.12342v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Navve Wasserman, Roi Pony, Oshri Naparstek, Adi Raz Goldfarb, Eli Schwartz, Udi Barzelay, Leonid Karlinsky
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurate multi-modal document retrieval is crucial for Retrieval-Augmented
Generation (RAG), yet existing benchmarks do not fully capture real-world
challenges with their current design. We introduce REAL-MM-RAG, an
automatically generated benchmark designed to address four key properties
essential for real-world retrieval: (i) multi-modal documents, (ii) enhanced
difficulty, (iii) Realistic-RAG queries and (iv) accurate labeling.
Additionally, we propose a multi-difficulty-level scheme based on query
rephrasing to evaluate models' semantic understanding beyond keyword matching.
Our benchmark reveals significant model weaknesses, particularly in handling
table-heavy documents and robustness to query rephrasing. To mitigate these
shortcomings, we curate a rephrased training set and introduce a new
finance-focused, table-heavy dataset. Fine-tuning on these datasets enables
models to achieve state-of-the-art retrieval performance on REAL-MM-RAG
benchmark. Our work offers a better way to evaluate and improve retrieval in
multi-modal RAG systems while also providing training data and models that
address current limitations.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Fast or Better? Balancing Accuracy and Cost in Retrieval-Augmented
  Generation with Flexible User Control 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.12145v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.12145v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jinyan Su, Jennifer Healey, Preslav Nakov, Claire Cardie
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Retrieval-Augmented Generation (RAG) has emerged as a powerful approach to
mitigate large language model (LLM) hallucinations by incorporating external
knowledge retrieval. However, existing RAG frameworks often apply retrieval
indiscriminately,leading to inefficiencies-over-retrieving when unnecessary or
failing to retrieve iteratively when required for complex reasoning. Recent
adaptive retrieval strategies, though adaptively navigates these retrieval
strategies, predict only based on query complexity and lacks user-driven
flexibility, making them infeasible for diverse user application needs. In this
paper, we introduce a novel user-controllable RAG framework that enables
dynamic adjustment of the accuracy-cost trade-off. Our approach leverages two
classifiers: one trained to prioritize accuracy and another to prioritize
retrieval efficiency. Via an interpretable control parameter $\alpha$, users
can seamlessly navigate between minimal-cost retrieval and high-accuracy
retrieval based on their specific requirements. We empirically demonstrate that
our approach effectively balances accuracy, retrieval cost, and user
controllability, making it a practical and adaptable solution for real-world
applications.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ REVERSUM: A Multi-staged Retrieval-Augmented Generation Method to
  Enhance Wikipedia Tail Biographies through Personal Narratives <span class="chip">COLING2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.12137v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.12137v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sayantan Adak, Pauras Mangesh Meher, Paramita Das, Animesh Mukherjee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Wikipedia is an invaluable resource for factual information about a wide
range of entities. However, the quality of articles on less-known entities
often lags behind that of the well-known ones. This study proposes a novel
approach to enhancing Wikipedia's B and C category biography articles by
leveraging personal narratives such as autobiographies and biographies. By
utilizing a multi-staged retrieval-augmented generation technique -- REVerSum
-- we aim to enrich the informational content of these lesser-known articles.
Our study reveals that personal narratives can significantly improve the
quality of Wikipedia articles, providing a rich source of reliable information
that has been underutilized in previous studies. Based on crowd-based
evaluation, REVerSum generated content outperforms the best performing baseline
by 17% in terms of integrability to the original Wikipedia article and 28.5\%
in terms of informativeness. Code and Data are available at:
https://github.com/sayantan11995/wikipedia_enrichment
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at COLING2025 Industry Track</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Joint Evaluation of Fairness and Relevance in Recommender Systems with
  Pareto Frontier <span class="chip">WWW 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11921v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11921v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Theresia Veronika Rampisela, Tuukka Ruotsalo, Maria Maistro, Christina Lioma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Fairness and relevance are two important aspects of recommender systems
(RSs). Typically, they are evaluated either (i) separately by individual
measures of fairness and relevance, or (ii) jointly using a single measure that
accounts for fairness with respect to relevance. However, approach (i) often
does not provide a reliable joint estimate of the goodness of the models, as it
has two different best models: one for fairness and another for relevance.
Approach (ii) is also problematic because these measures tend to be ad-hoc and
do not relate well to traditional relevance measures, like NDCG. Motivated by
this, we present a new approach for jointly evaluating fairness and relevance
in RSs: Distance to Pareto Frontier (DPFR). Given some user-item interaction
data, we compute their Pareto frontier for a pair of existing relevance and
fairness measures, and then use the distance from the frontier as a measure of
the jointly achievable fairness and relevance. Our approach is modular and
intuitive as it can be computed with existing measures. Experiments with 4 RS
models, 3 re-ranking strategies, and 6 datasets show that existing metrics have
inconsistent associations with our Pareto-optimal solution, making DPFR a more
robust and theoretically well-founded joint measure for assessing fairness and
relevance. Our code: https://github.com/theresiavr/DPFR-recsys-evaluation
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to TheWebConf/WWW 2025 (Oral)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FairDiverse: A Comprehensive Toolkit for Fair and Diverse Information
  Retrieval Algorithms 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11883v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11883v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chen Xu, Zhirui Deng, Clara Rus, Xiaopeng Ye, Yuanna Liu, Jun Xu, Zhicheng Dou, Ji-Rong Wen, Maarten de Rijke
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In modern information retrieval (IR). achieving more than just accuracy is
essential to sustaining a healthy ecosystem, especially when addressing
fairness and diversity considerations. To meet these needs, various datasets,
algorithms, and evaluation frameworks have been introduced. However, these
algorithms are often tested across diverse metrics, datasets, and experimental
setups, leading to inconsistencies and difficulties in direct comparisons. This
highlights the need for a comprehensive IR toolkit that enables standardized
evaluation of fairness- and diversity-aware algorithms across different IR
tasks. To address this challenge, we present FairDiverse, an open-source and
standardized toolkit. FairDiverse offers a framework for integrating fair and
diverse methods, including pre-processing, in-processing, and post-processing
techniques, at different stages of the IR pipeline. The toolkit supports the
evaluation of 28 fairness and diversity algorithms across 16 base models,
covering two core IR tasks (search and recommendation) thereby establishing a
comprehensive benchmark. Moreover, FairDiverse is highly extensible, providing
multiple APIs that empower IR researchers to swiftly develop and evaluate their
own fairness and diversity aware models, while ensuring fair comparisons with
existing baselines. The project is open-sourced and available on
https://github.com/XuChen0427/FairDiverse.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ChordFormer: A Conformer-Based Architecture for Large-Vocabulary Audio
  Chord Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11840v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11840v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Muhammad Waseem Akram, Stefano Dettori, Valentina Colla, Giorgio Carlo Buttazzo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Chord recognition serves as a critical task in music information retrieval
due to the abstract and descriptive nature of chords in music analysis. While
audio chord recognition systems have achieved significant accuracy for small
vocabularies (e.g., major/minor chords), large-vocabulary chord recognition
remains a challenging problem. This complexity also arises from the inherent
long-tail distribution of chords, where rare chord types are underrepresented
in most datasets, leading to insufficient training samples. Effective chord
recognition requires leveraging contextual information from audio sequences,
yet existing models, such as combinations of convolutional neural networks,
bidirectional long short-term memory networks, and bidirectional transformers,
face limitations in capturing long-term dependencies and exhibit suboptimal
performance on large-vocabulary chord recognition tasks. This work proposes
ChordFormer, a novel conformer-based architecture designed to tackle structural
chord recognition (e.g., triads, bass, sevenths) for large vocabularies.
ChordFormer leverages conformer blocks that integrate convolutional neural
networks with transformers, thus enabling the model to capture both local
patterns and global dependencies effectively. By addressing challenges such as
class imbalance through a reweighted loss function and structured chord
representations, ChordFormer outperforms state-of-the-art models, achieving a
2% improvement in frame-wise accuracy and a 6% increase in class-wise accuracy
on large-vocabulary chord datasets. Furthermore, ChordFormer excels in handling
class imbalance, providing robust and balanced recognition across chord types.
This approach bridges the gap between theoretical music knowledge and practical
applications, advancing the field of large-vocabulary chord recognition.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Enhancing Recommendation Explanations through User-Centric Refinement 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11721v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11721v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jingsen Zhang, Zihang Tian, Xueyang Feng, Xu Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generating natural language explanations for recommendations has become
increasingly important in recommender systems. Traditional approaches typically
treat user reviews as ground truth for explanations and focus on improving
review prediction accuracy by designing various model architectures. However,
due to limitations in data scale and model capability, these explanations often
fail to meet key user-centric aspects such as factuality, personalization, and
sentiment coherence, significantly reducing their overall helpfulness to users.
In this paper, we propose a novel paradigm that refines initial explanations
generated by existing explainable recommender models during the inference stage
to enhance their quality in multiple aspects. Specifically, we introduce a
multi-agent collaborative refinement framework based on large language models.
To ensure alignment between the refinement process and user demands, we employ
a plan-then-refine pattern to perform targeted modifications. To enable
continuous improvements, we design a hierarchical reflection mechanism that
provides feedback on the refinement process from both strategic and content
perspectives. Extensive experiments on three datasets demonstrate the
effectiveness of our framework.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Accuracy Assessment of OpenAlex and Clarivate Scholar ID with an
  LLM-Assisted Benchmark 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11610v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11610v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Renyu Zhao, Yunxin Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In quantitative SciSci (science of science) studies, accurately identifying
individual scholars is paramount for scientific data analysis. However, the
variability in how names are represented-due to commonality, abbreviations, and
different spelling conventions-complicates this task. While identifier systems
like ORCID are being developed, many scholars remain unregistered, and numerous
publications are not included. Scholarly databases such as Clarivate and
OpenAlex have introduced their own ID systems as preliminary name
disambiguation solutions. This study evaluates the effectiveness of these
systems across different groups to determine their suitability for various
application scenarios. We sampled authors from the top quartile (Q1) of Web of
Science (WOS) journals based on country, discipline, and number of
corresponding author papers. For each group, we selected 100 scholars and
meticulously annotated all their papers using a Search-enhanced Large Language
Model method. Using these annotations, we identified the corresponding IDs in
OpenAlex and Clarivate, extracted all associated papers, filtered for Q1 WOS
journals, and calculated precision and recall by comparing against the
annotated dataset.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FaMTEB: Massive Text Embedding Benchmark in Persian Language <span class="chip">ACL 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11571v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11571v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Erfan Zinvandi, Morteza Alikhani, Mehran Sarmadi, Zahra Pourbahman, Sepehr Arvin, Reza Kazemi, Arash Amini
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we introduce a comprehensive benchmark for Persian (Farsi)
text embeddings, built upon the Massive Text Embedding Benchmark (MTEB). Our
benchmark includes 63 datasets spanning seven different tasks: classification,
clustering, pair classification, reranking, retrieval, summary retrieval, and
semantic textual similarity. The datasets are formed as a combination of
existing, translated, and newly generated data, offering a diverse evaluation
framework for Persian language models. Given the increasing use of text
embedding models in chatbots, evaluation datasets are becoming inseparable
ingredients in chatbot challenges and Retrieval-Augmented Generation systems.
As a contribution, we include chatbot evaluation datasets in the MTEB benchmark
for the first time. In addition, in this paper, we introduce the new task of
summary retrieval which is not part of the tasks included in standard MTEB.
Another contribution of this paper is the introduction of a substantial number
of new Persian language NLP datasets suitable for training and evaluation, some
of which have no previous counterparts in Persian. We evaluate the performance
of several Persian and multilingual embedding models in a range of tasks. This
work introduces an open-source benchmark with datasets, code and a public
leaderboard.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>to appear in ACL 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GPU-accelerated Multi-relational Parallel Graph Retrieval for Web-scale
  Recommendations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11490v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11490v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhuoning Guo, Guangxing Chen, Qian Gao, Xiaochao Liao, Jianjia Zheng, Lu Shen, Hao Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Web recommendations provide personalized items from massive catalogs for
users, which rely heavily on retrieval stages to trade off the effectiveness
and efficiency of selecting a small relevant set from billion-scale candidates
in online digital platforms. As one of the largest Chinese search engine and
news feed providers, Baidu resorts to Deep Neural Network (DNN) and graph-based
Approximate Nearest Neighbor Search (ANNS) algorithms for accurate relevance
estimation and efficient search for relevant items. However, current retrieval
at Baidu fails in comprehensive user-item relational understanding due to
dissected interaction modeling, and performs inefficiently in large-scale
graph-based ANNS because of suboptimal traversal navigation and the GPU
computational bottleneck under high concurrency. To this end, we propose a
GPU-accelerated Multi-relational Parallel Graph Retrieval (GMP-GR) framework to
achieve effective yet efficient retrieval in web-scale recommendations. First,
we propose a multi-relational user-item relevance metric learning method that
unifies diverse user behaviors through multi-objective optimization and employs
a self-covariant loss to enhance pathfinding performance. Second, we develop a
hierarchical parallel graph-based ANNS to boost graph retrieval throughput,
which conducts breadth-depth-balanced searches on a large-scale item graph and
cost-effectively handles irregular neural computation via adaptive aggregation
on GPUs. In addition, we integrate system optimization strategies in the
deployment of GMP-GR in Baidu. Extensive experiments demonstrate the
superiority of GMP-GR in retrieval accuracy and efficiency. Deployed across
more than twenty applications at Baidu, GMP-GR serves hundreds of millions of
users with a throughput exceeding one hundred million requests per second.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GLTW: Joint Improved Graph <span class="highlight-title">Transformer</span> and LLM via Three-Word Language
  for Knowledge Graph Completion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11471v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11471v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kangyang Luo, Yuzhuo Bai, Cheng Gao, Shuzheng Si, Yingli Shen, Zhu Liu, Zhitong Wang, Cunliang Kong, Wenhao Li, Yufei Huang, Ye Tian, Xuantang Xiong, Lei Han, Maosong Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Knowledge Graph Completion (KGC), which aims to infer missing or incomplete
facts, is a crucial task for KGs. However, integrating the vital structural
information of KGs into Large Language Models (LLMs) and outputting predictions
deterministically remains challenging. To address this, we propose a new method
called GLTW, which encodes the structural information of KGs and merges it with
LLMs to enhance KGC performance. Specifically, we introduce an improved Graph
Transformer (iGT) that effectively encodes subgraphs with both local and global
structural information and inherits the characteristics of language model,
bypassing training from scratch. Also, we develop a subgraph-based
multi-classification training objective, using all entities within KG as
classification objects, to boost learning efficiency.Importantly, we combine
iGT with an LLM that takes KG language prompts as input.Our extensive
experiments on various KG datasets show that GLTW achieves significant
performance gains compared to SOTA baselines.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multi-Turn Multi-Modal Question Clarification for Enhanced
  Conversational Understanding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11442v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11442v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kimia Ramezan, Alireza Amiri Bavandpour, Yifei Yuan, Clemencia Siro, Mohammad Aliannejadi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Conversational query clarification enables users to refine their search
queries through interactive dialogue, improving search effectiveness.
Traditional approaches rely on text-based clarifying questions, which often
fail to capture complex user preferences, particularly those involving visual
attributes. While recent work has explored single-turn multi-modal
clarification with images alongside text, such methods do not fully support the
progressive nature of user intent refinement over multiple turns. Motivated by
this, we introduce the Multi-turn Multi-modal Clarifying Questions (MMCQ) task,
which combines text and visual modalities to refine user queries in a
multi-turn conversation. To facilitate this task, we create a large-scale
dataset named ClariMM comprising over 13k multi-turn interactions and 33k
question-answer pairs containing multi-modal clarifying questions. We propose
Mario, a retrieval framework that employs a two-phase ranking strategy: initial
retrieval with BM25, followed by a multi-modal generative re-ranking model that
integrates textual and visual information from conversational history. Our
experiments show that multi-turn multi-modal clarification outperforms
uni-modal and single-turn approaches, improving MRR by 12.88%. The gains are
most significant in longer interactions, demonstrating the value of progressive
refinement for complex queries.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ RAG vs. GraphRAG: A Systematic Evaluation and Key Insights 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11371v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11371v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haoyu Han, Harry Shomer, Yu Wang, Yongjia Lei, Kai Guo, Zhigang Hua, Bo Long, Hui Liu, Jiliang Tang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Retrieval-Augmented Generation (RAG) enhances the performance of LLMs across
various tasks by retrieving relevant information from external sources,
particularly on text-based data. For structured data, such as knowledge graphs,
GraphRAG has been widely used to retrieve relevant information. However, recent
studies have revealed that structuring implicit knowledge from text into graphs
can benefit certain tasks, extending the application of GraphRAG from graph
data to general text-based data. Despite their successful extensions, most
applications of GraphRAG for text data have been designed for specific tasks
and datasets, lacking a systematic evaluation and comparison between RAG and
GraphRAG on widely used text-based benchmarks. In this paper, we systematically
evaluate RAG and GraphRAG on well-established benchmark tasks, such as Question
Answering and Query-based Summarization. Our results highlight the distinct
strengths of RAG and GraphRAG across different tasks and evaluation
perspectives. Inspired by these observations, we investigate strategies to
integrate their strengths to improve downstream tasks. Additionally, we provide
an in-depth discussion of the shortcomings of current GraphRAG approaches and
outline directions for future research.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Personalized Ranking on Cascading Behavior Graphs for Accurate
  Multi-Behavior Recommendation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11335v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11335v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Geonwoo Ko, Minseo Jeon, Jinhong Jung
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-behavior recommendation predicts items a user may purchase by analyzing
diverse behaviors like viewing, adding to a cart, and purchasing. Existing
methods fall into two categories: representation learning and graph ranking.
Representation learning generates user and item embeddings to capture latent
interaction patterns, leveraging multi-behavior properties for better
generalization. However, these methods often suffer from over-smoothing and
bias toward frequent interactions, limiting their expressiveness. Graph ranking
methods, on the other hand, directly compute personalized ranking scores,
capturing user preferences more effectively. Despite their potential, graph
ranking approaches have been primarily explored in single-behavior settings and
remain underutilized for multi-behavior recommendation. In this paper, we
propose CascadingRank, a novel graph ranking method for multi-behavior
recommendation. It models the natural sequence of user behaviors (e.g.,
viewing, adding to cart, and purchasing) through a cascading behavior graph. An
iterative algorithm computes ranking scores, ensuring smoothness, query
fitting, and cascading alignment. Experiments on three real-world datasets
demonstrate that CascadingRank outperforms state-of-the-art methods, with up to
9.56% and 7.16% improvements in HR@10 and NDCG@10, respectively. Furthermore,
we provide theoretical analysis highlighting its effectiveness, convergence,
and scalability, showcasing the advantages of graph ranking in multi-behavior
recommendation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>26 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Ask in Any Modality: A Comprehensive <span class="highlight-title">Survey</span> on Multimodal
  Retrieval-Augmented Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.08826v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.08826v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mohammad Mahdi Abootorabi, Amirhosein Zobeiri, Mahdi Dehghani, Mohammadali Mohammadkhani, Bardia Mohammadi, Omid Ghahroodi, Mahdieh Soleymani Baghshah, Ehsaneddin Asgari
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) struggle with hallucinations and outdated
knowledge due to their reliance on static training data. Retrieval-Augmented
Generation (RAG) mitigates these issues by integrating external dynamic
information enhancing factual and updated grounding. Recent advances in
multimodal learning have led to the development of Multimodal RAG,
incorporating multiple modalities such as text, images, audio, and video to
enhance the generated outputs. However, cross-modal alignment and reasoning
introduce unique challenges to Multimodal RAG, distinguishing it from
traditional unimodal RAG. This survey offers a structured and comprehensive
analysis of Multimodal RAG systems, covering datasets, metrics, benchmarks,
evaluation, methodologies, and innovations in retrieval, fusion, augmentation,
and generation. We precisely review training strategies, robustness
enhancements, and loss functions, while also exploring the diverse Multimodal
RAG scenarios. Furthermore, we discuss open challenges and future research
directions to support advancements in this evolving field. This survey lays the
foundation for developing more capable and reliable AI systems that effectively
leverage multimodal dynamic external knowledge bases. Resources are available
at https://github.com/llm-lab-org/Multimodal-RAG-Survey.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>GitHub repository:
  https://github.com/llm-lab-org/Multimodal-RAG-Survey</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ RDSA: A Robust Deep Graph Clustering Framework via Dual Soft Assignment <span class="chip">DASFAA 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.21745v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.21745v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yang Xiang, Li Fan, Tulika Saha, Xiaoying Pang, Yushan Pan, Haiyang Zhang, Chengtao Ji
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Graph clustering is an essential aspect of network analysis that involves
grouping nodes into separate clusters. Recent developments in deep learning
have resulted in graph clustering, which has proven effective in many
applications. Nonetheless, these methods often encounter difficulties when
dealing with real-world graphs, particularly in the presence of noisy edges.
Additionally, many denoising graph clustering methods tend to suffer from lower
performance, training instability, and challenges in scaling to large datasets
compared to non-denoised models. To tackle these issues, we introduce a new
framework called the Robust Deep Graph Clustering Framework via Dual Soft
Assignment (RDSA). RDSA consists of three key components: (i) a node embedding
module that effectively integrates the graph's topological features and node
attributes; (ii) a structure-based soft assignment module that improves graph
modularity by utilizing an affinity matrix for node assignments; and (iii) a
node-based soft assignment module that identifies community landmarks and
refines node assignments to enhance the model's robustness. We assess RDSA on
various real-world datasets, demonstrating its superior performance relative to
existing state-of-the-art methods. Our findings indicate that RDSA provides
robust clustering across different graph types, excelling in clustering
effectiveness and robustness, including adaptability to noise, stability, and
scalability.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by DASFAA 2025; Complete version</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Large Memory Network for Recommendation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.05558v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.05558v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hui Lu, Zheng Chai, Yuchao Zheng, Zhe Chen, Deping Xie, Peng Xu, Xun Zhou, Di Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Modeling user behavior sequences in recommender systems is essential for
understanding user preferences over time, enabling personalized and accurate
recommendations for improving user retention and enhancing business values.
Despite its significance, there are two challenges for current sequential
modeling approaches. From the spatial dimension, it is difficult to mutually
perceive similar users' interests for a generalized intention understanding;
from the temporal dimension, current methods are generally prone to forgetting
long-term interests due to the fixed-length input sequence. In this paper, we
present Large Memory Network (LMN), providing a novel idea by compressing and
storing user history behavior information in a large-scale memory block. With
the elaborated online deployment strategy, the memory block can be easily
scaled up to million-scale in the industry. Extensive offline comparison
experiments, memory scaling up experiments, and online A/B test on Douyin
E-Commerce Search (ECS) are performed, validating the superior performance of
LMN. Currently, LMN has been fully deployed in Douyin ECS, serving millions of
users each day.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Reason4Rec: Large Language Models for Recommendation with Deliberative
  User Preference Alignment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.02061v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.02061v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yi Fang, Wenjie Wang, Yang Zhang, Fengbin Zhu, Qifan Wang, Fuli Feng, Xiangnan He
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While recent advancements in aligning Large Language Models (LLMs) with
recommendation tasks have shown great potential and promising performance
overall, these aligned recommendation LLMs still face challenges in complex
scenarios. This is primarily due to the current alignment approach focusing on
optimizing LLMs to generate user feedback directly, without incorporating
deliberation. To overcome this limitation and develop more reliable LLMs for
recommendations, we propose a new Deliberative Recommendation task, which
incorporates explicit reasoning about user preferences as an additional
alignment goal. We then introduce the Reasoning-powered Recommender framework
for deliberative user preference alignment, designed to enhance reasoning
capabilities by utilizing verbalized user feedback in a step-wise manner to
tackle this task. The framework employs collaborative step-wise experts and
tailored training strategies for each expert. Experimental results across three
real-world datasets demonstrate the rationality of the deliberative task
formulation and the superior performance of the proposed framework in improving
both prediction accuracy and reasoning quality.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Dual-Channel Multiplex Graph Neural Networks for Recommendation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.11624v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.11624v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiang Li, Chaofan Fu, Zhongying Zhao, Guanjie Zheng, Chao Huang, Yanwei Yu, Junyu Dong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Effective recommender systems play a crucial role in accurately capturing
user and item attributes that mirror individual preferences. Some existing
recommendation techniques have started to shift their focus towards modeling
various types of interactive relations between users and items in real-world
recommendation scenarios, such as clicks, marking favorites, and purchases on
online shopping platforms. Nevertheless, these approaches still grapple with
two significant challenges: (1) Insufficient modeling and exploitation of the
impact of various behavior patterns formed by multiplex relations between users
and items on representation learning, and (2) ignoring the effect of different
relations within behavior patterns on the target relation in recommender system
scenarios. In this work, we introduce a novel recommendation framework,
Dual-Channel Multiplex Graph Neural Network (DCMGNN), which addresses the
aforementioned challenges. It incorporates an explicit behavior pattern
representation learner to capture the behavior patterns composed of multiplex
user-item interactive relations, and includes a relation chain representation
learner and a relation chain-aware encoder to discover the impact of various
auxiliary relations on the target relation, the dependencies between different
relations, and mine the appropriate order of relations in a behavior pattern.
Extensive experiments on three real-world datasets demonstrate that our \model
surpasses various state-of-the-art recommendation methods. It outperforms the
best baselines by 10.06% and 12.15% on average across all datasets in terms of
Recall@10 and NDCG@10 respectively.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SciPIP: An LLM-based Scientific Paper Idea Proposer 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.23166v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.23166v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenxiao Wang, Lihui Gu, Liye Zhang, Yunxiang Luo, Yi Dai, Chen Shen, Liang Xie, Binbin Lin, Xiaofei He, Jieping Ye
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rapid advancement of large language models (LLMs) has opened new
possibilities for automating the proposal of innovative scientific ideas. This
process involves two key phases: literature retrieval and idea generation.
However, existing approaches often fall short due to their reliance on
keyword-based search tools during the retrieval phase, which neglects crucial
semantic information and frequently results in incomplete retrieval outcomes.
Similarly, in the idea generation phase, current methodologies tend to depend
solely on the internal knowledge of LLMs or metadata from retrieved papers,
thereby overlooking significant valuable insights contained within the full
texts. To address these limitations, we introduce SciPIP, an innovative
framework designed to enhance the LLM-based proposal of scientific ideas
through improvements in both literature retrieval and idea generation. Our
approach begins with the construction of a comprehensive literature database
that supports advanced retrieval based not only on keywords but also on
semantics and citation relationships. This is complemented by the introduction
of a multi-granularity retrieval algorithm aimed at ensuring more thorough and
exhaustive retrieval results. For the idea generation phase, we propose a
dual-path framework that effectively integrates both the content of retrieved
papers and the extensive internal knowledge of LLMs. This integration
significantly boosts the novelty, feasibility, and practical value of proposed
ideas. Our experiments, conducted across various domains such as natural
language processing and computer vision, demonstrate SciPIP's capability to
generate a multitude of innovative and useful ideas. These findings underscore
SciPIP's potential as a valuable tool for researchers seeking to advance their
fields with groundbreaking concepts.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>20 pages, 5 figures, 12 tables. The code has been availabel:
  https://github.com/cheerss/SciPIP</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Cross-domain Recommender Systems via Multimodal Domain Adaptation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.13887v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.13887v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Adamya Shyam, Ramya Kamani, Venkateswara Rao Kagita, Vikas Kumar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Collaborative Filtering (CF) has emerged as one of the most prominent
implementation strategies for building recommender systems. The key idea is to
exploit the usage patterns of individuals to generate personalized
recommendations. CF techniques, especially for newly launched platforms, often
face a critical issue known as the data sparsity problem, which greatly limits
their performance. Cross-domain CF alleviates the problem of data sparsity by
finding a common set of entities (users or items) across the domains, which
then act as a conduit for knowledge transfer. Nevertheless, most real-world
datasets are collected from different domains, so they often lack information
about anchor points or reference information for entity alignment. This paper
introduces a domain adaptation technique to align the embeddings of entities
across domains. Our approach first exploits the available textual and visual
information to independently learn a multi-view latent representation for each
entity in the auxiliary and target domains. The different representations of
the entity are then fused to generate the corresponding unified representation.
A domain classifier is then trained to learn the embedding for the domain
alignment by fixing the unified features as the anchor points. Experiments on
\AS{four} publicly available benchmark datasets indicate the effectiveness of
our proposed approach.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>20 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Multimodal semantic retrieval for product search <span class="chip">WWW 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.07365v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.07365v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dong Liu, Esther Lopez Ramos
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Semantic retrieval (also known as dense retrieval) based on textual data has
been extensively studied for both web search and product search application
fields, where the relevance of a query and a potential target document is
computed by their dense vector representation comparison. Product image is
crucial for e-commerce search interactions and is a key factor for customers at
product explorations. However, its impact on semantic retrieval has not been
well studied yet. In this research, we build a multimodal representation for
product items in e-commerce search in contrast to pure-text representation of
products, and investigate the impact of such representations. The models are
developed and evaluated on e-commerce datasets. We demonstrate that a
multimodal representation scheme for a product can show improvement either on
purchase recall or relevance accuracy in semantic retrieval. Additionally, we
provide numerical analysis for exclusive matches retrieved by a multimodal
semantic retrieval model versus a text-only semantic retrieval model, to
demonstrate the validation of multimodal solutions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at EReL@MIR WWW 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Data and Decision Traceability for SDA TAP Lab's Prototype Battle
  Management System 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.09827v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.09827v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Latha Pratti, Samya Bagchi, Yasir Latif
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Space Protocol is applying the principles derived from MITRE and NIST's
Supply Chain Traceability: Manufacturing Meta-Framework (NIST IR 8536) to a
complex multi party system to achieve introspection, auditing, and replay of
data and decisions that ultimately lead to a end decision. The core goal of
decision traceability is to ensure transparency, accountability, and integrity
within the WA system. This is accomplished by providing a clear, auditable path
from the system's inputs all the way to the final decision. This traceability
enables the system to track the various algorithms and data flows that have
influenced a particular outcome.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Chain-of-Factors Paper-<span class="highlight-title">Review</span>er Matching <span class="chip">WWW 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.14483v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.14483v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yu Zhang, Yanzhen Shen, SeongKu Kang, Xiusi Chen, Bowen Jin, Jiawei Han
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the rapid increase in paper submissions to academic conferences, the
need for automated and accurate paper-reviewer matching is more critical than
ever. Previous efforts in this area have considered various factors to assess
the relevance of a reviewer's expertise to a paper, such as the semantic
similarity, shared topics, and citation connections between the paper and the
reviewer's previous works. However, most of these studies focus on only one
factor, resulting in an incomplete evaluation of the paper-reviewer relevance.
To address this issue, we propose a unified model for paper-reviewer matching
that jointly considers semantic, topic, and citation factors. To be specific,
during training, we instruction-tune a contextualized language model shared
across all factors to capture their commonalities and characteristics; during
inference, we chain the three factors to enable step-by-step, coarse-to-fine
search for qualified reviewers given a submission. Experiments on four datasets
(one of which is newly contributed by us) spanning various fields such as
machine learning, computer vision, information retrieval, and data mining
consistently demonstrate the effectiveness of our proposed Chain-of-Factors
model in comparison with state-of-the-art paper-reviewer matching methods and
scientific pre-trained language models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages; Accepted to WWW 2025 (Code:
  https://github.com/yuzhimanhua/CoF)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ FunnelRAG: A Coarse-to-Fine Progressive Retrieval Paradigm for RAG <span class="chip">NAACL 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.10293v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.10293v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinping Zhao, Yan Zhong, Zetian Sun, Xinshuo Hu, Zhenyu Liu, Dongfang Li, Baotian Hu, Min Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Retrieval-Augmented Generation (RAG) prevails in Large Language Models. It
mainly consists of retrieval and generation. The retrieval modules (a.k.a.
retrievers) aim to find useful information used to facilitate the generation
modules (a.k.a. generators). As such, generators' performance largely depends
on the effectiveness and efficiency of retrievers. However, the widely used
retrieval paradigm remains flat. It treats retrieval procedures as a one-off
deal with constant granularity. Despite effectiveness, we argue that they
suffer from two limitations: (1) flat retrieval exerts a significant burden on
one retriever; (2) constant granularity limits the ceiling of retrieval
performance. In this work, we propose a progressive retrieval paradigm with
coarse-to-fine granularity for RAG, termed FunnelRAG, so as to balance
effectiveness and efficiency. Specifically, FunnelRAG establishes a progressive
retrieval pipeline by collaborating coarse-to-fine granularity, large-to-small
quantity, and low-to-high capacity, which can relieve the burden on one
retriever and also promote the ceiling of retrieval performance. Extensive
experiments manifest that FunnelRAG achieves comparable retrieval performance
while the time overhead is reduced by nearly 40 percent.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>18 pages, 6 figures, 13 tables. Accepted by NAACL 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Graph Foundation Models for Recommendation: A Comprehensive <span class="highlight-title">Survey</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.08346v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.08346v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bin Wu, Yihang Wang, Yuanhao Zeng, Jiawei Liu, Jiashu Zhao, Cheng Yang, Yawen Li, Long Xia, Dawei Yin, Chuan Shi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recommender systems (RS) serve as a fundamental tool for navigating the vast
expanse of online information, with deep learning advancements playing an
increasingly important role in improving ranking accuracy. Among these, graph
neural networks (GNNs) excel at extracting higher-order structural information,
while large language models (LLMs) are designed to process and comprehend
natural language, making both approaches highly effective and widely adopted.
Recent research has focused on graph foundation models (GFMs), which integrate
the strengths of GNNs and LLMs to model complex RS problems more efficiently by
leveraging the graph-based structure of user-item relationships alongside
textual understanding. In this survey, we provide a comprehensive overview of
GFM-based RS technologies by introducing a clear taxonomy of current
approaches, diving into methodological details, and highlighting key challenges
and future directions. By synthesizing recent advancements, we aim to offer
valuable insights into the evolving landscape of GFM-based recommender systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Multi-granularity Interest Retrieval and Refinement Network for
  Long-Term User Behavior Modeling in CTR Prediction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.15005v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.15005v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiang Xu, Hao Wang, Wei Guo, Luankang Zhang, Wanshan Yang, Runlong Yu, Yong Liu, Defu Lian, Enhong Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Click-through Rate (CTR) prediction is crucial for online personalization
platforms. Recent advancements have shown that modeling rich user behaviors can
significantly improve the performance of CTR prediction. Current long-term user
behavior modeling algorithms predominantly follow two cascading stages. The
first stage retrieves subsequence related to the target item from the long-term
behavior sequence, while the second stage models the relationship between the
subsequence and the target item. Despite significant progress, these methods
have two critical flaws. First, the retrieval query typically includes only
target item information, limiting the ability to capture the user's diverse
interests. Second, relational information, such as sequential and interactive
information within the subsequence, is frequently overlooked. Therefore, it
requires to be further mined to more accurately model user interests.
  To this end, we propose Multi-granularity Interest Retrieval and Refinement
Network (MIRRN). Specifically, we first construct queries based on behaviors
observed at different time scales to obtain subsequences, each capturing users'
interest at various granularities. We then introduce an noval multi-head
Fourier transformer to efficiently learn sequential and interactive information
within the subsequences, leading to more accurate modeling of user interests.
Finally, we employ multi-head target attention to adaptively assess the impact
of these multi-granularity interests on the target item. Extensive experiments
have demonstrated that MIRRN significantly outperforms state-of-the-art
baselines. Furthermore, an A/B test shows that MIRRN increases the average
number of listening songs by 1.32% and the average time of listening songs by
0.55% on the Huawei Music App. The implementation code is publicly available at
https://github.com/USTC-StarTeam/MIRRN.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MIRe: Enhancing Multimodal Queries Representation via Fusion-Free
  Modality Interaction for Multimodal Retrieval 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.08334v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.08334v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yeong-Joon Ju, Ho-Joong Kim, Seong-Whan Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent multimodal retrieval methods have endowed text-based retrievers with
multimodal capabilities by utilizing pre-training strategies for visual-text
alignment. They often directly fuse the two modalities for cross-reference
during the alignment to understand multimodal queries. However, existing
methods often overlook crucial visual information due to a text-dominant issue,
which overly depends on text-driven signals. In this paper, we introduce MIRe,
a retrieval framework that achieves modality interaction without fusing textual
features during the alignment. Our method allows the textual query to attend to
visual embeddings while not feeding text-driven signals back into the visual
representations. Additionally, we construct a pre-training dataset for
multimodal query retrieval by transforming concise question-answer pairs into
extended passages. Our experiments demonstrate that our pre-training strategy
significantly enhances the understanding of multimodal queries, resulting in
strong performance across four multimodal retrieval benchmarks under zero-shot
settings. Our code is publicly available: https://github.com/yeongjoonJu/MIRe.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>preprint</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Diffusion-EXR: Controllable <span class="highlight-title">Review</span> Generation for Explainable
  Recommendation via Diffusion Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.15490v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.15490v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ling Li, Shaohua Li, Winda Marantika, Alex C. Kot, Huijing Zhan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Denoising Diffusion Probabilistic Model (DDPM) has shown great competence in
image and audio generation tasks. However, there exist few attempts to employ
DDPM in the text generation, especially review generation under recommendation
systems. Fueled by the predicted reviews explainability that justifies
recommendations could assist users better understand the recommended items and
increase the transparency of recommendation system, we propose a Diffusion
Model-based Review Generation towards EXplainable Recommendation named
Diffusion-EXR. Diffusion-EXR corrupts the sequence of review embeddings by
incrementally introducing varied levels of Gaussian noise to the sequence of
word embeddings and learns to reconstruct the original word representations in
the reverse process. The nature of DDPM enables our lightweight Transformer
backbone to perform excellently in the recommendation review generation task.
Extensive experimental results have demonstrated that Diffusion-EXR can achieve
state-of-the-art review generation for recommendation on two publicly available
benchmark datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>We request to withdraw our paper from the archive due to significant
  errors identified in the analysis and conclusions. Upon further review, we
  realized that these errors undermine the validity of our findings. We plan to
  conduct additional research to correct these issues and resubmit a revised
  version in the future</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Multimedia <span class="chip" style="font-size: 60%">5</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Token Communications: A Unified Framework for Cross-modal Context-aware
  Semantic Communications 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.12096v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.12096v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Li Qiao, Mahdi Boloursaz Mashhadi, Zhen Gao, Rahim Tafazolli, Mehdi Bennis, Dusit Niyato
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we introduce token communications (TokCom), a unified
framework to leverage cross-modal context information in generative semantic
communications (GenSC). TokCom is a new paradigm, motivated by the recent
success of generative foundation models and multimodal large language models
(GFM/MLLMs), where the communication units are tokens, enabling efficient
transformer-based token processing at the transmitter and receiver. In this
paper, we introduce the potential opportunities and challenges of leveraging
context in GenSC, explore how to integrate GFM/MLLMs-based token processing
into semantic communication systems to leverage cross-modal context
effectively, present the key principles for efficient TokCom at various layers
in future wireless networks. We demonstrate the corresponding TokCom benefits
in a GenSC setup for image, leveraging cross-modal context information, which
increases the bandwidth efficiency by 70.8% with negligible loss of
semantic/perceptual quality. Finally, the potential research directions are
identified to facilitate adoption of TokCom in future wireless networks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Both Text and Images Leaked! A Systematic Analysis of Multimodal LLM
  Data Contamination 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03823v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03823v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dingjie Song, Sicheng Lai, Shunian Chen, Lichao Sun, Benyou Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rapid progression of multimodal large language models (MLLMs) has
demonstrated superior performance on various multimodal benchmarks. However,
the issue of data contamination during training creates challenges in
performance evaluation and comparison. While numerous methods exist for
detecting models' contamination in large language models (LLMs), they are less
effective for MLLMs due to their various modalities and multiple training
phases. In this study, we introduce a multimodal data contamination detection
framework, MM-Detect, designed for MLLMs. Our experimental results indicate
that MM-Detect is quite effective and sensitive in identifying varying degrees
of contamination, and can highlight significant performance improvements due to
the leakage of multimodal benchmark training sets. Furthermore, we explore
whether the contamination originates from the base LLMs used by MLLMs or the
multimodal training phase, providing new insights into the stages at which
contamination may be introduced.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code Available: https://github.com/MLLM-Data-Contamination/MM-Detect</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Bridging Compressed Image Latents and Multimodal Large Language Models <span class="chip">ICLR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.19651v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.19651v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chia-Hao Kao, Cheng Chien, Yu-Jen Tseng, Yi-Hsin Chen, Alessandro Gnutti, Shao-Yuan Lo, Wen-Hsiao Peng, Riccardo Leonardi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents the first-ever study of adapting compressed image latents
to suit the needs of downstream vision tasks that adopt Multimodal Large
Language Models (MLLMs). MLLMs have extended the success of large language
models to modalities (e.g. images) beyond text, but their billion scale hinders
deployment on resource-constrained end devices. While cloud-hosted MLLMs could
be available, transmitting raw, uncompressed images captured by end devices to
the cloud requires an efficient image compression system. To address this, we
focus on emerging neural image compression and propose a novel framework with a
lightweight transform-neck and a surrogate loss to adapt compressed image
latents for MLLM-based vision tasks. Given the huge scale of MLLMs, our
framework excludes the entire downstream MLLM except part of its visual encoder
from training our system. This stands out from most existing coding for machine
approaches that involve downstream networks in training and thus could be
impractical when the networks are MLLMs. The proposed framework is general in
that it is applicable to various MLLMs, neural image codecs, and multiple
application scenarios, where the neural image codec can be (1) pre-trained for
human perception without updating, (2) fully updated for joint human and
machine perception, or (3) fully updated for only machine perception. Extensive
experiments on different neural image codecs and various MLLMs show that our
method achieves great rate-accuracy performance with much less complexity.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICLR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Object-Attribute-Relation Representation Based Video Semantic
  Communication 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.10469v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.10469v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qiyuan Du, Yiping Duan, Qianqian Yang, Xiaoming Tao, Mérouane Debbah
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the rapid growth of multimedia data volume, there is an increasing need
for efficient video transmission in applications such as virtual reality and
future video streaming services. Semantic communication is emerging as a vital
technique for ensuring efficient and reliable transmission in low-bandwidth,
high-noise settings. However, most current approaches focus on joint
source-channel coding (JSCC) that depends on end-to-end training. These methods
often lack an interpretable semantic representation and struggle with
adaptability to various downstream tasks. In this paper, we introduce the use
of object-attribute-relation (OAR) as a semantic framework for videos to
facilitate low bit-rate coding and enhance the JSCC process for more effective
video transmission. We utilize OAR sequences for both low bit-rate
representation and generative video reconstruction. Additionally, we
incorporate OAR into the image JSCC model to prioritize communication resources
for areas more critical to downstream tasks. Our experiments on traffic
surveillance video datasets assess the effectiveness of our approach in terms
of video transmission performance. The empirical findings demonstrate that our
OAR-based video coding method not only outperforms H.265 coding at lower
bit-rates but also synergizes with JSCC to deliver robust and efficient video
transmission.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MIRe: Enhancing Multimodal Queries Representation via Fusion-Free
  Modality Interaction for Multimodal Retrieval 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.08334v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.08334v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yeong-Joon Ju, Ho-Joong Kim, Seong-Whan Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent multimodal retrieval methods have endowed text-based retrievers with
multimodal capabilities by utilizing pre-training strategies for visual-text
alignment. They often directly fuse the two modalities for cross-reference
during the alignment to understand multimodal queries. However, existing
methods often overlook crucial visual information due to a text-dominant issue,
which overly depends on text-driven signals. In this paper, we introduce MIRe,
a retrieval framework that achieves modality interaction without fusing textual
features during the alignment. Our method allows the textual query to attend to
visual embeddings while not feeding text-driven signals back into the visual
representations. Additionally, we construct a pre-training dataset for
multimodal query retrieval by transforming concise question-answer pairs into
extended passages. Our experiments demonstrate that our pre-training strategy
significantly enhances the understanding of multimodal queries, resulting in
strong performance across four multimodal retrieval benchmarks under zero-shot
settings. Our code is publicly available: https://github.com/yeongjoonJu/MIRe.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>preprint</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>

</body>

<footer>
    <div>
        <time id="build-timestamp" datetime="2025-02-25T05:27:01.340460022Z">
            2025-02-25 05:27:01 UTC
        </time>
    </div>
</footer>
<script src="index.js"></script>
</html>
